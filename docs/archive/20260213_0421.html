<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-13 04:21</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260213_0421</div>
    <div class="row"><div class="card">
<div class="title">SurfPhase: 3D Interfacial Dynamics in Two-Phase Flows from Sparse Videos</div>
<div class="meta-line">Authors: Yue Gao, Hong-Xing Yu, Sanghyeon Chang, Qianxi Fu, Bo Zhu, Yoonjin Won, Juan Carlos Niebles, Jiajun Wu</div>
<div class="meta-line">First: 2026-02-11T18:59:55+00:00 · Latest: 2026-02-11T18:59:55+00:00</div>
<div class="meta-line">Comments: The first two authors contributed equally. Project website: https://yuegao.me/SurfPhase</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11154v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11154v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Interfacial dynamics in two-phase flows govern momentum, heat, and mass transfer, yet remain difficult to measure experimentally. Classical techniques face intrinsic limitations near moving interfaces, while existing neural rendering methods target single-phase flows with diffuse boundaries and cannot handle sharp, deformable liquid-vapor interfaces. We propose SurfPhase, a novel model for reconstructing 3D interfacial dynamics from sparse camera views. Our approach integrates dynamic Gaussian surfels with a signed distance function formulation for geometric consistency, and leverages a video diffusion model to synthesize novel-view videos to refine reconstruction from sparse observations. We evaluate on a new dataset of high-speed pool boiling videos, demonstrating high-quality view synthesis and velocity estimation from only two camera views. Project website: https://yuegao.me/SurfPhase.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SurfPhase：从稀疏视频中重建两相流的三维界面动力学</div>
<div class="mono" style="margin-top:8px">两相流中的界面动力学控制动量、热量和质量传递，但实验测量仍具挑战性。经典技术在移动界面附近存在固有局限，而现有的神经渲染方法主要针对边界模糊的单相流，无法处理清晰且可变形的液-气界面。我们提出SurfPhase，一种新颖的模型，用于从稀疏的摄像视角重建三维界面动力学。我们的方法结合动态高斯表面点（surfels）与符号距离函数（SDF）的几何一致性表示，并利用视频扩散模型合成新视角视频以优化稀疏观测的重建效果。我们在一个新的高速池沸腾视频数据集上进行了评估，展示了仅需两个摄像视角即可实现高质量的视角合成和速度估计。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion-Pretrained Dense and Contextual Embeddings</div>
<div class="meta-line">Authors: Sedigheh Eslami, Maksim Gaiduk, Markus Krimmel, Louis Milliken, Bo Wang, Denis Bykov</div>
<div class="meta-line">First: 2026-02-11T18:59:08+00:00 · Latest: 2026-02-11T18:59:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11151v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11151v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this report, we introduce pplx-embed, a family of multilingual embedding models that employ multi-stage contrastive learning on a diffusion-pretrained language model backbone for web-scale retrieval. By leveraging bidirectional attention through diffusion-based pretraining, our models capture comprehensive bidirectional context within passages, enabling the use of mean pooling and a late chunking strategy to better preserve global context across long documents. We release two model types: pplx-embed-v1 for standard retrieval, and pplx-embed-context-v1 for contextualized embeddings that incorporate global document context into passage representations. pplx-embed-v1 achieves competitive performance on the MTEB(Multilingual, v2), MTEB(Code), MIRACL, BERGEN, and ToolRet retrieval benchmarks, while pplx-embed-context-v1 sets new records on the ConTEB benchmark. Beyond public benchmarks, pplx-embed-v1 demonstrates strong performance on our internal evaluation suite, which focuses on real-world, large-scale search scenarios over tens of millions of documents. These results validate the models&#x27; effectiveness in production environments where retrieval quality and efficiency are critical at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散预训练的密集且上下文感知嵌入</div>
<div class="mono" style="margin-top:8px">本报告介绍了pplx-embed，这是一个多语言嵌入模型家族，它在扩散预训练语言模型主干上采用多阶段对比学习，用于网络规模的检索。通过基于扩散的预训练，我们的模型能够捕捉段落中的全面双向上下文，从而使用均值池化和后期分块策略更好地保留长文档中的全局上下文。我们发布了两种模型类型：pplx-embed-v1用于标准检索，以及pplx-embed-context-v1用于上下文感知嵌入，该模型将全局文档上下文整合到段落表示中。pplx-embed-v1在MTEB（多语言，v2）、MTEB（代码）、MIRACL、BERGEN和ToolRet检索基准上表现优异，而pplx-embed-context-v1在ConTEB基准上创下了新纪录。除了公开基准，pplx-embed-v1在我们内部的评估套件中也表现出色，该套件专注于数千万文档上的实际大规模搜索场景。这些结果验证了模型在生产环境中对检索质量和效率的实用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this report, we introduce pplx-embed, a family of multilingual embedding models that employ multi-stage contrastive learning on a diffusion-pretrained language model backbone for web-scale retrieval.</div>
</details>
</div>
<div class="card">
<div class="title">YOR: Your Own Mobile Manipulator for Generalizable Robotics</div>
<div class="meta-line">Authors: Manan H Anjaria, Mehmet Enes Erciyes, Vedant Ghatnekar, Neha Navarkar, Haritheja Etukuru, Xiaole Jiang, Kanad Patel, Dhawal Kabra, Nicholas Wojno, Radhika Ajay Prayage, Soumith Chintala, Lerrel Pinto, Nur Muhammad Mahi Shafiullah, Zichen Jeff Cui</div>
<div class="meta-line">First: 2026-02-11T18:59:00+00:00 · Latest: 2026-02-11T18:59:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11150v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11150v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://www.yourownrobot.ai/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in robot learning have generated significant interest in capable platforms that may eventually approach human-level competence. This interest, combined with the commoditization of actuators, has propelled growth in low-cost robotic platforms. However, the optimal form factor for mobile manipulation, especially on a budget, remains an open question. We introduce YOR, an open-source, low-cost mobile manipulator that integrates an omnidirectional base, a telescopic vertical lift, and two arms with grippers to achieve whole-body mobility and manipulation. Our design emphasizes modularity, ease of assembly using off-the-shelf components, and affordability, with a bill-of-materials cost under 10,000 USD. We demonstrate YOR&#x27;s capability by completing tasks that require coordinated whole-body control, bimanual manipulation, and autonomous navigation. Overall, YOR offers competitive functionality for mobile manipulation research at a fraction of the cost of existing platforms. Project website: https://www.yourownrobot.ai/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>YOR：适用于通用机器人的自主移动机械臂</div>
<div class="mono" style="margin-top:8px">近年来，机器人学习的进展引发了对具备人类水平能力平台的浓厚兴趣。这种兴趣与执行器的普及化相结合，推动了低成本机器人平台的发展。然而，预算有限情况下移动操作的最佳形态仍是一个开放性问题。我们介绍了YOR，一款开源、低成本的移动机械臂，集成了全向移动基座、可伸缩垂直升降装置以及两个带有夹爪的手臂，以实现全身移动与操作。我们的设计强调模块化、易于使用现成组件组装以及经济性，其物料清单成本低于10,000美元。我们通过完成需要全身协调控制、双臂操作和自主导航的任务，展示了YOR的能力。总体而言，YOR在成本上远低于现有平台，同时提供了具有竞争力的移动操作研究功能。项目网站：https://www.yourownrobot.ai/</div>
</details>
</div>
<div class="card">
<div class="title">Beyond VLM-Based Rewards: Diffusion-Native Latent Reward Modeling</div>
<div class="meta-line">Authors: Gongye Liu, Bo Yang, Yida Zhi, Zhizhou Zhong, Lei Ke, Didan Deng, Han Gao, Yongxiang Huang, Kaihao Zhang, Hongbo Fu, Wenhan Luo</div>
<div class="meta-line">First: 2026-02-11T18:57:29+00:00 · Latest: 2026-02-11T18:57:29+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/HKUST-C4G/diffusion-rm</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11146v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11146v1">PDF</a> · <a href="https://github.com/HKUST-C4G/diffusion-rm">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Preference optimization for diffusion and flow-matching models relies on reward functions that are both discriminatively robust and computationally efficient. Vision-Language Models (VLMs) have emerged as the primary reward provider, leveraging their rich multimodal priors to guide alignment. However, their computation and memory cost can be substantial, and optimizing a latent diffusion generator through a pixel-space reward introduces a domain mismatch that complicates alignment. In this paper, we propose DiNa-LRM, a diffusion-native latent reward model that formulates preference learning directly on noisy diffusion states. Our method introduces a noise-calibrated Thurstone likelihood with diffusion-noise-dependent uncertainty. DiNa-LRM leverages a pretrained latent diffusion backbone with a timestep-conditioned reward head, and supports inference-time noise ensembling, providing a diffusion-native mechanism for test-time scaling and robust rewarding. Across image alignment benchmarks, DiNa-LRM substantially outperforms existing diffusion-based reward baselines and achieves performance competitive with state-of-the-art VLMs at a fraction of the computational cost. In preference optimization, we demonstrate that DiNa-LRM improves preference optimization dynamics, enabling faster and more resource-efficient model alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越基于VLM的奖励：扩散原生的潜在奖励建模</div>
<div class="mono" style="margin-top:8px">扩散和流匹配模型的偏好优化依赖于既具有判别鲁棒性又计算高效的奖励函数。视觉-语言模型（VLM）已成为主要的奖励提供者，利用其丰富的多模态先验知识来引导对齐。然而，它们的计算和内存成本可能很高，并且通过像素空间奖励优化潜在扩散生成器会引入领域不匹配，从而复杂化对齐过程。在本文中，我们提出了DiNa-LRM，这是一种基于扩散的潜在奖励模型，直接在噪声扩散状态上进行偏好学习。我们的方法引入了与扩散噪声校准相关的Thurstone似然和不确定性。DiNa-LRM利用了一个预训练的潜在扩散主干网络，并结合了时间步条件的奖励头，支持推理时的噪声集成，提供了一种扩散原生的机制，用于测试时的扩展和鲁棒奖励。在图像对齐基准测试中，DiNa-LRM显著优于现有的基于扩散的奖励基线，并在计算成本仅为现有方法的几分之一的情况下，实现了与最先进的VLM相当的性能。在偏好优化方面，我们证明DiNa-LRM改进了偏好优化动态，使模型对齐更快且更节省资源。</div>
</details>
</div>
<div class="card">
<div class="title">SCRAPL: Scattering Transform with Random Paths for Machine Learning</div>
<div class="meta-line">Authors: Christopher Mitcheltree, Vincent Lostanlen, Emmanouil Benetos, Mathieu Lagrange</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-11T18:57:08+00:00 · Latest: 2026-02-11T18:57:08+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026. Code, audio samples, and Python package provided at https://christhetree.github.io/scrapl/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11145v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11145v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://christhetree.github.io/scrapl/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Euclidean distance between wavelet scattering transform coefficients (known as paths) provides informative gradients for perceptual quality assessment of deep inverse problems in computer vision, speech, and audio processing. However, these transforms are computationally expensive when employed as differentiable loss functions for stochastic gradient descent due to their numerous paths, which significantly limits their use in neural network training. Against this problem, we propose &quot;Scattering transform with Random Paths for machine Learning&quot; (SCRAPL): a stochastic optimization scheme for efficient evaluation of multivariable scattering transforms. We implement SCRAPL for the joint time-frequency scattering transform (JTFS) which demodulates spectrotemporal patterns at multiple scales and rates, allowing a fine characterization of intermittent auditory textures. We apply SCRAPL to differentiable digital signal processing (DDSP), specifically, unsupervised sound matching of a granular synthesizer and the Roland TR-808 drum machine. We also propose an initialization heuristic based on importance sampling, which adapts SCRAPL to the perceptual content of the dataset, improving neural network convergence and evaluation performance. We make our code and audio samples available and provide SCRAPL as a Python package.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SCRAPL: 用于机器学习的随机路径散射变换</div>
<div class="mono" style="margin-top:8px">小波散射变换系数（称为路径）之间的欧几里得距离为计算机视觉、语音和音频处理中深度逆问题的感知质量评估提供了有信息量的梯度。然而，由于路径数量众多，当这些变换作为可微分损失函数用于随机梯度下降时，计算成本很高，这显著限制了它们在神经网络训练中的应用。针对这一问题，我们提出了&quot;用于机器学习的随机路径散射变换&quot;（SCRAPL）：一种用于高效评估多变量散射变换的随机优化方案。我们为联合时频散射变换（JTFS）实现了SCRAPL，该变换能够在多个尺度和速率上解调频时模式，从而实现对间歇性听觉纹理的精细表征。我们将SCRAPL应用于可微分数字信号处理（DDSP），具体包括颗粒合成器和Roland TR-808鼓机的无监督声音匹配。此外，我们还提出了一种基于重要性采样的初始化启发式方法，使SCRAPL能够适应数据集的感知内容，从而提升神经网络的收敛性和评估性能。我们提供了代码和音频样本，并将SCRAPL作为Python包发布。</div>
</details>
</div>
<div class="card">
<div class="title">GENIUS: Generative Fluid Intelligence Evaluation Suite</div>
<div class="meta-line">Authors: Ruichuan An, Sihan Yang, Ziyu Guo, Wei Dai, Zijun Shen, Haodong Li, Renrui Zhang, Xinyu Wei, Guopeng Li, Wenshan Wu, Wentao Zhang</div>
<div class="meta-line">First: 2026-02-11T18:55:54+00:00 · Latest: 2026-02-11T18:55:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11144v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11144v1">PDF</a> · <a href="https://github.com/arctanxarc/GENIUS}{https://github.com/arctanxarc/GENIUS}$">Code1</a> · <a href="https://github.com/arctanxarc/GENIUS">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unified Multimodal Models (UMMs) have shown remarkable progress in visual generation. Yet, existing benchmarks predominantly assess $\textit{Crystallized Intelligence}$, which relies on recalling accumulated knowledge and learned schemas. This focus overlooks $\textit{Generative Fluid Intelligence (GFI)}$: the capacity to induce patterns, reason through constraints, and adapt to novel scenarios on the fly. To rigorously assess this capability, we introduce $\textbf{GENIUS}$ ($\textbf{GEN}$ Fluid $\textbf{I}$ntelligence Eval$\textbf{U}$ation $\textbf{S}$uite). We formalize $\textit{GFI}$ as a synthesis of three primitives. These include $\textit{Inducing Implicit Patterns}$ (e.g., inferring personalized visual preferences), $\textit{Executing Ad-hoc Constraints}$ (e.g., visualizing abstract metaphors), and $\textit{Adapting to Contextual Knowledge}$ (e.g., simulating counter-intuitive physics). Collectively, these primitives challenge models to solve problems grounded entirely in the immediate context. Our systematic evaluation of 12 representative models reveals significant performance deficits in these tasks. Crucially, our diagnostic analysis disentangles these failure modes. It demonstrates that deficits stem from limited context comprehension rather than insufficient intrinsic generative capability. To bridge this gap, we propose a training-free attention intervention strategy. Ultimately, $\textbf{GENIUS}$ establishes a rigorous standard for $\textit{GFI}$, guiding the field beyond knowledge utilization toward dynamic, general-purpose reasoning. Our dataset and code will be released at: $\href{https://github.com/arctanxarc/GENIUS}{https://github.com/arctanxarc/GENIUS}$.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GENIUS：生成性流体智能评估套件</div>
<div class="mono" style="margin-top:8px">统一多模态模型（UMMs）在视觉生成方面取得了显著进展。然而，现有的基准测试主要评估$\textit{晶体智力}$，这依赖于回忆积累的知识和已学习的模式。这种关注点忽略了$\textit{生成性流体智能（GFI）}$：即在即时情境中诱导模式、通过约束进行推理并适应新场景的能力。为了严格评估这种能力，我们引入了$\textbf{GENIUS}$（$\textbf{GEN}$ Fluid $\textbf{I}$ntelligence Eval$\textbf{U}$ation $\textbf{S}$uite）。我们将$\textit{GFI}$形式化为三种基本要素的综合。这些要素包括$\textit{诱导隐式模式}$（例如，推断个性化的视觉偏好）、$\textit{执行临时约束}$（例如，可视化抽象隐喻）以及$\textit{适应情境知识}$（例如，模拟反直觉物理）。这些基本要素共同挑战模型在完全基于即时情境的问题上进行求解。我们对12个代表性模型的系统评估揭示了这些任务中的显著性能缺陷。关键的是，我们的诊断分析区分了这些失败模式。它表明，缺陷源于有限的情境理解能力，而非内在生成能力不足。为弥合这一差距，我们提出了一种无需训练的注意力干预策略。最终，$\textbf{GENIUS}$为$\textit{GFI}$建立了严格的标准，引导该领域从知识利用转向动态、通用的推理。我们的数据集和代码将在以下链接发布：$\href{https://github.com/arctanxarc/GENIUS}{https://github.com/arctanxarc/GENIUS}$。</div>
</details>
</div>
<div class="card">
<div class="title">Data-Efficient Hierarchical Goal-Conditioned Reinforcement Learning via Normalizing Flows</div>
<div class="meta-line">Authors: Shaswat Garg, Matin Moezzi, Brandon Da Silva</div>
<div class="meta-line">First: 2026-02-11T18:54:48+00:00 · Latest: 2026-02-11T18:54:48+00:00</div>
<div class="meta-line">Comments: 9 pages, 3 figures, IEEE International Conference on Robotics and Automation 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11142v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11142v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hierarchical goal-conditioned reinforcement learning (H-GCRL) provides a powerful framework for tackling complex, long-horizon tasks by decomposing them into structured subgoals. However, its practical adoption is hindered by poor data efficiency and limited policy expressivity, especially in offline or data-scarce regimes. In this work, Normalizing flow-based hierarchical implicit Q-learning (NF-HIQL), a novel framework that replaces unimodal gaussian policies with expressive normalizing flow policies at both the high- and low-levels of the hierarchy is introduced. This design enables tractable log-likelihood computation, efficient sampling, and the ability to model rich multimodal behaviors. New theoretical guarantees are derived, including explicit KL-divergence bounds for Real-valued non-volume preserving (RealNVP) policies and PAC-style sample efficiency results, showing that NF-HIQL preserves stability while improving generalization. Empirically, NF-HIQL is evaluted across diverse long-horizon tasks in locomotion, ball-dribbling, and multi-step manipulation from OGBench. NF-HIQL consistently outperforms prior goal-conditioned and hierarchical baselines, demonstrating superior robustness under limited data and highlighting the potential of flow-based architectures for scalable, data-efficient hierarchical reinforcement learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过归一化流实现的数据高效分层目标条件强化学习</div>
<div class="mono" style="margin-top:8px">分层目标条件强化学习（H-GCRL）提供了一个强大的框架，通过将复杂、长时域任务分解为结构化的子目标来解决这些问题。然而，其在实际应用中受到数据效率低下和策略表达能力有限的阻碍，尤其是在离线或数据稀缺的场景下。本文提出了一种新的框架：基于归一化流的分层隐式Q学习（NF-HIQL），该框架在分层结构的高层和低层均用具有表达能力的归一化流策略替代传统的单模态高斯策略。这种设计使得对数似然的计算变得可行，采样更加高效，并且能够建模丰富的多模态行为。我们推导了新的理论保证，包括针对RealNVP策略的显式KL散度界以及PAC风格的样本效率结果，表明NF-HIQL在保持稳定性的同时提升了泛化能力。实验上，NF-HIQL在OGBench中的运动、控球和多步操作等长时域任务中进行了评估，其表现始终优于先前的目标条件和分层基线方法，展示了基于流的架构在可扩展、数据高效的分层强化学习中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Hierarchical goal-conditioned reinforcement learning (H-GCRL) provides a powerful framework for tackling complex, long-horizon tasks by decomposing them into structured subgoals.</div>
</details>
</div>
<div class="card">
<div class="title">LCIP: Loss-Controlled Inverse Projection of High-Dimensional Image Data</div>
<div class="meta-line">Authors: Yu Wang, Frederik L. Dennig, Michael Behrisch, Alexandru Telea</div>
<div class="meta-line">First: 2026-02-11T18:52:46+00:00 · Latest: 2026-02-11T18:52:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11141v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11141v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Projections (or dimensionality reduction) methods $P$ aim to map high-dimensional data to typically 2D scatterplots for visual exploration. Inverse projection methods $P^{-1}$ aim to map this 2D space to the data space to support tasks such as data augmentation, classifier analysis, and data imputation. Current $P^{-1}$ methods suffer from a fundamental limitation -- they can only generate a fixed surface-like structure in data space, which poorly covers the richness of this space. We address this by a new method that can `sweep&#x27; the data space under user control. Our method works generically for any $P$ technique and dataset, is controlled by two intuitive user-set parameters, and is simple to implement. We demonstrate it by an extensive application involving image manipulation for style transfer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LCIP：高维图像数据的损失控制逆投影</div>
<div class="mono" style="margin-top:8px">投影（或降维）方法 $P$ 的目标是将高维数据映射到通常的 2D 散点图以支持可视化探索。逆投影方法 $P^{-1}$ 的目标是将 2D 空间映射回数据空间，以支持数据增强、分类器分析和数据填补等任务。当前的 $P^{-1}$ 方法存在一个根本性限制——它们只能在数据空间中生成固定的表面结构，无法充分覆盖该空间的丰富性。我们通过一种新的方法来解决这一问题，该方法可以在用户控制下对数据空间进行`扫描&#x27;。我们的方法适用于任何 $P$ 技术和数据集，由两个直观的用户设置参数控制，并且易于实现。我们通过一个广泛的应用案例来展示该方法，涉及用于风格迁移的图像处理。</div>
</details>
</div>
<div class="card">
<div class="title">AlignTune: Modular Toolkit for Post-Training Alignment of Large Language Models</div>
<div class="meta-line">Authors: R E Zera Marveen Lyngkhoi, Chirag Chawla, Pratinav Seth, Utsav Avaiya, Soham Bhattacharjee, Mykola Khandoga, Rui Yuan, Vinay Kumar Sankarapu</div>
<div class="meta-line">First: 2026-02-10T10:08:51+00:00 · Latest: 2026-02-11T18:51:19+00:00</div>
<div class="meta-line">Comments: Library opensource and available at https://github.com/Lexsi-Labs/aligntune</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.09621v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.09621v2">PDF</a> · <a href="https://github.com/Lexsi-Labs/aligntune">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-training alignment is central to deploying large language models (LLMs), yet practical workflows remain split across backend-specific tools and ad-hoc glue code, making experiments hard to reproduce. We identify backend interference, reward fragmentation, and irreproducible pipelines as key obstacles in alignment research. We introduce AlignTune, a modular toolkit exposing a unified interface for supervised fine-tuning (SFT) and RLHF-style optimization with interchangeable TRL and Unsloth backends. AlignTune standardizes configuration, provides an extensible reward layer (rule-based and learned), and integrates evaluation over standard benchmarks and custom tasks. By isolating backend-specific logic behind a single factory boundary, AlignTune enables controlled comparisons and reproducible alignment experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AlignTune：大型语言模型微调对齐的模块化工具包</div>
<div class="mono" style="margin-top:8px">后训练对齐是部署大型语言模型（LLMs）的核心，但实际工作流程仍分散在后端特定工具和临时粘合代码中，导致实验难以复现。我们识别出后端干扰、奖励碎片化和不可复现的流程是对齐研究中的关键障碍。我们引入了AlignTune，这是一个模块化工具包，提供统一的接口用于监督微调（SFT）和RLHF风格的优化，并支持可替换的TRL和Unsloth后端。AlignTune标准化了配置，提供可扩展的奖励层（基于规则和学习），并整合了在标准基准和自定义任务上的评估。通过将后端特定的逻辑隔离在单一工厂边界后，AlignTune实现了可控的比较和可复现的对齐实验。</div>
</details>
</div>
<div class="card">
<div class="title">TabICLv2: A better, faster, scalable, and open tabular foundation model</div>
<div class="meta-line">Authors: Jingang Qu, David Holzmüller, Gaël Varoquaux, Marine Le Morvan</div>
<div class="meta-line">First: 2026-02-11T18:51:02+00:00 · Latest: 2026-02-11T18:51:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11139v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11139v1">PDF</a> · <a href="https://github.com/soda-inria/tabicl">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tabular foundation models, such as TabPFNv2 and TabICL, have recently dethroned gradient-boosted trees at the top of predictive benchmarks, demonstrating the value of in-context learning for tabular data. We introduce TabICLv2, a new state-of-the-art foundation model for regression and classification built on three pillars: (1) a novel synthetic data generation engine designed for high pretraining diversity; (2) various architectural innovations, including a new scalable softmax in attention improving generalization to larger datasets without prohibitive long-sequence pretraining; and (3) optimized pretraining protocols, notably replacing AdamW with the Muon optimizer. On the TabArena and TALENT benchmarks, TabICLv2 without any tuning surpasses the performance of the current state of the art, RealTabPFN-2.5 (hyperparameter-tuned, ensembled, and fine-tuned on real data). With only moderate pretraining compute, TabICLv2 generalizes effectively to million-scale datasets under 50GB GPU memory while being markedly faster than RealTabPFN-2.5. We provide extensive ablation studies to quantify these contributions and commit to open research by first releasing inference code and model weights at https://github.com/soda-inria/tabicl, with synthetic data engine and pretraining code to follow.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TabICLv2：一个更优、更快、可扩展且开放的表格基础模型</div>
<div class="mono" style="margin-top:8px">近期，表格基础模型如TabPFNv2和TabICL在预测基准测试中超越了梯度提升树，展示了上下文学习在表格数据中的价值。我们引入了TabICLv2，这是一个基于三个支柱构建的新一代回归和分类基础模型：(1) 一种新型合成数据生成引擎，旨在提高预训练的多样性；(2) 多种架构创新，包括一种新的可扩展softmax机制，以提升对大规模数据集的泛化能力，而无需耗费高昂的长序列预训练；(3) 优化的预训练协议，特别是用Muon优化器替代AdamW。在TabArena和TALENT基准测试中，TabICLv2无需任何调优即可超越当前最先进的RealTabPFN-2.5（超参数调优、集成和在真实数据上微调）。在仅中等预训练计算资源下，TabICLv2在50GB GPU内存限制下能有效泛化到百万级数据集，同时显著快于RealTabPFN-2.5。我们提供了广泛的消融研究以量化这些贡献，并承诺通过开放研究，首先在https://github.com/soda-inria/tabicl上发布推理代码和模型权重，后续将发布合成数据引擎和预训练代码。</div>
</details>
</div>
<div class="card">
<div class="title">MOTGNN: Interpretable Graph Neural Networks for Multi-Omics Disease Classification</div>
<div class="meta-line">Authors: Tiantian Yang, Zhiqian Chen</div>
<div class="meta-line">First: 2025-08-10T19:35:53+00:00 · Latest: 2026-02-11T18:50:44+00:00</div>
<div class="meta-line">Comments: 11 pages, 6 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.07465v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.07465v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Integrating multi-omics data, such as DNA methylation, mRNA expression, and microRNA (miRNA) expression, offers a comprehensive view of the biological mechanisms underlying disease. However, the high dimensionality of multi-omics data, the heterogeneity across modalities, and the lack of reliable biological interaction networks make meaningful integration challenging. In addition, many existing models rely on handcrafted similarity graphs, are vulnerable to class imbalance, and often lack built-in interpretability, limiting their usefulness in biomedical applications. We propose Multi-Omics integration with Tree-generated Graph Neural Network (MOTGNN), a novel and interpretable framework for binary disease classification. MOTGNN employs eXtreme Gradient Boosting (XGBoost) for omics-specific supervised graph construction, followed by modality-specific Graph Neural Networks (GNNs) for hierarchical representation learning, and a deep feedforward network for cross-omics integration. Across three real-world disease datasets, MOTGNN outperforms state-of-the-art baselines by 5-10% in accuracy, ROC-AUC, and F1-score, and remains robust to severe class imbalance. The model maintains computational efficiency through the use of sparse graphs and provides built-in interpretability, revealing both top-ranked biomarkers and the relative contributions of each omics modality. These results highlight the potential of MOTGNN to improve both predictive accuracy and interpretability in multi-omics disease modeling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MOTGNN：用于多组学疾病分类的可解释图神经网络</div>
<div class="mono" style="margin-top:8px">整合多组学数据，如DNA甲基化、mRNA表达和微小RNA（miRNA）表达，可以提供对疾病生物学机制的全面理解。然而，多组学数据的高维度、不同模态之间的异质性以及缺乏可靠的生物相互作用网络，使得有意义的整合具有挑战性。此外，许多现有模型依赖手工构建的相似性图，容易受到类别不平衡的影响，并且通常缺乏内置的可解释性，这限制了其在生物医学应用中的实用性。我们提出了一种新的、可解释的框架Multi-Omics integration with Tree-generated Graph Neural Network（MOTGNN），用于二分类疾病分类。MOTGNN使用极端梯度提升（XGBoost）进行组学特异性监督图构建，随后使用模态特异性的图神经网络（GNNs）进行分层表示学习，并通过深度前馈网络实现跨组学整合。在三个真实世界疾病数据集上，MOTGNN在准确率、ROC-AUC和F1分数上均优于最先进的基线模型5-10%，并且在严重的类别不平衡情况下仍保持鲁棒性。该模型通过使用稀疏图保持计算效率，并提供内置的可解释性，揭示了排名靠前的生物标志物以及每种组学模态的相对贡献。这些结果突显了MOTGNN在提升多组学疾病建模的预测准确性和可解释性方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Integrating multi-omics data, such as DNA methylation, mRNA expression, and microRNA (miRNA) expression, offers a comprehensive view of the biological mechanisms underlying disease.</div>
</details>
</div>
<div class="card">
<div class="title">Weight Decay Improves Language Model Plasticity</div>
<div class="meta-line">Authors: Tessa Han, Sebastian Bordt, Hanlin Zhang, Sham Kakade</div>
<div class="meta-line">First: 2026-02-11T18:49:26+00:00 · Latest: 2026-02-11T18:49:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11137v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11137v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The prevailing paradigm in large language model (LLM) development is to pretrain a base model, then perform further training to improve performance and model behavior. However, hyperparameter optimization and scaling laws have been studied primarily from the perspective of the base model&#x27;s validation loss, ignoring downstream adaptability. In this work, we study pretraining from the perspective of model plasticity, that is, the ability of the base model to successfully adapt to downstream tasks through fine-tuning. We focus on the role of weight decay, a key regularization parameter during pretraining. Through systematic experiments, we show that models trained with larger weight decay values are more plastic, meaning they show larger performance gains when fine-tuned on downstream tasks. This phenomenon can lead to counterintuitive trade-offs where base models that perform worse after pretraining can perform better after fine-tuning. Further investigation of weight decay&#x27;s mechanistic effects on model behavior reveals that it encourages linearly separable representations, regularizes attention matrices, and reduces overfitting on the training data. In conclusion, this work demonstrates the importance of using evaluation metrics beyond cross-entropy loss for hyperparameter optimization and casts light on the multifaceted role of that a single optimization hyperparameter plays in shaping model behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>权重衰减提升语言模型的可塑性</div>
<div class="mono" style="margin-top:8px">在大型语言模型（LLM）开发中，主流范式是先预训练一个基础模型，再通过进一步训练来提升性能和模型行为。然而，超参数优化和扩展定律的研究主要从基础模型的验证损失角度出发，忽略了下游任务的适应性。在本工作中，我们从模型可塑性的角度研究预训练，即基础模型通过微调成功适应下游任务的能力。我们重点关注权重衰减，这是预训练过程中一个关键的正则化参数。通过系统实验，我们表明使用较大权重衰减值训练的模型具有更高的可塑性，这意味着它们在微调下游任务时能获得更大的性能提升。这种现象可能导致反直觉的权衡，即某些预训练后表现较差的基础模型，在微调后可能表现更好。进一步研究权重衰减对模型行为的机制作用表明，它鼓励线性可分的表示，正则化注意力矩阵，并减少对训练数据的过拟合。总之，本工作展示了在超参数优化中使用超越交叉熵损失的评估指标的重要性，并揭示了单个优化超参数在塑造模型行为中的多方面作用。</div>
</details>
</div>
<div class="card">
<div class="title">Proficient Graph Neural Network Design by Accumulating Knowledge on Large Language Models</div>
<div class="meta-line">Authors: Jialiang Wang, Hanmo Liu, Shimin Di, Zhili Wang, Jiachuan Wang, Lei Chen, Xiaofang Zhou</div>
<div class="meta-line">Venue: WSDM 2026</div>
<div class="meta-line">First: 2024-08-13T08:22:01+00:00 · Latest: 2026-02-11T18:49:00+00:00</div>
<div class="meta-line">Comments: Accepted at WSDM 2026. Title changed from &quot;Computation-friendly graph neural network design by accumulating knowledge on large language models&quot; to &quot;Proficient Graph Neural Network Design by Accumulating Knowledge on Large Language Models&quot;</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2408.06717v3">Abs</a> · <a href="https://arxiv.org/pdf/2408.06717v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-level automation is increasingly critical in AI, driven by rapid advances in large language models (LLMs) and AI agents. However, LLMs, despite their general reasoning power, struggle significantly in specialized, data-sensitive tasks such as designing Graph Neural Networks (GNNs). This difficulty arises from (1) the inherent knowledge gaps in modeling the intricate, varying relationships between graph properties and suitable architectures and (2) the external noise from misleading descriptive inputs, often resulting in generic or even misleading model suggestions. Achieving proficiency in designing data-aware models -- defined as the meta-level capability to systematically accumulate, interpret, and apply data-specific design knowledge -- remains challenging for existing automated approaches, due to their inefficient construction and application of meta-knowledge. To achieve meta-level proficiency, we propose DesiGNN, a knowledge-centered framework that systematically converts past model design experience into structured, fine-grained knowledge priors well-suited for meta-learning with LLMs. To account for the inherent variability and external noise, DesiGNN aligns empirical property filtering from extensive benchmarks with adaptive elicitation of literature insights via LLMs. By constructing a solid meta-knowledge between unseen graph understanding and known effective architecture patterns, DesiGNN can deliver top-5.77% initial model proposals for unseen datasets within seconds and achieve consistently superior performance with minimal search cost compared to baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过在大语言模型上积累知识实现高效的图神经网络设计</div>
<div class="mono" style="margin-top:8px">在AI领域，高水平的自动化正变得越来越关键，这得益于大语言模型（LLMs）和AI代理的快速发展。然而，尽管LLMs具备强大的通用推理能力，在诸如图神经网络（GNNs）设计等专业且数据敏感的任务中仍面临显著困难。这种困难源于（1）在建模图属性与合适架构之间复杂且多变的关系时固有的知识缺口，以及（2）来自误导性描述输入的外部噪声，常常导致通用甚至误导性的模型建议。现有自动化方法在构建和应用元知识方面效率低下，因此在实现数据感知模型设计的元级能力——即系统地积累、解释和应用数据特定的设计知识——方面仍面临挑战。为实现元级能力，我们提出了DesiGNN，一个以知识为中心的框架，系统地将以往的模型设计经验转化为适合与LLMs结合进行元学习的结构化、细粒度知识先验。为了应对固有的变异性与外部噪声，DesiGNN通过LLMs自适应地提取文献见解，并与大量基准测试中的经验属性过滤对齐。通过构建未见过的图理解与已知有效架构模式之间的坚实元知识，DesiGNN能够在几秒内为未见过的数据集生成前5.77%的模型建议，并在搜索成本极低的情况下实现优于基线的稳定性能。</div>
</details>
</div>
<div class="card">
<div class="title">FormalJudge: A Neuro-Symbolic Paradigm for Agentic Oversight</div>
<div class="meta-line">Authors: Jiayi Zhou, Yang Sheng, Hantao Lou, Yaodong Yang, Jie Fu</div>
<div class="meta-line">First: 2026-02-11T18:48:11+00:00 · Latest: 2026-02-11T18:48:11+00:00</div>
<div class="meta-line">Comments: 27 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11136v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11136v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As LLM-based agents increasingly operate in high-stakes domains with real-world consequences, ensuring their behavioral safety becomes paramount. The dominant oversight paradigm, LLM-as-a-Judge, faces a fundamental dilemma: how can probabilistic systems reliably supervise other probabilistic systems without inheriting their failure modes? We argue that formal verification offers a principled escape from this dilemma, yet its adoption has been hindered by a critical bottleneck: the translation from natural language requirements to formal specifications. This paper bridges this gap by proposing , a neuro-symbolic framework that employs a bidirectional Formal-of-Thought architecture: LLMs serve as specification compilers that top-down decompose high-level human intent into atomic, verifiable constraints, then bottom-up prove compliance using Dafny specifications and Z3 Satisfiability modulo theories solving, which produces mathematical guarantees rather than probabilistic scores. We validate across three benchmarks spanning behavioral safety, multi-domain constraint adherence, and agentic upward deception detection. Experiments on 7 agent models demonstrate that achieves an average improvement of 16.6% over LLM-as-a-Judge baselines, enables weak-to-strong generalization where a 7B judge achieves over 90% accuracy detecting deception from 72B agents, and provides near-linear safety improvement through iterative refinement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FormalJudge：一种用于代理监督的神经符号范式</div>
<div class="mono" style="margin-top:8px">随着基于大语言模型（LLM）的代理越来越多地在具有现实后果的高风险领域中运行，确保其行为安全变得至关重要。主流的监督范式LLM-as-a-Judge面临一个根本性的困境：如何让概率系统可靠地监督其他概率系统而不继承其失效模式？我们认为，形式化验证提供了一种解决这一困境的原则性方法，但其应用却受到一个关键瓶颈的阻碍：从自然语言需求到形式化规范的转换。本文通过提出一种神经符号框架来弥合这一差距，该框架采用双向的Formal-of-Thought架构：LLM作为规范编译器，自上而下地将高层次的人类意图分解为原子、可验证的约束，然后自下而上地使用Dafny规范和Z3可满足性模理论求解器来证明合规性，从而产生数学保证而非概率评分。我们在三个基准上验证了该方法，涵盖行为安全、多领域约束遵循以及代理向上欺骗检测。在7个代理模型上的实验表明，该方法在LLM-as-a-Judge基线之上平均提升了16.6%，实现了从弱到强的泛化，其中70亿参数的监督模型在检测720亿参数代理的欺骗行为时准确率超过90%，并通过迭代优化实现了近线性的安全提升。</div>
</details>
</div>
<div class="card">
<div class="title">Occlusion-Aware Consistent Model Predictive Control for Robot Navigation in Occluded Obstacle-Dense Environments</div>
<div class="meta-line">Authors: Minzhe Zheng, Lei Zheng, Lei Zhu, Jun Ma</div>
<div class="meta-line">First: 2025-03-06T15:52:59+00:00 · Latest: 2026-02-11T18:47:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.04563v5">Abs</a> · <a href="https://arxiv.org/pdf/2503.04563v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring safety and motion consistency for robot navigation in occluded, obstacle-dense environments is a critical challenge. In this context, this study presents an occlusion-aware Consistent Model Predictive Control (CMPC) strategy. To account for the occluded obstacles, it incorporates adjustable risk regions that represent their potential future locations. Subsequently, dynamic risk boundary constraints are developed online to enhance safety. Based on these constraints, the CMPC constructs multiple locally optimal trajectory branches (each tailored to different risk regions) to strike a balance between safety and performance. A shared consensus segment is generated to ensure smooth transitions between branches without significant velocity fluctuations, preserving motion consistency. To facilitate high computational efficiency and ensure coordination across local trajectories, we use the alternating direction method of multipliers (ADMM) to decompose the CMPC into manageable sub-problems for parallel solving. The proposed strategy is validated through simulations and real-world experiments on an Ackermann-steering robot platform. The results demonstrate the effectiveness of the proposed CMPC strategy through comparisons with baseline approaches in occluded, obstacle-dense environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向遮挡的自洽模型预测控制在遮挡密集障碍环境中用于机器人导航</div>
<div class="mono" style="margin-top:8px">确保机器人在遮挡密集障碍环境中导航的安全性和运动一致性是一个关键挑战。本文在此背景下提出了一种面向遮挡的自洽模型预测控制（CMPC）策略。为考虑遮挡障碍物，该策略引入了可调节的风险区域，以表示其潜在的未来位置。随后，开发了在线动态风险边界约束以提高安全性。基于这些约束，CMPC构建了多个局部最优轨迹分支（每个分支针对不同的风险区域），以在安全性和性能之间取得平衡。生成一个共享共识段，以确保分支之间的平滑过渡，避免显著的速度波动，从而保持运动一致性。为提高计算效率并确保局部轨迹之间的协调，我们采用交替方向乘子法（ADMM）将CMPC分解为可并行求解的子问题。所提出的策略通过在Ackermann转向机器人平台上的仿真和现实世界实验进行了验证。结果通过与基线方法的比较，展示了所提出的CMPC策略在遮挡密集障碍环境中的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Equivariant symmetry-aware head pose estimation for fetal MRI</div>
<div class="meta-line">Authors: Ramya Muthukrishnan, Borjan Gagoski, Aryn Lee, P. Ellen Grant, Elfar Adalsteinsson, Polina Golland, Benjamin Billot</div>
<div class="meta-line">First: 2025-12-04T15:15:55+00:00 · Latest: 2026-02-11T18:47:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.04890v5">Abs</a> · <a href="https://arxiv.org/pdf/2512.04890v5">PDF</a> · <a href="http://github.com/MedicalVisionGroup/E3-Pose">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present E(3)-Pose, a novel fast pose estimation method that jointly and explicitly models rotation equivariance and object symmetry. Our work is motivated by the challenging problem of accounting for fetal head motion during a diagnostic MRI scan. We aim to enable automatic adaptive prescription of 2D diagnostic MRI slices with 6-DoF head pose estimation, supported by 3D MRI volumes rapidly acquired before each 2D slice. Existing methods struggle to generalize to clinical volumes, due to pose ambiguities induced by inherent anatomical symmetries, as well as low resolution, noise, and artifacts. In contrast, E(3)-Pose captures anatomical symmetries and rigid pose equivariance by construction, and yields robust estimates of the fetal head pose. Our experiments on publicly available and representative clinical fetal MRI datasets demonstrate the superior robustness and generalization of our method across domains. Crucially, E(3)-Pose achieves state-of-the-art accuracy on clinical MRI volumes, supporting future clinical translation. Our implementation is publicly available at github.com/MedicalVisionGroup/E3-Pose.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>针对胎儿MRI的等变对称感知头部姿态估计</div>
<div class="mono" style="margin-top:8px">我们提出了E(3)-Pose，一种新颖的快速姿态估计方法，联合且显式地建模旋转等变性和物体对称性。我们的工作受到在诊断MRI扫描过程中考虑胎儿头部运动这一具有挑战性问题的启发。我们旨在通过在每次获取2D切片前快速采集的3D MRI体数据支持下，实现自动适应性的2D诊断MRI切片处方。现有方法由于由固有解剖对称性引起的姿态歧义，以及低分辨率、噪声和伪影等问题，难以推广到临床数据。相反，E(3)-Pose通过构造方式捕捉解剖对称性和刚性姿态等变性，从而获得稳健的胎儿头部姿态估计。我们在公开且具有代表性的临床胎儿MRI数据集上的实验表明，我们的方法在跨领域具有优越的鲁棒性和泛化能力。关键的是，E(3)-Pose在临床MRI体数据上实现了最先进的精度，支持未来的临床转化。我们的实现可在github.com/MedicalVisionGroup/E3-Pose上公开获取。</div>
</details>
</div>
<div class="card">
<div class="title">Just on Time: Token-Level Early Stopping for Diffusion Language Models</div>
<div class="meta-line">Authors: Zahar Kohut, Severyn Shykula, Dmytro Khamula, Mykola Vysotskyi, Taras Rumezhak, Volodymyr Karpiv</div>
<div class="meta-line">First: 2026-02-11T18:44:04+00:00 · Latest: 2026-02-11T18:44:04+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11133v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11133v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion language models generate text through iterative refinement, a process that is often computationally inefficient because many tokens reach stability long before the final denoising step. We introduce a training-free, token-level early stopping approach that identifies convergence independently at each position. Our method leverages lightweight signals derived from the model&#x27;s predictions and local context to dynamically determine when individual tokens can be finalized. This yields adaptive per-token freezing without task-specific fine-tuning, substantially reducing the total number of diffusion steps required. Across diverse benchmarks, spanning mathematical reasoning, general question answering, and scientific understanding, our approach achieves state-of-the-art efficiency gains while preserving generation quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>及时完成：扩散语言模型的令牌级提前终止</div>
<div class="mono" style="margin-top:8px">扩散语言模型通过迭代优化生成文本，这一过程通常计算效率低下，因为许多令牌在最终去噪步骤之前就已经趋于稳定。我们提出了一种无需训练的令牌级提前终止方法，能够在每个位置独立识别收敛状态。我们的方法利用模型预测和局部上下文中的轻量信号，动态判断每个令牌何时可以完成生成。这种方法实现了无需任务特定微调的自适应令牌冻结，显著减少了所需的扩散步骤总数。在涵盖数学推理、通用问答和科学理解等多个基准测试中，我们的方法在保持生成质量的同时，实现了最先进的效率提升。</div>
</details>
</div>
<div class="card">
<div class="title">On Qualitative Preference in Alternating-time Temporal Logic with Strategy Contexts</div>
<div class="meta-line">Authors: Dimitar P. Guelev</div>
<div class="meta-line">First: 2025-02-19T05:17:18+00:00 · Latest: 2026-02-11T18:44:00+00:00</div>
<div class="meta-line">Comments: 30 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.13436v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.13436v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We show how to add and eliminate binary preference on plays in Alternating-time Temporal Logic (ATL) with strategy contexts on Concurrent Game Models (CGMs) by means of a translation which preserves satisfaction in models where preference-indiscernibility between plays is an equivalence relation of finite index. The elimination technique also works for a companion second-order path quantifier, which makes quantified path variables range over sets of plays that are closed under preference-indiscernibility. We argue that the preference operator and the specialized quantifier facilitate formulating interesting solution concepts such as Nash equilibrium and secure equilibrium in a straightforward way. We also present a novel translation from ATL with strategy contexts to Quantified Computation Tree Logic (QCTL). Together with the translation which eliminates preference and the specialized form of quantification, this translation allows reasoning about infinite multiplayer synchronous games on CGMs to be translated from the proposed extension of ATL with strategy contexts into QCTL. The setting is related to that of ordered objectives in the works of Bouyer, Brenguier, Markey and Ummels, except that our focus is on the use of the temporal logic languages mentioned above, and we rely on translations into QCTL for the algorithmic solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在具有策略上下文的交替时间时态逻辑中的定性偏好</div>
<div class="mono" style="margin-top:8px">我们展示了一种翻译方法，该方法在具有策略上下文的并发博弈模型（CGMs）中，通过保持在偏好不可区分性为有限指数等价关系的模型中的满足性，实现对交替时间时态逻辑（ATL）中游戏路径的二元偏好添加与消除。消除技术同样适用于一个伴随的二阶路径量词，使得量化路径变量在偏好不可区分性闭合的路径集合上进行范围变化。我们论证了偏好操作符和专门化的量词有助于以直接的方式表达有趣的解概念，如纳什均衡和安全均衡。我们还提出了一种从具有策略上下文的ATL到量化计算树逻辑（QCTL）的新翻译方法。结合消除偏好和专门化量词的翻译方法，这种翻译使得在CGMs上对无限多玩家同步博弈的推理可以转换为对具有策略上下文的ATL扩展的QCTL形式。该设定与Bouyer、Brenguier、Markey和Ummels等人关于有序目标的研究相关，但我们的重点在于上述时态逻辑语言的使用，并依赖于翻译到QCTL来实现算法解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We show how to add and eliminate binary preference on plays in Alternating-time Temporal Logic (ATL) with strategy contexts on Concurrent Game Models (CGMs) by means of a translation which preserves satisfaction in models where preference-indiscernibility between plays is an equivalence relation of finite index.</div>
</details>
</div>
<div class="card">
<div class="title">Expanding the Capabilities of Reinforcement Learning via Text Feedback</div>
<div class="meta-line">Authors: Yuda Song, Lili Chen, Fahim Tajwar, Remi Munos, Deepak Pathak, J. Andrew Bagnell, Aarti Singh, Andrea Zanette</div>
<div class="meta-line">First: 2026-02-02T18:56:56+00:00 · Latest: 2026-02-11T18:43:26+00:00</div>
<div class="meta-line">Comments: 43 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02482v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02482v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过文本反馈扩展强化学习的能力</div>
<div class="mono" style="margin-top:8px">强化学习在大语言模型（LLM）微调中的成功源于一个不合理的信息来源：每个rollout仅提供一个二进制奖励或偏好标签的单比特信息。在另一极端，蒸馏方法提供密集的监督，但需要示范数据，而示范数据的获取成本高且难以扩展。我们研究文本反馈作为一种中间信号：比标量奖励更丰富，但比完整示范更便宜。文本反馈是人类交互的自然形式，并且在许多现实场景中已经非常丰富，用户、标注者和自动评估者经常对LLM的输出进行评论。为了大规模利用文本反馈，我们形式化了一个多轮强化学习设置，称为文本反馈强化学习（RL from Text Feedback, RLTF），其中在训练过程中提供文本反馈，但在推理时不提供。因此，模型必须学会将反馈内化以提升其推理时的单轮性能。为此，我们提出了两种方法：自蒸馏（RLTF-SD），它训练单轮策略以匹配其自身的反馈条件下的第二轮生成；以及反馈建模（RLTF-FM），它将反馈预测作为辅助目标。我们对这两种方法进行了理论分析，并在推理谜题、竞赛数学和创意写作任务上进行了实证评估。我们的结果表明，这两种方法在多个基准测试中均优于强大的基线模型，突显了在大规模场景下引入丰富监督源的强化学习的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label.</div>
</details>
</div>
<div class="card">
<div class="title">End to End Collaborative Synthetic Data Generation</div>
<div class="meta-line">Authors: Sikha Pentyala, Geetha Sitaraman, Trae Claar, Martine De Cock</div>
<div class="meta-line">Venue: AAAI 2025</div>
<div class="meta-line">First: 2024-12-04T23:10:51+00:00 · Latest: 2026-02-11T18:43:08+00:00</div>
<div class="meta-line">Comments: Accepted at PPAI Workshop, AAAI 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.03766v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.03766v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of AI is based on the availability of data to train models. While in some cases a single data custodian may have sufficient data to enable AI, often multiple custodians need to collaborate to reach a cumulative size required for meaningful AI research. The latter is, for example, often the case for rare diseases, with each clinical site having data for only a small number of patients. Recent algorithms for federated synthetic data generation are an important step towards collaborative, privacy-preserving data sharing. Existing techniques, however, focus exclusively on synthesizer training, assuming that the training data is already preprocessed and that the desired synthetic data can be delivered in one shot, without any hyperparameter tuning. In this paper, we propose an end-to-end collaborative framework for publishing of synthetic data that accounts for privacy-preserving preprocessing as well as evaluation. We instantiate this framework with Secure Multiparty Computation (MPC) protocols and evaluate it in a use case for privacy-preserving publishing of synthetic genomic data for leukemia.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>端到端协作式合成数据生成</div>
<div class="mono" style="margin-top:8px">人工智能的成功依赖于可用于训练模型的数据。在某些情况下，单一数据持有者可能拥有足够的数据以支持AI研究，但通常多个持有者需要协作以达到有意义的AI研究所需的累积数据规模。例如，在罕见病研究中，每个临床机构可能仅拥有少量患者的数据。最近的联邦合成数据生成算法是实现协作、隐私保护数据共享的重要一步。然而，现有技术仅专注于合成器的训练，假设训练数据已经预处理完成，并且期望的合成数据可以一次性交付，而无需任何超参数调整。在本文中，我们提出了一种端到端的协作框架，用于合成数据的发布，该框架同时考虑隐私保护的预处理和评估。我们使用安全多方计算（MPC）协议实例化该框架，并在一个用于隐私保护发布白血病合成基因组数据的用例中进行了评估。</div>
</details>
</div>
<div class="card">
<div class="title">MIND: Benchmarking Memory Consistency and Action Control in World Models</div>
<div class="meta-line">Authors: Yixuan Ye, Xuanyu Lu, Yuxin Jiang, Yuchao Gu, Rui Zhao, Qiwei Liang, Jiachun Pan, Fengda Zhang, Weijia Wu, Alex Jinpeng Wang</div>
<div class="meta-line">First: 2026-02-08T15:57:23+00:00 · Latest: 2026-02-11T18:42:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08025v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08025v2">PDF</a> · <a href="https://github.com/CSU-JPG/MIND">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">World models aim to understand, remember, and predict dynamic visual environments, yet a unified benchmark for evaluating their fundamental abilities remains lacking. To address this gap, we introduce MIND, the first open-domain closed-loop revisited benchmark for evaluating Memory consIstency and action coNtrol in worlD models. MIND contains 250 high-quality videos at 1080p and 24 FPS, including 100 (first-person) + 100 (third-person) video clips under a shared action space and 25 + 25 clips across varied action spaces covering eight diverse scenes. We design an efficient evaluation framework to measure two core abilities: memory consistency and action control, capturing temporal stability and contextual coherence across viewpoints. Furthermore, we design various action spaces, including different character movement speeds and camera rotation angles, to evaluate the action generalization capability across different action spaces under shared scenes. To facilitate future performance benchmarking on MIND, we introduce MIND-World, a novel interactive Video-to-World baseline. Extensive experiments demonstrate the completeness of MIND and reveal key challenges in current world models, including the difficulty of maintaining long-term memory consistency and generalizing across action spaces. Code: https://github.com/CSU-JPG/MIND.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MIND：世界模型中记忆一致性与动作控制的基准测试</div>
<div class="mono" style="margin-top:8px">世界模型旨在理解、记忆和预测动态视觉环境，但目前仍缺乏统一的基准来评估其基本能力。为解决这一问题，我们引入了MIND，这是首个开放领域闭环重访基准，用于评估世界模型中的记忆一致性和动作控制。MIND包含250个高质量的1080p和24 FPS视频，包括在共享动作空间下的100个（第一视角）+ 100个（第三人称）视频片段，以及覆盖八个多样化场景的不同动作空间下的25 + 25个片段。我们设计了一个高效的评估框架，用于衡量两种核心能力：记忆一致性和动作控制，捕捉不同视角下的时间稳定性与上下文一致性。此外，我们设计了多种动作空间，包括不同的角色移动速度和摄像机旋转角度，以评估在共享场景下不同动作空间中的动作泛化能力。为便于未来在MIND上的性能基准测试，我们引入了MIND-World，一种新颖的视频到世界的交互基线。大量实验验证了MIND的完整性，并揭示了当前世界模型中的关键挑战，包括维持长期记忆一致性的困难以及在不同动作空间中的泛化能力问题。代码：https://github.com/CSU-JPG/MIND。</div>
</details>
</div>
<div class="card">
<div class="title">From Circuits to Dynamics: Understanding and Stabilizing Failure in 3D Diffusion Transformers</div>
<div class="meta-line">Authors: Maximilian Plattner, Fabian Paischer, Johannes Brandstetter, Arturs Berzins</div>
<div class="meta-line">First: 2026-02-11T18:42:05+00:00 · Latest: 2026-02-11T18:42:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11130v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11130v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reliable surface completion from sparse point clouds underpins many applications spanning content creation and robotics. While 3D diffusion transformers attain state-of-the-art results on this task, we uncover that they exhibit a catastrophic mode of failure: arbitrarily small on-surface perturbations to the input point cloud can fracture the output into multiple disconnected pieces -- a phenomenon we call Meltdown. Using activation-patching from mechanistic interpretability, we localize Meltdown to a single early denoising cross-attention activation. We find that the singular-value spectrum of this activation provides a scalar proxy: its spectral entropy rises when fragmentation occurs and returns to baseline when patched. Interpreted through diffusion dynamics, we show that this proxy tracks a symmetry-breaking bifurcation of the reverse process. Guided by this insight, we introduce PowerRemap, a test-time control that stabilizes sparse point-cloud conditioning. We demonstrate that Meltdown persists across state-of-the-art architectures (WaLa, Make-a-Shape), datasets (GSO, SimJEB) and denoising strategies (DDPM, DDIM), and that PowerRemap effectively counters this failure with stabilization rates of up to 98.3%. Overall, this work is a case study on how diffusion model behavior can be understood and guided based on mechanistic analysis, linking a circuit-level cross-attention mechanism to diffusion-dynamics accounts of trajectory bifurcations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从电路到动态：理解并稳定3D扩散Transformer中的故障</div>
<div class="mono" style="margin-top:8px">可靠的稀疏点云表面补全是内容创作和机器人技术等多个应用的基础。尽管3D扩散Transformer在该任务上取得了最先进的结果，但我们发现它们存在一种灾难性故障模式：对输入点云表面的任意微小扰动可能导致输出被分割成多个不连通的部分——我们称之为Meltdown。通过从机制可解释性中提取的激活补丁方法，我们将Meltdown定位到一个早期去噪交叉注意力激活上。我们发现，该激活的奇异值谱提供了一个标量代理：当出现碎片化时，其谱熵上升，而补丁后则恢复到基线水平。通过扩散动态的视角，我们展示了该代理如何追踪反向过程中的对称性破缺分岔。基于这一洞察，我们引入了PowerRemap，这是一种测试时的控制方法，用于稳定稀疏点云的条件。我们证明了Meltdown在最先进的架构（WaLa、Make-a-Shape）、数据集（GSO、SimJEB）和去噪策略（DDPM、DDIM）中普遍存在，并且PowerRemap能够有效应对这种故障，稳定率高达98.3%。总体而言，这项工作是一个案例研究，展示了如何通过机制分析来理解并引导扩散模型的行为，将电路级的交叉注意力机制与扩散动态中轨迹分岔的解释联系起来。</div>
</details>
</div>
<div class="card">
<div class="title">Asymmetric Prompt Weighting for Reinforcement Learning with Verifiable Rewards</div>
<div class="meta-line">Authors: Reinhard Heckel, Mahdi Soltanolkotabi, Christos Thramboulidis</div>
<div class="meta-line">First: 2026-02-11T18:39:42+00:00 · Latest: 2026-02-11T18:39:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11128v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11128v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards has driven recent advances in LLM post-training, in particular for reasoning. Policy optimization algorithms generate a number of responses for a given prompt and then effectively weight the corresponding gradients depending on the rewards. The most popular algorithms including GRPO, DAPO, and RLOO focus on ambiguous prompts, i.e., prompts with intermediate success probability, while downgrading gradients with very easy and very hard prompts. In this paper, we consider asymmetric prompt weightings that assign higher weights to prompts with low, or even zero, empirical success probability. We find that asymmetric weighting particularly benefits from-scratch RL (as in R1-Zero), where training traverses a wide accuracy range, and less so in post-SFT RL where the model already starts at high accuracy. We also provide theory that characterizes prompt weights which minimize the time needed to raise success probability from an initial level to a target accuracy under a fixed update budget. In low-success regimes, where informative responses are rare and response cost dominates, these optimal weights become asymmetric, upweighting low success probabilities and thereby accelerating effective-time convergence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可验证奖励的强化学习中的非对称提示加权</div>
<div class="mono" style="margin-top:8px">可验证奖励的强化学习推动了大语言模型（LLM）微调后的近期进展，尤其是在推理方面。策略优化算法会为给定的提示生成多个响应，并根据奖励对相应的梯度进行加权。目前流行的算法包括GRPO、DAPO和RLOO，它们主要关注具有中等成功概率的模糊提示，而降低非常容易和非常困难提示的梯度权重。本文考虑了非对称提示加权方法，为具有低甚至零经验成功概率的提示分配更高的权重。我们发现，非对称加权在从零开始的强化学习（如R1-Zero）中特别有益，其中训练过程跨越了广泛的准确率范围；而在微调后的强化学习（post-SFT RL）中，由于模型初始准确率较高，其效果不那么显著。我们还提供了理论分析，该理论描述了在固定更新预算下，能够最小化从初始水平提升成功概率到目标准确率所需时间的提示权重。在低成功概率的环境中，由于信息性响应稀少且响应成本占主导地位，这些最优权重变得非对称，通过提升低成功概率来加速有效时间收敛。</div>
</details>
</div>
<div class="card">
<div class="title">The Offline-Frontier Shift: Diagnosing Distributional Limits in Generative Multi-Objective Optimization</div>
<div class="meta-line">Authors: Stephanie Holly, Alexandru-Ciprian Zăvoianu, Siegfried Silber, Sepp Hochreiter, Werner Zellinger</div>
<div class="meta-line">First: 2026-02-11T18:38:40+00:00 · Latest: 2026-02-11T18:38:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11126v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11126v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Offline multi-objective optimization (MOO) aims to recover Pareto-optimal designs given a finite, static dataset. Recent generative approaches, including diffusion models, show strong performance under hypervolume, yet their behavior under other established MOO metrics is less understood. We show that generative methods systematically underperform evolutionary alternatives with respect to other metrics, such as generational distance. We relate this failure mode to the offline-frontier shift, i.e., the displacement of the offline dataset from the Pareto front, which acts as a fundamental limitation in offline MOO. We argue that overcoming this limitation requires out-of-distribution sampling in objective space (via an integral probability metric) and empirically observe that generative methods remain conservatively close to the offline objective distribution. Our results position offline MOO as a distribution-shift--limited problem and provide a diagnostic lens for understanding when and why generative optimization methods fail.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>离线前沿转移：生成多目标优化中的分布限制诊断</div>
<div class="mono" style="margin-top:8px">离线多目标优化（MOO）旨在基于有限的静态数据集恢复帕累托最优设计。近期的生成方法，包括扩散模型，在超体积指标上表现出色，但其在其他已建立的MOO指标下的表现尚不明确。我们证明，生成方法在诸如世代距离等其他指标上系统性地表现不如进化方法。我们将这种失败模式归因于离线前沿转移，即离线数据集偏离帕累托前沿，这成为离线MOO的一个基本限制。我们认为，克服这一限制需要在目标空间中进行分布外采样（通过积分概率度量），并实证观察到生成方法仍然保守地接近离线目标分布。我们的结果将离线MOO定位为一个分布转移受限的问题，并提供了一个理解生成优化方法何时以及为何失败的诊断视角。</div>
</details>
</div>
<div class="card">
<div class="title">Min-Sum Uniform Coverage Problem by Autonomous Mobile Robots</div>
<div class="meta-line">Authors: Animesh Maiti, Abhinav Chakraborty, Bibhuti Das, Subhash Bhagat, Krishnendu Mukhopadhyaya</div>
<div class="meta-line">First: 2026-02-11T18:38:24+00:00 · Latest: 2026-02-11T18:38:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11125v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11125v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the \textit{min-sum uniform coverage} problem for a swarm of $n$ mobile robots on a given finite line segment and on a circle having finite positive radius, where the circle is given as an input. The robots must coordinate their movements to reach a uniformly spaced configuration that minimizes the total distance traveled by all robots. The robots are autonomous, anonymous, identical, and homogeneous, and operate under the \textit{Look-Compute-Move} (LCM) model with \textit{non-rigid} motion controlled by a fair asynchronous scheduler. They are oblivious and silent, possessing neither persistent memory nor a means of explicit communication. In the \textbf{line-segment setting}, the \textit{min-sum uniform coverage} problem requires placing the robots at uniformly spaced points along the segment so as to minimize the total distance traveled by all robots. In the \textbf{circle setting} for this problem, the robots have to arrange themselves uniformly around the given circle to form a regular $n$-gon. There is no fixed orientation or designated starting vertex, and the goal is to minimize the total distance traveled by all the robots. We present a deterministic distributed algorithm that achieves uniform coverage in the line-segment setting with minimum total movement cost. For the circle setting, we characterize all initial configurations for which the \textit{min-sum uniform coverage} problem is deterministically unsolvable under the considered robot model. For all the other remaining configurations, we provide a deterministic distributed algorithm that achieves uniform coverage while minimizing the total distance traveled. These results characterize the deterministic solvability of min-sum coverage for oblivious robots and achieve optimal cost whenever solvable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自主移动机器人在有限线段和圆上的最小总距离均匀覆盖问题</div>
<div class="mono" style="margin-top:8px">我们研究了在给定有限线段和有限正半径圆上，由 $n $ 个移动机器人组成的群体的 \textit{min-sum uniform coverage} 问题。机器人必须协调移动，以达到最小化所有机器人总移动距离的均匀分布配置。这些机器人是自主的、匿名的、相同的且同质的，且在 \textit{Look-Compute-Move} (LCM) 模型下运行，其非刚性运动由公平的异步调度器控制。它们是无记忆的且无声的，既没有持久存储，也没有显式通信的手段。在 \textbf{线段设置} 中，\textit{min-sum uniform coverage} 问题要求将机器人放置在段上的均匀间距点上，以最小化所有机器人的总移动距离。在 \textbf{圆设置} 中，机器人需要围绕给定的圆均匀分布，形成一个正 $n$ 边形。没有固定的朝向或指定的起始顶点，目标是最小化所有机器人的总移动距离。我们提出了一种确定性的分布式算法，用于在有限线段设置中实现最小总移动成本的均匀覆盖。对于圆设置，我们界定了所有使得 \textit{min-sum uniform coverage} 问题在所考虑的机器人模型下无法确定性求解的初始配置。对于所有其他剩余配置，我们提供了一种确定性的分布式算法，以实现均匀覆盖并最小化总移动距离。这些结果界定了无记忆机器人在 min-sum 覆盖问题中的确定性可解性，并在可解时达到最优成本。</div>
</details>
</div>
<div class="card">
<div class="title">PhyCritic: Multimodal Critic Models for Physical AI</div>
<div class="meta-line">Authors: Tianyi Xiong, Shihao Wang, Guilin Liu, Yi Dong, Ming Li, Heng Huang, Jan Kautz, Zhiding Yu</div>
<div class="meta-line">First: 2026-02-11T18:35:39+00:00 · Latest: 2026-02-11T18:35:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11124v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11124v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rapid development of large multimodal models, reliable judge and critic models have become essential for open-ended evaluation and preference alignment, providing pairwise preferences, numerical scores, and explanatory justifications for assessing model-generated responses. However, existing critics are primarily trained in general visual domains such as captioning or image question answering, leaving physical AI tasks involving perception, causal reasoning, and planning largely underexplored. We introduce PhyCritic, a multimodal critic model optimized for physical AI through a two-stage RLVR pipeline: a physical skill warmup stage that enhances physically oriented perception and reasoning, followed by self-referential critic finetuning, where the critic generates its own prediction as an internal reference before judging candidate responses, improving judgment stability and physical correctness. Across both physical and general-purpose multimodal judge benchmarks, PhyCritic achieves strong performance gains over open-source baselines and, when applied as a policy model, further improves perception and reasoning in physically grounded tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PhyCritic：面向物理AI的多模态批评模型</div>
<div class="mono" style="margin-top:8px">随着大规模多模态模型的快速发展，可靠的评判和批评模型对于开放性评估和偏好对齐变得至关重要，能够提供成对偏好、数值评分和解释性理由来评估模型生成的响应。然而，现有的批评模型主要在通用视觉领域（如图像描述和图像问答）进行训练，使得涉及感知、因果推理和规划的物理AI任务仍被广泛忽视。我们引入PhyCritic，这是一种通过两阶段RLVR流程优化用于物理AI的多模态批评模型：首先是一个物理技能预热阶段，以增强面向物理的感知和推理能力；随后是自指式批评微调阶段，其中批评模型在评判候选响应前会生成自身的预测作为内部参考，从而提升评判的稳定性与物理正确性。在物理和通用多模态评判基准测试中，PhyCritic在开源基线模型上表现出显著的性能提升，并且当用作策略模型时，还能进一步提升基于物理任务的感知和推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">With the rapid development of large multimodal models, reliable judge and critic models have become essential for open-ended evaluation and preference alignment, providing pairwise preferences, numerical scores, and explanatory justifications for assessing model-generated responses.</div>
</details>
</div>
<div class="card">
<div class="title">From Natural Language to Materials Discovery:The Materials Knowledge Navigation Agent</div>
<div class="meta-line">Authors: Genmao Zhuang, Amir Barati Farimani</div>
<div class="meta-line">First: 2026-02-11T18:34:24+00:00 · Latest: 2026-02-11T18:34:24+00:00</div>
<div class="meta-line">Comments: 22 pages,5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11123v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11123v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accelerating the discovery of high-performance materials remains a central challenge across energy, electronics, and aerospace technologies, where traditional workflows depend heavily on expert intuition and computationally expensive simulations. Here we introduce the Materials Knowledge Navigation Agent (MKNA), a language-driven system that translates natural-language scientific intent into executable actions for database retrieval, property prediction, structure generation, and stability evaluation. Beyond automating tool invocation, MKNA autonomously extracts quantitative thresholds and chemically meaningful design motifs from literature and database evidence, enabling data-grounded hypothesis formation. Applied to the search for high-Debye-temperature ceramics, the agent identifies a literature-supported screening criterion (Theta_D &gt; 800 K), rediscovers canonical ultra-stiff materials such as diamond, SiC, SiN, and BeO, and proposes thermodynamically stable, previously unreported Be-C-rich compounds that populate the sparsely explored 1500-1700 K regime. These results demonstrate that MKNA not only finds stable candidates but also reconstructs interpretable design heuristics, establishing a generalizable platform for autonomous, language-guided materials exploration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从自然语言到材料发现：材料知识导航代理</div>
<div class="mono" style="margin-top:8px">加速高性能材料的发现仍然是能源、电子和航空航天技术中的核心挑战，传统的工作流程严重依赖专家直觉和计算成本高昂的模拟。本文介绍了材料知识导航代理（MKNA），这是一个语言驱动的系统，能够将自然语言的科学意图转化为数据库检索、性质预测、结构生成和稳定性评估等可执行操作。MKNA不仅自动化工具调用，还能自主从文献和数据库证据中提取定量阈值和具有化学意义的设计模式，从而实现基于数据的假设生成。在寻找高德拜温度陶瓷的应用中，该代理识别出文献支持的筛选标准（Theta_D &gt; 800 K），重新发现如金刚石、SiC、SiN 和 BeO 等经典超刚性材料，并提出之前未报告的富含铍和碳的热力学稳定化合物，这些化合物填充了1500-1700 K这一稀疏探索的温度区间。这些结果表明，MKNA 不仅能找到稳定的候选材料，还能重建可解释的设计启发式方法，建立一个可推广的自主、语言引导的材料探索平台。</div>
</details>
</div>
<div class="card">
<div class="title">A New Dataset and Performance Benchmark for Real-time Spacecraft Segmentation in Onboard Computers</div>
<div class="meta-line">Authors: Jeffrey Joan Sam, Janhavi Sathe, Nikhil Chigali, Naman Gupta, Radhey Ruparel, Yicheng Jiang, Janmajay Singh, James W. Berck, Arko Barman</div>
<div class="meta-line">First: 2025-07-14T20:02:40+00:00 · Latest: 2026-02-11T18:32:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.10775v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.10775v2">PDF</a> · <a href="https://github.com/RiceD2KLab/SWiM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spacecraft deployed in outer space are routinely subjected to various forms of damage due to exposure to hazardous environments. In addition, there are significant risks to the subsequent process of in-space repairs through human extravehicular activity or robotic manipulation, incurring substantial operational costs. Recent developments in image segmentation could enable the development of reliable and cost-effective autonomous inspection systems. While these models often require large amounts of training data to achieve satisfactory results, publicly available annotated spacecraft segmentation data are very scarce. Here, we present a new dataset of nearly 64k annotated spacecraft images that was created using real spacecraft models, superimposed on a mixture of real and synthetic backgrounds generated using NASA&#x27;s TTALOS pipeline. To mimic camera distortions and noise in real-world image acquisition, we also added different types of noise and distortion to the images. Our dataset includes images with several real-world challenges, including noise, camera distortions, glare, varying lighting conditions, varying field of view, partial spacecraft visibility, brightly-lit city backgrounds, densely patterned and confounding backgrounds, aurora borealis, and a wide variety of spacecraft geometries. Finally, we finetuned YOLOv8 and YOLOv11 models for spacecraft segmentation to generate performance benchmarks for the dataset under well-defined hardware and inference time constraints to mimic real-world image segmentation challenges for real-time onboard applications in space on NASA&#x27;s inspector spacecraft. The resulting models, when tested under these constraints, achieved a Dice score of 0.92, Hausdorff distance of 0.69, and an inference time of about 0.5 second. The dataset and models for performance benchmark are available at https://github.com/RiceD2KLab/SWiM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种用于航天器机载计算机实时分割的新数据集和性能基准</div>
<div class="mono" style="margin-top:8px">部署在太空中的航天器由于暴露在危险环境中，经常遭受各种形式的损坏。此外，通过人工出舱活动或机器人操作进行在轨维修的过程也存在重大风险，导致运营成本显著增加。近年来，图像分割技术的发展使得开发可靠且经济高效的自主检测系统成为可能。然而，这些模型通常需要大量训练数据才能获得令人满意的结果，而公开可用的航天器分割标注数据却非常有限。为此，我们提出了一种包含近64,000张航天器标注图像的新数据集，这些图像基于真实航天器模型，并叠加在由NASA的TTALOS管道生成的真实与合成背景混合图像上。为了模拟现实世界图像采集中的相机失真和噪声，我们还向图像中添加了不同类型的噪声和失真。该数据集包含多种现实世界挑战的图像，包括噪声、相机失真、反光、光照条件变化、视场变化、航天器部分可见性、明亮的城市背景、密集且干扰性强的背景、极光以及多种航天器几何形状。最后，我们针对航天器分割对YOLOv8和YOLOv11模型进行了微调，以在明确的硬件和推理时间限制下生成数据集的性能基准，从而模拟现实世界中的图像分割挑战，适用于NASA巡检航天器的实时机载应用。在这些限制条件下测试后，所得到的模型实现了Dice得分为0.92，Hausdorff距离为0.69，推理时间为约0.5秒。该数据集和性能基准模型可在https://github.com/RiceD2KLab/SWiM上获取。</div>
</details>
</div>
<div class="card">
<div class="title">Implicit Hypothesis Testing and Divergence Preservation in Neural Network Representations</div>
<div class="meta-line">Authors: Kadircan Aksoy, Protim Bhattacharjee, Peter Jung</div>
<div class="meta-line">First: 2026-01-28T10:46:44+00:00 · Latest: 2026-02-11T18:32:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20477v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.20477v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the supervised training dynamics of neural classifiers through the lens of binary hypothesis testing. We model classification as a set of binary tests between class-conditional distributions of representations and empirically show that, along training trajectories, well-generalizing networks increasingly align with Neyman-Pearson optimal decision rules via monotonic improvements in KL divergence that relate to error rate exponents. We finally discuss how this yields an explanation and possible training or regularization strategies for different classes of neural networks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>神经网络表示中的隐式假设检验与差异保持</div>
<div class="mono" style="margin-top:8px">我们通过二元假设检验的视角研究神经分类器的监督训练动态。我们将分类建模为表示的类条件分布之间的二元检验，并实证表明，在训练过程中，具有良好泛化能力的网络会通过与KL散度相关的误差率指数的单调改进，越来越符合Neyman-Pearson最优决策规则。最后，我们讨论了这一发现如何为不同类别的神经网络提供解释以及可能的训练或正则化策略。</div>
</details>
</div>
<div class="card">
<div class="title">HairWeaver: Few-Shot Photorealistic Hair Motion Synthesis with Sim-to-Real Guided Video Diffusion</div>
<div class="meta-line">Authors: Di Chang, Ji Hou, Aljaz Bozic, Assaf Neuberger, Felix Juefei-Xu, Olivier Maury, Gene Wei-Chin Lin, Tuur Stuyck, Doug Roble, Mohammad Soleymani, Stephane Grabli</div>
<div class="meta-line">First: 2026-02-11T18:31:47+00:00 · Latest: 2026-02-11T18:31:47+00:00</div>
<div class="meta-line">Comments: Website: https://boese0601.github.io/hairweaver/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.11117v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.11117v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://boese0601.github.io/hairweaver/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present HairWeaver, a diffusion-based pipeline that animates a single human image with realistic and expressive hair dynamics. While existing methods successfully control body pose, they lack specific control over hair, and as a result, fail to capture the intricate hair motions, resulting in stiff and unrealistic animations. HairWeaver overcomes this limitation using two specialized modules: a Motion-Context-LoRA to integrate motion conditions and a Sim2Real-Domain-LoRA to preserve the subject&#x27;s photoreal appearance across different data domains. These lightweight components are designed to guide a video diffusion backbone while maintaining its core generative capabilities. By training on a specialized dataset of dynamic human motion generated from a CG simulator, HairWeaver affords fine control over hair motion and ultimately learns to produce highly realistic hair that responds naturally to movement. Comprehensive evaluations demonstrate that our approach sets a new state of the art, producing lifelike human hair animations with dynamic details.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HairWeaver: 以模拟到现实引导的视频扩散实现少样本逼真头发运动合成</div>
<div class="mono" style="margin-top:8px">我们提出了HairWeaver，这是一个基于扩散的管道，能够以真实且富有表现力的头发动态来动画化单个人类图像。尽管现有方法能够成功控制身体姿态，但它们缺乏对头发的特定控制，因此无法捕捉复杂的头发运动，导致动画僵硬且不真实。HairWeaver通过两个专用模块克服了这一限制：一个Motion-Context-LoRA模块用于整合运动条件，一个Sim2Real-Domain-LoRA模块用于在不同数据域中保持主体的逼真外观。这些轻量级组件旨在引导视频扩散主干网络，同时保持其核心生成能力。通过在由CG模拟器生成的动态人体运动专用数据集上进行训练，HairWeaver实现了对头发运动的精细控制，并最终学习生成高度逼真的头发，自然地响应运动。全面的评估表明，我们的方法达到了新的技术前沿，能够生成具有动态细节的真实人类头发动画。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260212_0425.html">20260212_0425</a>
<a href="archive/20260210_0435.html">20260210_0435</a>
<a href="archive/20260209_0404.html">20260209_0404</a>
<a href="archive/20260208_0353.html">20260208_0353</a>
<a href="archive/20260207_0410.html">20260207_0410</a>
<a href="archive/20260206_0412.html">20260206_0412</a>
<a href="archive/20260205_0417.html">20260205_0417</a>
<a href="archive/20260204_0421.html">20260204_0421</a>
<a href="archive/20260202_0402.html">20260202_0402</a>
<a href="archive/20260201_0354.html">20260201_0354</a>
<a href="archive/20260131_0408.html">20260131_0408</a>
<a href="archive/20260130_0406.html">20260130_0406</a>
<a href="archive/20260129_0407.html">20260129_0407</a>
<a href="archive/20260128_0407.html">20260128_0407</a>
<a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
