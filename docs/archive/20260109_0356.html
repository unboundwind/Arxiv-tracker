<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-09 03:56</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260109_0356</div>
    <div class="row"><div class="card">
<div class="title">Choreographing a World of Dynamic Objects</div>
<div class="meta-line">Authors: Yanzhe Lyu, Chen Geng, Karthik Dharmarajan, Yunzhi Zhang, Hadi Alzayer, Shangzhe Wu, Jiajun Wu</div>
<div class="meta-line">First: 2026-01-07T18:59:40+00:00 · Latest: 2026-01-07T18:59:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04194v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04194v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://yanzhelyu.github.io/chord">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dynamic objects in our physical 4D (3D + time) world are constantly evolving, deforming, and interacting with other objects, leading to diverse 4D scene dynamics. In this paper, we present a universal generative pipeline, CHORD, for CHOReographing Dynamic objects and scenes and synthesizing this type of phenomena. Traditional rule-based graphics pipelines to create these dynamics are based on category-specific heuristics, yet are labor-intensive and not scalable. Recent learning-based methods typically demand large-scale datasets, which may not cover all object categories in interest. Our approach instead inherits the universality from the video generative models by proposing a distillation-based pipeline to extract the rich Lagrangian motion information hidden in the Eulerian representations of 2D videos. Our method is universal, versatile, and category-agnostic. We demonstrate its effectiveness by conducting experiments to generate a diverse range of multi-body 4D dynamics, show its advantage compared to existing methods, and demonstrate its applicability in generating robotics manipulation policies. Project page: https://yanzhelyu.github.io/chord</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>编排动态物体的世界</div>
<div class="mono" style="margin-top:8px">在我们物理上的4D（3D + 时间）世界中，动态物体不断演化、变形并与其它物体相互作用，从而产生多样的4D场景动态。本文提出了一种通用的生成式流程CHORD，用于编排动态物体和场景，并合成此类现象。传统的基于规则的图形流程依赖于特定类别的启发式方法，但劳动密集且难以扩展。近期基于学习的方法通常需要大规模数据集，这可能无法涵盖所有感兴趣的物体类别。我们的方法通过提出一种基于蒸馏的流程，继承了视频生成模型的通用性，以提取隐藏在2D视频欧拉表示中的丰富拉格朗日运动信息。我们的方法具有通用性、多功能性和类别无关性。我们通过实验生成多样化的多体4D动态，展示了其有效性，并与现有方法进行了比较，证明了其优势，同时展示了其在生成机器人操作策略中的应用潜力。项目页面：https://yanzhelyu.github.io/chord</div>
</details>
</div>
<div class="card">
<div class="title">Prediction Intervals for Interim Events in Randomized Clinical Trials with Time-to-Event Endpoints</div>
<div class="meta-line">Authors: Edoardo Ratti, Federico L. Perlino, Stefania Galimberti, Maria G. Valsecchi</div>
<div class="meta-line">First: 2026-01-07T18:58:45+00:00 · Latest: 2026-01-07T18:58:45+00:00</div>
<div class="meta-line">Comments: 35 pages, 18 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04192v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04192v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time-to-event endpoints are central to evaluate treatment efficacy across many disease areas. Many trial protocols include interim analyses within group-sequential designs that control type I error via spending functions or boundary methods. The corresponding operating characteristics depend on the number of looks and the information accrued. Planning interim analyses with time-to-event endpoints is challenging because statistical information depends on the number of observed events. Ensuring adequate follow-up to accrue the required events is therefore critical, making interim prediction of information at scheduled looks and at the final analysis essential. While several methods have been developed to predict the calendar time required to reach a target number of events, to the best of our knowledge there is no established framework that addresses the prediction of the number of events at a future date with corresponding prediction intervals. Starting from an prediction interval approach originally developed in reliability engineering for the number of future component failures, we reformulated and extended it to the context of interim monitoring in clinical trials. This adaptation yields a general framework for event-count prediction intervals in the clinical setting, taking the patient as the unit of analysis and accommodating a range of parametric survival models, patient-level covariates, stagged entry and possible dependence between entry dates and lost to follow-up. Prediction intervals are obtained in a frequentist framework from a bootstrap estimator of the conditional distribution of future events. The performance of the proposed approach is investigated via simulation studies and illustrated by analyzing a real-world phase III trial in childhood acute lymphoblastic leukaemia.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有生存终点的随机临床试验中中间事件的预测区间</div>
<div class="mono" style="margin-top:8px">生存终点在评估多种疾病领域的治疗效果中起着核心作用。许多试验方案包含基于分组序贯设计的中间分析，通过花费函数或边界方法控制I类错误。相应的操作特性依赖于观察次数和累积的信息量。由于统计信息依赖于观察到的事件数量，因此在具有生存终点的试验中规划中间分析具有挑战性。确保足够的随访以累积所需的事件数量因此至关重要，使得在计划的观察点和最终分析时对信息量进行中间预测变得必不可少。尽管已有多种方法用于预测达到目标事件数所需的日历时间，据我们所知，目前尚无已建立的框架用于预测未来某日期的事件数量及其相应的预测区间。我们从可靠性工程中最初为预测未来组件故障数量而开发的预测区间方法出发，对其进行重新表述并扩展，以适应临床试验中的中间监测情境。这种适应性方法在临床环境中提供了一个通用的事件数量预测区间框架，以患者为分析单位，并可容纳多种参数生存模型、患者层面的协变量、分阶段入组以及入组日期与失访之间的潜在依赖关系。预测区间基于频率学派框架，通过未来事件条件分布的自举估计器获得。通过模拟研究评估了所提出方法的性能，并通过分析一个真实世界中的儿童急性淋巴细胞白血病III期试验进行了说明。</div>
</details>
</div>
<div class="card">
<div class="title">Embedding Autonomous Agents in Resource-Constrained Robotic Platforms</div>
<div class="meta-line">Authors: Negar Halakou, Juan F. Gutierrez, Ye Sun, Han Jiang, Xueming Wu, Yilun Song, Andres Gomez</div>
<div class="meta-line">First: 2026-01-07T18:57:32+00:00 · Latest: 2026-01-07T18:57:32+00:00</div>
<div class="meta-line">Comments: This is an open-access, author-archived version of a manuscript published in European Conference on Multi-Agent Systems 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04191v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04191v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many embedded devices operate under resource constraints and in dynamic environments, requiring local decision-making capabilities. Enabling devices to make independent decisions in such environments can improve the responsiveness of the system and reduce the dependence on constant external control. In this work, we integrate an autonomous agent, programmed using AgentSpeak, with a small two-wheeled robot that explores a maze using its own decision-making and sensor data. Experimental results show that the agent successfully solved the maze in 59 seconds using 287 reasoning cycles, with decision phases taking less than one millisecond. These results indicate that the reasoning process is efficient enough for real-time execution on resource-constrained hardware. This integration demonstrates how high-level agent-based control can be applied to resource-constrained embedded systems for autonomous operation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将自主代理嵌入资源受限的机器人平台</div>
<div class="mono" style="margin-top:8px">许多嵌入式设备在资源受限和动态环境中运行，需要具备本地决策能力。在这样的环境中使设备能够独立决策可以提高系统的响应性并减少对外部持续控制的依赖。在本工作中，我们将一个使用AgentSpeak编程的自主代理集成到一台小型双轮机器人上，该机器人通过自身的决策和传感器数据探索迷宫。实验结果表明，代理在59秒内成功解迷，使用了287个推理周期，其中决策阶段耗时不足一毫秒。这些结果表明，推理过程足够高效，可以在资源受限的硬件上实现实时执行。这种集成展示了如何将高级代理控制应用于资源受限的嵌入式系统以实现自主操作。</div>
</details>
</div>
<div class="card">
<div class="title">ImLoc: Revisiting Visual Localization with Image-based Representation</div>
<div class="meta-line">Authors: Xudong Jiang, Fangjinhua Wang, Silvano Galliani, Christoph Vogel, Marc Pollefeys</div>
<div class="meta-line">First: 2026-01-07T18:51:51+00:00 · Latest: 2026-01-07T18:51:51+00:00</div>
<div class="meta-line">Comments: Code will be available at https://github.com/cvg/Hierarchical-Localization</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04185v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04185v1">PDF</a> · <a href="https://github.com/cvg/Hierarchical-Localization">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing visual localization methods are typically either 2D image-based, which are easy to build and maintain but limited in effective geometric reasoning, or 3D structure-based, which achieve high accuracy but require a centralized reconstruction and are difficult to update. In this work, we revisit visual localization with a 2D image-based representation and propose to augment each image with estimated depth maps to capture the geometric structure. Supported by the effective use of dense matchers, this representation is not only easy to build and maintain, but achieves highest accuracy in challenging conditions. With compact compression and a GPU-accelerated LO-RANSAC implementation, the whole pipeline is efficient in both storage and computation and allows for a flexible trade-off between accuracy and highest memory efficiency. Our method achieves a new state-of-the-art accuracy on various standard benchmarks and outperforms existing memory-efficient methods at comparable map sizes. Code will be available at https://github.com/cvg/Hierarchical-Localization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ImLoc：基于图像表示的视觉定位再审视</div>
<div class="mono" style="margin-top:8px">现有的视觉定位方法通常分为两类：一类是基于2D图像的方法，易于构建和维护，但在有效的几何推理方面存在局限；另一类是基于3D结构的方法，虽然精度较高，但需要集中重建且难以更新。在本工作中，我们重新审视基于2D图像表示的视觉定位，并提出通过添加估计的深度图来增强每张图像，以捕捉几何结构。借助密集匹配器的有效使用，这种表示不仅易于构建和维护，而且在具有挑战性的条件下实现了最高的精度。通过紧凑的压缩方式和GPU加速的LO-RANSAC实现，整个流程在存储和计算上都高效，并允许在精度和内存效率之间灵活权衡。我们的方法在多个标准基准测试中实现了新的最先进精度，并在相同地图尺寸下优于现有的内存高效方法。</div>
</details>
</div>
<div class="card">
<div class="title">HONEYBEE: Efficient Role-based Access Control for Vector Databases via Dynamic Partitioning[Technical Report]</div>
<div class="meta-line">Authors: Hongbin Zhong, Matthew Lentz, Nina Narodytska, Adriana Szekeres, Kexin Rong</div>
<div class="meta-line">First: 2025-05-02T18:59:31+00:00 · Latest: 2026-01-07T18:49:03+00:00</div>
<div class="meta-line">Comments: Accepted by SIGMOD 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.01538v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.01538v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Enterprise deployments of vector databases require access control policies to protect sensitive data. These systems often implement access control through hybrid vector queries that combine nearest-neighbor search with relational predicates based on user permissions. However, existing approaches face a fundamental trade-off: dedicated per-user indexes minimize query latency but incur high memory redundancy, while shared indexes with post-search filtering reduce memory overhead at the cost of increased latency. This paper introduces HONEYBEE, a dynamic partitioning framework that leverages the structure of Role-Based Access Control (RBAC) policies to create a smooth trade-off between these extremes. RBAC policies organize users into roles and assign permissions at the role level, creating a natural ``thin waist`` in the permission structure that is ideal for partitioning decisions. Specifically, HONEYBEE produces overlapping partitions where vectors can be strategically replicated across different partitions to reduce query latency while controlling memory overhead. To guide these decisions, HONEYBEE develops analytical models of vector search performance and recall, and formulates partitioning as a constrained optimization problem that balances memory usage, query efficiency, and recall. Evaluations on RBAC workloads demonstrate that HONEYBEE achieves up to 13.5X lower query latency than row-level security with only a 1.24X increase in memory usage, while achieving comparable query performance to dedicated, per-role indexes with 90.4% reduction in additional memory consumption, offering a practical middle ground for secure and efficient vector search.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HONEYBEE：通过动态分区实现向量数据库的高效基于角色的访问控制[技术报告]</div>
<div class="mono" style="margin-top:8px">企业部署向量数据库需要访问控制策略来保护敏感数据。这些系统通常通过结合最近邻搜索与基于用户权限的关系谓词的混合向量查询来实现访问控制。然而，现有方法面临根本性的权衡：为每个用户单独创建索引可以最小化查询延迟，但会带来高内存冗余；而使用共享索引并进行后搜索过滤则能减少内存开销，但会增加延迟。本文提出HONEYBEE，这是一个动态分区框架，利用基于角色的访问控制（RBAC）策略的结构，在这两种极端之间实现平滑的权衡。RBAC策略将用户组织为角色，并在角色级别分配权限，这在权限结构中自然形成了一个“瘦腰”结构，非常适合进行分区决策。具体而言，HONEYBEE生成重叠的分区，将向量策略性地复制到不同的分区中，以减少查询延迟并控制内存开销。为了指导这些决策，HONEYBEE建立了向量搜索性能和召回率的分析模型，并将分区问题表述为一个受约束的优化问题，以平衡内存使用、查询效率和召回率。在RBAC工作负载上的评估表明，HONEYBEE在仅增加1.24倍内存使用的情况下，实现了比行级安全低达13.5倍的查询延迟，同时在额外内存消耗减少90.4%的情况下，其查询性能可与专用的、基于角色的索引相媲美，为安全且高效的向量搜索提供了一个实用的中间方案。</div>
</details>
</div>
<div class="card">
<div class="title">Lightweight Test-Time Adaptation for EMG-Based Gesture Recognition</div>
<div class="meta-line">Authors: Nia Touko, Matthew O A Ellis, Cristiano Capone, Alessio Burrello, Elisa Donati, Luca Manneschi</div>
<div class="meta-line">First: 2026-01-07T18:48:31+00:00 · Latest: 2026-01-07T18:48:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04181v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04181v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reliable long-term decoding of surface electromyography (EMG) is hindered by signal drift caused by electrode shifts, muscle fatigue, and posture changes. While state-of-the-art models achieve high intra-session accuracy, their performance often degrades sharply. Existing solutions typically demand large datasets or high-compute pipelines that are impractical for energy-efficient wearables. We propose a lightweight framework for Test-Time Adaptation (TTA) using a Temporal Convolutional Network (TCN) backbone. We introduce three deployment-ready strategies: (i) causal adaptive batch normalization for real-time statistical alignment; (ii) a Gaussian Mixture Model (GMM) alignment with experience replay to prevent forgetting; and (iii) meta-learning for rapid, few-shot calibration. Evaluated on the NinaPro DB6 multi-session dataset, our framework significantly bridges the inter-session accuracy gap with minimal overhead. Our results show that experience-replay updates yield superior stability under limited data, while meta-learning achieves competitive performance in one- and two-shot regimes using only a fraction of the data required by current benchmarks. This work establishes a path toward robust, &quot;plug-and-play&quot; myoelectric control for long-term prosthetic use.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于表面肌电信号的轻量级测试时自适应手势识别</div>
<div class="mono" style="margin-top:8px">由于电极位移、肌肉疲劳和姿势变化导致的信号漂移，使得表面肌电信号（EMG）的长期可靠解码受到阻碍。尽管最先进的模型在会话内能实现高准确率，但其性能在会话间往往急剧下降。现有解决方案通常需要大量数据集或高计算量的流程，这对于节能型可穿戴设备来说并不实际。我们提出了一种轻量级的测试时自适应（TTA）框架，采用时间卷积网络（TCN）作为主干。我们引入了三种适用于部署的策略：(i) 因果自适应批量归一化以实现实时统计对齐；(ii) 带经验回放的高斯混合模型（GMM）对齐以防止遗忘；以及 (iii) 元学习用于快速、少样本校准。在NinaPro DB6多会话数据集上的评估表明，我们的框架在最小开销下显著弥合了会话间准确率差距。实验结果表明，在有限数据情况下，经验回放更新提供了更优的稳定性，而元学习仅使用当前基准所需数据量的一小部分，即可在单样本和双样本模式下实现具有竞争力的性能。本工作为长期假肢使用提供了稳健的、即插即用的肌电信号控制路径。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Physics Discovery from Highly Corrupted Data: A PINN Framework Applied to the Nonlinear Schrödinger Equation</div>
<div class="meta-line">Authors: Pietro de Oliveira Esteves</div>
<div class="meta-line">First: 2026-01-07T18:43:11+00:00 · Latest: 2026-01-07T18:43:11+00:00</div>
<div class="meta-line">Comments: 9 pages, 4 figures, 2 tables. Code available at https://github.com/p-esteves/pinn-nlse-2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04176v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04176v1">PDF</a> · <a href="https://github.com/p-esteves/pinn-nlse-2026">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We demonstrate a deep learning framework capable of recovering physical parameters from the Nonlinear Schrodinger Equation (NLSE) under severe noise conditions. By integrating Physics-Informed Neural Networks (PINNs) with automatic differentiation, we achieve reconstruction of the nonlinear coefficient beta with less than 0.2 percent relative error using only 500 sparse, randomly sampled data points corrupted by 20 percent additive Gaussian noise, a regime where traditional finite difference methods typically fail due to noise amplification in numerical derivatives. We validate the method&#x27;s generalization capabilities across different physical regimes (beta between 0.5 and 2.0) and varying data availability (between 100 and 1000 training points), demonstrating consistent sub-1 percent accuracy. Statistical analysis over multiple independent runs confirms robustness (standard deviation less than 0.15 percent for beta equals 1.0). The complete pipeline executes in approximately 80 minutes on modest cloud GPU resources (NVIDIA Tesla T4), making the approach accessible for widespread adoption. Our results indicate that physics-based regularization acts as an effective filter against high measurement uncertainty, positioning PINNs as a viable alternative to traditional optimization methods for inverse problems in spatiotemporal dynamics where experimental data is scarce and noisy. All code is made publicly available to facilitate reproducibility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从高度损坏数据中稳健地发现物理规律：非线性薛定谔方程的PINN框架应用</div>
<div class="mono" style="margin-top:8px">我们展示了一种深度学习框架，能够在严重噪声条件下从非线性薛定谔方程（NLSE）中恢复物理参数。通过将物理信息神经网络（PINNs）与自动微分相结合，我们仅使用500个稀疏、随机采样的数据点，这些数据点受到20%的加性高斯噪声污染，成功重建了非线性系数beta，相对误差低于0.2%。在传统有限差分方法因数值导数中的噪声放大而通常失效的条件下，这种方法表现出色。我们验证了该方法在不同物理条件（beta在0.5到2.0之间）和不同数据可用性（训练点数在100到1000之间）下的泛化能力，展示了其在亚1%精度范围内的稳定性。通过对多个独立运行的统计分析，确认了该方法的鲁棒性（当beta等于1.0时，标准差低于0.15%）。整个流程在普通的云GPU资源（NVIDIA Tesla T4）上大约需要80分钟，使得该方法易于广泛采用。我们的结果表明，基于物理的正则化在高测量不确定性条件下是一种有效的过滤手段，使PINNs成为解决时空动力学逆问题的可行替代方案，特别是在实验数据稀缺且嘈杂的情况下。所有代码均已公开，以促进可重复性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We demonstrate a deep learning framework capable of recovering physical parameters from the Nonlinear Schrodinger Equation (NLSE) under severe noise conditions.</div>
</details>
</div>
<div class="card">
<div class="title">Agentic Rubrics as Contextual Verifiers for SWE Agents</div>
<div class="meta-line">Authors: Mohit Raghavendra, Anisha Gunjal, Bing Liu, Yunzhong He</div>
<div class="meta-line">First: 2026-01-07T18:38:23+00:00 · Latest: 2026-01-07T18:38:23+00:00</div>
<div class="meta-line">Comments: 31 pages, 11 Figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04171v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04171v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Verification is critical for improving agents: it provides the reward signal for Reinforcement Learning and enables inference-time gains through Test-Time Scaling (TTS). Despite its importance, verification in software engineering (SWE) agent settings often relies on code execution, which can be difficult to scale due to environment setup overhead. Scalable alternatives such as patch classifiers and heuristic methods exist, but they are less grounded in codebase context and harder to interpret. To this end, we explore Agentic Rubrics: an expert agent interacts with the repository to create a context-grounded rubric checklist, and candidate patches are then scored against it without requiring test execution. On SWE-Bench Verified under parallel TTS evaluation, Agentic Rubrics achieve a score of 54.2% on Qwen3-Coder-30B-A3B and 40.6% on Qwen3-32B, with at least a +3.5 percentage-point gain over the strongest baseline in our comparison set. We further analyze rubric behavior, showing that rubric scores are consistent with ground-truth tests while also flagging issues that tests do not capture. Our ablations show that agentic context gathering is essential for producing codebase-specific, unambiguous criteria. Together, these results suggest that Agentic Rubrics provide an efficient, scalable, and granular verification signal for SWE agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>作为SWE代理情境验证器的代理评分标准</div>
<div class="mono" style="margin-top:8px">验证对于提升代理至关重要：它为强化学习提供奖励信号，并通过测试时扩展（TTS）实现推理时的性能提升。尽管其重要性不言而喻，但在软件工程（SWE）代理环境中，验证通常依赖于代码执行，这由于环境设置的开销而难以扩展。存在可扩展的替代方案，如补丁分类器和启发式方法，但它们在代码库情境上的关联性较低且难以解释。为此，我们探索了代理评分标准：专家代理与仓库交互以创建基于情境的评分标准清单，候选补丁随后根据该清单评分，而无需执行测试。在SWE-Bench Verified下的并行TTS评估中，代理评分标准在Qwen3-Coder-30B-A3B上达到54.2%的得分，在Qwen3-32B上达到40.6%，相较于我们比较集中的最强基线至少提升了3.5个百分点。我们进一步分析了评分标准的行为，表明评分结果与真实测试结果一致，同时还能检测出测试无法捕捉的问题。我们的消融实验表明，代理情境收集对于生成特定于代码库且明确的评分标准至关重要。综上所述，这些结果表明代理评分标准为SWE代理提供了一种高效、可扩展且细致的验证信号。</div>
</details>
</div>
<div class="card">
<div class="title">Agent Drift: Quantifying Behavioral Degradation in Multi-Agent LLM Systems Over Extended Interactions</div>
<div class="meta-line">Authors: Abhishek Rath</div>
<div class="meta-line">First: 2026-01-07T18:37:26+00:00 · Latest: 2026-01-07T18:37:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04170v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04170v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent Large Language Model (LLM) systems have emerged as powerful architectures for complex task decomposition and collaborative problem-solving. However, their long-term behavioral stability remains largely unexamined. This study introduces the concept of agent drift, defined as the progressive degradation of agent behavior, decision quality, and inter-agent coherence over extended interaction sequences. We present a comprehensive theoretical framework for understanding drift phenomena, proposing three distinct manifestations: semantic drift (progressive deviation from original intent), coordination drift (breakdown in multi-agent consensus mechanisms), and behavioral drift (emergence of unintended strategies).
  We introduce the Agent Stability Index (ASI), a novel composite metric framework for quantifying drift across twelve dimensions, including response consistency, tool usage patterns, reasoning pathway stability, and inter-agent agreement rates. Through simulation-based analysis and theoretical modeling, we demonstrate how unchecked agent drift can lead to substantial reductions in task completion accuracy and increased human intervention requirements.
  We propose three mitigation strategies: episodic memory consolidation, drift-aware routing protocols, and adaptive behavioral anchoring. Theoretical analysis suggests these approaches can significantly reduce drift-related errors while maintaining system throughput. This work establishes a foundational methodology for monitoring, measuring, and mitigating agent drift in production agentic AI systems, with direct implications for enterprise deployment reliability and AI safety research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>智能体漂移：多智能体大语言模型系统在长期交互中的行为退化量化</div>
<div class="mono" style="margin-top:8px">多智能体大语言模型（LLM）系统已成为复杂任务分解和协作问题解决的强大架构。然而，其长期行为稳定性仍缺乏深入研究。本研究引入了智能体漂移的概念，定义为在长期交互序列中，智能体行为、决策质量及智能体间一致性逐步退化。我们提出了一个全面的理论框架来理解漂移现象，提出了三种不同的表现形式：语义漂移（逐步偏离原始意图）、协调漂移（多智能体共识机制的失效）和行为漂移（非预期策略的出现）。
我们提出了智能体稳定性指数（ASI），这是一种新型的综合度量框架，用于在包括响应一致性、工具使用模式、推理路径稳定性及智能体间一致率在内的十二个维度上量化漂移。通过基于仿真的分析和理论建模，我们展示了未经控制的智能体漂移如何导致任务完成准确率显著下降，并增加对人工干预的需求。
我们提出了三种缓解策略：周期性记忆巩固、漂移感知的路由协议以及自适应行为锚定。理论分析表明，这些方法可以显著减少与漂移相关的错误，同时保持系统吞吐量。本工作为监控、测量和缓解生产环境中的智能体漂移建立了基础方法论，对企业部署的可靠性及人工智能安全研究具有直接意义。</div>
</details>
</div>
<div class="card">
<div class="title">FedDUAL: A Dual-Strategy with Adaptive Loss and Dynamic Aggregation for Mitigating Data Heterogeneity in Federated Learning</div>
<div class="meta-line">Authors: Pranab Sahoo, Ashutosh Tripathi, Sriparna Saha, Samrat Mondal</div>
<div class="meta-line">First: 2024-12-05T18:42:29+00:00 · Latest: 2026-01-07T18:33:05+00:00</div>
<div class="meta-line">Comments: Transactions on Machine Learning Research (TMLR)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.04416v2">Abs</a> · <a href="https://arxiv.org/pdf/2412.04416v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated Learning (FL) marks a transformative approach to distributed model training by combining locally optimized models from various clients into a unified global model. While FL preserves data privacy by eliminating centralized storage, it encounters significant challenges such as performance degradation, slower convergence, and reduced robustness of the global model due to the heterogeneity in client data distributions. Among the various forms of data heterogeneity, label skew emerges as a particularly formidable and prevalent issue, especially in domains such as image classification. To address these challenges, we begin with comprehensive experiments to pinpoint the underlying issues in the FL training process. Based on our findings, we then introduce an innovative dual-strategy approach designed to effectively resolve these issues. First, we introduce an adaptive loss function for client-side training, meticulously crafted to preserve previously acquired knowledge while maintaining an optimal equilibrium between local optimization and global model coherence. Secondly, we develop a dynamic aggregation strategy for aggregating client models at the server. This approach adapts to each client&#x27;s unique learning patterns, effectively addressing the challenges of diverse data across the network. Our comprehensive evaluation, conducted across three diverse real-world datasets, coupled with theoretical convergence guarantees, demonstrates the superior efficacy of our method compared to several established state-of-the-art approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FedDUAL: 一种用于缓解联邦学习中数据异质性的双策略方法，具有自适应损失函数和动态聚合</div>
<div class="mono" style="margin-top:8px">联邦学习（FL）通过将来自不同客户端的局部优化模型结合为一个统一的全局模型，标志着分布式模型训练的一种变革性方法。尽管FL通过消除集中存储来保护数据隐私，但由于客户端数据分布的异质性，它仍面临诸如性能下降、收敛速度变慢和全局模型鲁棒性降低等重大挑战。在各种数据异质性形式中，标签偏移（label skew）尤为突出且普遍，尤其是在图像分类等领域。为了解决这些问题，我们首先通过全面的实验来识别FL训练过程中存在的根本问题。基于我们的发现，我们提出了一种创新的双策略方法，旨在有效解决这些问题。首先，我们引入了一种自适应损失函数用于客户端训练，该函数精心设计，旨在在保持已有知识的同时，在局部优化和全局模型一致性之间维持最佳平衡。其次，我们开发了一种动态聚合策略用于服务器端聚合客户端模型。这种方法能够适应每个客户端独特的学习模式，从而有效应对网络中多样化的数据挑战。我们在三个不同的现实世界数据集上进行了全面评估，并结合理论收敛保证，证明了我们的方法相较于几种现有的先进方法具有更优的效果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Federated Learning (FL) marks a transformative approach to distributed model training by combining locally optimized models from various clients into a unified global model.</div>
</details>
</div>
<div class="card">
<div class="title">Clinical Data Goes MEDS? Let&#x27;s OWL make sense of it</div>
<div class="meta-line">Authors: Alberto Marfoglia, Jong Ho Jhee, Adrien Coulet</div>
<div class="meta-line">First: 2026-01-07T18:25:02+00:00 · Latest: 2026-01-07T18:25:02+00:00</div>
<div class="meta-line">Comments: 12 pages, 5 tables, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04164v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04164v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The application of machine learning on healthcare data is often hindered by the lack of standardized and semantically explicit representation, leading to limited interoperability and reproducibility across datasets and experiments. The Medical Event Data Standard (MEDS) addresses these issues by introducing a minimal, event-centric data model designed for reproducible machine-learning workflows from health data. However, MEDS is defined as a data-format specification and does not natively provide integration with the Semantic Web ecosystem. In this article, we introduce MEDS-OWL, a lightweight OWL ontology that provides formal concepts and relations to enable representing MEDS datasets as RDF graphs. Additionally, we implemented meds2rdf, a Python conversion library that transforms MEDS events into RDF graphs, ensuring conformance with the ontology. We demonstrate the approach on a synthetic clinical dataset that describes patient care pathways for ruptured intracranial aneurysms and validate the resulting graph using SHACL constraints. The first release of MEDS-OWL comprises 13 classes, 10 object properties, 20 data properties, and 24 OWL axioms. Combined with meds2rdf, it enables data transformation into FAIR-aligned datasets, provenance-aware publishing, and interoperability of event-based clinical data. By bridging MEDS with the Semantic Web, this work contributes a reusable semantic layer for event-based clinical data and establishes a robust foundation for subsequent graph-based analytics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>临床数据是否需要MEDS？让我们用OWL来理解它</div>
<div class="mono" style="margin-top:8px">机器学习在医疗数据上的应用常常受到缺乏标准化和语义明确表示的阻碍，导致数据集和实验之间的互操作性和可重复性受限。医疗事件数据标准（MEDS）通过引入一个最小化、以事件为中心的数据模型来解决这些问题，该模型旨在从健康数据中实现可重复的机器学习工作流。然而，MEDS被定义为一种数据格式规范，不原生支持与语义网生态系统的集成。本文介绍了MEDS-OWL，这是一种轻量级的OWL本体，提供了形式化的概念和关系，以实现将MEDS数据集表示为RDF图。此外，我们实现了meds2rdf，一个Python转换库，将MEDS事件转换为RDF图，确保符合该本体。我们在一个合成的临床数据集上展示了该方法，该数据集描述了颅内动脉瘤破裂患者的护理路径，并使用SHACL约束验证了生成的图。MEDS-OWL的第一个版本包含13个类、10个对象属性、20个数据属性和24个OWL公理。结合meds2rdf，它能够将数据转换为符合FAIR原则的数据集，实现来源感知的发布，并促进基于事件的临床数据的互操作性。通过将MEDS与语义网连接起来，这项工作为基于事件的临床数据提供了一个可重用的语义层，并为后续的图分析建立了坚实的基础。</div>
</details>
</div>
<div class="card">
<div class="title">Scanner-Induced Domain Shifts Undermine the Robustness of Pathology Foundation Models</div>
<div class="meta-line">Authors: Erik Thiringer, Fredrik K. Gustafsson, Kajsa Ledesma Eriksson, Mattias Rantalainen</div>
<div class="meta-line">First: 2026-01-07T18:24:12+00:00 · Latest: 2026-01-07T18:24:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04163v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04163v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pathology foundation models (PFMs) have become central to computational pathology, aiming to offer general encoders for feature extraction from whole-slide images (WSIs). Despite strong benchmark performance, PFM robustness to real-world technical domain shifts, such as variability from whole-slide scanner devices, remains poorly understood. We systematically evaluated the robustness of 14 PFMs to scanner-induced variability, including state-of-the-art models, earlier self-supervised models, and a baseline trained on natural images. Using a multiscanner dataset of 384 breast cancer WSIs scanned on five devices, we isolated scanner effects independently from biological and laboratory confounders. Robustness is assessed via complementary unsupervised embedding analyses and a set of clinicopathological supervised prediction tasks. Our results demonstrate that current PFMs are not invariant to scanner-induced domain shifts. Most models encode pronounced scanner-specific variability in their embedding spaces. While AUC often remains stable, this masks a critical failure mode: scanner variability systematically alters the embedding space and impacts calibration of downstream model predictions, resulting in scanner-dependent bias that can impact reliability in clinical use cases. We further show that robustness is not a simple function of training data scale, model size, or model recency. None of the models provided reliable robustness against scanner-induced variability. While the models trained on the most diverse data, here represented by vision-language models, appear to have an advantage with respect to robustness, they underperformed on downstream supervised tasks. We conclude that development and evaluation of PFMs requires moving beyond accuracy-centric benchmarks toward explicit evaluation and optimisation of embedding stability and calibration under realistic acquisition variability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扫描仪引起的领域偏移削弱了病理学基础模型的鲁棒性</div>
<div class="mono" style="margin-top:8px">病理学基础模型（PFMs）已成为计算病理学的核心，旨在为全切片图像（WSIs）的特征提取提供通用编码器。尽管在基准测试中表现强劲，但PFMs对现实世界中由扫描设备引起的领域偏移的鲁棒性仍缺乏深入理解。我们系统评估了14个PFMs对扫描仪引起的变异的鲁棒性，包括最先进的模型、早期的自监督模型以及在自然图像上训练的基线模型。我们使用包含384个乳腺癌WSIs的多扫描仪数据集，这些图像由五种不同的设备扫描，从而将扫描仪效应独立于生物和实验室混杂因素。鲁棒性通过互补的无监督嵌入分析和一组临床病理学监督预测任务进行评估。我们的结果表明，当前的PFMs并不对扫描仪引起的领域偏移具有不变性。大多数模型在其嵌入空间中编码了显著的扫描仪特异性变异。虽然AUC通常保持稳定，但这掩盖了关键的失效模式：扫描仪的变异系统性地改变了嵌入空间，并影响了下游模型预测的校准，导致扫描仪依赖性的偏差，这可能影响临床使用场景的可靠性。我们进一步表明，鲁棒性并非仅由训练数据规模、模型大小或模型更新时间简单决定。没有任何模型能提供对扫描仪引起的变异的可靠鲁棒性。虽然在最多样化数据上训练的模型（此处由视觉-语言模型表示）似乎在鲁棒性方面具有优势，但它们在下游监督任务中表现不佳。我们得出结论，PFMs的开发和评估需要超越以准确率为中心的基准测试，转向在现实获取变异条件下对嵌入稳定性和校准的显式评估与优化。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring Iterative Controllable Summarization with Large Language Models</div>
<div class="meta-line">Authors: Sangwon Ryu, Heejin Do, Daehee Kim, Hwanjo Yu, Dongwoo Kim, Yunsu Kim, Gary Geunbae Lee, Jungseul Ok</div>
<div class="meta-line">First: 2024-11-19T12:36:02+00:00 · Latest: 2026-01-07T18:22:44+00:00</div>
<div class="meta-line">Comments: EACL Findings 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.12460v3">Abs</a> · <a href="https://arxiv.org/pdf/2411.12460v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have demonstrated remarkable performance in abstractive summarization tasks. However, their ability to precisely control summary attributes (e.g., length or topic) remains underexplored, limiting their adaptability to specific user preferences. In this paper, we systematically explore the controllability of LLMs. To this end, we revisit summary attribute measurements and introduce iterative evaluation metrics, failure rate and average iteration count to precisely evaluate controllability of LLMs, rather than merely assessing errors. Our findings show that LLMs struggle more with numerical attributes than with linguistic attributes. To address this challenge, we propose a guide-to-explain framework (GTE) for controllable summarization. Our GTE framework enables the model to identify misaligned attributes in the initial draft and guides it in self-explaining errors in the previous output. By allowing the model to reflect on its misalignment, GTE generates well-adjusted summaries that satisfy the desired attributes with robust effectiveness, requiring surprisingly fewer iterations than other iterative approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的迭代可控摘要研究</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在抽象摘要任务中表现出色。然而，其对摘要属性（如长度或主题）的精确控制能力仍待深入探索，限制了其适应特定用户偏好的能力。本文系统地探讨了LLMs的可控性。为此，我们重新审视摘要属性的度量方法，并引入迭代评估指标，包括失败率和平均迭代次数，以更精确地评估LLMs的可控性，而不仅仅是评估错误。我们的研究发现，LLMs在数值属性上的控制能力不如在语言属性上。为解决这一挑战，我们提出了一种引导-解释框架（GTE）用于可控摘要。我们的GTE框架使模型能够在初稿中识别不匹配的属性，并引导其自我解释前一步输出中的错误。通过允许模型反思其不匹配之处，GTE生成的摘要能够满足所需属性，且效果稳健，所需的迭代次数比其他迭代方法少得多。</div>
</details>
</div>
<div class="card">
<div class="title">Attention Needs to Focus: A Unified Perspective on Attention Allocation</div>
<div class="meta-line">Authors: Zichuan Fu, Wentao Song, Guojing Li, Yejing Wang, Xian Wu, Yimin Deng, Hanyu Yan, Yefeng Zheng, Xiangyu Zhao</div>
<div class="meta-line">First: 2026-01-01T08:39:15+00:00 · Latest: 2026-01-07T18:20:49+00:00</div>
<div class="meta-line">Comments: preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00919v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.00919v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Transformer architecture, a cornerstone of modern Large Language Models (LLMs), has achieved extraordinary success in sequence modeling, primarily due to its attention mechanism. However, despite its power, the standard attention mechanism is plagued by well-documented issues: representational collapse and attention sink. Although prior work has proposed approaches for these issues, they are often studied in isolation, obscuring their deeper connection. In this paper, we present a unified perspective, arguing that both can be traced to a common root -- improper attention allocation. We identify two failure modes: 1) Attention Overload, where tokens receive comparable high weights, blurring semantic features that lead to representational collapse; 2) Attention Underload, where no token is semantically relevant, yet attention is still forced to distribute, resulting in spurious focus such as attention sink. Building on this insight, we introduce Lazy Attention, a novel mechanism designed for a more focused attention distribution. To mitigate overload, it employs positional discrimination across both heads and dimensions to sharpen token distinctions. To counteract underload, it incorporates Elastic-Softmax, a modified normalization function that relaxes the standard softmax constraint to suppress attention on irrelevant tokens. Experiments on the FineWeb-Edu corpus, evaluated across nine diverse benchmarks, demonstrate that Lazy Attention successfully mitigates attention sink and achieves competitive performance compared to both standard attention and modern architectures, while reaching up to 59.58% attention sparsity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>注意力需要聚焦：注意力分配的统一视角</div>
<div class="mono" style="margin-top:8px">Transformer架构作为现代大型语言模型（LLMs）的核心，因其注意力机制在序列建模中取得了巨大成功。然而，尽管其强大，标准注意力机制仍存在已记录的问题：表征崩溃和注意力沉降。虽然已有研究提出了应对这些问题的方法，但这些方法往往被孤立研究，掩盖了它们之间的深层联系。本文提出一个统一的视角，认为这两种问题都可以追溯到一个共同的根源——注意力分配不当。我们识别出两种失败模式：1）注意力过载，其中某些标记获得相似高的权重，模糊了导致表征崩溃的语义特征；2）注意力不足，其中没有标记具有语义相关性，但注意力仍被迫分配，导致虚假聚焦，如注意力沉降。基于这一洞察，我们引入了Lazy Attention，这是一种旨在实现更聚焦注意力分配的新机制。为缓解过载问题，它在头部和维度上采用位置区分技术以增强标记区分度；为应对不足问题，它结合了Elastic-Softmax，一种修改后的归一化函数，放松标准softmax的约束以抑制对无关标记的注意力。我们在FineWeb-Edu语料库上进行的实验，经过九个多样化的基准测试，表明Lazy Attention有效缓解了注意力沉降问题，并在与标准注意力和现代架构的对比中取得了具有竞争力的性能，同时实现了高达59.58%的注意力稀疏性。</div>
</details>
</div>
<div class="card">
<div class="title">All That Glisters Is Not Gold: A Benchmark for Reference-Free Counterfactual Financial Misinformation Detection</div>
<div class="meta-line">Authors: Yuechen Jiang, Zhiwei Liu, Yupeng Cao, Yueru He, Ziyang Xu, Chen Xu, Zhiyang Deng, Prayag Tiwari, Xi Chen, Alejandro Lopez-Lira, Jimin Huang, Junichi Tsujii, Sophia Ananiadou</div>
<div class="meta-line">First: 2026-01-07T18:18:28+00:00 · Latest: 2026-01-07T18:18:28+00:00</div>
<div class="meta-line">Comments: 39 pages; 24 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04160v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04160v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce RFC Bench, a benchmark for evaluating large language models on financial misinformation under realistic news. RFC Bench operates at the paragraph level and captures the contextual complexity of financial news where meaning emerges from dispersed cues. The benchmark defines two complementary tasks: reference free misinformation detection and comparison based diagnosis using paired original perturbed inputs. Experiments reveal a consistent pattern: performance is substantially stronger when comparative context is available, while reference free settings expose significant weaknesses, including unstable predictions and elevated invalid outputs. These results indicate that current models struggle to maintain coherent belief states without external grounding. By highlighting this gap, RFC Bench provides a structured testbed for studying reference free reasoning and advancing more reliable financial misinformation detection in real world settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>All That Glisters Is Not Gold: 一种用于参考无关金融虚假信息检测的基准</div>
<div class="mono" style="margin-top:8px">我们引入RFC Bench，这是一个用于在现实新闻环境下评估大型语言模型在金融虚假信息检测方面的基准。RFC Bench在段落层面运行，捕捉金融新闻中意义从分散线索中浮现的上下文复杂性。该基准定义了两项互补任务：参考无关的虚假信息检测和基于比较的诊断，使用成对的原始扰动输入。实验揭示了一个一致的模式：当有比较上下文时，模型表现显著更强，而在参考无关的设置下，模型暴露出重大缺陷，包括预测不稳定和无效输出增加。这些结果表明，当前模型在缺乏外部锚定的情况下难以维持连贯的信念状态。通过突出这一差距，RFC Bench为研究参考无关推理提供了结构化的测试平台，并推动了在现实场景中更可靠金融虚假信息检测的发展。</div>
</details>
</div>
<div class="card">
<div class="title">Non-markovian reservoir profile effects on dynamics of light within a single waveguide</div>
<div class="meta-line">Authors: J. R. Silva, C. Antunis B. S. Santos</div>
<div class="meta-line">First: 2025-12-18T15:25:44+00:00 · Latest: 2026-01-07T18:17:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16657v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.16657v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we investigate how different reservoir memory profiles influence the dynamical evolution of a single waveguide coupled to an external environment. We compare three representative memory kernels: Lorentzian, Gaussian and Uniform, highlighting their distinct spatial correlations and their impact on system behavior. We compute the transmission amplitude, transparency properties, as well as long-time behavior of the system under each memory model. To quantify deviations from Markovian dynamics, we employ a non-Markovianity measure based on information backflow, allowing a direct comparison between the structured reservoirs and the Markovian limit. Our results reveal clear signatures of memoryless-induced modifications in the transmission spectrum and demonstrate how specific reservoir profiles enhance or suppress non-Markovian effects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非马尔可夫环境对单波导中光动力学的影响</div>
<div class="mono" style="margin-top:8px">本工作中，我们研究了不同环境记忆谱对单波导与外部环境耦合系统动力学演化的影响。我们比较了三种代表性记忆核：洛伦兹型、高斯型和均匀型，突出了它们不同的空间相关性及其对系统行为的影响。我们计算了在每种记忆模型下系统的传输幅值、透明特性以及长期行为。为了量化与马尔可夫动力学的偏差，我们采用基于信息回流的非马尔可夫性度量方法，从而能够直接比较结构化环境与马尔可夫极限之间的差异。我们的结果揭示了记忆诱导修改在传输谱中的明显特征，并展示了特定环境谱如何增强或抑制非马尔可夫效应。</div>
</details>
</div>
<div class="card">
<div class="title">Does Memory Need Graphs? A Unified Framework and Empirical Analysis for Long-Term Dialog Memory</div>
<div class="meta-line">Authors: Sen Hu, Yuxiang Wei, Jiaxin Ran, Zhiyuan Yao, Xueran Han, Huacan Wang, Ronghao Chen, Lei Zou</div>
<div class="meta-line">First: 2026-01-03T20:39:39+00:00 · Latest: 2026-01-07T18:16:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01280v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.01280v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph structures are increasingly used in dialog memory systems, but empirical findings on their effectiveness remain inconsistent, making it unclear which design choices truly matter. We present an experimental, system-oriented analysis of long-term dialog memory architectures. We introduce a unified framework that decomposes dialog memory systems into core components and supports both graph-based and non-graph approaches. Under this framework, we conduct controlled, stage-wise experiments on LongMemEval and HaluMem, comparing common design choices in memory representation, organization, maintenance, and retrieval. Our results show that many performance differences are driven by foundational system settings rather than specific architectural innovations. Based on these findings, we identify stable and reliable strong baselines for future dialog memory research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对话记忆是否需要图结构？一种统一框架与实证分析</div>
<div class="mono" style="margin-top:8px">图结构在对话记忆系统中被越来越多地使用，但其有效性方面的实证研究结果仍不一致，使得难以确定哪些设计选择真正重要。我们提出了一种面向系统的实验分析方法，用于研究长期对话记忆架构。我们引入了一个统一框架，将对话记忆系统分解为核心组件，并支持基于图和非图的方法。在此框架下，我们在LongMemEval和HaluMem上进行了受控的分阶段实验，比较了记忆表示、组织、维护和检索中常见的设计选择。我们的结果表明，许多性能差异是由基础系统设置驱动的，而非特定的架构创新。基于这些发现，我们为未来对话记忆研究确定了稳定且可靠的强大基线。</div>
</details>
</div>
<div class="card">
<div class="title">ToTMNet: FFT-Accelerated Toeplitz Temporal Mixing Network for Lightweight Remote Photoplethysmography</div>
<div class="meta-line">Authors: Vladimir Frants, Sos Agaian, Karen Panetta</div>
<div class="meta-line">First: 2026-01-07T18:15:09+00:00 · Latest: 2026-01-07T18:15:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04159v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04159v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Remote photoplethysmography (rPPG) estimates a blood volume pulse (BVP) waveform from facial videos captured by commodity cameras. Although recent deep models improve robustness compared to classical signal-processing approaches, many methods increase computational cost and parameter count, and attention-based temporal modeling introduces quadratic scaling with respect to the temporal length. This paper proposes ToTMNet, a lightweight rPPG architecture that replaces temporal attention with an FFT-accelerated Toeplitz temporal mixing layer. The Toeplitz operator provides full-sequence temporal receptive field using a linear number of parameters in the clip length and can be applied in near-linear time using circulant embedding and FFT-based convolution. ToTMNet integrates the global Toeplitz temporal operator into a compact gated temporal mixer that combines a local depthwise temporal convolution branch with gated global Toeplitz mixing, enabling efficient long-range temporal filtering while only having 63k parameters. Experiments on two datasets, UBFC-rPPG (real videos) and SCAMPS (synthetic videos), show that ToTMNet achieves strong heart-rate estimation accuracy with a compact design. On UBFC-rPPG intra-dataset evaluation, ToTMNet reaches 1.055 bpm MAE with Pearson correlation 0.996. In a synthetic-to-real setting (SCAMPS to UBFC-rPPG), ToTMNet reaches 1.582 bpm MAE with Pearson correlation 0.994. Ablation results confirm that the gating mechanism is important for effectively using global Toeplitz mixing, especially under domain shift. The main limitation of this preprint study is the use of only two datasets; nevertheless, the results indicate that Toeplitz-structured temporal mixing is a practical and efficient alternative to attention for rPPG.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ToTMNet：基于FFT加速的托普利兹时序混合网络用于轻量级远程光电容积描记法</div>
<div class="mono" style="margin-top:8px">远程光电容积描记法（rPPG）通过普通摄像头捕获的面部视频估计血容量脉冲（BVP）波形。尽管近期的深度模型相比经典信号处理方法提高了鲁棒性，但许多方法增加了计算成本和参数数量，基于注意力的时序建模引入了与时间长度相关的二次方缩放。本文提出ToTMNet，这是一种轻量级的rPPG架构，用基于FFT加速的托普利兹时序混合层替代了时序注意力机制。托普利兹算子通过线性数量的参数提供完整的序列时序感受野，并可通过循环嵌入和基于FFT的卷积在近线性时间内应用。ToTMNet将全局托普利兹时序算子整合到一个紧凑的门控时序混合器中，结合局部深度时序卷积分支与门控全局托普利兹混合，从而实现高效的长距离时序滤波，仅需63k个参数。在两个数据集UBFC-rPPG（真实视频）和SCAMPS（合成视频）上的实验表明，ToTMNet在紧凑设计下实现了强大的心率估计精度。在UBFC-rPPG数据集内的评估中，ToTMNet达到1.055 bpm的MAE和0.996的皮尔逊相关系数。在合成到真实场景（SCAMPS到UBFC-rPPG）中，ToTMNet达到1.582 bpm的MAE和0.994的皮尔逊相关系数。消融实验结果确认，门控机制对于有效利用全局托普利兹混合至关重要，尤其是在领域偏移的情况下。本预印本研究的主要局限是仅使用了两个数据集；然而，结果表明托普利兹结构的时序混合是一种实用且高效的rPPG替代注意力机制的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Remote photoplethysmography (rPPG) estimates a blood volume pulse (BVP) waveform from facial videos captured by commodity cameras.</div>
</details>
</div>
<div class="card">
<div class="title">FLEx: Language Modeling with Few-shot Language Explanations</div>
<div class="meta-line">Authors: Adar Avsian, Christopher Richardson, Anirudh Sundar, Larry Heck</div>
<div class="meta-line">First: 2026-01-07T18:12:05+00:00 · Latest: 2026-01-07T18:12:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04157v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04157v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language models have become effective at a wide range of tasks, from math problem solving to open-domain question answering. However, they still make mistakes, and these mistakes are often repeated across related queries. Natural language explanations can help correct these errors, but collecting them at scale may be infeasible, particularly in domains where expert annotators are required. To address this issue, we introduce FLEx ($\textbf{F}$ew-shot $\textbf{L}$anguage $\textbf{Ex}$planations), a method for improving model behavior using a small number of explanatory examples. FLEx selects representative model errors using embedding-based clustering, verifies that the associated explanations correct those errors, and summarizes them into a prompt prefix that is prepended at inference-time. This summary guides the model to avoid similar errors on new inputs, without modifying model weights. We evaluate FLEx on CounterBench, GSM8K, and ReasonIF. We find that FLEx consistently outperforms chain-of-thought (CoT) prompting across all three datasets and reduces up to 83\% of CoT&#x27;s remaining errors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FLEx：使用少样本语言解释的语言建模</div>
<div class="mono" style="margin-top:8px">语言模型在从数学问题求解到开放领域问答等广泛任务中表现出色。然而，它们仍然会出错，且这些错误常在相关查询中重复出现。自然语言解释可以帮助纠正这些错误，但大规模收集解释可能不可行，尤其是在需要专家标注的领域。为了解决这一问题，我们引入了FLEx（Few-shot Language Explanations），一种通过少量解释性示例来改进模型行为的方法。FLEx利用基于嵌入的聚类选择具有代表性的模型错误，验证相关解释是否能纠正这些错误，并将其总结为一个提示前缀，在推理时添加。这种总结引导模型在新的输入上避免类似错误，而无需修改模型权重。我们在CounterBench、GSM8K和ReasonIF数据集上评估了FLEx，发现它在所有三个数据集上都优于链式思维（CoT）提示，并能减少多达83%的CoT剩余错误。</div>
</details>
</div>
<div class="card">
<div class="title">CktGen: Automated Analog Circuit Design with Generative Artificial Intelligence</div>
<div class="meta-line">Authors: Yuxuan Hou, Hehe Fan, Jianrong Zhang, Yue Zhang, Hua Chen, Min Zhou, Faxin Yu, Roger Zimmermann, Yi Yang</div>
<div class="meta-line">First: 2024-10-01T18:35:44+00:00 · Latest: 2026-01-07T18:11:26+00:00</div>
<div class="meta-line">Comments: Paper accepted by Engineering</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.00995v2">Abs</a> · <a href="https://arxiv.org/pdf/2410.00995v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The automatic synthesis of analog circuits presents significant challenges. Most existing approaches formulate the problem as a single-objective optimization task, overlooking that design specifications for a given circuit type vary widely across applications. To address this, we introduce specification-conditioned analog circuit generation, a task that directly generates analog circuits based on target specifications. The motivation is to leverage existing well-designed circuits to improve automation in analog circuit design. Specifically, we propose CktGen, a simple yet effective variational autoencoder that maps discretized specifications and circuits into a joint latent space and reconstructs the circuit from that latent vector. Notably, as a single specification may correspond to multiple valid circuits, naively fusing specification information into the generative model does not capture these one-to-many relationships. To address this, we decouple the encoding of circuits and specifications and align their mapped latent space. Then, we employ contrastive training with a filter mask to maximize differences between encoded circuits and specifications. Furthermore, classifier guidance along with latent feature alignment promotes the clustering of circuits sharing the same specification, avoiding model collapse into trivial one-to-one mappings. By canonicalizing the latent space with respect to specifications, we can search for an optimal circuit that meets valid target specifications. We conduct comprehensive experiments on the open circuit benchmark and introduce metrics to evaluate cross-model consistency. Experimental results demonstrate that CktGen achieves substantial improvements over state-of-the-art methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CktGen: 基于生成人工智能的模拟电路自动化设计</div>
<div class="mono" style="margin-top:8px">模拟电路的自动综合面临重大挑战。大多数现有方法将该问题建模为单目标优化任务，忽略了特定电路类型在不同应用场景下的设计规范差异。为了解决这一问题，我们引入了基于规范的模拟电路生成方法，该方法直接根据目标规范生成模拟电路。其动机是利用已有的良好设计电路来提升模拟电路设计的自动化水平。具体而言，我们提出了CktGen，这是一种简单而有效的变分自编码器，将离散化的规范和电路映射到联合潜在空间，并从该潜在向量中重构电路。值得注意的是，由于一个规范可能对应多个有效的电路，简单地将规范信息融合到生成模型中无法捕捉这种一对多的关系。为此，我们解耦了电路和规范的编码过程，并对齐它们的潜在空间映射。随后，我们采用带有过滤掩码的对比训练，以最大化编码电路与规范之间的差异。此外，结合分类器引导和潜在特征对齐，有助于具有相同规范的电路聚类，避免模型坍缩为简单的单对单映射。通过将潜在空间规范化为与规范相关，我们可以搜索满足有效目标规范的最优电路。我们在公开的电路基准上进行了全面实验，并引入了评估跨模型一致性的指标。实验结果表明，CktGen在现有方法之上取得了显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion-DRF: Differentiable Reward Flow for Video Diffusion Fine-Tuning</div>
<div class="meta-line">Authors: Yifan Wang, Yanyu Li, Sergey Tulyakov, Yun Fu, Anil Kag</div>
<div class="meta-line">First: 2026-01-07T18:05:08+00:00 · Latest: 2026-01-07T18:05:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04153v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04153v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Direct Preference Optimization (DPO) has recently improved Text-to-Video (T2V) generation by enhancing visual fidelity and text alignment. However, current methods rely on non-differentiable preference signals from human annotations or learned reward models. This reliance makes training label-intensive, bias-prone, and easy-to-game, which often triggers reward hacking and unstable training. We propose Diffusion-DRF, a differentiable reward flow for fine-tuning video diffusion models using a frozen, off-the-shelf Vision-Language Model (VLM) as a training-free critic. Diffusion-DRF directly backpropagates VLM feedback through the diffusion denoising chain, converting logit-level responses into token-aware gradients for optimization. We propose an automated, aspect-structured prompting pipeline to obtain reliable multi-dimensional VLM feedback, while gradient checkpointing enables efficient updates through the final denoising steps. Diffusion-DRF improves video quality and semantic alignment while mitigating reward hacking and collapse -- without additional reward models or preference datasets. It is model-agnostic and readily generalizes to other diffusion-based generative tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Diffusion-DRF：用于视频扩散微调的可微分奖励流</div>
<div class="mono" style="margin-top:8px">直接偏好优化（DPO）最近通过提升视觉保真度和文本对齐，改进了文本到视频（T2V）生成。然而，当前方法依赖于人类注释或学习到的奖励模型提供的非可微分偏好信号。这种依赖性导致训练过程需要大量标签、容易产生偏差且易于被操控，通常会引发奖励黑客行为和不稳定的训练。我们提出Diffusion-DRF，一种使用冻结的、现成的视觉-语言模型（VLM）作为无训练批评者的可微分奖励流，用于视频扩散模型的微调。Diffusion-DRF直接通过扩散去噪链反向传播VLM反馈，将logit级别的响应转换为token感知的梯度以进行优化。我们提出了一种自动化、结构化的提示生成管道，以获取可靠的多维VLM反馈，同时梯度检查点技术使最终去噪步骤的更新更加高效。Diffusion-DRF在提升视频质量和语义对齐的同时，缓解了奖励黑客和崩溃问题，而无需额外的奖励模型或偏好数据集。它与模型无关，且可轻松推广到其他基于扩散的生成任务。</div>
</details>
</div>
<div class="card">
<div class="title">Causal Invariance Learning via Efficient Nonconvex Optimization</div>
<div class="meta-line">Authors: Zhenyu Wang, Yifan Hu, Peter Bühlmann, Zijian Guo</div>
<div class="meta-line">First: 2024-12-16T15:11:02+00:00 · Latest: 2026-01-07T18:04:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.11850v3">Abs</a> · <a href="https://arxiv.org/pdf/2412.11850v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Identifying the causal relationship among variables from observational data is an important yet challenging task. This work focuses on identifying the direct causes of an outcome and estimating their magnitude, i.e., learning the causal outcome model. Data from multiple environments provide valuable opportunities to uncover causality by exploiting the invariance principle that the causal outcome model holds across heterogeneous environments. Based on the invariance principle, we propose the Negative Weighted Distributionally Robust Optimization (NegDRO) framework to learn an invariant prediction model. NegDRO minimizes the worst-case combination of risks across multiple environments and enforces invariance by allowing potential negative weights. Under the additive interventions regime, we establish three major contributions: (i) On the statistical side, we provide sufficient and nearly necessary identification conditions under which the invariant prediction model coincides with the causal outcome model; (ii) On the optimization side, despite the nonconvexity of NegDRO, we establish its benign optimization landscape, where all stationary points lie close to the true causal outcome model; (iii) On the computational side, we develop a gradient-based algorithm that provably converges to the causal outcome model, with non-asymptotic convergence rates in both sample size and gradient-descent iterations. In particular, our method avoids exhaustive combinatorial searches over exponentially many subsets of covariates found in the literature, ensuring scalability even when the dimension of the covariates is large. To our knowledge, this is the first causal invariance learning method that finds the approximate global optimality for a nonconvex optimization problem efficiently.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过高效非凸优化进行因果不变性学习</div>
<div class="mono" style="margin-top:8px">从观测数据中识别变量间的因果关系是一项重要但具有挑战性的任务。本工作专注于识别结果的直接原因并估计其大小，即学习因果结果模型。来自多个环境的数据为通过利用因果结果模型在异构环境中的不变性原理来揭示因果关系提供了宝贵的机会。基于不变性原理，我们提出了负加权分布鲁棒优化（NegDRO）框架，以学习一个不变预测模型。NegDRO通过允许潜在的负权重来强制不变性，最小化多个环境中的最坏情况风险组合。在加性干预框架下，我们建立了三个主要贡献：(i) 在统计方面，我们提供了充分且几乎必要的识别条件，使得不变预测模型与因果结果模型一致；(ii) 在优化方面，尽管NegDRO是非凸的，我们证明了其优化景观是良性的，所有平稳点都接近真实的因果结果模型；(iii) 在计算方面，我们开发了一种基于梯度的算法，该算法可以证明收敛到因果结果模型，并在样本量和梯度下降迭代次数上都具有非渐近收敛速率。特别地，我们的方法避免了文献中指数级协变量子集上的穷举组合搜索，即使在协变量维度较大时也能确保可扩展性。据我们所知，这是首个能够高效地在非凸优化问题中找到近似全局最优解的因果不变性学习方法。</div>
</details>
</div>
<div class="card">
<div class="title">Klear: Unified Multi-Task Audio-Video Joint Generation</div>
<div class="meta-line">Authors: Jun Wang, Chunyu Qiang, Yuxin Guo, Yiran Wang, Xijuan Zeng, Chen Zhang, Pengfei Wan</div>
<div class="meta-line">First: 2026-01-07T18:03:45+00:00 · Latest: 2026-01-07T18:03:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04151v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04151v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Audio-video joint generation has progressed rapidly, yet substantial challenges still remain. Non-commercial approaches still suffer audio-visual asynchrony, poor lip-speech alignment, and unimodal degradation, which can be stemmed from weak audio-visual correspondence modeling, limited generalization, and scarce high-quality dense-caption data. To address these issues, we introduce Klear and delve into three axes--model architecture, training strategy, and data curation. Architecturally, we adopt a single-tower design with unified DiT blocks and an Omni-Full Attention mechanism, achieving tight audio-visual alignment and strong scalability. Training-wise, we adopt a progressive multitask regime--random modality masking to joint optimization across tasks, and a multistage curriculum, yielding robust representations, strengthening A-V aligned world knowledge, and preventing unimodal collapse. For datasets, we present the first large-scale audio-video dataset with dense captions, and introduce a novel automated data-construction pipeline which annotates and filters millions of diverse, high-quality, strictly aligned audio-video-caption triplets. Building on this, Klear scales to large datasets, delivering high-fidelity, semantically and temporally aligned, instruction-following generation in both joint and unimodal settings while generalizing robustly to out-of-distribution scenarios. Across tasks, it substantially outperforms prior methods by a large margin and achieves performance comparable to Veo 3, offering a unified, scalable path toward next-generation audio-video synthesis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Klear：统一的多任务音频-视频联合生成</div>
<div class="mono" style="margin-top:8px">音频-视频联合生成技术发展迅速，但仍面临诸多挑战。非商业方法仍存在视听不同步、唇语对齐差以及单模态退化等问题，这些问题可能源于弱的视听对应建模、有限的泛化能力以及缺乏高质量的密集字幕数据。为了解决这些问题，我们引入了Klear，并从模型架构、训练策略和数据构建三个维度进行深入探索。在架构上，我们采用单塔设计，结合统一的DiT块和Omni-Full Attention机制，实现了紧密的视听对齐和强大的可扩展性。在训练方面，我们采用渐进式多任务策略——通过随机模态掩码实现跨任务的联合优化，并引入多阶段课程学习，从而获得稳健的表示，增强视听对齐的世界知识，并防止单模态崩溃。在数据集方面，我们提出了首个大规模的音频-视频密集字幕数据集，并引入了一种新颖的自动化数据构建流程，用于标注和过滤数百万个多样、高质量且严格对齐的音频-视频-字幕三元组。基于此，Klear能够扩展到大规模数据集，实现高保真、语义和时间对齐的指令跟随生成，适用于联合和单模态场景，并且在分布外场景中表现出强大的泛化能力。在各项任务中，Klear显著优于以往方法，并且其性能与Veo 3相当，为下一代音频-视频合成提供了一条统一且可扩展的路径。</div>
</details>
</div>
<div class="card">
<div class="title">A Theoretical and Empirical Taxonomy of Imbalance in Binary Classification</div>
<div class="meta-line">Authors: Rose Yvette Bandolo Essomba, Ernest Fokoué</div>
<div class="meta-line">First: 2026-01-07T18:02:11+00:00 · Latest: 2026-01-07T18:02:11+00:00</div>
<div class="meta-line">Comments: 24 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04149v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04149v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Class imbalance significantly degrades classification performance, yet its effects are rarely analyzed from a unified theoretical perspective. We propose a principled framework based on three fundamental scales: the imbalance coefficient $η$, the sample--dimension ratio $κ$, and the intrinsic separability $Δ$. Starting from the Gaussian Bayes classifier, we derive closed-form Bayes errors and show how imbalance shifts the discriminant boundary, yielding a deterioration slope that predicts four regimes: Normal, Mild, Extreme, and Catastrophic. Using a balanced high-dimensional genomic dataset, we vary only $η$ while keeping $κ$ and $Δ$ fixed. Across parametric and non-parametric models, empirical degradation closely follows theoretical predictions: minority Recall collapses once $\log(η)$ exceeds $Δ\sqrtκ$, Precision increases asymmetrically, and F1-score and PR-AUC decline in line with the predicted regimes. These results show that the triplet $(η,κ,Δ)$ provides a model-agnostic, geometrically grounded explanation of imbalance-induced deterioration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>二分类中不平衡性的理论与实证分类学</div>
<div class="mono" style="margin-top:8px">类别不平衡会显著降低分类性能，但其影响很少从统一的理论视角进行分析。我们提出一个基于三个基本尺度的原理性框架：不平衡系数 $η$、样本-维度比 $κ$ 以及内在可分性 $Δ$。从高斯贝叶斯分类器出发，我们推导出闭式贝叶斯误差，并展示不平衡如何改变判别边界，从而产生一个退化斜率，预测出四种状态：正常、轻微、极端和灾难性。我们使用一个高维平衡的基因组数据集，仅改变 $η$ 而保持 $κ$ 和 $Δ$ 不变。在参数化和非参数化模型中，实证退化结果与理论预测高度吻合：少数类 Recall 在 $\log(η)$ 超过 $Δ\sqrtκ$ 时崩溃，Precision 不对称增加，F1-score 和 PR-AUC 的下降与预测的状态一致。这些结果表明，三元组 $(η,κ,Δ)$ 提供了一个模型无关、几何基础的不平衡性导致性能退化的解释。</div>
</details>
</div>
<div class="card">
<div class="title">Reward Is Enough: LLMs Are In-Context Reinforcement Learners</div>
<div class="meta-line">Authors: Kefan Song, Amir Moeini, Peng Wang, Lei Gong, Rohan Chandra, Shangtong Zhang, Yanjun Qi</div>
<div class="meta-line">First: 2025-05-21T16:15:01+00:00 · Latest: 2026-01-07T17:58:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.06303v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.06303v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is a framework for solving sequential decision-making problems. In this work, we demonstrate that, surprisingly, RL emerges during the inference time of large language models (LLMs), a phenomenon we term in-context RL (ICRL). To reveal this capability, we introduce a simple multi-round prompting framework, we call ICRL prompting, for inference-time self-improvement. The goal of ICRL prompting is to guide LLMs to perform reinforcement learning during inference for self-improvement on a given task. After each response, the model receives numerical scalar feedback, denoted as a reward. In the next round, we prompt the LLM again together with a context that concatenates all prior responses and their associated rewards. We consistently observe that response quality improves as the context grows. In other words, the LLM can optimize scalar reward signals during inference, exhibiting behavior analogous to reinforcement learning. We evaluate ICRL prompting on Game of 24, creative writing, ScienceWorld, and Olympiad-level math competitions (AIME and HMMT), demonstrating significant improvements over baselines such as Self-Refine and Reflexion. Notably, even when the reward signals are generated by the same LLM, ICRL prompting still improves performance, highlighting a promising new paradigm for test-time scaling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励就足够了：LLMs 是上下文强化学习者</div>
<div class="mono" style="margin-top:8px">强化学习（RL）是一种解决序列决策问题的框架。在这项工作中，我们展示了令人惊讶的现象：在大语言模型（LLMs）的推理过程中，强化学习会自然出现，我们将这一现象称为上下文强化学习（ICRL）。为了揭示这一能力，我们引入了一个简单的多轮提示框架，称为 ICRL 提示，用于推理时的自我提升。ICRL 提示的目标是引导 LLM 在推理过程中执行强化学习，以在给定任务上实现自我提升。在每次响应后，模型会接收到一个数值标量反馈，我们称之为奖励。在下一轮中，我们将 LLM 与包含所有先前响应及其相关奖励的上下文一起提示。我们一致观察到，随着上下文的增长，响应质量不断提高。换句话说，LLM 可以在推理过程中优化标量奖励信号，表现出与强化学习类似的行为。我们在 24 点游戏、创意写作、ScienceWorld 和奥林匹克级别的数学竞赛（AIME 和 HMMT）上评估了 ICRL 提示，结果表明其在 Self-Refine 和 Reflexion 等基线方法上取得了显著提升。值得注意的是，即使奖励信号是由同一 LLM 生成的，ICRL 提示仍然能提升性能，这凸显了一种有前景的新范式，用于测试时的扩展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reinforcement learning (RL) is a framework for solving sequential decision-making problems.</div>
</details>
</div>
<div class="card">
<div class="title">Implications of Non-equatorial Relativistic Accretion Flows for Ultra-Fast Inflows in AGNs</div>
<div class="meta-line">Authors: Keigo Fukumura, Alessandro Peca, Roberto Serafinelli, Mauro Dadina</div>
<div class="meta-line">First: 2026-01-07T17:54:28+00:00 · Latest: 2026-01-07T17:54:28+00:00</div>
<div class="meta-line">Comments: accepted to ApJ (11 pages, 4 figures)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04141v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04141v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Motivated by a number of X-ray observations of active galactic nuclei (AGNs) that exhibit a potential signature of ultra-fast inflows (UFIs), we consider in this work a scenario that UFIs can be physically identified as weakly-magnetized hydrodynamic accretion flows that is guided and channeled by poloidal magnetic field into low-to-mid latitude above the equatorial disk. In the context of general relativistic hydrodynamics (GRHD) under a weak-field limit in Kerr spacetime, we present a set of preliminary results by numerically calculating the physical property of GRHD flows (e.g. kinematics and density distribution) in an effort to simulate redshifted absorption line spectra. Our model demonstrates that such GRHD accretion off the equatorial plane (i.e. $v \gsim 0.1c$ where $c$ is the speed of light in the vicinity of AGN closer than $\sim 100$ \sw radii) can manifest itself as UFIs in the form of redshifted absorption signature assuming the observed characteristics such as column density of $N_H \sim 10^{23}$ cm$^{-2}$ and ionization parameter of $\log (ξ\rm{[erg~cm~s^{-1}])} \sim 3$ as also seen in recent multi-epoch {\it NuSTAR} observations among other data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非赤道相对论吸积流对活动星系核中超快速流入的含义</div>
<div class="mono" style="margin-top:8px">受一系列活动星系核（AGNs）X射线观测结果的启发，这些观测结果显示出超快速流入（UFIs）的潜在特征，我们考虑了一种情景：UFIs可以被物理识别为弱磁化的流体动力学吸积流，这些吸积流在赤道盘上方低至中纬度区域被极向磁场引导和约束。在克尔时空弱场极限下的广义相对论流体动力学（GRHD）框架下，我们通过数值计算GRHD流的物理特性（如运动学和密度分布），试图模拟红移吸收线谱。我们的模型表明，这种偏离赤道平面的GRHD吸积流（即速度 $v \gsim 0.1c$，其中 $c$ 是AGN附近约 $\sim 100$ \sw 半径处的光速）可以表现为超快速流入，其形式为红移吸收特征，假设观测到的特征如 $N_H \sim 10^{23}$ cm$^{-2}$ 的柱密度和 $\log (ξ\rm{[erg~cm~s^{-1}])} \sim 3$ 的电离参数，这些特征也出现在最近的多时相NuSTAR观测数据及其他数据中。</div>
</details>
</div>
<div class="card">
<div class="title">Wow, wo, val! A Comprehensive Embodied World Model Evaluation Turing Test</div>
<div class="meta-line">Authors: Chun-Kai Fan, Xiaowei Chi, Xiaozhu Ju, Hao Li, Yong Bao, Yu-Kai Wang, Lizhang Chen, Zhiyuan Jiang, Kuangzhi Ge, Ying Li, Weishi Mi, Qingpo Wuwu, Peidong Jia, Yulin Luo, Kevin Zhang, Zhiyuan Qin, Yong Dai, Sirui Han, Yike Guo, Shanghang Zhang, Jian Tang</div>
<div class="meta-line">First: 2026-01-07T17:50:37+00:00 · Latest: 2026-01-07T17:50:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04137v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04137v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As world models gain momentum in Embodied AI, an increasing number of works explore using video foundation models as predictive world models for downstream embodied tasks like 3D prediction or interactive generation. However, before exploring these downstream tasks, video foundation models still have two critical questions unanswered: (1) whether their generative generalization is sufficient to maintain perceptual fidelity in the eyes of human observers, and (2) whether they are robust enough to serve as a universal prior for real-world embodied agents. To provide a standardized framework for answering these questions, we introduce the Embodied Turing Test benchmark: WoW-World-Eval (Wow,wo,val). Building upon 609 robot manipulation data, Wow-wo-val examines five core abilities, including perception, planning, prediction, generalization, and execution. We propose a comprehensive evaluation protocol with 22 metrics to assess the models&#x27; generation ability, which achieves a high Pearson Correlation between the overall score and human preference (&gt;0.93) and establishes a reliable foundation for the Human Turing Test. On Wow-wo-val, models achieve only 17.27 on long-horizon planning and at best 68.02 on physical consistency, indicating limited spatiotemporal consistency and physical reasoning. For the Inverse Dynamic Model Turing Test, we first use an IDM to evaluate the video foundation models&#x27; execution accuracy in the real world. However, most models collapse to $\approx$ 0% success, while WoW maintains a 40.74% success rate. These findings point to a noticeable gap between the generated videos and the real world, highlighting the urgency and necessity of benchmarking World Model in Embodied AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>哇哦，哇哦，瓦尔！一个全面的具身世界模型评估图灵测试</div>
<div class="mono" style="margin-top:8px">随着世界模型在具身人工智能领域的发展，越来越多的研究探索使用视频基础模型作为预测性世界模型，用于下游的具身任务，如3D预测或交互式生成。然而，在探索这些下游任务之前，视频基础模型仍存在两个关键问题未解决：(1)它们的生成泛化能力是否足以在人类观察者眼中保持感知保真度；(2)它们是否足够稳健，可以作为现实世界具身代理的通用先验。为提供一个标准化的框架来回答这些问题，我们引入了具身图灵测试基准：Wow-wo-val（哇哦，哇哦，瓦尔）。基于609个机器人操作数据，Wow-wo-val考察了五项核心能力，包括感知、规划、预测、泛化和执行。我们提出了一套包含22项指标的全面评估协议，用于评估模型的生成能力，该协议在总体得分与人类偏好之间实现了高皮尔逊相关性（&gt;0.93），并为人类图灵测试奠定了可靠的基础。在Wow-wo-val上，模型在长视距规划任务中仅达到17.27分，在物理一致性任务中最高也仅达到68.02分，表明其时空一致性和物理推理能力有限。对于逆动态模型图灵测试，我们首先使用IDM来评估视频基础模型在现实世界中的执行准确性。然而，大多数模型的成功率接近0%，而Wow则保持了40.74%的成功率。这些发现表明生成视频与现实世界之间存在明显的差距，突显了在具身人工智能中对世界模型进行基准测试的紧迫性和必要性。</div>
</details>
</div>
<div class="card">
<div class="title">ContextFocus: Activation Steering for Contextual Faithfulness in Large Language Models</div>
<div class="meta-line">Authors: Nikhil Anand, Shwetha Somasundaram, Anirudh Phukan, Apoorv Saxena, Koyel Mukherjee</div>
<div class="meta-line">First: 2026-01-07T17:45:20+00:00 · Latest: 2026-01-07T17:45:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04131v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04131v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) encode vast amounts of parametric knowledge during pre-training. As world knowledge evolves, effective deployment increasingly depends on their ability to faithfully follow externally retrieved context. When such evidence conflicts with the model&#x27;s internal knowledge, LLMs often default to memorized facts, producing unfaithful outputs. In this work, we introduce ContextFocus, a lightweight activation steering approach that improves context faithfulness in such knowledge-conflict settings while preserving fluency and efficiency. Unlike prior approaches, our solution requires no model finetuning and incurs minimal inference-time overhead, making it highly efficient. We evaluate ContextFocus on the ConFiQA benchmark, comparing it against strong baselines including ContextDPO, COIECD, and prompting-based methods. Furthermore, we show that our method is complementary to prompting strategies and remains effective on larger models. Extensive experiments show that ContextFocus significantly improves contextual-faithfulness. Our results highlight the effectiveness, robustness, and efficiency of ContextFocus in improving contextual-faithfulness of LLM outputs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ContextFocus：大型语言模型中用于上下文忠实性的激活引导方法</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在预训练过程中编码了大量参数化知识。随着世界知识的演变，有效部署越来越依赖于其遵循外部检索上下文的能力。当此类证据与模型内部知识冲突时，LLMs通常会默认使用记忆中的事实，从而产生不忠实的输出。在本工作中，我们引入了ContextFocus，这是一种轻量级的激活引导方法，能够在知识冲突的场景下提升上下文忠实性，同时保持流畅性和效率。与以往的方法不同，我们的解决方案无需模型微调，并且在推理时间上的开销极小，使其具有高度的效率。我们在ConFiQA基准上评估了ContextFocus，将其与包括ContextDPO、COIECD和基于提示的方法在内的强大基线进行比较。此外，我们还展示了该方法与提示策略的互补性，并且在更大的模型上仍保持有效性。大量实验表明，ContextFocus显著提升了LLMs输出的上下文忠实性。我们的结果突显了ContextFocus在提升LLMs输出上下文忠实性方面的有效性、鲁棒性和效率。</div>
</details>
</div>
<div class="card">
<div class="title">Pixel-Wise Multimodal Contrastive Learning for Remote Sensing Images</div>
<div class="meta-line">Authors: Leandro Stival, Ricardo da Silva Torres, Helio Pedrini</div>
<div class="meta-line">First: 2026-01-07T17:41:11+00:00 · Latest: 2026-01-07T17:41:11+00:00</div>
<div class="meta-line">Comments: 21 pages, 9 Figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04127v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04127v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Satellites continuously generate massive volumes of data, particularly for Earth observation, including satellite image time series (SITS). However, most deep learning models are designed to process either entire images or complete time series sequences to extract meaningful features for downstream tasks. In this study, we propose a novel multimodal approach that leverages pixel-wise two-dimensional (2D) representations to encode visual property variations from SITS more effectively. Specifically, we generate recurrence plots from pixel-based vegetation index time series (NDVI, EVI, and SAVI) as an alternative to using raw pixel values, creating more informative representations. Additionally, we introduce PIxel-wise Multimodal Contrastive (PIMC), a new multimodal self-supervision approach that produces effective encoders based on two-dimensional pixel time series representations and remote sensing imagery (RSI). To validate our approach, we assess its performance on three downstream tasks: pixel-level forecasting and classification using the PASTIS dataset, and land cover classification on the EuroSAT dataset. Moreover, we compare our results to state-of-the-art (SOTA) methods on all downstream tasks. Our experimental results show that the use of 2D representations significantly enhances feature extraction from SITS, while contrastive learning improves the quality of representations for both pixel time series and RSI. These findings suggest that our multimodal method outperforms existing models in various Earth observation tasks, establishing it as a robust self-supervision framework for processing both SITS and RSI. Code avaliable on</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向遥感图像的像素级多模态对比学习</div>
<div class="mono" style="margin-top:8px">卫星持续生成大量数据，特别是在地球观测中，包括卫星图像时间序列（SITS）。然而，大多数深度学习模型是为处理整张图像或完整的时序序列而设计的，以提取可用于下游任务的有意义特征。在本研究中，我们提出了一种新颖的多模态方法，利用像素级的二维（2D）表示来更有效地编码SITS中的视觉属性变化。具体而言，我们从基于像素的植被指数时间序列（NDVI、EVI和SAVI）生成递归图，作为使用原始像素值的替代方案，从而创建更具信息量的表示。此外，我们引入了一种新的多模态自监督方法——像素级多模态对比学习（PIMC），该方法基于二维像素时间序列表示和遥感图像（RSI）构建有效的编码器。为了验证我们的方法，我们在三个下游任务上评估其性能：使用PASTIS数据集进行像素级预测和分类，以及使用EuroSAT数据集进行土地覆盖分类。此外，我们在所有下游任务上将我们的结果与最先进的（SOTA）方法进行了比较。实验结果表明，使用二维表示显著增强了从SITS中提取特征的能力，而对比学习则提高了像素时间序列和遥感图像表示的质量。这些发现表明，我们的多模态方法在多种地球观测任务中优于现有模型，确立了其作为处理SITS和RSI的稳健自监督框架的潜力。代码可在</div>
</details>
</div>
<div class="card">
<div class="title">InfiniteWeb: Scalable Web Environment Synthesis for GUI Agent Training</div>
<div class="meta-line">Authors: Ziyun Zhang, Zezhou Wang, Xiaoyi Zhang, Zongyu Guo, Jiahao Li, Bin Li, Yan Lu</div>
<div class="meta-line">First: 2026-01-07T17:40:08+00:00 · Latest: 2026-01-07T17:40:08+00:00</div>
<div class="meta-line">Comments: Work In Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04126v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.04126v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">GUI agents that interact with graphical interfaces on behalf of users represent a promising direction for practical AI assistants. However, training such agents is hindered by the scarcity of suitable environments. We present InfiniteWeb, a system that automatically generates functional web environments at scale for GUI agent training. While LLMs perform well on generating a single webpage, building a realistic and functional website with many interconnected pages faces challenges. We address these challenges through unified specification, task-centric test-driven development, and a combination of website seed with reference design image to ensure diversity. Our system also generates verifiable task evaluators enabling dense reward signals for reinforcement learning. Experiments show that InfiniteWeb surpasses commercial coding agents at realistic website construction, and GUI agents trained on our generated environments achieve significant performance improvements on OSWorld and Online-Mind2Web, demonstrating the effectiveness of proposed system.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InfiniteWeb：面向GUI代理训练的可扩展网页环境合成</div>
<div class="mono" style="margin-top:8px">代表用户与图形界面交互的GUI代理是实用AI助手的一个有前景方向。然而，适合训练此类代理的环境稀缺，限制了其发展。我们提出了InfiniteWeb，这是一个能够大规模自动生成功能网页环境的系统。虽然LLMs在生成单个网页方面表现良好，但构建一个真实且功能完善的多页面网站面临挑战。我们通过统一规范、以任务为中心的测试驱动开发以及结合网站种子与参考设计图像的方法来解决这些问题，以确保多样性。我们的系统还生成可验证的任务评估器，从而为强化学习提供密集的奖励信号。实验表明，InfiniteWeb在真实网站构建任务中优于商业编码代理，且在我们生成的环境中训练的GUI代理在OSWorld和Online-Mind2Web任务中表现出显著的性能提升，证明了所提系统的有效性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
