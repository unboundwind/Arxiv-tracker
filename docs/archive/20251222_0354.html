<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-22 03:54</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251222_0354</div>
    <div class="row"><div class="card">
<div class="title">Generative Refocusing: Flexible Defocus Control from a Single Image</div>
<div class="meta-line">Authors: Chun-Wei Tuan Mu, Jia-Bin Huang, Yu-Lun Liu</div>
<div class="meta-line">First: 2025-12-18T18:59:59+00:00 · Latest: 2025-12-18T18:59:59+00:00</div>
<div class="meta-line">Comments: Project website: https://generative-refocusing.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16923v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16923v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://generative-refocusing.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Depth-of-field control is essential in photography, but getting the perfect focus often takes several tries or special equipment. Single-image refocusing is still difficult. It involves recovering sharp content and creating realistic bokeh. Current methods have significant drawbacks. They need all-in-focus inputs, depend on synthetic data from simulators, and have limited control over aperture. We introduce Generative Refocusing, a two-step process that uses DeblurNet to recover all-in-focus images from various inputs and BokehNet for creating controllable bokeh. Our main innovation is semi-supervised training. This method combines synthetic paired data with unpaired real bokeh images, using EXIF metadata to capture real optical characteristics beyond what simulators can provide. Our experiments show we achieve top performance in defocus deblurring, bokeh synthesis, and refocusing benchmarks. Additionally, our Generative Refocusing allows text-guided adjustments and custom aperture shapes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成式重聚焦：从单张图像实现灵活的失焦控制</div>
<div class="mono" style="margin-top:8px">景深控制在摄影中至关重要，但获得完美的对焦通常需要多次尝试或特殊设备。单图像重聚焦仍然具有挑战性，它涉及恢复清晰内容并生成逼真的景深效果。当前方法存在显著缺陷，需要全焦输入，依赖模拟器生成的合成数据，并且对光圈控制有限。我们引入生成式重聚焦，这是一种两步流程：使用DeblurNet从各种输入中恢复全焦图像，使用BokehNet生成可控的景深效果。我们的主要创新是半监督训练方法，该方法结合合成配对数据与非配对的真实景深图像，利用EXIF元数据捕捉模拟器无法提供的真实光学特性。我们的实验表明，在失焦去模糊、景深合成和重聚焦基准测试中，我们取得了最佳性能。此外，我们的生成式重聚焦还支持文本引导的调整和自定义光圈形状。</div>
</details>
</div>
<div class="card">
<div class="title">The World is Your Canvas: Painting Promptable Events with Reference Images, Trajectories, and Text</div>
<div class="meta-line">Authors: Hanlin Wang, Hao Ouyang, Qiuyu Wang, Yue Yu, Yihao Meng, Wen Wang, Ka Leong Cheng, Shuailei Ma, Qingyan Bai, Yixuan Li, Cheng Chen, Yanhong Zeng, Xing Zhu, Yujun Shen, Qifeng Chen</div>
<div class="meta-line">First: 2025-12-18T18:59:59+00:00 · Latest: 2025-12-18T18:59:59+00:00</div>
<div class="meta-line">Comments: Project page and code: https://worldcanvas.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16924v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16924v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://worldcanvas.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present WorldCanvas, a framework for promptable world events that enables rich, user-directed simulation by combining text, trajectories, and reference images. Unlike text-only approaches and existing trajectory-controlled image-to-video methods, our multimodal approach combines trajectories -- encoding motion, timing, and visibility -- with natural language for semantic intent and reference images for visual grounding of object identity, enabling the generation of coherent, controllable events that include multi-agent interactions, object entry/exit, reference-guided appearance and counterintuitive events. The resulting videos demonstrate not only temporal coherence but also emergent consistency, preserving object identity and scene despite temporary disappearance. By supporting expressive world events generation, WorldCanvas advances world models from passive predictors to interactive, user-shaped simulators. Our project page is available at: https://worldcanvas.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>世界是你的画布：使用参考图像、轨迹和文本绘制可提示事件</div>
<div class="mono" style="margin-top:8px">我们提出了WorldCanvas，一个用于可提示世界事件的框架，通过结合文本、轨迹和参考图像，实现丰富且由用户主导的模拟。与仅文本的方法和现有的轨迹控制图像到视频方法不同，我们的多模态方法将轨迹（编码运动、时间与可见性）与自然语言结合，用于语义意图，同时使用参考图像对物体身份进行视觉定位，从而生成连贯且可控的事件，包括多智能体交互、物体进入/离开、参考引导的外观以及反直觉事件。生成的视频不仅展示了时间上的连贯性，还表现出涌现的一致性，即使物体暂时消失，也能保持其身份和场景的连续性。通过支持表达性世界事件的生成，WorldCanvas将世界模型从被动预测者推进为可交互、由用户塑造的模拟器。</div>
</details>
</div>
<div class="card">
<div class="title">Next-Embedding Prediction Makes Strong Vision Learners</div>
<div class="meta-line">Authors: Sihan Xu, Ziqiao Ma, Wenhao Chai, Xuweiyi Chen, Weiyang Jin, Joyce Chai, Saining Xie, Stella X. Yu</div>
<div class="meta-line">First: 2025-12-18T18:59:58+00:00 · Latest: 2025-12-18T18:59:58+00:00</div>
<div class="meta-line">Comments: Project Page: https://sihanxu.me/nepa</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16922v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16922v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inspired by the success of generative pretraining in natural language, we ask whether the same principles can yield strong self-supervised visual learners. Instead of training models to output features for downstream use, we train them to generate embeddings to perform predictive tasks directly. This work explores such a shift from learning representations to learning models. Specifically, models learn to predict future patch embeddings conditioned on past ones, using causal masking and stop gradient, which we refer to as Next-Embedding Predictive Autoregression (NEPA). We demonstrate that a simple Transformer pretrained on ImageNet-1k with next embedding prediction as its sole learning objective is effective - no pixel reconstruction, discrete tokens, contrastive loss, or task-specific heads. This formulation retains architectural simplicity and scalability, without requiring additional design complexity. NEPA achieves strong results across tasks, attaining 83.8% and 85.3% top-1 accuracy on ImageNet-1K with ViT-B and ViT-L backbones after fine-tuning, and transferring effectively to semantic segmentation on ADE20K. We believe generative pretraining from embeddings provides a simple, scalable, and potentially modality-agnostic alternative to visual self-supervised learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Next-Embedding预测打造强大的视觉学习者</div>
<div class="mono" style="margin-top:8px">受自然语言中生成式预训练成功的启发，我们探讨是否可以将相同原则应用于构建强大的自监督视觉学习模型。我们不训练模型输出特征以供下游任务使用，而是训练它们直接生成嵌入以执行预测任务。本工作探索了从学习表示到学习模型的转变。具体而言，模型通过因果掩码和停止梯度机制，基于过去的嵌入来预测未来的嵌入，我们称之为Next-Embedding预测自回归（NEPA）。我们证明，一个仅以Next-Embedding预测为目标进行预训练的简单Transformer，在ImageNet-1K上经过微调后，能够达到83.8%和85.3%的top-1准确率（使用ViT-B和ViT-L主干网络），并在ADE20K上有效迁移至语义分割任务。我们认为，从嵌入中进行生成式预训练为视觉自监督学习提供了一种简单、可扩展且可能模态无关的替代方案。</div>
</details>
</div>
<div class="card">
<div class="title">EasyV2V: A High-quality Instruction-based Video Editing Framework</div>
<div class="meta-line">Authors: Jinjie Mai, Chaoyang Wang, Guocheng Gordon Qian, Willi Menapace, Sergey Tulyakov, Bernard Ghanem, Peter Wonka, Ashkan Mirzaei</div>
<div class="meta-line">First: 2025-12-18T18:59:57+00:00 · Latest: 2025-12-18T18:59:57+00:00</div>
<div class="meta-line">Comments: Project page: https://snap-research.github.io/easyv2v/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16920v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16920v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://snap-research.github.io/easyv2v/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While image editing has advanced rapidly, video editing remains less explored, facing challenges in consistency, control, and generalization. We study the design space of data, architecture, and control, and introduce \emph{EasyV2V}, a simple and effective framework for instruction-based video editing. On the data side, we compose existing experts with fast inverses to build diverse video pairs, lift image edit pairs into videos via single-frame supervision and pseudo pairs with shared affine motion, mine dense-captioned clips for video pairs, and add transition supervision to teach how edits unfold. On the model side, we observe that pretrained text-to-video models possess editing capability, motivating a simplified design. Simple sequence concatenation for conditioning with light LoRA fine-tuning suffices to train a strong model. For control, we unify spatiotemporal control via a single mask mechanism and support optional reference images. Overall, EasyV2V works with flexible inputs, e.g., video+text, video+mask+text, video+mask+reference+text, and achieves state-of-the-art video editing results, surpassing concurrent and commercial systems. Project page: https://snap-research.github.io/easyv2v/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EasyV2V：一种高质量的基于指令的视频编辑框架</div>
<div class="mono" style="margin-top:8px">尽管图像编辑技术迅速发展，视频编辑仍较少被探索，面临一致性、控制和泛化方面的挑战。我们研究了数据、架构和控制的设计空间，并引入了\emph{EasyV2V}，一种简单且有效的基于指令的视频编辑框架。在数据方面，我们通过组合现有专家与快速逆向生成来构建多样化的视频对，利用单帧监督和共享仿射运动的伪对将图像编辑对提升为视频对，通过密集字幕片段挖掘视频对，并添加过渡监督以教授编辑如何展开。在模型方面，我们观察到预训练的文本到视频模型具备编辑能力，这促使我们采用简化设计。仅通过简单的序列拼接和轻量级LoRA微调即可训练出强大的模型。在控制方面，我们通过单一掩码机制统一时空控制，并支持可选的参考图像。总体而言，EasyV2V能够灵活处理多种输入，例如视频+文本、视频+掩码+文本、视频+掩码+参考图像+文本，并实现了最先进的视频编辑结果，超越了同时期和商业系统。项目页面：https://snap-research.github.io/easyv2v/</div>
</details>
</div>
<div class="card">
<div class="title">DVGT: Driving Visual Geometry Transformer</div>
<div class="meta-line">Authors: Sicheng Zuo, Zixun Xie, Wenzhao Zheng, Shaoqing Xu, Fang Li, Shengyin Jiang, Long Chen, Zhi-Xin Yang, Jiwen Lu</div>
<div class="meta-line">First: 2025-12-18T18:59:57+00:00 · Latest: 2025-12-18T18:59:57+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/wzzheng/DVGT</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16919v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16919v1">PDF</a> · <a href="https://github.com/wzzheng/DVGT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Perceiving and reconstructing 3D scene geometry from visual inputs is crucial for autonomous driving. However, there still lacks a driving-targeted dense geometry perception model that can adapt to different scenarios and camera configurations. To bridge this gap, we propose a Driving Visual Geometry Transformer (DVGT), which reconstructs a global dense 3D point map from a sequence of unposed multi-view visual inputs. We first extract visual features for each image using a DINO backbone, and employ alternating intra-view local attention, cross-view spatial attention, and cross-frame temporal attention to infer geometric relations across images. We then use multiple heads to decode a global point map in the ego coordinate of the first frame and the ego poses for each frame. Unlike conventional methods that rely on precise camera parameters, DVGT is free of explicit 3D geometric priors, enabling flexible processing of arbitrary camera configurations. DVGT directly predicts metric-scaled geometry from image sequences, eliminating the need for post-alignment with external sensors. Trained on a large mixture of driving datasets including nuScenes, OpenScene, Waymo, KITTI, and DDAD, DVGT significantly outperforms existing models on various scenarios. Code is available at https://github.com/wzzheng/DVGT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DVGT：面向驾驶的视觉几何变换器</div>
<div class="mono" style="margin-top:8px">从视觉输入中感知和重建三维场景几何结构对于自动驾驶至关重要。然而，目前仍缺乏一种面向驾驶的密集几何感知模型，能够适应不同的场景和相机配置。为了解决这一问题，我们提出了一种驾驶视觉几何变换器（DVGT），它从一系列无姿态的多视角视觉输入中重建全局密集的三维点云图。我们首先使用DINO主干网络提取每张图像的视觉特征，并通过交替的单视角局部注意力、跨视角空间注意力和跨帧时序注意力来推断图像间的几何关系。随后，我们使用多个解码头来生成第一帧的自车坐标系下的全局点云图以及每帧的自车姿态。与依赖精确相机参数的传统方法不同，DVGT不依赖显式的三维几何先验知识，从而能够灵活处理任意相机配置。DVGT直接从图像序列中预测度量尺度的几何结构，消除了与外部传感器进行后对齐的需要。在包含nuScenes、OpenScene、Waymo、KITTI和DDAD等多个驾驶数据集的大型混合数据集上进行训练，DVGT在各种场景中显著优于现有模型。</div>
</details>
</div>
<div class="card">
<div class="title">Differences That Matter: Auditing Models for Capability Gap Discovery and Rectification</div>
<div class="meta-line">Authors: Qihao Liu, Chengzhi Mao, Yaojie Liu, Alan Yuille, Wen-Sheng Chu</div>
<div class="meta-line">First: 2025-12-18T18:59:57+00:00 · Latest: 2025-12-18T18:59:57+00:00</div>
<div class="meta-line">Comments: project page: https://auditdm.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16921v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16921v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://auditdm.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conventional evaluation methods for multimodal LLMs (MLLMs) lack interpretability and are often insufficient to fully disclose significant capability gaps across models. To address this, we introduce AuditDM, an automated framework that actively discovers and rectifies MLLM failure modes by auditing their divergence. AuditDM fine-tunes an MLLM as an auditor via reinforcement learning to generate challenging questions and counterfactual images that maximize disagreement among target models. Once trained, the auditor uncovers diverse, interpretable exemplars that reveal model weaknesses and serve as annotation-free data for rectification. When applied to SoTA models like Gemma-3 and PaliGemma-2, AuditDM discovers more than 20 distinct failure types. Fine-tuning on these discoveries consistently improves all models across 16 benchmarks, and enables a 3B model to surpass its 28B counterpart. Our results suggest that as data scaling hits diminishing returns, targeted model auditing offers an effective path to model diagnosis and improvement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>差异即关键：用于能力差距发现与修复的审计模型</div>
<div class="mono" style="margin-top:8px">传统的多模态大语言模型（MLLMs）评估方法缺乏可解释性，且往往不足以全面揭示模型间显著的能力差距。为了解决这一问题，我们引入了AuditDM，一个自动框架，通过审计模型的分歧来主动发现和修复MLLMs的失效模式。AuditDM通过强化学习对MLLM进行微调，使其作为审计员生成具有挑战性的问题和反事实图像，以最大化目标模型之间的分歧。训练完成后，审计员能够发现多样且可解释的示例，揭示模型的弱点，并作为无标注数据用于修复。当应用于Gemma-3和PaliGemma-2等最先进的模型时，AuditDM发现了超过20种不同的失效类型。在这些发现基础上进行微调，能够持续提升所有模型在16个基准测试中的表现，并使3B参数模型超越其28B参数的同类模型。我们的结果表明，当数据扩展达到边际效益递减时，有针对性的模型审计为模型诊断和改进提供了一条有效的途径。</div>
</details>
</div>
<div class="card">
<div class="title">AdaTooler-V: Adaptive Tool-Use for Images and Videos</div>
<div class="meta-line">Authors: Chaoyang Wang, Kaituo Feng, Dongyang Chen, Zhongyu Wang, Zhixun Li, Sicheng Gao, Meng Meng, Xu Zhou, Manyuan Zhang, Yuzhang Shang, Xiangyu Yue</div>
<div class="meta-line">First: 2025-12-18T18:59:55+00:00 · Latest: 2025-12-18T18:59:55+00:00</div>
<div class="meta-line">Comments: Project page: https://github.com/CYWang735/AdaTooler-V</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16918v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16918v1">PDF</a> · <a href="https://github.com/CYWang735/AdaTooler-V">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions. However, existing open-source models often exhibit blind tool-use reasoning patterns, invoking vision tools even when they are unnecessary, which significantly increases inference overhead and degrades model performance. To this end, we propose AdaTooler-V, an MLLM that performs adaptive tool-use by determining whether a visual problem truly requires tools. First, we introduce AT-GRPO, a reinforcement learning algorithm that adaptively adjusts reward scales based on the Tool Benefit Score of each sample, encouraging the model to invoke tools only when they provide genuine improvements. Moreover, we construct two datasets to support training: AdaTooler-V-CoT-100k for SFT cold start and AdaTooler-V-300k for RL with verifiable rewards across single-image, multi-image, and video data. Experiments across twelve benchmarks demonstrate the strong reasoning capability of AdaTooler-V, outperforming existing methods in diverse visual reasoning tasks. Notably, AdaTooler-V-7B achieves an accuracy of 89.8\% on the high-resolution benchmark V*, surpassing the commercial proprietary model GPT-4o and Gemini 1.5 Pro. All code, models, and data are released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AdaTooler-V: 针对图像和视频的自适应工具使用</div>
<div class="mono" style="margin-top:8px">最近的研究进展表明，多模态大语言模型（MLLMs）可以通过结合视觉工具交互的多模态交错思维链（CoT）获益。然而，现有的开源模型常常表现出盲目的工具使用推理模式，即使在不需要工具的情况下也会调用视觉工具，这显著增加了推理开销并降低了模型性能。为此，我们提出了AdaTooler-V，这是一种能够根据视觉问题是否真正需要工具而进行自适应工具调用的MLLM。首先，我们引入了AT-GRPO，一种基于样本工具收益分数自适应调整奖励尺度的强化学习算法，鼓励模型仅在工具能带来真实改进时调用。此外，我们构建了两个数据集以支持训练：AdaTooler-V-CoT-100k用于SFT冷启动，AdaTooler-V-300k用于跨单图像、多图像和视频数据的可验证奖励强化学习训练。在十二个基准测试中的实验结果表明，AdaTooler-V具有强大的推理能力，在多种视觉推理任务中优于现有方法。值得注意的是，AdaTooler-V-7B在高分辨率基准V*上达到了89.8\%的准确率，超过了商业专有模型GPT-4o和Gemini 1.5 Pro。所有代码、模型和数据均已发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advances have shown that multimodal large language models (MLLMs) benefit from multimodal interleaved chain-of-thought (CoT) with vision tool interactions.</div>
</details>
</div>
<div class="card">
<div class="title">Generative Adversarial Reasoner: Enhancing LLM Reasoning with Adversarial Reinforcement Learning</div>
<div class="meta-line">Authors: Qihao Liu, Luoxin Ye, Wufei Ma, Yu-Cheng Chou, Alan Yuille</div>
<div class="meta-line">First: 2025-12-18T18:59:54+00:00 · Latest: 2025-12-18T18:59:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16917v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16917v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps. In this paper, we introduce Generative Adversarial Reasoner, an on-policy joint training framework designed to enhance reasoning by co-evolving an LLM reasoner and an LLM-based discriminator through adversarial reinforcement learning. A compute-efficient review schedule partitions each reasoning chain into logically complete slices of comparable length, and the discriminator evaluates each slice&#x27;s soundness with concise, structured justifications. Learning couples complementary signals: the LLM reasoner is rewarded for logically consistent steps that yield correct answers, while the discriminator earns rewards for correctly detecting errors or distinguishing traces in the reasoning process. This produces dense, well-calibrated, on-policy step-level rewards that supplement sparse exact-match signals, improving credit assignment, increasing sample efficiency, and enhancing overall reasoning quality of LLMs. Across various mathematical benchmarks, the method delivers consistent gains over strong baselines with standard RL post-training. Specifically, on AIME24, we improve DeepSeek-R1-Distill-Qwen-7B from 54.0 to 61.3 (+7.3) and DeepSeek-R1-Distill-Llama-8B from 43.7 to 53.7 (+10.0). The modular discriminator also enables flexible reward shaping for objectives such as teacher distillation, preference alignment, and mathematical proof-based reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成对抗推理器：通过对抗强化学习增强大语言模型推理能力</div>
<div class="mono" style="margin-top:8px">具备显式推理能力的大语言模型（LLMs）在数学推理方面表现出色，但仍会犯过程错误，如计算错误、逻辑脆弱以及表面上合理但无效的步骤。本文提出生成对抗推理器，这是一种基于策略的联合训练框架，旨在通过对抗强化学习共同演化一个LLM推理器和一个基于LLM的判别器，以提升推理能力。一种计算高效的审查机制将每个推理链分割为长度相近且逻辑完整的片段，判别器则通过简洁的结构化理由评估每个片段的合理性。学习过程结合互补信号：LLM推理器因产生逻辑一致且得出正确答案的步骤而获得奖励，而判别器则因正确检测错误或区分推理过程中的痕迹而获得奖励。这产生了密集、校准良好的基于策略的步骤级奖励，补充了稀疏的精确匹配信号，从而改善了信用分配，提高了样本效率，并增强了LLMs的整体推理质量。在多个数学基准测试中，该方法在标准RL后训练下均优于强大的基线模型。具体而言，在AIME24测试中，我们将DeepSeek-R1-Distill-Qwen-7B的得分从54.0提升至61.3（+7.3），将DeepSeek-R1-Distill-Llama-8B的得分从43.7提升至53.7（+10.0）。此外，模块化的判别器还支持灵活的奖励塑造，适用于诸如教师蒸馏、偏好对齐和基于数学证明的推理等目标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) with explicit reasoning capabilities excel at mathematical reasoning yet still commit process errors, such as incorrect calculations, brittle logic, and superficially plausible but invalid steps.</div>
</details>
</div>
<div class="card">
<div class="title">StereoPilot: Learning Unified and Efficient Stereo Conversion via Generative Priors</div>
<div class="meta-line">Authors: Guibao Shen, Yihua Du, Wenhang Ge, Jing He, Chirui Chang, Donghao Zhou, Zhen Yang, Luozhou Wang, Xin Tao, Ying-Cong Chen</div>
<div class="meta-line">First: 2025-12-18T18:59:50+00:00 · Latest: 2025-12-18T18:59:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16915v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16915v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hit-perfect.github.io/StereoPilot/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content. However, producing 3D videos remains costly and complex, while automatic Monocular-to-Stereo conversion is hindered by the limitations of the multi-stage ``Depth-Warp-Inpaint&#x27;&#x27; (DWI) pipeline. This paradigm suffers from error propagation, depth ambiguity, and format inconsistency between parallel and converged stereo configurations. To address these challenges, we introduce UniStereo, the first large-scale unified dataset for stereo video conversion, covering both stereo formats to enable fair benchmarking and robust model training. Building upon this dataset, we propose StereoPilot, an efficient feed-forward model that directly synthesizes the target view without relying on explicit depth maps or iterative diffusion sampling. Equipped with a learnable domain switcher and a cycle consistency loss, StereoPilot adapts seamlessly to different stereo formats and achieves improved consistency. Extensive experiments demonstrate that StereoPilot significantly outperforms state-of-the-art methods in both visual fidelity and computational efficiency. Project page: https://hit-perfect.github.io/StereoPilot/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StereoPilot: 通过生成先验学习统一且高效的立体转换</div>
<div class="mono" style="margin-top:8px">立体显示技术的快速发展，包括VR头显和3D影院，导致对高质量立体视频内容的需求不断增加。然而，制作3D视频仍然成本高昂且复杂，而自动单目到立体的转换受到多阶段的『深度变形-补全』(DWI) 管线的限制。这种范式存在误差传播、深度歧义以及并行与收敛立体配置之间的格式不一致问题。为了解决这些挑战，我们引入了UniStereo，这是首个用于立体视频转换的大规模统一数据集，涵盖两种立体格式，以实现公平的基准测试和稳健的模型训练。基于此数据集，我们提出了StereoPilot，这是一种高效的前馈模型，能够直接合成目标视角，而无需依赖显式的深度图或迭代扩散采样。StereoPilot配备了可学习的域切换器和循环一致性损失，能够无缝适应不同的立体格式，并实现更高的一致性。大量实验表明，StereoPilot在视觉保真度和计算效率方面均显著优于现有最先进的方法。项目页面：https://hit-perfect.github.io/StereoPilot/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid growth of stereoscopic displays, including VR headsets and 3D cinemas, has led to increasing demand for high-quality stereo video content.</div>
</details>
</div>
<div class="card">
<div class="title">Depth Any Panoramas: A Foundation Model for Panoramic Depth Estimation</div>
<div class="meta-line">Authors: Xin Lin, Meixi Song, Dizhe Zhang, Wenxuan Lu, Haodong Li, Bo Du, Ming-Hsuan Yang, Truong Nguyen, Lu Qi</div>
<div class="meta-line">First: 2025-12-18T18:59:29+00:00 · Latest: 2025-12-18T18:59:29+00:00</div>
<div class="meta-line">Comments: Project Page: https://insta360-research-team.github.io/DAP_website/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16913v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16913v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://insta360-research-team.github.io/DAP_website/">Project1</a> · <a href="https://insta360-research-team.github.io/DAP\_website/">Project2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances. We explore a data-in-the-loop paradigm from the view of both data construction and framework design. We collect a large-scale dataset by combining public datasets, high-quality synthetic data from our UE5 simulator and text-to-image models, and real panoramic images from the web. To reduce domain gaps between indoor/outdoor and synthetic/real data, we introduce a three-stage pseudo-label curation pipeline to generate reliable ground truth for unlabeled images. For the model, we adopt DINOv3-Large as the backbone for its strong pre-trained generalization, and introduce a plug-and-play range mask head, sharpness-centric optimization, and geometry-centric optimization to improve robustness to varying distances and enforce geometric consistency across views. Experiments on multiple benchmarks (e.g., Stanford2D3D, Matterport3D, and Deep360) demonstrate strong performance and zero-shot generalization, with particularly robust and stable metric predictions in diverse real-world scenes. The project page can be found at: \href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\_website/}</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度任意全景图：全景深度估计的基础模型</div>
<div class="mono" style="margin-top:8px">在这项工作中，我们提出了一种全景度量深度基础模型，能够跨多样化的场景距离进行泛化。我们从数据构建和框架设计两个角度探索了数据闭环范式。我们通过结合公开数据集、来自我们UE5模拟器和文本到图像模型的高质量合成数据，以及从网络上获取的真实全景图像，构建了一个大规模数据集。为减少室内/室外和合成/真实数据之间的领域差异，我们引入了一个三阶段伪标签整理流程，以生成未标记图像的可靠真实标签。在模型设计上，我们采用DINOv3-Large作为主干网络，因其强大的预训练泛化能力，并引入了一种即插即用的范围掩码头、以锐度为中心的优化和以几何为中心的优化，以提高对不同距离的鲁棒性，并在不同视角间强制几何一致性。在多个基准测试（如Stanford2D3D、Matterport3D和Deep360）上的实验表明，该模型具有强大的性能和零样本泛化能力，在多样化的现实场景中表现出特别稳健和稳定的度量预测。项目页面可访问：\href{https://insta360-research-team.github.io/DAP_website/} {https://insta360-research-team.github.io/DAP\_website/}</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this work, we present a panoramic metric depth foundation model that generalizes across diverse scene distances.</div>
</details>
</div>
<div class="card">
<div class="title">Exploration v.s. Exploitation: Rethinking RLVR through Clipping, Entropy, and Spurious Reward</div>
<div class="meta-line">Authors: Peter Chen, Xiaopeng Li, Ziniu Li, Wotao Yin, Xi Chen, Tianyi Lin</div>
<div class="meta-line">First: 2025-12-18T18:59:27+00:00 · Latest: 2025-12-18T18:59:27+00:00</div>
<div class="meta-line">Comments: 35 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16912v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16912v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper examines the exploration-exploitation trade-off in reinforcement learning with verifiable rewards (RLVR), a framework for improving the reasoning of Large Language Models (LLMs). Recent studies suggest that RLVR can elicit strong mathematical reasoning in LLMs through two seemingly paradoxical mechanisms: spurious rewards, which suppress exploitation by rewarding outcomes unrelated to the ground truth, and entropy minimization, which suppresses exploration by pushing the model toward more confident and deterministic outputs, highlighting a puzzling dynamic: both discouraging exploitation and discouraging exploration improve reasoning performance, yet the underlying principles that reconcile these effects remain poorly understood. We focus on two fundamental questions: (i) how policy entropy relates to performance, and (ii) whether spurious rewards yield gains, potentially through the interplay of clipping bias and model contamination. Our results show that clipping bias under spurious rewards reduces policy entropy, leading to more confident and deterministic outputs, while entropy minimization alone is insufficient for improvement. We further propose a reward-misalignment model explaining why spurious rewards can enhance performance beyond contaminated settings. Our findings clarify the mechanisms behind spurious-reward benefits and provide principles for more effective RLVR training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索与利用：通过截断、熵和虚假奖励重新思考可验证奖励强化学习（RLVR）</div>
<div class="mono" style="margin-top:8px">本文探讨了可验证奖励强化学习（RLVR）框架中探索与利用的权衡，该框架旨在提升大型语言模型（LLMs）的推理能力。近期研究表明，RLVR可以通过两种看似矛盾的机制促使LLMs产生强大的数学推理能力：虚假奖励通过奖励与真实结果无关的结果来抑制利用，而熵最小化则通过推动模型趋向更自信和确定性的输出来抑制探索，突显了一种令人困惑的动态：同时抑制利用和探索反而能提升推理表现，但这些效果背后的原理仍不明确。我们聚焦于两个基本问题：(i) 政策熵如何影响性能，以及 (ii) 虚假奖励是否能带来性能提升，可能通过截断偏差与模型污染之间的相互作用。我们的结果表明，在虚假奖励下截断偏差会降低政策熵，从而产生更自信和确定性的输出，而单独的熵最小化不足以带来性能提升。我们进一步提出了一种奖励错配模型，解释了为何虚假奖励能在污染环境下提升性能。我们的发现阐明了虚假奖励带来的性能提升机制，并为更有效的RLVR训练提供了指导原则。</div>
</details>
</div>
<div class="card">
<div class="title">Posterior Behavioral Cloning: Pretraining BC Policies for Efficient RL Finetuning</div>
<div class="meta-line">Authors: Andrew Wagenmaker, Perry Dong, Raymond Tsao, Chelsea Finn, Sergey Levine</div>
<div class="meta-line">First: 2025-12-18T18:59:17+00:00 · Latest: 2025-12-18T18:59:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16911v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16911v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Standard practice across domains from robotics to language is to first pretrain a policy on a large-scale demonstration dataset, and then finetune this policy, typically with reinforcement learning (RL), in order to improve performance on deployment domains. This finetuning step has proved critical in achieving human or super-human performance, yet while much attention has been given to developing more effective finetuning algorithms, little attention has been given to ensuring the pretrained policy is an effective initialization for RL finetuning. In this work we seek to understand how the pretrained policy affects finetuning performance, and how to pretrain policies in order to ensure they are effective initializations for finetuning. We first show theoretically that standard behavioral cloning (BC) -- which trains a policy to directly match the actions played by the demonstrator -- can fail to ensure coverage over the demonstrator&#x27;s actions, a minimal condition necessary for effective RL finetuning. We then show that if, instead of exactly fitting the observed demonstrations, we train a policy to model the posterior distribution of the demonstrator&#x27;s behavior given the demonstration dataset, we do obtain a policy that ensures coverage over the demonstrator&#x27;s actions, enabling more effective finetuning. Furthermore, this policy -- which we refer to as the posterior behavioral cloning (PostBC) policy -- achieves this while ensuring pretrained performance is no worse than that of the BC policy. We then show that PostBC is practically implementable with modern generative models in robotic control domains -- relying only on standard supervised learning -- and leads to significantly improved RL finetuning performance on both realistic robotic control benchmarks and real-world robotic manipulation tasks, as compared to standard behavioral cloning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>后验行为克隆：为高效强化学习微调预训练BC策略</div>
<div class="mono" style="margin-top:8px">在从机器人学到语言学等多个领域中，标准做法是首先在大规模演示数据集上预训练策略，然后通过强化学习（RL）进行微调，以提升在部署领域的性能。这一微调步骤在实现人类或超人类表现方面已被证明是关键的，然而尽管人们广泛关注如何开发更有效的微调算法，却很少关注确保预训练策略是有效的RL微调初始化。在本工作中，我们旨在理解预训练策略如何影响微调性能，并探索如何预训练策略以确保其作为微调的优良初始化。我们首先从理论上表明，标准行为克隆（BC）——即训练策略直接匹配演示者的行为——可能无法确保覆盖演示者的行为，这是有效RL微调的必要条件。随后我们证明，如果我们训练策略以建模演示者行为的后验分布，而不是精确拟合观察到的演示，就能获得一个确保覆盖演示者行为的策略，从而实现更有效的微调。此外，这种策略——我们称之为后验行为克隆（PostBC）策略——在保证预训练性能不劣于BC策略的同时，实现了这一目标。最后，我们展示了PostBC在机器人控制领域中可以通过现代生成模型进行实际实现——仅依赖标准监督学习——并在现实的机器人控制基准和实际机器人操作任务中显著提升了RL微调性能，相较于标准行为克隆。</div>
</details>
</div>
<div class="card">
<div class="title">SFTok: Bridging the Performance Gap in Discrete Tokenizers</div>
<div class="meta-line">Authors: Qihang Rao, Borui Zhang, Wenzhao Zheng, Jie Zhou, Jiwen Lu</div>
<div class="meta-line">First: 2025-12-18T18:59:04+00:00 · Latest: 2025-12-18T18:59:04+00:00</div>
<div class="meta-line">Comments: Under review. Code is available at https://github.com/Neur-IO/SFTok</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16910v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16910v1">PDF</a> · <a href="https://github.com/Neur-IO/SFTok">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation. By compressing images into compact latent representations, tokenizers enable generative models to operate in lower-dimensional spaces, thereby improving computational efficiency and reducing complexity. Discrete tokenizers naturally align with the autoregressive paradigm but still lag behind continuous ones, limiting their adoption in multimodal systems. To address this, we propose \textbf{SFTok}, a discrete tokenizer that incorporates a multi-step iterative mechanism for precise reconstruction. By integrating \textbf{self-forcing guided visual reconstruction} and \textbf{debias-and-fitting training strategy}, SFTok resolves the training-inference inconsistency in multi-step process, significantly enhancing image reconstruction quality. At a high compression rate of only 64 tokens per image, SFTok achieves state-of-the-art reconstruction quality on ImageNet (rFID = 1.21) and demonstrates exceptional performance in class-to-image generation tasks (gFID = 2.29).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SFTok: 缩小离散分词器在高分辨率图像生成中的性能差距</div>
<div class="mono" style="margin-top:8px">近期多模态模型的发展突显了图像分词在高分辨率图像生成中的关键作用。通过将图像压缩为紧凑的潜在表示，分词器使生成模型能够在低维空间中运行，从而提高计算效率并降低复杂度。离散分词器天然地与自回归范式相契合，但仍落后于连续分词器，限制了其在多模态系统中的应用。为了解决这一问题，我们提出了\textbf{SFTok}，一种结合多步迭代机制以实现精确重建的离散分词器。通过整合\textbf{自引导视觉重建}和\textbf{去偏倚与拟合训练策略}，SFTok解决了多步过程中的训练-推理不一致性，显著提升了图像重建质量。在仅需每张图像64个标记的高压缩率下，SFTok在ImageNet上实现了最先进的重建质量（rFID = 1.21），并在类别到图像生成任务中表现出色（gFID = 2.29）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advances in multimodal models highlight the pivotal role of image tokenization in high-resolution image generation.</div>
</details>
</div>
<div class="card">
<div class="title">MomaGraph: State-Aware Unified Scene Graphs with Vision-Language Model for Embodied Task Planning</div>
<div class="meta-line">Authors: Yuanchen Ju, Yongyuan Liang, Yen-Jen Wang, Nandiraju Gireesh, Yuanliang Ju, Seungjae Lee, Qiao Gu, Elvis Hsieh, Furong Huang, Koushil Sreenath</div>
<div class="meta-line">First: 2025-12-18T18:59:03+00:00 · Latest: 2025-12-18T18:59:03+00:00</div>
<div class="meta-line">Comments: 25 pages, 10 figures. Project page:https://hybridrobotics.github.io/MomaGraph/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16909v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16909v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hybridrobotics.github.io/MomaGraph/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mobile manipulators in households must both navigate and manipulate. This requires a compact, semantically rich scene representation that captures where objects are, how they function, and which parts are actionable. Scene graphs are a natural choice, yet prior work often separates spatial and functional relations, treats scenes as static snapshots without object states or temporal updates, and overlooks information most relevant for accomplishing the current task. To address these limitations, we introduce MomaGraph, a unified scene representation for embodied agents that integrates spatial-functional relationships and part-level interactive elements. However, advancing such a representation requires both suitable data and rigorous evaluation, which have been largely missing. We thus contribute MomaGraph-Scenes, the first large-scale dataset of richly annotated, task-driven scene graphs in household environments, along with MomaGraph-Bench, a systematic evaluation suite spanning six reasoning capabilities from high-level planning to fine-grained scene understanding. Built upon this foundation, we further develop MomaGraph-R1, a 7B vision-language model trained with reinforcement learning on MomaGraph-Scenes. MomaGraph-R1 predicts task-oriented scene graphs and serves as a zero-shot task planner under a Graph-then-Plan framework. Extensive experiments demonstrate that our model achieves state-of-the-art results among open-source models, reaching 71.6% accuracy on the benchmark (+11.4% over the best baseline), while generalizing across public benchmarks and transferring effectively to real-robot experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MomaGraph: 基于视觉语言模型的具身任务规划中具有状态感知的统一场景图</div>
<div class="mono" style="margin-top:8px">家庭中的移动操作机械臂必须同时进行导航和操作。这需要一种紧凑且语义丰富的场景表示，以捕捉物体的位置、功能以及哪些部分可以被操作。场景图是自然的选择，但以往的工作往往将空间关系和功能关系分开处理，将场景视为静态快照，忽略了物体状态和时间更新，也忽视了完成当前任务最相关的信息。为了解决这些限制，我们引入了MomaGraph，这是一种为具身智能体设计的统一场景表示，整合了空间-功能关系和部件级的交互元素。然而，要推进这种表示，需要合适的数据和严格的评估，而这在以往研究中大多缺失。因此，我们贡献了MomaGraph-Scenes，这是首个包含丰富标注、任务驱动的场景图的大规模数据集，以及MomaGraph-Bench，一个涵盖从高层规划到细粒度场景理解的六种推理能力的系统性评估套件。在此基础上，我们进一步开发了MomaGraph-R1，这是一个基于MomaGraph-Scenes进行强化学习训练的70亿参数视觉语言模型。MomaGraph-R1可以预测任务导向的场景图，并在基于图-然后-规划的框架下作为零样本任务规划器。大量实验表明，我们的模型在开源模型中取得了最先进的结果，在基准测试中达到71.6%的准确率（比最佳基线高11.4%），并且能够跨公共基准测试泛化，并有效迁移到真实机器人实验中。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Mobile manipulators in households must both navigate and manipulate.</div>
</details>
</div>
<div class="card">
<div class="title">SceneDiff: A Benchmark and Method for Multiview Object Change Detection</div>
<div class="meta-line">Authors: Yuqun Wu, Chih-hao Lin, Henry Che, Aditi Tiwari, Chuhang Zou, Shenlong Wang, Derek Hoiem</div>
<div class="meta-line">First: 2025-12-18T18:59:02+00:00 · Latest: 2025-12-18T18:59:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16908v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16908v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times. Detecting such changes is important for many applications, such as robotic tidying or construction progress and safety monitoring. A major challenge is that varying viewpoints can cause objects to falsely appear changed. We introduce SceneDiff Benchmark, the first multiview change detection benchmark with object instance annotations, comprising 350 diverse video pairs with thousands of changed objects. We also introduce the SceneDiff method, a new training-free approach for multiview object change detection that leverages pretrained 3D, segmentation, and image encoding models to robustly predict across multiple benchmarks. Our method aligns the captures in 3D, extracts object regions, and compares spatial and semantic region features to detect changes. Experiments on multi-view and two-view benchmarks demonstrate that our method outperforms existing approaches by large margins (94% and 37.4% relative AP improvements). The benchmark and code will be publicly released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SceneDiff：多视角物体变化检测的基准和方法</div>
<div class="mono" style="margin-top:8px">我们研究了在不同时间对同一场景进行两次捕获（图像或视频）之间识别新增、移除或移动物体的问题。检测此类变化对于许多应用非常重要，例如机器人整理或建筑进度与安全监控。一个主要挑战是不同的视角可能导致物体被错误地判断为发生变化。我们引入了SceneDiff基准，这是首个包含物体实例标注的多视角变化检测基准，包含350个多样化的视频对，涉及数千个变化的物体。我们还提出了SceneDiff方法，这是一种新的无需训练的多视角物体变化检测方法，利用预训练的3D、分割和图像编码模型，以稳健的方式在多个基准上进行预测。我们的方法在三维空间中对捕获进行对齐，提取物体区域，并比较空间和语义区域特征以检测变化。在多视角和双视角基准上的实验表明，我们的方法在现有方法的基础上取得了显著的性能提升（相对AP提升分别为94%和37.4%）。该基准和代码将公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We investigate the problem of identifying objects that have been added, removed, or moved between a pair of captures (images or videos) of the same scene at different times.</div>
</details>
</div>
<div class="card">
<div class="title">Flowing from Reasoning to Motion: Learning 3D Hand Trajectory Prediction from Egocentric Human Interaction Videos</div>
<div class="meta-line">Authors: Mingfei Chen, Yifan Wang, Zhengqin Li, Homanga Bharadhwaj, Yujin Chen, Chuan Qin, Ziyi Kou, Yuan Tian, Eric Whitmire, Rajinder Sodhi, Hrvoje Benko, Eli Shlizerman, Yue Liu</div>
<div class="meta-line">First: 2025-12-18T18:59:01+00:00 · Latest: 2025-12-18T18:59:01+00:00</div>
<div class="meta-line">Comments: Project website: https://egoman-project.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16907v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16907v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://egoman-project.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prior works on 3D hand trajectory prediction are constrained by datasets that decouple motion from semantic supervision and by models that weakly link reasoning and action. To address these, we first present the EgoMAN dataset, a large-scale egocentric dataset for interaction stage-aware 3D hand trajectory prediction with 219K 6DoF trajectories and 3M structured QA pairs for semantic, spatial, and motion reasoning. We then introduce the EgoMAN model, a reasoning-to-motion framework that links vision-language reasoning and motion generation via a trajectory-token interface. Trained progressively to align reasoning with motion dynamics, our approach yields accurate and stage-aware trajectories with generalization across real-world scenes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从推理到运动：从第一视角人类交互视频中学习3D手轨迹预测</div>
<div class="mono" style="margin-top:8px">先前的3D手轨迹预测工作受到数据集的限制，这些数据集将运动与语义监督分离，并且模型仅弱地连接推理与行动。为了解决这些问题，我们首先提出了EgoMAN数据集，这是一个大规模的第一视角数据集，用于交互阶段感知的3D手轨迹预测，包含219,000条6DoF轨迹和300万个结构化问答对，用于语义、空间和运动推理。随后，我们引入了EgoMAN模型，这是一个推理到运动的框架，通过轨迹-标记接口将视觉-语言推理与运动生成连接起来。我们的方法通过逐步训练对齐推理与运动动态，从而在真实场景中实现准确且阶段感知的轨迹预测。</div>
</details>
</div>
<div class="card">
<div class="title">VIVA: VLM-Guided Instruction-Based Video Editing with Reward Optimization</div>
<div class="meta-line">Authors: Xiaoyan Cong, Haotian Yang, Angtian Wang, Yizhi Wang, Yiding Yang, Canyu Zhang, Chongyang Ma</div>
<div class="meta-line">First: 2025-12-18T18:58:42+00:00 · Latest: 2025-12-18T18:58:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16906v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16906v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://viva-paper.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Instruction-based video editing aims to modify an input video according to a natural-language instruction while preserving content fidelity and temporal coherence. However, existing diffusion-based approaches are often trained on paired data of simple editing operations, which fundamentally limits their ability to generalize to diverse and complex, real-world instructions. To address this generalization gap, we propose VIVA, a scalable framework for instruction-based video editing that leverages VLM-guided encoding and reward optimization. First, we introduce a VLM-based instructor that encodes the textual instruction, the first frame of the source video, and an optional reference image into visually-grounded instruction representations, providing fine-grained spatial and semantic context for the diffusion transformer backbone. Second, we propose a post-training stage, Edit-GRPO, which adapts Group Relative Policy Optimization to the domain of video editing, directly optimizing the model for instruction-faithful, content-preserving, and aesthetically pleasing edits using relative rewards. Furthermore, we propose a data construction pipeline designed to synthetically generate diverse, high-fidelity paired video-instruction data of basic editing operations. Extensive experiments show that VIVA achieves superior instruction following, generalization, and editing quality over state-of-the-art methods. Website: https://viva-paper.github.io</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VIVA：基于视觉语言模型引导的指令视频编辑与奖励优化</div>
<div class="mono" style="margin-top:8px">基于指令的视频编辑旨在根据自然语言指令修改输入视频，同时保持内容保真度和时间连贯性。然而，现有的扩散模型方法通常基于简单的编辑操作配对数据进行训练，这从根本上限制了它们对多样且复杂的现实指令的泛化能力。为了解决这一泛化差距，我们提出了VIVA，一个可扩展的基于指令的视频编辑框架，利用视觉语言模型引导的编码和奖励优化。首先，我们引入了一个基于VLM的指令生成器，将文本指令、源视频的第一帧以及可选的参考图像编码为视觉基础的指令表示，为扩散Transformer主干提供细粒度的空间和语义上下文。其次，我们提出了一种后训练阶段Edit-GRPO，将Group Relative Policy Optimization应用于视频编辑领域，直接通过相对奖励优化模型，以实现符合指令、保持内容并具有审美吸引力的编辑。此外，我们还提出了一种数据构建流程，旨在合成多样化、高保真的基础编辑操作视频-指令配对数据。大量实验表明，VIVA在指令遵循、泛化能力和编辑质量方面均优于现有最先进的方法。网站：https://viva-paper.github.io</div>
</details>
</div>
<div class="card">
<div class="title">Alchemist: Unlocking Efficiency in Text-to-Image Model Training via Meta-Gradient Data Selection</div>
<div class="meta-line">Authors: Kaixin Ding, Yang Zhou, Xi Chen, Miao Yang, Jiarong Ou, Rui Chen, Xin Tao, Hengshuang Zhao</div>
<div class="meta-line">First: 2025-12-18T18:57:58+00:00 · Latest: 2025-12-18T18:57:58+00:00</div>
<div class="meta-line">Comments: project page: https://kxding.github.io/project/Alchemist/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16905v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16905v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://kxding.github.io/project/Alchemist/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in Text-to-Image (T2I) generative models, such as Imagen, Stable Diffusion, and FLUX, have led to remarkable improvements in visual quality. However, their performance is fundamentally limited by the quality of training data. Web-crawled and synthetic image datasets often contain low-quality or redundant samples, which lead to degraded visual fidelity, unstable training, and inefficient computation. Hence, effective data selection is crucial for improving data efficiency. Existing approaches rely on costly manual curation or heuristic scoring based on single-dimensional features in Text-to-Image data filtering. Although meta-learning based method has been explored in LLM, there is no adaptation for image modalities. To this end, we propose **Alchemist**, a meta-gradient-based framework to select a suitable subset from large-scale text-image data pairs. Our approach automatically learns to assess the influence of each sample by iteratively optimizing the model from a data-centric perspective. Alchemist consists of two key stages: data rating and data pruning. We train a lightweight rater to estimate each sample&#x27;s influence based on gradient information, enhanced with multi-granularity perception. We then use the Shift-Gsampling strategy to select informative subsets for efficient model training. Alchemist is the first automatic, scalable, meta-gradient-based data selection framework for Text-to-Image model training. Experiments on both synthetic and web-crawled datasets demonstrate that Alchemist consistently improves visual quality and downstream performance. Training on an Alchemist-selected 50% of the data can outperform training on the full dataset.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>炼金术：通过元梯度数据选择提升文本到图像模型训练效率</div>
<div class="mono" style="margin-top:8px">近年来，文本到图像（T2I）生成模型如Imagen、Stable Diffusion和FLUX在视觉质量方面取得了显著进展。然而，其性能从根本上受到训练数据质量的限制。网络爬取和合成图像数据集通常包含低质量或冗余的样本，导致视觉保真度下降、训练不稳定和计算效率低下。因此，有效的数据选择对于提升数据效率至关重要。现有方法依赖于昂贵的人工标注或基于文本到图像数据过滤中单维特征的启发式评分。尽管在大语言模型（LLM）中已有基于元学习的方法，但尚未针对图像模态进行适配。为此，我们提出**Alchemist**，一个基于元梯度的框架，用于从大规模文本-图像数据对中选择合适的子集。我们的方法通过从数据为中心的角度迭代优化模型，自动学习评估每个样本的影响。Alchemist包含两个关键阶段：数据评分和数据修剪。我们训练一个轻量级评分器，基于梯度信息并结合多粒度感知来估计每个样本的影响。随后，我们使用Shift-Gsampling策略选择信息量大的子集以实现高效的模型训练。Alchemist是首个自动、可扩展、基于元梯度的数据选择框架，用于文本到图像模型训练。在合成数据集和网络爬取数据集上的实验表明，Alchemist能够持续提升视觉质量和下游任务性能。使用Alchemist选择的50%数据进行训练，其效果可优于使用完整数据集进行训练。</div>
</details>
</div>
<div class="card">
<div class="title">In-Context Algebra</div>
<div class="meta-line">Authors: Eric Todd, Jannik Brinkmann, Rohit Gandikota, David Bau</div>
<div class="meta-line">First: 2025-12-18T18:56:50+00:00 · Latest: 2025-12-18T18:56:50+00:00</div>
<div class="meta-line">Comments: 28 pages, 18 figures. Code and data at https://algebra.baulab.info</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16902v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16902v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate the mechanisms that arise when transformers are trained to solve arithmetic on sequences where tokens are variables whose meaning is determined only through their interactions. While prior work has found that transformers develop geometric embeddings that mirror algebraic structure, those previous findings emerge from settings where arithmetic-valued tokens have fixed meanings. We devise a new task in which the assignment of symbols to specific algebraic group elements varies from one sequence to another. Despite this challenging setup, transformers achieve near-perfect accuracy on the task and even generalize to unseen algebraic groups. We develop targeted data distributions to create causal tests of a set of hypothesized mechanisms, and we isolate three mechanisms models consistently learn: commutative copying where a dedicated head copies answers, identity element recognition that distinguishes identity-containing facts, and closure-based cancellation that tracks group membership to constrain valid answers. Complementary to the geometric representations found in fixed-symbol settings, our findings show that models develop symbolic reasoning mechanisms when trained to reason in-context with variables whose meanings are not fixed.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>上下文代数</div>
<div class="mono" style="margin-top:8px">我们研究了当变压器被训练以在序列上解决算术问题时，出现的机制。这些序列中的标记是变量，其含义仅通过相互作用确定。尽管先前的研究发现变压器会发展出几何嵌入，这些嵌入反映了代数结构，但这些发现来自于算术值标记具有固定含义的环境。我们设计了一个新任务，其中符号到特定代数群元素的映射在不同序列中有所不同。尽管这种设置具有挑战性，变压器在该任务上仍能实现接近完美的准确率，并且甚至能推广到未见过的代数群。我们开发了有针对性的数据分布，以创建一组假设机制的因果测试，并隔离出三个模型始终学习的机制：专门的复制头复制答案、识别单位元素以区分包含单位元素的事实，以及基于闭合性的消去机制，通过跟踪群成员关系来限制有效答案。我们的研究结果表明，当模型被训练以在变量含义不固定的情况下进行上下文推理时，它们会发展出符号推理机制，这与固定符号设置中的几何表示相辅相成。</div>
</details>
</div>
<div class="card">
<div class="title">Impacts of Racial Bias in Historical Training Data for News AI</div>
<div class="meta-line">Authors: Rahul Bhargava, Malene Hornstrup Jespersen, Emily Boardman Ndulue, Vivica Dsouza</div>
<div class="meta-line">First: 2025-12-18T18:56:11+00:00 · Latest: 2025-12-18T18:56:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16901v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16901v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings. These models, trained on extant data from various sources, can be conceptualized as historical artifacts that encode decades-old attitudes and stereotypes. This paper investigates one such example trained on the broadly-used New York Times Annotated Corpus to create a multi-label classifier. Our use in research settings surfaced the concerning &quot;blacks&quot; thematic topic label. Through quantitative and qualitative means we investigate this label&#x27;s use in the training corpus, what concepts it might be encoding in the trained classifier, and how those concepts impact our model use. Via the application of explainable AI methods, we find that the &quot;blacks&quot; label operates partially as a general &quot;racism detector&quot; across some minoritized groups. However, it performs poorly against expectations on modern examples such as COVID-19 era anti-Asian hate stories, and reporting on the Black Lives Matter movement. This case study of interrogating embedded biases in a model reveals how similar applications in newsroom settings can lead to unexpected outputs that could impact a wide variety of potential uses of any large language model-story discovery, audience targeting, summarization, etc. The fundamental tension this exposes for newsrooms is how to adopt AI-enabled workflow tools while reducing the risk of reproducing historical biases in news coverage.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>新闻AI历史训练数据中的种族偏见影响</div>
<div class="mono" style="margin-top:8px">人工智能技术已迅速应用于涉及大规模文本语料库的商业和研究领域，包括计算新闻学研究和新闻编辑室环境。这些模型基于来自不同来源的现有数据进行训练，可以被概念化为承载数十年来态度和刻板印象的历史文物。本文研究了一个基于广泛使用的《纽约时报》注释语料库训练的示例，旨在创建一个多标签分类器。我们在研究环境中使用该模型时，发现了一个令人担忧的&quot;黑人&quot;主题标签。通过定量和定性方法，我们探讨了该标签在训练语料库中的使用情况，它可能在训练后的分类器中编码的概念，以及这些概念如何影响我们的模型应用。通过应用可解释AI方法，我们发现&quot;黑人&quot;标签在某些少数族裔群体中部分充当了一种通用的&quot;种族主义检测器&quot;。然而，它在现代案例（如新冠疫情时期的反亚裔仇恨言论和对黑命攸关运动的报道）上的表现却令人失望。这项对模型中嵌入偏见的案例研究揭示了在新闻编辑室环境中类似应用可能导致意外输出，这些输出可能影响任何大型语言模型的广泛应用，如故事发现、受众定位、摘要生成等。本文揭示了新闻编辑室在采用AI驱动的工作流程工具时所面临的根本性矛盾，即如何在减少新闻报道中再现历史偏见风险的同时，充分利用AI技术。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">AI technologies have rapidly moved into business and research applications that involve large text corpora, including computational journalism research and newsroom settings.</div>
</details>
</div>
<div class="card">
<div class="title">FlashPortrait: 6x Faster Infinite Portrait Animation with Adaptive Latent Prediction</div>
<div class="meta-line">Authors: Shuyuan Tu, Yueming Pan, Yinming Huang, Xintong Han, Zhen Xing, Qi Dai, Kai Qiu, Chong Luo, Zuxuan Wu</div>
<div class="meta-line">First: 2025-12-18T18:56:05+00:00 · Latest: 2025-12-18T18:56:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16900v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16900v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current diffusion-based acceleration methods for long-portrait animation struggle to ensure identity (ID) consistency. This paper presents FlashPortrait, an end-to-end video diffusion transformer capable of synthesizing ID-preserving, infinite-length videos while achieving up to 6x acceleration in inference speed. In particular, FlashPortrait begins by computing the identity-agnostic facial expression features with an off-the-shelf extractor. It then introduces a Normalized Facial Expression Block to align facial features with diffusion latents by normalizing them with their respective means and variances, thereby improving identity stability in facial modeling. During inference, FlashPortrait adopts a dynamic sliding-window scheme with weighted blending in overlapping areas, ensuring smooth transitions and ID consistency in long animations. In each context window, based on the latent variation rate at particular timesteps and the derivative magnitude ratio among diffusion layers, FlashPortrait utilizes higher-order latent derivatives at the current timestep to directly predict latents at future timesteps, thereby skipping several denoising steps and achieving 6x speed acceleration. Experiments on benchmarks show the effectiveness of FlashPortrait both qualitatively and quantitatively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlashPortrait: 6倍加速的无限人像动画，具有自适应潜在预测</div>
<div class="mono" style="margin-top:8px">当前基于扩散的长人像动画加速方法难以保证身份（ID）一致性。本文提出FlashPortrait，这是一种端到端的视频扩散Transformer，能够在保持身份一致性的前提下合成无限长度的视频，并在推理速度上实现最高6倍的加速。具体而言，FlashPortrait首先使用现成的提取器计算与身份无关的面部表情特征。随后引入归一化面部表情块，通过用各自均值和方差对特征进行归一化，将面部特征与扩散潜在空间对齐，从而提升面部建模的身份稳定性。在推理过程中，FlashPortrait采用动态滑动窗口方案，在重叠区域进行加权融合，确保长动画的平滑过渡和身份一致性。在每个上下文窗口中，基于特定时间步的潜在变化率以及扩散层之间的导数幅度比，FlashPortrait利用当前时间步的高阶潜在导数直接预测未来时间步的潜在值，从而跳过多个去噪步骤，实现6倍的速度加速。在基准测试上的实验表明，FlashPortrait在定性和定量方面均有效。</div>
</details>
</div>
<div class="card">
<div class="title">Multimodal RewardBench 2: Evaluating Omni Reward Models for Interleaved Text and Image</div>
<div class="meta-line">Authors: Yushi Hu, Reyhane Askari-Hemmat, Melissa Hall, Emily Dinan, Luke Zettlemoyer, Marjan Ghazvininejad</div>
<div class="meta-line">First: 2025-12-18T18:56:04+00:00 · Latest: 2025-12-18T18:56:04+00:00</div>
<div class="meta-line">Comments: Code and data available at https://github.com/facebookresearch/MMRB2</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16899v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16899v1">PDF</a> · <a href="https://github.com/facebookresearch/MMRB2">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences. We introduce Multimodal RewardBench 2 (MMRB2), the first comprehensive benchmark for reward models on multimodal understanding and (interleaved) generation. MMRB2 spans four tasks: text-to-image, image editing, interleaved generation, and multimodal reasoning (&quot;thinking-with-images&quot;), providing 1,000 expert-annotated preference pairs per task from 23 models and agents across 21 source tasks. MMRB2 is designed with: (1) practical but challenging prompts; (2) responses from state-of-the-art models and agents; and (3) preference pairs with strong human-expert consensus, curated via an ensemble filtering strategy. Using MMRB2, we study existing judges for each subtask, including multimodal LLM-as-a-judge and models trained with human preferences. The latest Gemini 3 Pro attains 75-80% accuracy. GPT-5 and Gemini 2.5 Pro reach 66-75% accuracy, compared to &gt;90% for humans, yet surpass the widely used GPT-4o (59%). The best performing open-source model Qwen3-VL-32B achieves similar accuracies as Gemini 2.5 Flash (64%). We also show that MMRB2 performance strongly correlates with downstream task success using Best-of-N sampling and conduct an in-depth analysis that shows key areas to improve the reward models going forward.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多模态RewardBench 2：评估处理交错文本和图像的通用奖励模型</div>
<div class="mono" style="margin-top:8px">奖励模型（RMs）对于训练大型语言模型（LLMs）至关重要，但在处理交错图像和文本序列的通用模型中仍被较少研究。我们引入了多模态RewardBench 2（MMRB2），这是首个针对多模态理解和（交错）生成任务的奖励模型综合基准。MMRB2涵盖四个任务：文本到图像生成、图像编辑、交错生成以及多模态推理（&quot;图像思维&quot;），每个任务提供了来自21个源任务、23个模型和代理的1,000对专家标注的偏好对。MMRB2的设计包括：（1）实用但具有挑战性的提示；（2）来自最先进模型和代理的响应；以及（3）通过集成过滤策略精选的具有强烈人类专家共识的偏好对。利用MMRB2，我们研究了每个子任务中现有的评估者，包括多模态LLM作为评估者以及基于人类偏好训练的模型。最新版的Gemini 3 Pro达到了75-80%的准确率。GPT-5和Gemini 2.5 Pro的准确率分别为66-75%，相比人类的&gt;90%仍有差距，但超过了广泛使用的GPT-4o（59%）。表现最好的开源模型Qwen3-VL-32B的准确率与Gemini 2.5 Flash相似（64%）。我们还展示了MMRB2的性能与下游任务的成功率之间存在强相关性，并通过Best-of-N采样进行了深入分析，揭示了未来改进奖励模型的关键领域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reward models (RMs) are essential for training large language models (LLMs), but remain underexplored for omni models that handle interleaved image and text sequences.</div>
</details>
</div>
<div class="card">
<div class="title">Sceniris: A Fast Procedural Scene Generation Framework</div>
<div class="meta-line">Authors: Jinghuan Shang, Harsh Patel, Ran Gong, Karl Schmeckpeper</div>
<div class="meta-line">First: 2025-12-18T18:55:03+00:00 · Latest: 2025-12-18T18:55:03+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/rai-inst/sceniris</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16896v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16896v1">PDF</a> · <a href="https://github.com/rai-inst/sceniris">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Synthetic 3D scenes are essential for developing Physical AI and generative models. Existing procedural generation methods often have low output throughput, creating a significant bottleneck in scaling up dataset creation. In this work, we introduce Sceniris, a highly efficient procedural scene generation framework for rapidly generating large-scale, collision-free scene variations. Sceniris also provides an optional robot reachability check, providing manipulation-feasible scenes for robot tasks. Sceniris is designed for maximum efficiency by addressing the primary performance limitations of the prior method, Scene Synthesizer. Leveraging batch sampling and faster collision checking in cuRobo, Sceniris achieves at least 234x speed-up over Scene Synthesizer. Sceniris also expands the object-wise spatial relationships available in prior work to support diverse scene requirements. Our code is available at https://github.com/rai-inst/sceniris</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Sceniris：一种快速的场景生成框架</div>
<div class="mono" style="margin-top:8px">合成三维场景对于开发物理AI和生成模型至关重要。现有的程序化生成方法通常输出吞吐量较低，成为扩展数据集创建的主要瓶颈。在本工作中，我们引入了Sceniris，这是一种高度高效的程序化场景生成框架，能够快速生成大规模、无碰撞的场景变体。Sceniris还提供可选的机器人可达性检查，为机器人任务生成可操作的场景。Sceniris通过解决先前方法Scene Synthesizer的主要性能限制，实现了最大效率的设计。借助cuRobo中的批量采样和更快的碰撞检测，Sceniris在Scene Synthesizer的基础上实现了至少234倍的速度提升。此外，Sceniris还扩展了先前工作中可用的物体间空间关系，以支持多样化的场景需求。我们的代码可在https://github.com/rai-inst/sceniris获取。</div>
</details>
</div>
<div class="card">
<div class="title">Instant Expressive Gaussian Head Avatar via 3D-Aware Expression Distillation</div>
<div class="meta-line">Authors: Kaiwen Jiang, Xueting Li, Seonwook Park, Ravi Ramamoorthi, Shalini De Mello, Koki Nagano</div>
<div class="meta-line">First: 2025-12-18T18:53:28+00:00 · Latest: 2025-12-18T18:53:28+00:00</div>
<div class="meta-line">Comments: Project website is https://research.nvidia.com/labs/amri/projects/instant4d</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16893v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16893v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://research.nvidia.com/labs/amri/projects/instant4d">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Portrait animation has witnessed tremendous quality improvements thanks to recent advances in video diffusion models. However, these 2D methods often compromise 3D consistency and speed, limiting their applicability in real-world scenarios, such as digital twins or telepresence. In contrast, 3D-aware facial animation feedforward methods -- built upon explicit 3D representations, such as neural radiance fields or Gaussian splatting -- ensure 3D consistency and achieve faster inference speed, but come with inferior expression details. In this paper, we aim to combine their strengths by distilling knowledge from a 2D diffusion-based method into a feed-forward encoder, which instantly converts an in-the-wild single image into a 3D-consistent, fast yet expressive animatable representation. Our animation representation is decoupled from the face&#x27;s 3D representation and learns motion implicitly from data, eliminating the dependency on pre-defined parametric models that often constrain animation capabilities. Unlike previous computationally intensive global fusion mechanisms (e.g., multiple attention layers) for fusing 3D structural and animation information, our design employs an efficient lightweight local fusion strategy to achieve high animation expressivity. As a result, our method runs at 107.31 FPS for animation and pose control while achieving comparable animation quality to the state-of-the-art, surpassing alternative designs that trade speed for quality or vice versa. Project website is https://research.nvidia.com/labs/amri/projects/instant4d</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于3D感知表情蒸馏的即时表达高斯头像</div>
<div class="mono" style="margin-top:8px">得益于近期视频扩散模型的进展，人像动画在质量上取得了显著提升。然而，这些2D方法通常会牺牲3D一致性与速度，限制了其在现实场景中的应用，例如数字孪生或远程临场。相比之下，基于显式3D表示（如神经辐射场或高斯点云）的3D感知面部动画前馈方法能够确保3D一致性并实现更快的推理速度，但表情细节较差。本文旨在通过将2D扩散方法的知识蒸馏到一个前馈编码器中，结合两者的优势，从而即时地将野外采集的单张图像转换为具有3D一致性、快速且富有表现力的可动画表示。我们的动画表示与面部的3D表示解耦，并通过数据隐式学习运动，消除了对预定义参数化模型的依赖，这些模型通常会限制动画能力。与之前计算密集型的全局融合机制（如多个注意力层）不同，我们的设计采用了一种高效轻量的局部融合策略，以实现高动画表现力。因此，我们的方法在动画和姿态控制上达到了107.31 FPS的速度，同时动画质量可与当前最先进的方法相媲美，超越了那些在速度与质量之间进行权衡的替代设计。项目网站为 https://research.nvidia.com/labs/amri/projects/instant4d</div>
</details>
</div>
<div class="card">
<div class="title">LinkedOut: Linking World Knowledge Representation Out of Video LLM for Next-Generation Video Recommendation</div>
<div class="meta-line">Authors: Haichao Zhang, Yao Lu, Lichen Wang, Yunzhe Li, Daiwei Chen, Yunpeng Xu, Yun Fu</div>
<div class="meta-line">First: 2025-12-18T18:52:18+00:00 · Latest: 2025-12-18T18:52:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16891v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16891v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering. However, deploying VLLMs for downstream tasks such as video recommendation remains challenging, since real systems require multi-video inputs, lightweight backbones, low-latency sequential inference, and rapid response. In practice, (1) decode-only generation yields high latency for sequential inference, (2) typical interfaces do not support multi-video inputs, and (3) constraining outputs to language discards fine-grained visual details that matter for downstream vision tasks. We argue that these limitations stem from the absence of a representation that preserves pixel-level detail while leveraging world knowledge. We present LinkedOut, a representation that extracts VLLM world knowledge directly from video to enable fast inference, supports multi-video histories, and removes the language bottleneck. LinkedOut extracts semantically grounded, knowledge-aware tokens from raw frames using VLLMs, guided by promptable queries and optional auxiliary modalities. We introduce a cross-layer knowledge fusion MoE that selects the appropriate level of abstraction from the rich VLLM features, enabling personalized, interpretable, and low-latency recommendation. To our knowledge, LinkedOut is the first VLLM-based video recommendation method that operates on raw frames without handcrafted labels, achieving state-of-the-art results on standard benchmarks. Interpretability studies and ablations confirm the benefits of layer diversity and layer-wise fusion, pointing to a practical path that fully leverages VLLM world-knowledge priors and visual reasoning for downstream vision tasks such as recommendation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LinkedOut：从视频大语言模型中提取世界知识表示以实现下一代视频推荐</div>
<div class="mono" style="margin-top:8px">视频大语言模型（VLLMs）通过在互联网规模数据上进行预训练，解锁了具有世界知识感知能力的视频理解。然而，将VLLMs部署到下游任务如视频推荐中仍然具有挑战性，因为实际系统需要多视频输入、轻量级主干网络、低延迟的序列推理和快速响应。在实践中，(1) 仅解码生成会导致序列推理高延迟，(2) 典型接口不支持多视频输入，(3) 将输出限制为语言则会丢弃对下游视觉任务至关重要的细粒度视觉细节。我们认为这些限制源于缺乏一种在利用世界知识的同时保留像素级细节的表示。我们提出了LinkedOut，一种从视频中直接提取VLLM世界知识的表示方法，支持多视频历史记录，并消除了语言瓶颈。LinkedOut通过可提示查询和可选的辅助模态，从原始帧中提取语义基础、知识感知的标记。我们引入了一种跨层知识融合的Mixture of Experts（MoE），从丰富的VLLM特征中选择适当的抽象层次，从而实现个性化、可解释且低延迟的推荐。据我们所知，LinkedOut是首个基于VLLM、无需手工标注标签即可在原始帧上运行的视频推荐方法，在标准基准测试中取得了最先进的结果。可解释性研究和消融实验验证了层多样性与层间融合的优势，指明了一条充分利用VLLM世界知识先验和视觉推理的实用路径，以实现如推荐等下游视觉任务。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Video Large Language Models (VLLMs) unlock world-knowledge-aware video understanding through pretraining on internet-scale data and have already shown promise on tasks such as movie analysis and video question answering.</div>
</details>
</div>
<div class="card">
<div class="title">Team Westwood Solution for MIDOG 2025 Challenge: An Ensemble-CNN-Based Approach For Mitosis Detection And Classification</div>
<div class="meta-line">Authors: Tengyou Xu, Haochen Yang, Xiang &#x27;Anthony&#x27; Chen, Hongyan Gu, Mohammad Haeri</div>
<div class="meta-line">First: 2025-08-29T17:07:10+00:00 · Latest: 2025-12-18T18:50:29+00:00</div>
<div class="meta-line">Comments: To appear Lecture Notes in Computer Science</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.02600v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.02600v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This abstract presents our solution (Team Westwood) for mitosis detection and atypical mitosis classification in the MItosis DOmain Generalization (MIDOG) 2025 challenge. For mitosis detection, we trained an nnUNetV2 for initial mitosis candidate screening with high sensitivity, followed by a random forest classifier ensembling predictions of three convolutional neural networks (CNNs): EfficientNet-b3, EfficientNet-b5, and EfficientNetV2-s. For the atypical mitosis classification, we trained another random forest classifier ensembling the predictions of three CNNs: EfficientNet-b3, EfficientNet-b5, and InceptionV3. On the preliminary test set, our solution achieved an F1 score of 0.7450 for track 1 mitosis detection, and a balanced accuracy of 0.8722 for track 2 atypical mitosis classification. On the final test set, our solution achieved an F1 score of 0.6972 for track 1 mitosis detection, and a balanced accuracy of 0.8242 for track 2 atypical mitosis classification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Team Westwood 解决方案：MIDOG 2025 挑战赛中的有丝分裂检测与分类：基于集成卷积神经网络的方法</div>
<div class="mono" style="margin-top:8px">本文介绍了我们（Team Westwood）在 MItosis DOmain Generalization (MIDOG) 2025 挑战赛中针对有丝分裂检测和非典型有丝分裂分类的解决方案。对于有丝分裂检测，我们使用 nnUNetV2 进行初始有丝分裂候选筛选，具有高灵敏度，随后使用随机森林分类器集成三个卷积神经网络（CNNs）的预测结果：EfficientNet-b3、EfficientNet-b5 和 EfficientNetV2-s。对于非典型有丝分裂分类，我们训练了另一个随机森林分类器，集成三个 CNNs 的预测结果：EfficientNet-b3、EfficientNet-b5 和 InceptionV3。在初步测试集上，我们的解决方案在 track 1 有丝分裂检测中取得了 0.7450 的 F1 分数，在 track 2 非典型有丝分裂分类中取得了 0.8722 的平衡准确率。在最终测试集上，我们的解决方案在 track 1 有丝分裂检测中取得了 0.6972 的 F1 分数，在 track 2 非典型有丝分裂分类中取得了 0.8242 的平衡准确率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This abstract presents our solution (Team Westwood) for mitosis detection and atypical mitosis classification in the MItosis DOmain Generalization (MIDOG) 2025 challenge.</div>
</details>
</div>
<div class="card">
<div class="title">M-PhyGs: Multi-Material Object Dynamics from Video</div>
<div class="meta-line">Authors: Norika Wada, Kohei Yamashita, Ryo Kawahara, Ko Nishino</div>
<div class="meta-line">First: 2025-12-18T18:50:08+00:00 · Latest: 2025-12-18T18:50:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16885v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16885v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Knowledge of the physical material properties governing the dynamics of a real-world object becomes necessary to accurately anticipate its response to unseen interactions. Existing methods for estimating such physical material parameters from visual data assume homogeneous single-material objects, pre-learned dynamics, or simplistic topologies. Real-world objects, however, are often complex in material composition and geometry lying outside the realm of these assumptions. In this paper, we particularly focus on flowers as a representative common object. We introduce Multi-material Physical Gaussians (M-PhyGs) to estimate the material composition and parameters of such multi-material complex natural objects from video. From a short video captured in a natural setting, M-PhyGs jointly segments the object into similar materials and recovers their continuum mechanical parameters while accounting for gravity. M-PhyGs achieves this efficiently with newly introduced cascaded 3D and 2D losses, and by leveraging temporal mini-batching. We introduce a dataset, Phlowers, of people interacting with flowers as a novel platform to evaluate the accuracy of this challenging task of multi-material physical parameter estimation. Experimental results on Phlowers dataset demonstrate the accuracy and effectiveness of M-PhyGs and its components.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>M-PhyGs：从视频中估计多材料物体的动态</div>
<div class="mono" style="margin-top:8px">为了准确预测现实世界物体对未见过的交互的响应，需要了解其物理材料属性。现有的基于视觉数据估计此类物理材料参数的方法通常假设物体是单一均匀材料、预学习的动态或简单的拓扑结构。然而，现实世界中的物体往往在材料组成和几何结构上较为复杂，超出了这些假设的范围。本文特别以花作为具有代表性的常见物体进行研究，提出多材料物理高斯（M-PhyGs）方法，从视频中估计多材料复杂自然物体的材料组成和参数。M-PhyGs通过引入新的级联3D和2D损失函数，并利用时间维度的微批次处理，高效地从自然场景中捕获的短视频中对物体进行联合分割，恢复其连续力学参数并考虑重力影响。我们还引入了一个名为Phlowers的新数据集，作为评估多材料物理参数估计这一具有挑战性的任务的平台。实验结果表明，M-PhyGs及其组成部分在Phlowers数据集上具有较高的准确性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Cartesian-nj: Extending e3nn to Irreducible Cartesian Tensor Product and Contracion</div>
<div class="meta-line">Authors: Zemin Xu, Chenyu Wu, Wenbo Xie, Daiqian Xie, P. Hu</div>
<div class="meta-line">First: 2025-12-18T18:49:50+00:00 · Latest: 2025-12-18T18:49:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16882v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16882v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Equivariant atomistic machine learning models have brought substantial gains in both extrapolation capability and predictive accuracy. Depending on the basis of the space, two distinct types of irreducible representations are utilized. From architectures built upon spherical tensors (STs) to more recent formulations employing irreducible Cartesian tensors (ICTs), STs have remained dominant owing to their compactness, elegance, and theoretical completeness. Nevertheless, questions have persisted regarding whether ST constructions are the only viable design principle, motivating continued development of Cartesian networks. In this work, we introduce the Cartesian-3j and Cartesian-nj symbol, which serve as direct analogues of the Wigner-3j and Wigner-nj symbol defined for tensor coupling. These coefficients enable the combination of any two ICTs into a new ICT. Building on this foundation, we extend e3nn to support irreducible Cartesian tensor product, and we release the resulting Python package as cartnn. Within this framework, we implement Cartesian counterparts of MACE, NequIP, and Allegro, allowing the first systematic comparison of Cartesian and spherical models to assess whether Cartesian formulations may offer advantages under specific conditions. Using TACE as a representative example, we further examine whether architectures constructed from irreducible Cartesian tensor product and contraction(ICTP and ICTC) are conceptually well-founded in Cartesian space and whether opportunities remain for improving their design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>笛卡尔-nj：将e3nn扩展到不可约笛卡尔张量积和收缩</div>
<div class="mono" style="margin-top:8px">等变原子机器学习模型在插值能力和预测精度方面带来了显著提升。根据空间的基础，两种不同的不可约表示被使用。从基于球张量（STs）的架构到最近使用不可约笛卡尔张量（ICTs）的公式，STs因其紧凑性、优雅性和理论完整性而一直占据主导地位。然而，关于ST构建是否是唯一可行的设计原则的问题仍然存在，这推动了对笛卡尔网络的持续开发。在本工作中，我们引入了笛卡尔-3j和笛卡尔-nj符号，它们是张量耦合中定义的Wigner-3j和Wigner-nj符号的直接类比。这些系数能够将任意两个ICTs组合成一个新的ICT。在此基础上，我们将e3nn扩展以支持不可约笛卡尔张量积，并将结果作为Python包cartnn发布。在此框架中，我们实现了MACE、NequIP和Allegro的笛卡尔版本，从而允许首次对笛卡尔和球模型进行系统比较，以评估在特定条件下笛卡尔公式是否可能提供优势。以TACE为例，我们进一步探讨基于不可约笛卡尔张量积和收缩（ICTP和ICTC）的架构在笛卡尔空间中是否在概念上是合理的，以及其设计是否仍有改进空间。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Equivariant atomistic machine learning models have brought substantial gains in both extrapolation capability and predictive accuracy.</div>
</details>
</div>
<div class="card">
<div class="title">PolaRiS: Scalable Real-to-Sim Evaluations for Generalist Robot Policies</div>
<div class="meta-line">Authors: Arhan Jain, Mingtong Zhang, Kanav Arora, William Chen, Marcel Torne, Muhammad Zubair Irshad, Sergey Zakharov, Yue Wang, Sergey Levine, Chelsea Finn, Wei-Chiu Ma, Dhruv Shah, Abhishek Gupta, Karl Pertsch</div>
<div class="meta-line">First: 2025-12-18T18:49:41+00:00 · Latest: 2025-12-18T18:49:41+00:00</div>
<div class="meta-line">Comments: Website: https://polaris-evals.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16881v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16881v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://polaris-evals.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A significant challenge for robot learning research is our ability to accurately measure and compare the performance of robot policies. Benchmarking in robotics is historically challenging due to the stochasticity, reproducibility, and time-consuming nature of real-world rollouts. This challenge is exacerbated for recent generalist policies, which has to be evaluated across a wide variety of scenes and tasks. Evaluation in simulation offers a scalable complement to real world evaluations, but the visual and physical domain gap between existing simulation benchmarks and the real world has made them an unreliable signal for policy improvement. Furthermore, building realistic and diverse simulated environments has traditionally required significant human effort and expertise. To bridge the gap, we introduce Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS), a scalable real-to-sim framework for high-fidelity simulated robot evaluation. PolaRiS utilizes neural reconstruction methods to turn short video scans of real-world scenes into interactive simulation environments. Additionally, we develop a simple simulation data co-training recipe that bridges remaining real-to-sim gaps and enables zero-shot evaluation in unseen simulation environments. Through extensive paired evaluations between simulation and the real world, we demonstrate that PolaRiS evaluations provide a much stronger correlation to real world generalist policy performance than existing simulated benchmarks. Its simplicity also enables rapid creation of diverse simulated environments. As such, this work takes a step towards distributed and democratized evaluation for the next generation of robotic foundation models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PolaRiS：面向通用机器人策略的可扩展真实到仿真评估框架</div>
<div class="mono" style="margin-top:8px">机器人学习研究中的一个重大挑战是我们准确衡量和比较机器人策略性能的能力。由于现实世界中的 rollout 具有随机性、可重复性差且耗时，机器人领域的基准测试历来具有挑战性。这一挑战在近期的通用策略中尤为突出，因为这些策略需要在各种场景和任务中进行评估。仿真中的评估为现实世界评估提供了可扩展的补充，但现有仿真基准与现实世界之间在视觉和物理领域的差距，使其成为策略改进的不可靠信号。此外，构建真实且多样的仿真环境传统上需要大量的人工努力和专业知识。为弥合这一差距，我们引入了 Policy Evaluation and Environment Reconstruction in Simulation (PolaRiS)，这是一个可扩展的真实到仿真的框架，用于高保真度的机器人仿真评估。PolaRiS 利用神经重建方法，将现实场景的短视频扫描转换为可交互的仿真环境。此外，我们开发了一种简单的仿真数据协同训练方法，以弥合剩余的真实到仿真差距，并实现未见过的仿真环境中的零样本评估。通过大量现实与仿真之间的配对评估，我们证明了 PolaRiS 的评估结果与现实世界通用策略性能之间具有更强的相关性。其简单性也使得创建多样化的仿真环境更加高效。因此，这项工作朝着下一代机器人基础模型的分布式和民主化评估迈出了一步。</div>
</details>
</div>
<div class="card">
<div class="title">Memory-Enhanced SAM3 for Occlusion-Robust Surgical Instrument Segmentation</div>
<div class="meta-line">Authors: Valay Bundele, Mehran Hosseinzadeh, Hendrik P. A. Lensch</div>
<div class="meta-line">First: 2025-12-18T18:49:33+00:00 · Latest: 2025-12-18T18:49:33+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16880v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.16880v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://valaybundele.github.io/remedi-sam3/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate surgical instrument segmentation in endoscopic videos is crucial for computer-assisted interventions, yet remains challenging due to frequent occlusions, rapid motion, specular artefacts, and long-term instrument re-entry. While SAM3 provides a powerful spatio-temporal framework for video object segmentation, its performance in surgical scenes is limited by indiscriminate memory updates, fixed memory capacity, and weak identity recovery after occlusions. We propose ReMeDI-SAM3, a training-free memory-enhanced extension of SAM3, that addresses these limitations through three components: (i) relevance-aware memory filtering with a dedicated occlusion-aware memory for storing pre-occlusion frames, (ii) a piecewise interpolation scheme that expands the effective memory capacity, and (iii) a feature-based re-identification module with temporal voting for reliable post-occlusion identity disambiguation. Together, these components mitigate error accumulation and enable reliable recovery after occlusions. Evaluations on EndoVis17 and EndoVis18 under a zero-shot setting show absolute mcIoU improvements of around 7% and 16%, respectively, over vanilla SAM3, outperforming even prior training-based approaches. Project page: https://valaybundele.github.io/remedi-sam3/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向遮挡鲁棒的手术器械分割的增强型SAM3</div>
<div class="mono" style="margin-top:8px">内窥镜视频中准确的手术器械分割对于计算机辅助手术至关重要，但由于频繁的遮挡、快速运动、镜面伪影和长期器械重新进入，这一任务仍然具有挑战性。尽管SAM3为视频对象分割提供了一个强大的时空框架，但其在手术场景中的表现受到无差别记忆更新、固定记忆容量和遮挡后弱身份恢复的限制。我们提出了一种无需训练的记忆增强型ReMeDI-SAM3，通过三个组件解决上述问题：(i) 基于相关性的记忆过滤机制，结合专门用于存储遮挡前帧的遮挡感知记忆；(ii) 分段插值方案，扩展有效记忆容量；(iii) 基于特征的重新识别模块，结合时间投票实现可靠的遮挡后身份区分。这些组件共同抑制了误差累积，并实现了遮挡后的可靠恢复。在零样本设置下，我们在EndoVis17和EndoVis18数据集上的评估显示，ReMeDI-SAM3相比原始SAM3分别实现了约7%和16%的mcIoU提升，甚至优于先前的训练方法。项目页面：https://valaybundele.github.io/remedi-sam3/</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
