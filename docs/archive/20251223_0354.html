<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-23 03:54</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251223_0354</div>
    <div class="row"><div class="card">
<div class="title">Both Semantics and Reconstruction Matter: Making Representation Encoders Ready for Text-to-Image Generation and Editing</div>
<div class="meta-line">Authors: Shilong Zhang, He Zhang, Zhifei Zhang, Chongjian Ge, Shuchen Xue, Shaoteng Liu, Mengwei Ren, Soo Ye Kim, Yuqian Zhou, Qing Liu, Daniil Pakhomov, Kai Zhang, Zhe Lin, Ping Luo</div>
<div class="meta-line">First: 2025-12-19T18:59:57+00:00 · Latest: 2025-12-19T18:59:57+00:00</div>
<div class="meta-line">Comments: Project Page: https://jshilong.github.io/PS-VAE-PAGE/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17909v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17909v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://jshilong.github.io/PS-VAE-PAGE/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction. To unify vision generation and understanding, a burgeoning trend is to adopt high-dimensional features from representation encoders as generative latents. However, we empirically identify two fundamental obstacles in this paradigm: (1) the discriminative feature space lacks compact regularization, making diffusion models prone to off-manifold latents that lead to inaccurate object structures; and (2) the encoder&#x27;s inherently weak pixel-level reconstruction hinders the generator from learning accurate fine-grained geometry and texture. In this paper, we propose a systematic framework to adapt understanding-oriented encoder features for generative tasks. We introduce a semantic-pixel reconstruction objective to regularize the latent space, enabling the compression of both semantic information and fine-grained details into a highly compact representation (96 channels with 16x16 spatial downsampling). This design ensures that the latent space remains semantically rich and achieves state-of-the-art image reconstruction, while remaining compact enough for accurate generation. Leveraging this representation, we design a unified Text-to-Image (T2I) and image editing model. Benchmarking against various feature spaces, we demonstrate that our approach achieves state-of-the-art reconstruction, faster convergence, and substantial performance gains in both T2I and editing tasks, validating that representation encoders can be effectively adapted into robust generative components.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语义与重构同样重要：为文本到图像生成与编辑准备表示编码器</div>
<div class="mono" style="margin-top:8px">现代潜在扩散模型（LDMs）通常在低级变分自编码器（VAE）的潜在空间中运行，这些潜在空间主要优化了像素级重构。为了统一视觉生成与理解，一个新兴趋势是采用表示编码器的高维特征作为生成潜在变量。然而，我们通过实证发现这一范式存在两个根本性障碍：(1) 判别特征空间缺乏紧凑的正则化，导致扩散模型容易产生离流形的潜在变量，从而导致物体结构不准确；(2) 编码器在像素级重构上固有的弱性能阻碍了生成器学习准确的细粒度几何和纹理。在本文中，我们提出一个系统框架，以适应面向理解的编码器特征用于生成任务。我们引入了一个语义-像素重构目标，以正则化潜在空间，使得语义信息和细粒度细节能够被压缩到一个高度紧凑的表示中（96通道，16x16空间下采样）。这种设计确保了潜在空间在保持语义丰富性的同时，实现最先进的图像重构，并且足够紧凑以支持准确的生成。利用这一表示，我们设计了一个统一的文本到图像（T2I）和图像编辑模型。通过与多种特征空间的基准测试，我们证明了我们的方法在T2I和编辑任务中实现了最先进的重构效果、更快的收敛速度以及显著的性能提升，验证了表示编码器可以有效地被适配为稳健的生成组件。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modern Latent Diffusion Models (LDMs) typically operate in low-level Variational Autoencoder (VAE) latent spaces that are primarily optimized for pixel-level reconstruction.</div>
</details>
</div>
<div class="card">
<div class="title">Re-Depth Anything: Test-Time Depth Refinement via Self-Supervised Re-lighting</div>
<div class="meta-line">Authors: Ananta R. Bhattarai, Helge Rhodin</div>
<div class="meta-line">First: 2025-12-19T18:59:56+00:00 · Latest: 2025-12-19T18:59:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17908v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17908v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution. We introduce Re-Depth Anything, a test-time self-supervision framework that bridges this domain gap by fusing DA-V2 with the powerful priors of large-scale 2D diffusion models. Our method performs label-free refinement directly on the input image by re-lighting predicted depth maps and augmenting the input. This re-synthesis method replaces classical photometric reconstruction by leveraging shape from shading (SfS) cues in a new, generative context with Score Distillation Sampling (SDS). To prevent optimization collapse, our framework employs a targeted optimization strategy: rather than optimizing depth directly or fine-tuning the full model, we freeze the encoder and only update intermediate embeddings while also fine-tuning the decoder. Across diverse benchmarks, Re-Depth Anything yields substantial gains in depth accuracy and realism over the DA-V2, showcasing new avenues for self-supervision by augmenting geometric reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Re-Depth Anything：通过自监督重照明进行测试时深度细化</div>
<div class="mono" style="margin-top:8px">单目深度估计仍然具有挑战性，因为最近的基础模型，如Depth Anything V2（DA-V2），在远离训练分布的真实图像上表现不佳。我们引入了Re-Depth Anything，这是一个测试时自监督框架，通过将DA-V2与大规模2D扩散模型的强大先验知识结合，弥合这一领域差距。我们的方法通过重照明预测的深度图并增强输入图像，在无需标签的情况下直接对输入图像进行细化。这种方法利用Score Distillation Sampling（SDS）在新的生成上下文中，通过形状从光照（SfS）线索替代了传统的光度重建。为了防止优化崩溃，我们的框架采用了一种有针对性的优化策略：我们不直接优化深度或微调整个模型，而是冻结编码器，仅更新中间嵌入并同时微调解码器。在多个多样化的基准测试中，Re-Depth Anything在深度准确性和现实感方面显著优于DA-V2，展示了通过增强几何推理进行自监督的新途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Monocular depth estimation remains challenging as recent foundation models, such as Depth Anything V2 (DA-V2), struggle with real-world images that are far from the training distribution.</div>
</details>
</div>
<div class="card">
<div class="title">Dexterous World Models</div>
<div class="meta-line">Authors: Byungjun Kim, Taeksoo Kim, Junyoung Lee, Hanbyul Joo</div>
<div class="meta-line">First: 2025-12-19T18:59:51+00:00 · Latest: 2025-12-19T18:59:51+00:00</div>
<div class="meta-line">Comments: Project Page: snuvclab.github.io/dwm</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17907v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17907v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in 3D reconstruction has made it easy to create realistic digital twins from everyday environments. However, current digital twins remain largely static and are limited to navigation and view synthesis without embodied interactivity. To bridge this gap, we introduce Dexterous World Model (DWM), a scene-action-conditioned video diffusion framework that models how dexterous human actions induce dynamic changes in static 3D scenes.
  Given a static 3D scene rendering and an egocentric hand motion sequence, DWM generates temporally coherent videos depicting plausible human-scene interactions. Our approach conditions video generation on (1) static scene renderings following a specified camera trajectory to ensure spatial consistency, and (2) egocentric hand mesh renderings that encode both geometry and motion cues to model action-conditioned dynamics directly. To train DWM, we construct a hybrid interaction video dataset. Synthetic egocentric interactions provide fully aligned supervision for joint locomotion and manipulation learning, while fixed-camera real-world videos contribute diverse and realistic object dynamics.
  Experiments demonstrate that DWM enables realistic and physically plausible interactions, such as grasping, opening, and moving objects, while maintaining camera and scene consistency. This framework represents a first step toward video diffusion-based interactive digital twins and enables embodied simulation from egocentric actions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>灵巧世界模型</div>
<div class="mono" style="margin-top:8px">近年来，3D重建技术的进步使得从日常环境中创建逼真的数字孪生变得容易。然而，当前的数字孪生大多保持静态，仅限于导航和视图合成，缺乏具身交互能力。为弥合这一差距，我们引入了灵巧世界模型（Dexterous World Model, DWM），这是一个基于场景-动作-条件的视频扩散框架，用于建模灵巧的人类动作如何在静态3D场景中引发动态变化。给定一个静态3D场景渲染和一个以第一人称视角的手部运动序列，DWM可以生成时间上连贯的视频，展示合理的场景与人类的交互。我们的方法通过以下两个条件进行视频生成：(1) 按照指定相机轨迹的静态场景渲染，以确保空间一致性；(2) 编码几何和运动信息的第一人称视角手部网格渲染，直接建模基于动作的动态变化。为了训练DWM，我们构建了一个混合交互视频数据集。合成的第一人称视角交互提供了联合移动和操作学习的完全对齐监督，而固定相机的真实世界视频则贡献了多样且真实的物体动态。实验表明，DWM能够实现逼真且符合物理规律的交互，如抓取、打开和移动物体，同时保持相机和场景的一致性。该框架是基于视频扩散的交互式数字孪生的第一步，并能够从第一人称动作中实现具身模拟。</div>
</details>
</div>
<div class="card">
<div class="title">Adversarial Robustness of Vision in Open Foundation Models</div>
<div class="meta-line">Authors: Jonathon Fox, William J Buchanan, Pavlos Papadopoulos</div>
<div class="meta-line">Venue: IEEE Access, 2025</div>
<div class="meta-line">First: 2025-12-19T18:59:16+00:00 · Latest: 2025-12-19T18:59:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17902v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17902v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects. Thus, an adversary could aim to modify an image by adding unseen elements, which will confuse the AI in its recognition of an entity. This paper thus investigates the adversarial robustness of LLaVA-1.5-13B and Meta&#x27;s Llama 3.2 Vision-8B-2. These are tested for untargeted PGD (Projected Gradient Descent) against the visual input modality, and empirically evaluated on the Visual Question Answering (VQA) v2 dataset subset. The results of these adversarial attacks are then quantified using the standard VQA accuracy metric. This evaluation is then compared with the accuracy degradation (accuracy drop) of LLaVA and Llama 3.2 Vision. A key finding is that Llama 3.2 Vision, despite a lower baseline accuracy in this setup, exhibited a smaller drop in performance under attack compared to LLaVA, particularly at higher perturbation levels. Overall, the findings confirm that the vision modality represents a viable attack vector for degrading the performance of contemporary open-weight VLMs, including Meta&#x27;s Llama 3.2 Vision. Furthermore, they highlight that adversarial robustness does not necessarily correlate directly with standard benchmark performance and may be influenced by underlying architectural and training factors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>开放基础模型中视觉的对抗鲁棒性</div>
<div class="mono" style="margin-top:8px">随着深度学习的发展，理解AI系统能够识别物体的模型变得越来越困难。因此，攻击者可能试图通过添加未见过的元素来修改图像，从而干扰AI对实体的识别。本文研究了LLaVA-1.5-13B和Meta的Llama 3.2 Vision-8B-2的对抗鲁棒性。这些模型针对视觉输入模态进行了无目标投影梯度下降（PGD）测试，并在视觉问答（VQA）v2数据集子集上进行了实证评估。使用标准的VQA准确率度量标准量化了这些对抗攻击的结果。随后，将这些评估结果与LLaVA和Llama 3.2 Vision的准确率下降情况进行比较。一个关键发现是，尽管在该设置下LLama 3.2 Vision的基线准确率较低，但在攻击下其性能下降幅度仍小于LLaVA，尤其是在较高扰动水平下。总体而言，这些发现证实了视觉模态是降低当前开放权重视觉语言模型（VLMs）性能的一种可行攻击向量，包括Meta的Llama 3.2 Vision。此外，它们还指出对抗鲁棒性并不一定与标准基准性能直接相关，可能受到底层架构和训练因素的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">With the increase in deep learning, it becomes increasingly difficult to understand the model in which AI systems can identify objects.</div>
</details>
</div>
<div class="card">
<div class="title">When Reasoning Meets Its Laws</div>
<div class="meta-line">Authors: Junyu Zhang, Yifan Sun, Tianang Leng, Jingyan Shen, Liu Ziyin, Paul Pu Liang, Huan Zhang</div>
<div class="meta-line">First: 2025-12-19T18:59:11+00:00 · Latest: 2025-12-19T18:59:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17901v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17901v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://lore-project.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the superior performance of Large Reasoning Models (LRMs), their reasoning behaviors are often counterintuitive, leading to suboptimal reasoning capabilities. To theoretically formalize the desired reasoning behaviors, this paper presents the Laws of Reasoning (LoRe), a unified framework that characterizes intrinsic reasoning patterns in LRMs. We first propose compute law with the hypothesis that the reasoning compute should scale linearly with question complexity. Beyond compute, we extend LoRe with a supplementary accuracy law. Since the question complexity is difficult to quantify in practice, we examine these hypotheses by two properties of the laws, monotonicity and compositionality. We therefore introduce LoRe-Bench, a benchmark that systematically measures these two tractable properties for large reasoning models. Evaluation shows that most reasoning models exhibit reasonable monotonicity but lack compositionality. In response, we develop an effective finetuning approach that enforces compute-law compositionality. Extensive empirical studies demonstrate that better compliance with compute laws yields consistently improved reasoning performance on multiple benchmarks, and uncovers synergistic effects across properties and laws. Project page: https://lore-project.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当推理遇见其定律</div>
<div class="mono" style="margin-top:8px">尽管大型推理模型（LRMs）表现出色，但它们的推理行为常常违反直觉，导致推理能力不佳。为理论化地形式化所需的推理行为，本文提出了推理定律（LoRe），一个统一的框架，用于描述LRMs中的内在推理模式。我们首先提出计算定律，假设推理计算应与问题复杂度线性增长。除了计算之外，我们还扩展LoRe，引入补充的准确性定律。由于在实践中难以量化问题复杂度，我们通过定律的两个属性——单调性和组合性来检验这些假设。因此，我们引入LoRe-Bench，一个系统性评估大型推理模型这两个可测量属性的基准。评估结果表明，大多数推理模型表现出合理的单调性，但缺乏组合性。为此，我们开发了一种有效的微调方法，强制实现计算定律的组合性。大量实证研究表明，更好地遵循计算定律可显著提升推理模型在多个基准上的表现，并揭示属性与定律之间的协同效应。项目页面：https://lore-project.github.io/</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Forcing for Multi-Agent Interaction Sequence Modeling</div>
<div class="meta-line">Authors: Vongani H. Maluleke, Kie Horiuchi, Lea Wilken, Evonne Ng, Jitendra Malik, Angjoo Kanazawa</div>
<div class="meta-line">First: 2025-12-19T18:59:02+00:00 · Latest: 2025-12-19T18:59:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17900v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17900v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://von31.github.io/MAGNet/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding and generating multi-person interactions is a fundamental challenge with broad implications for robotics and social computing. While humans naturally coordinate in groups, modeling such interactions remains difficult due to long temporal horizons, strong inter-agent dependencies, and variable group sizes. Existing motion generation methods are largely task-specific and do not generalize to flexible multi-agent generation. We introduce MAGNet (Multi-Agent Diffusion Forcing Transformer), a unified autoregressive diffusion framework for multi-agent motion generation that supports a wide range of interaction tasks through flexible conditioning and sampling. MAGNet performs dyadic prediction, partner inpainting, and full multi-agent motion generation within a single model, and can autoregressively generate ultra-long sequences spanning hundreds of v. Building on Diffusion Forcing, we introduce key modifications that explicitly model inter-agent coupling during autoregressive denoising, enabling coherent coordination across agents. As a result, MAGNet captures both tightly synchronized activities (e.g, dancing, boxing) and loosely structured social interactions. Our approach performs on par with specialized methods on dyadic benchmarks while naturally extending to polyadic scenarios involving three or more interacting people, enabled by a scalable architecture that is agnostic to the number of agents. We refer readers to the supplemental video, where the temporal dynamics and spatial coordination of generated interactions are best appreciated. Project page: https://von31.github.io/MAGNet/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多智能体交互序列建模的扩散强制</div>
<div class="mono" style="margin-top:8px">理解与生成多人交互是机器人和社交计算领域中的基本挑战，具有广泛的应用意义。尽管人类在群体中自然地进行协调，但由于长期的时间跨度、强智能体间依赖关系以及可变的群体规模，建模此类交互仍然困难。现有的运动生成方法大多任务特定，无法推广到灵活的多智能体生成。我们引入了MAGNet（多智能体扩散强制Transformer），这是一个统一的自回归扩散框架，用于多智能体运动生成，通过灵活的条件和采样支持广泛的交互任务。MAGNet在一个模型中执行双人预测、伙伴补全以及完整的多智能体运动生成，并能够自回归地生成数百步的超长序列。基于扩散强制，我们引入了关键修改，明确在自回归去噪过程中建模智能体间的耦合关系，从而实现智能体间的协调一致。因此，MAGNet能够捕捉紧密同步的活动（如舞蹈、拳击）和松散结构的社会交互。我们的方法在双人基准测试中与专用方法表现相当，同时自然地扩展到涉及三个或更多交互人员的多智能体场景，这得益于一种与智能体数量无关的可扩展架构。我们建议读者参考补充视频，以更好地欣赏生成交互的时间动态和空间协调。项目页面：https://von31.github.io/MAGNet/</div>
</details>
</div>
<div class="card">
<div class="title">Distributionally Robust Imitation Learning: Layered Control Architecture for Certifiable Autonomy</div>
<div class="meta-line">Authors: Aditya Gahlawat, Ahmed Aboudonia, Sandeep Banik, Naira Hovakimyan, Nikolai Matni, Aaron D. Ames, Gioele Zardini, Alberto Speranzon</div>
<div class="meta-line">First: 2025-12-19T18:58:11+00:00 · Latest: 2025-12-19T18:58:11+00:00</div>
<div class="meta-line">Comments: 18 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17899v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17899v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Imitation learning (IL) enables autonomous behavior by learning from expert demonstrations. While more sample-efficient than comparative alternatives like reinforcement learning, IL is sensitive to compounding errors induced by distribution shifts. There are two significant sources of distribution shifts when using IL-based feedback laws on systems: distribution shifts caused by policy error and distribution shifts due to exogenous disturbances and endogenous model errors due to lack of learning. Our previously developed approaches, Taylor Series Imitation Learning (TaSIL) and $\mathcal{L}_1$ -Distributionally Robust Adaptive Control (\ellonedrac), address the challenge of distribution shifts in complementary ways. While TaSIL offers robustness against policy error-induced distribution shifts, \ellonedrac offers robustness against distribution shifts due to aleatoric and epistemic uncertainties. To enable certifiable IL for learned and/or uncertain dynamical systems, we formulate \textit{Distributionally Robust Imitation Policy (DRIP)} architecture, a Layered Control Architecture (LCA) that integrates TaSIL and~\ellonedrac. By judiciously designing individual layer-centric input and output requirements, we show how we can guarantee certificates for the entire control pipeline. Our solution paves the path for designing fully certifiable autonomy pipelines, by integrating learning-based components, such as perception, with certifiable model-based decision-making through the proposed LCA approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分布鲁棒模仿学习：可认证自主性的分层控制架构</div>
<div class="mono" style="margin-top:8px">模仿学习（IL）通过从专家演示中学习来实现自主行为。虽然比强化学习等比较方法更具样本效率，但IL对分布转移引起的累积误差较为敏感。在使用基于IL的反馈法则时，有两个显著的分布转移来源：由策略误差引起的分布转移，以及由于缺乏学习而由外生扰动和内生模型误差引起的分布转移。我们之前开发的两种方法，泰勒级数模仿学习（TaSIL）和 $\mathcal{L}_1$-分布鲁棒自适应控制（\ellonedrac），以互补的方式应对分布转移的挑战。TaSIL 提供了对由策略误差引起的分布转移的鲁棒性，而 \ellonedrac 提供了对由随机不确定性和认知不确定性引起的分布转移的鲁棒性。为了实现对学习和/或不确定动态系统的可认证模仿学习，我们提出了分布鲁棒模仿策略（DRIP）架构，这是一种结合了 TaSIL 和 \ellonedrac 的分层控制架构（LCA）。通过精心设计各层的输入和输出要求，我们展示了如何为整个控制流程提供认证保证。我们的解决方案通过将基于学习的组件（如感知）与基于可认证模型的决策机制相结合，为设计完全可认证的自主性流程铺平了道路。</div>
</details>
</div>
<div class="card">
<div class="title">Humanlike AI Design Increases Anthropomorphism but Yields Divergent Outcomes on Engagement and Trust Globally</div>
<div class="meta-line">Authors: Robin Schimmelpfennig, Mark Díaz, Vinodkumar Prabhakaran, Aida Davani</div>
<div class="meta-line">First: 2025-12-19T18:57:53+00:00 · Latest: 2025-12-19T18:57:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17898v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17898v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Over a billion users across the globe interact with AI systems engineered with increasing sophistication to mimic human traits. This shift has triggered urgent debate regarding Anthropomorphism, the attribution of human characteristics to synthetic agents, and its potential to induce misplaced trust or emotional dependency. However, the causal link between more humanlike AI design and subsequent effects on engagement and trust has not been tested in realistic human-AI interactions with a global user pool. Prevailing safety frameworks continue to rely on theoretical assumptions derived from Western populations, overlooking the global diversity of AI users. Here, we address these gaps through two large-scale cross-national experiments (N=3,500) across 10 diverse nations, involving real-time and open-ended interactions with an AI system. We find that when evaluating an AI&#x27;s human-likeness, users focus less on the kind of theoretical aspects often cited in policy (e.g., sentience or consciousness), but rather applied, interactional cues like conversation flow or understanding the user&#x27;s perspective. We also experimentally demonstrate that humanlike design levers can causally increase anthropomorphism among users; however, we do not find that humanlike design universally increases behavioral measures for user engagement and trust, as previous theoretical work suggests. Instead, part of the connection between human-likeness and behavioral outcomes is fractured by culture: specific design choices that foster self-reported trust in AI-systems in some populations (e.g., Brazil) may trigger the opposite result in others (e.g., Japan). Our findings challenge prevailing narratives of inherent risk in humanlike AI design. Instead, we identify a nuanced, culturally mediated landscape of human-AI interaction, which demands that we move beyond a one-size-fits-all approach in AI governance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>拟人化AI设计增强了拟人化程度，但对全球参与度和信任度的影响存在差异</div>
<div class="mono" style="margin-top:8px">全球数十亿用户正在与日益复杂的AI系统互动，这些系统被设计成模仿人类特征。这种转变引发了关于拟人化（将人类特征归因于合成代理）的紧急讨论，以及其可能引发的错误信任或情感依赖。然而，拟人化AI设计与后续参与度和信任度之间的因果关系尚未在具有全球用户群体的真实人机互动中进行测试。目前的安全框架仍然依赖于从西方人群中得出的理论假设，忽视了全球AI用户的多样性。在此，我们通过两项大规模跨国实验（N=3,500），在10个不同的国家中，涉及实时和开放式的AI系统互动，来填补这些空白。我们发现，当评估AI的拟人化程度时，用户更关注实际的、互动性的线索，如对话流畅性或理解用户视角，而不是政策中常提到的理论方面（如意识或感知）。我们还实验证明，拟人化设计可以因果地增加用户对AI的拟人化程度；然而，我们并未发现拟人化设计普遍提高用户参与度和信任度的行为指标，如之前理论工作所建议的。相反，拟人化与行为结果之间的联系部分被文化所打破：某些设计选择可能在某些群体（如巴西）中促进对AI系统的自我报告信任，而在其他群体（如日本）中却可能引发相反的结果。我们的研究结果挑战了拟人化AI设计固有风险的主流观点。相反，我们识别出一个复杂且文化中介的人机互动景观，这要求我们在AI治理中超越一刀切的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Over a billion users across the globe interact with AI systems engineered with increasing sophistication to mimic human traits.</div>
</details>
</div>
<div class="card">
<div class="title">RadarGen: Automotive Radar Point Cloud Generation from Cameras</div>
<div class="meta-line">Authors: Tomer Borreda, Fangqiang Ding, Sanja Fidler, Shengyu Huang, Or Litany</div>
<div class="meta-line">First: 2025-12-19T18:57:33+00:00 · Latest: 2025-12-19T18:57:33+00:00</div>
<div class="meta-line">Comments: Project page: https://radargen.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17897v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17897v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://radargen.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present RadarGen, a diffusion model for synthesizing realistic automotive radar point clouds from multi-view camera imagery. RadarGen adapts efficient image-latent diffusion to the radar domain by representing radar measurements in bird&#x27;s-eye-view form that encodes spatial structure together with radar cross section (RCS) and Doppler attributes. A lightweight recovery step reconstructs point clouds from the generated maps. To better align generation with the visual scene, RadarGen incorporates BEV-aligned depth, semantic, and motion cues extracted from pretrained foundation models, which guide the stochastic generation process toward physically plausible radar patterns. Conditioning on images makes the approach broadly compatible, in principle, with existing visual datasets and simulation frameworks, offering a scalable direction for multimodal generative simulation. Evaluations on large-scale driving data show that RadarGen captures characteristic radar measurement distributions and reduces the gap to perception models trained on real data, marking a step toward unified generative simulation across sensing modalities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RadarGen：从摄像头生成汽车雷达点云</div>
<div class="mono" style="margin-top:8px">我们提出了RadarGen，这是一种扩散模型，能够从多视角摄像头图像合成逼真的汽车雷达点云。RadarGen通过将雷达测量值以鸟瞰图形式表示，适应高效的图像-潜在空间扩散模型，该形式编码了空间结构以及雷达散射截面（RCS）和多普勒属性。一个轻量级的恢复步骤可从生成的图中重建点云。为了更好地对齐生成与视觉场景，RadarGen结合了从预训练基础模型中提取的与鸟瞰图对齐的深度、语义和运动线索，这些线索引导随机生成过程朝向物理上合理的雷达模式。基于图像的条件使该方法原则上与现有的视觉数据集和仿真框架兼容，为多模态生成式仿真提供了可扩展的方向。在大规模驾驶数据上的评估表明，RadarGen能够捕捉到雷达测量值的典型分布，并缩小与基于真实数据训练的感知模型之间的差距，标志着跨感知模态统一生成式仿真的一步。</div>
</details>
</div>
<div class="card">
<div class="title">XAgen: An Explainability Tool for Identifying and Correcting Failures in Multi-Agent Workflows</div>
<div class="meta-line">Authors: Xinru Wang, Ming Yin, Eunyee Koh, Mustafa Doga Dogan</div>
<div class="meta-line">First: 2025-12-19T18:54:36+00:00 · Latest: 2025-12-19T18:54:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17896v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17896v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As multi-agent systems powered by Large Language Models (LLMs) are increasingly adopted in real-world workflows, users with diverse technical backgrounds are now building and refining their own agentic processes. However, these systems can fail in opaque ways, making it difficult for users to observe, understand, and correct errors. We conducted formative interviews with 12 practitioners to identify mismatches between existing observability tools and users&#x27; needs. Based on these insights, we designed XAgen, an explainability tool that supports users with varying AI expertise through three core capabilities: log visualization for glanceable workflow understanding, human-in-the-loop feedback to capture expert judgment, and automatic error detection via an LLM-as-a-judge. In a user study with 8 participants, XAgen helped users more easily locate failures, attribute to specific agents or steps, and iteratively improve configurations. Our findings surface human-centered design guidelines for explainable agentic AI development and highlights opportunities for more context-aware interactive debugging.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>XAgen：一种用于识别和纠正多智能体工作流故障的可解释性工具</div>
<div class="mono" style="margin-top:8px">随着基于大型语言模型（LLMs）的多智能体系统在现实工作流中被越来越多地采用，具有不同技术背景的用户现在正在构建和优化自己的智能体流程。然而，这些系统可能以不透明的方式失败，使用户难以观察、理解和纠正错误。我们通过对12位从业者进行了形成性访谈，以识别现有可观测性工具与用户需求之间的不匹配。基于这些见解，我们设计了XAgen，一种可解释性工具，通过三种核心功能支持不同AI专业知识水平的用户：日志可视化以实现快速的工作流理解、人机交互反馈以捕捉专家判断，以及通过LLM作为裁判进行自动错误检测。在一项针对8名用户的实验中，XAgen帮助用户更轻松地定位故障、归因于特定智能体或步骤，并迭代改进配置。我们的研究结果揭示了以用户为中心的可解释智能体AI开发设计指南，并突出了更具上下文感知的交互式调试机会。</div>
</details>
</div>
<div class="card">
<div class="title">Visualization of The Content of Surah al Fiil using Marker-Based Augmented Reality</div>
<div class="meta-line">Authors: Ahmad Badru Al Husaeni, Dzakwanfaiq Nauval, Farid Muhtar Fathir, Mahesa Adlan Falah, Muhammad Miftahur Rizki Awalin</div>
<div class="meta-line">First: 2025-12-19T18:53:46+00:00 · Latest: 2025-12-19T18:53:46+00:00</div>
<div class="meta-line">Comments: 12 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17895v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17895v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study presents the development of a marker-based augmented reality (AR) application designed to visualize the content of Surah al-Fil as an interactive and context-rich medium for Islamic education. Using a research and development approach, the system was developed through structured stages including data collection, user requirement analysis, interface design, 3D asset creation using Blender, and integration of Unity 3D with the Vuforia SDK. The application features key visual elements such as the elephant army, the Kaaba, and the Ababil birds, which were modeled in detail and linked to high-contrast image markers to ensure accurate and stable AR tracking. Functional testing demonstrated strong technical performance, achieving a 95 percent marker detection accuracy at an optimal distance of 30-40 cm with consistent real-time rendering across multiple Android devices. User evaluations involving students and Islamic education teachers indicated high acceptance, with an overall satisfaction score of 4.7 out of 5 in terms of usability, visual appeal, interactivity, and learning effectiveness. These findings indicate that AR-based learning media can enhance learner engagement, deepen understanding of Quranic narratives, and provide immersive insights into historical and spiritual contexts. Overall, this study demonstrates that marker-based AR technology has significant potential to support innovation in digital Islamic education by enriching traditional learning with interactive and visually intuitive experiences.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于标记的增强现实技术可视化《奉火经》内容</div>
<div class="mono" style="margin-top:8px">本研究提出了一种基于标记的增强现实（AR）应用程序的开发，旨在将《奉火经》的内容以交互性强且富有上下文的媒介形式呈现，用于伊斯兰教育。采用研究与开发的方法，系统通过结构化的阶段进行开发，包括数据收集、用户需求分析、界面设计、使用Blender创建3D资产，以及将Unity 3D与Vuforia SDK集成。应用程序包含关键视觉元素，如象军、克尔白和阿比勒鸟，这些元素被详细建模并连接到高对比度的图像标记，以确保AR跟踪的准确性和稳定性。功能测试表明该应用程序具有良好的技术性能，在30-40厘米的最佳距离下实现了95%的标记检测准确率，并在多种Android设备上实现了稳定的实时渲染。涉及学生和伊斯兰教育教师的用户评估显示了高度的接受度，整体满意度评分为5分制中的4.7分，涉及可用性、视觉吸引力、互动性和学习效果等方面。这些结果表明，基于AR的学习媒介可以增强学习者的参与度，加深对《古兰经》叙事的理解，并提供沉浸式的对历史和精神背景的洞察。总体而言，本研究展示了基于标记的AR技术在支持数字伊斯兰教育创新方面的巨大潜力，通过增强传统学习方式的互动性和视觉直观性。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring the Effect of Basis Rotation on NQS Performance</div>
<div class="meta-line">Authors: Sven Benjamin Kožić, Vinko Zlatić, Fabio Franchini, Salvatore Marco Giampaolo</div>
<div class="meta-line">First: 2025-12-19T18:49:33+00:00 · Latest: 2025-12-19T18:49:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17893v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17893v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural Quantum States (NQS) use neural networks to represent wavefunctions of quantum many-body systems, but their performance depends on the choice of basis, yet the underlying mechanism remains poorly understood. We use a fully solvable one-dimensional Ising model to show that local basis rotations leave the loss landscape unchanged while relocating the exact wavefunction in parameter space, effectively increasing its geometric distance from typical initializations. By sweeping a rotation angle, we compute quantum Fisher information and Fubini-Study distances to quantify how the rotated wavefunction moves within the loss landscape. Shallow architectures (with focus on Restricted Boltzmann Machines (RBMs)) trained with quantum natural gradient are more likely to fall into saddle-point regions depending on the rotation angle: they achieve low energy error but fail to reproduce correct coefficient distributions. In the ferromagnetic case, near-degenerate eigenstates create high-curvature barriers that trap optimization at intermediate fidelities. We introduce a framework based on an analytically solvable rotated Ising model to investigate how relocating the target wavefunction within a fixed loss landscape exposes information-geometric barriers,such as saddle points and high-curvature regions,that hinder shallow NQS optimization, underscoring the need for landscape-aware model design in variational training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索基矢旋转对NQS性能的影响</div>
<div class="mono" style="margin-top:8px">神经量子态（NQS）使用神经网络来表示量子多体系统的波函数，但其性能依赖于基矢的选择，而其背后的机制仍不明确。我们利用一个完全可解的一维Ising模型来展示，局部基矢旋转不会改变损失景观，但会将精确的波函数在参数空间中重新定位，从而有效增加其与典型初始化之间的几何距离。通过扫描旋转角度，我们计算量子费舍尔信息和弗比尼-斯图迪距离，以量化旋转后的波函数在损失景观中的移动情况。使用量子自然梯度训练的浅层架构（特别是受限玻尔兹曼机（RBMs））更容易根据旋转角度陷入鞍点区域：它们能够获得较低的能量误差，但无法重现正确的系数分布。在铁磁情况下，近简并的本征态会形成高曲率障碍，使优化过程在中间保真度处陷入停滞。我们引入一个基于可解析旋转Ising模型的框架，研究在固定损失景观中重新定位目标波函数如何暴露信息几何障碍，如鞍点和高曲率区域，这些障碍会阻碍浅层NQS的优化，强调了在变分训练中需要景观感知的模型设计的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Neural Quantum States (NQS) use neural networks to represent wavefunctions of quantum many-body systems, but their performance depends on the choice of basis, yet the underlying mechanism remains poorly understood.</div>
</details>
</div>
<div class="card">
<div class="title">Prefix Trees Improve Memory Consumption in Large-Scale Continuous-Time Stochastic Models</div>
<div class="meta-line">Authors: Landon Taylor, Joshua Jeppson, Ahmed Irfan, Lukas Buecherl, Chris Myers, Zhen Zhang</div>
<div class="meta-line">First: 2025-12-19T18:49:00+00:00 · Latest: 2025-12-19T18:49:00+00:00</div>
<div class="meta-line">Comments: 21 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17892v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17892v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Highly-concurrent system models with vast state spaces like Chemical Reaction Networks (CRNs) that model biological and chemical systems pose a formidable challenge to cutting-edge formal analysis tools. Although many symbolic approaches have been presented, transient probability analysis of CRNs, modeled as Continuous-Time Markov Chains (CTMCs), requires explicit state representation. For that purpose, current cutting-edge methods use hash maps, which boast constant average time complexity and linear memory complexity. However, hash maps often suffer from severe memory limitations on models with immense state spaces. To address this, we propose using prefix trees to store states for large, highly concurrent models (particularly CRNs) for memory savings. We present theoretical analyses and benchmarks demonstrating the favorability of prefix trees over hash maps for very large state spaces. Additionally, we propose using a Bounded Model Checking (BMC) pre-processing step to impose a variable ordering to further improve memory usage along with preliminary evaluations suggesting its effectiveness. We remark that while our work is motivated primarily by the challenges posed by CRNs, it is generalizable to all CTMC models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>前缀树在大规模连续时间随机模型中改善内存消耗</div>
<div class="mono" style="margin-top:8px">像化学反应网络（CRNs）这样的高并发系统模型，其庞大的状态空间对最先进的形式化分析工具提出了严峻挑战。尽管已经提出了许多符号方法，但将CRNs建模为连续时间马尔可夫链（CTMCs）时，瞬态概率分析需要显式的状态表示。为此，当前最先进的方法使用哈希表，其平均时间复杂度为常数，内存复杂度为线性。然而，哈希表在状态空间极其庞大的模型中常常面临严重的内存限制。为了解决这一问题，我们提出使用前缀树来存储大规模高并发模型（尤其是CRNs）的状态，以节省内存。我们提供了理论分析和基准测试，证明前缀树在非常大的状态空间中优于哈希表。此外，我们还提出使用有界模型检测（BMC）预处理步骤来施加变量顺序，以进一步优化内存使用，并进行了初步评估，表明其有效性。我们指出，尽管我们的工作主要受到CRNs挑战的启发，但其方法可推广到所有CTMC模型。</div>
</details>
</div>
<div class="card">
<div class="title">Keypoint Counting Classifiers: Turning Vision Transformers into Self-Explainable Models Without Training</div>
<div class="meta-line">Authors: Kristoffer Wickstrøm, Teresa Dorszewski, Siyan Chen, Michael Kampffmeyer, Elisabeth Wetzer, Robert Jenssen</div>
<div class="meta-line">First: 2025-12-19T18:47:04+00:00 · Latest: 2025-12-19T18:47:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17891v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17891v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current approaches for designing self-explainable models (SEMs) require complicated training procedures and specific architectures which makes them impractical. With the advance of general purpose foundation models based on Vision Transformers (ViTs), this impracticability becomes even more problematic. Therefore, new methods are necessary to provide transparency and reliability to ViT-based foundation models. In this work, we present a new method for turning any well-trained ViT-based model into a SEM without retraining, which we call Keypoint Counting Classifiers (KCCs). Recent works have shown that ViTs can automatically identify matching keypoints between images with high precision, and we build on these results to create an easily interpretable decision process that is inherently visualizable in the input. We perform an extensive evaluation which show that KCCs improve the human-machine communication compared to recent baselines. We believe that KCCs constitute an important step towards making ViT-based foundation models more transparent and reliable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关键点计数分类器：无需重新训练即可将视觉Transformer转化为自解释模型</div>
<div class="mono" style="margin-top:8px">当前设计自解释模型（SEMs）的方法需要复杂的训练过程和特定架构，这使得它们难以实际应用。随着基于视觉Transformer（ViTs）的通用基础模型的发展，这种不实用性变得更加严重。因此，有必要开发新方法，以提高基于ViT的基础模型的透明度和可靠性。在本工作中，我们提出了一种新方法，可以在不重新训练的情况下将任何已训练的ViT模型转化为SEMs，我们称之为关键点计数分类器（KCCs）。最近的研究表明，ViTs可以以高精度自动识别图像之间的匹配关键点，我们在此基础上构建了一个易于解释的决策过程，该过程本质上可以可视化输入。我们进行了广泛的评估，结果表明与最近的基线方法相比，KCCs能够提升人机交流效果。我们认为，KCCs是使基于ViT的基础模型更加透明和可靠的重要一步。</div>
</details>
</div>
<div class="card">
<div class="title">SpecCLIP: Aligning and Translating Spectroscopic Measurements for Stars</div>
<div class="meta-line">Authors: Xiaosheng Zhao, Yang Huang, Guirong Xue, Xiao Kong, Jifeng Liu, Xiaoyu Tang, Timothy C. Beers, Yuan-Sen Ting, A-Li Luo</div>
<div class="meta-line">First: 2025-07-02T17:49:52+00:00 · Latest: 2025-12-19T18:39:57+00:00</div>
<div class="meta-line">Comments: 29 pages, 8 figures, 6 tables. Accepted for publication in ApJ. Comments welcome</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.01939v4">Abs</a> · <a href="https://arxiv.org/pdf/2507.01939v4">PDF</a> · <a href="https://github.com/Xiaosheng-Zhao/SpecCLIP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, large language models (LLMs) have transformed natural language understanding through vast datasets and large-scale parameterization. Inspired by this success, we present SpecCLIP, a foundation model framework that extends LLM-inspired methodologies to stellar spectral analysis. Stellar spectra, akin to structured language, encode rich physical and chemical information about stars. By training foundation models on large-scale spectral datasets, our goal is to learn robust and informative embeddings that support diverse downstream applications. As a proof of concept, SpecCLIP involves pre-training on two spectral types--LAMOST low-resolution and Gaia XP--followed by contrastive alignment using the CLIP (Contrastive Language-Image Pre-training) framework, adapted to associate spectra from different instruments. This alignment is complemented by auxiliary decoders that preserve spectrum-specific information and enable translation (prediction) between spectral types, with the former achieved by maximizing mutual information between embeddings and input spectra. The result is a cross-spectrum framework enabling intrinsic calibration and flexible applications across instruments. We demonstrate that fine-tuning these models on moderate-sized labeled datasets improves adaptability to tasks such as stellar-parameter estimation and chemical-abundance determination. SpecCLIP also enhances the accuracy and precision of parameter estimates benchmarked against external survey data. Additionally, its similarity search and cross-spectrum prediction capabilities offer potential for anomaly detection. Our results suggest that contrastively trained foundation models enriched with spectrum-aware decoders can advance precision stellar spectroscopy. Our code SpecCLIP is publicly available at https://github.com/Xiaosheng-Zhao/SpecCLIP</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpecCLIP：用于恒星的光谱测量对齐与翻译</div>
<div class="mono" style="margin-top:8px">近年来，大型语言模型（LLMs）通过大规模数据集和参数化方法革新了自然语言理解。受此启发，我们提出了SpecCLIP，一个基础模型框架，将LLM启发的方法扩展到恒星光谱分析中。恒星光谱类似于结构化语言，编码了丰富的物理和化学信息。通过在大规模光谱数据集上训练基础模型，我们的目标是学习出稳健且信息丰富的嵌入表示，以支持多样化的下游应用。作为概念验证，SpecCLIP首先在两种光谱类型——LAMOST低分辨率光谱和Gaia XP光谱上进行预训练，随后使用CLIP（对比语言-图像预训练）框架进行对比对齐，该框架被调整以关联不同仪器的光谱。这种对齐由辅助解码器补充，这些解码器保留了光谱特定信息，并实现了光谱类型之间的翻译（预测），通过最大化嵌入表示与输入光谱之间的互信息来实现。结果是一个跨光谱框架，支持内在校准和跨仪器的灵活应用。我们证明，对这些模型进行中等规模标记数据集的微调可以提高其在恒星参数估计和化学丰度测定等任务中的适应性。SpecCLIP还提升了参数估计的准确性和精确度，与外部调查数据进行基准测试。此外，其相似性搜索和跨光谱预测能力为异常检测提供了潜力。我们的结果表明，结合光谱感知解码器的对比训练基础模型可以推动高精度恒星光谱学的发展。我们的代码SpecCLIP可在https://github.com/Xiaosheng-Zhao/SpecCLIP上公开获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In recent years, large language models (LLMs) have transformed natural language understanding through vast datasets and large-scale parameterization.</div>
</details>
</div>
<div class="card">
<div class="title">Regularized Random Fourier Features and Finite Element Reconstruction for Operator Learning in Sobolev Space</div>
<div class="meta-line">Authors: Xinyue Yu, Hayden Schaeffer</div>
<div class="meta-line">First: 2025-12-19T18:36:24+00:00 · Latest: 2025-12-19T18:36:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17884v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17884v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Operator learning is a data-driven approximation of mappings between infinite-dimensional function spaces, such as the solution operators of partial differential equations. Kernel-based operator learning can offer accurate, theoretically justified approximations that require less training than standard methods. However, they can become computationally prohibitive for large training sets and can be sensitive to noise. We propose a regularized random Fourier feature (RRFF) approach, coupled with a finite element reconstruction map (RRFF-FEM), for learning operators from noisy data. The method uses random features drawn from multivariate Student&#x27;s $t$ distributions, together with frequency-weighted Tikhonov regularization that suppresses high-frequency noise. We establish high-probability bounds on the extreme singular values of the associated random feature matrix and show that when the number of features $N$ scales like $m \log m$ with the number of training samples $m$, the system is well-conditioned, which yields estimation and generalization guarantees. Detailed numerical experiments on benchmark PDE problems, including advection, Burgers&#x27;, Darcy flow, Helmholtz, Navier-Stokes, and structural mechanics, demonstrate that RRFF and RRFF-FEM are robust to noise and achieve improved performance with reduced training time compared to the unregularized random feature model, while maintaining competitive accuracy relative to kernel and neural operator tests.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在Sobolev空间中算子学习的正则化随机傅里叶特征与有限元重建</div>
<div class="mono" style="margin-top:8px">算子学习是一种基于数据的无限维函数空间之间映射的近似方法，例如偏微分方程的解算子。基于核的算子学习可以提供准确且有理论依据的近似，其训练需求低于标准方法。然而，当训练集较大时，计算成本可能变得很高，并且对噪声敏感。我们提出了一种正则化随机傅里叶特征（RRFF）方法，并结合有限元重建映射（RRFF-FEM），用于从噪声数据中学习算子。该方法使用来自多元Student&#x27;s t分布的随机特征，并结合频率加权的Tikhonov正则化以抑制高频噪声。我们建立了与相关随机特征矩阵极端奇异值相关的高概率界限，并证明当特征数量 $N$ 与训练样本数量 $m$ 的关系为 $N$ 约等于 $m \log m$ 时，系统具有良好的条件性，从而提供了估计和泛化保证。在包括对流、Burgers方程、达西流、亥姆霍兹方程、纳维-斯托克斯方程和结构力学在内的基准PDE问题上的详细数值实验表明，RRFF和RRFF-FEM在噪声鲁棒性和训练时间减少方面优于无正则化的随机特征模型，同时在准确度上与核方法和神经算子测试保持竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">Your Eyes Controlled the Game: Real-Time Cognitive Training Adaptation based on Eye-Tracking and Physiological Data in Virtual Reality</div>
<div class="meta-line">Authors: Dominik Szczepaniak, Monika Harvey, Fani Deligianni</div>
<div class="meta-line">First: 2025-12-19T18:36:02+00:00 · Latest: 2025-12-19T18:36:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17882v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17882v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cognitive training for sustained attention and working memory is vital across domains relying on robust mental capacity such as education or rehabilitation. Adaptive systems are essential, dynamically matching difficulty to user ability to maintain engagement and accelerate learning. Current adaptive systems often rely on simple performance heuristics or predict visual complexity and affect instead of cognitive load. This study presents the first implementation of real-time adaptive cognitive load control in Virtual Reality cognitive training based on eye-tracking and physiological data. We developed a bidirectional LSTM model with a self-attention mechanism, trained on eye-tracking and physiological (PPG, GSR) data from 74 participants. We deployed it in real-time with 54 participants across single-task (sustained attention) and dual-task (sustained attention + mental arithmetic) paradigms. Difficulty was adjusted dynamically based on participant self-assessment or model&#x27;s real-time cognitive load predictions. Participants showed a tendency to estimate the task as too difficult, even though they were objectively performing at their best. Over the course of a 10-minute session, both adaptation methods converged at equivalent difficulty in single-task scenarios, with no significant differences in subjective workload or game performance. However, in the dual-task conditions, the model successfully pushed users to higher difficulty levels without performance penalties or increased frustration, highlighting a user tendency to underestimate capacity under high cognitive load. Findings indicate that machine learning models may provide more objective cognitive capacity assessments than self-directed approaches, mitigating subjective performance biases and enabling more effective training by pushing users beyond subjective comfort zones toward physiologically-determined optimal challenge levels.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>你的眼睛掌控了游戏：基于眼动追踪和生理数据的虚拟现实实时认知训练自适应控制</div>
<div class="mono" style="margin-top:8px">持续注意力和工作记忆的认知训练在依赖强大心理能力的领域（如教育和康复）至关重要。自适应系统对于动态匹配难度与用户能力以保持参与度和加速学习是必不可少的。当前的自适应系统通常依赖简单的性能启发式方法，或预测视觉复杂度和情绪，而非认知负荷。本研究提出了首个基于眼动追踪和生理数据的虚拟现实认知训练中的实时自适应认知负荷控制实现。我们开发了一个具有自注意力机制的双向LSTM模型，使用74名参与者的目光追踪和生理数据（PPG、GSR）进行训练，并在54名参与者中实时部署，涵盖单任务（持续注意力）和双任务（持续注意力+心算）范式。难度根据参与者自我评估或模型实时认知负荷预测动态调整。参与者倾向于高估任务难度，即使他们在客观表现上处于最佳状态。在单任务场景中，两种适应方法在10分钟会话内收敛至相同的难度水平，主观工作负荷和游戏表现无显著差异。然而，在双任务条件下，模型成功地将用户引导至更高难度水平，而不会造成表现下降或增加挫败感，突显了用户在高认知负荷下低估自身能力的倾向。研究结果表明，机器学习模型可能比自我导向方法提供更客观的认知能力评估，从而缓解主观表现偏差，并通过将用户推向生理学决定的最优挑战水平，实现更有效的训练。</div>
</details>
</div>
<div class="card">
<div class="title">Weighted Stochastic Differential Equation to Implement Wasserstein-Fisher-Rao Gradient Flow</div>
<div class="meta-line">Authors: Herlock Rahimi</div>
<div class="meta-line">First: 2025-12-19T18:31:27+00:00 · Latest: 2025-12-19T18:31:27+00:00</div>
<div class="meta-line">Comments: 26 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17878v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17878v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Score-based diffusion models currently constitute the state of the art in continuous generative modeling. These methods are typically formulated via overdamped or underdamped Ornstein--Uhlenbeck-type stochastic differential equations, in which sampling is driven by a combination of deterministic drift and Brownian diffusion, resulting in continuous particle trajectories in the ambient space. While such dynamics enjoy exponential convergence guarantees for strongly log-concave target distributions, it is well known that their mixing rates deteriorate exponentially in the presence of nonconvex or multimodal landscapes, such as double-well potentials. Since many practical generative modeling tasks involve highly non-log-concave target distributions, considerable recent effort has been devoted to developing sampling schemes that improve exploration beyond classical diffusion dynamics.
  A promising line of work leverages tools from information geometry to augment diffusion-based samplers with controlled mass reweighting mechanisms. This perspective leads naturally to Wasserstein--Fisher--Rao (WFR) geometries, which couple transport in the sample space with vertical (reaction) dynamics on the space of probability measures. In this work, we formulate such reweighting mechanisms through the introduction of explicit correction terms and show how they can be implemented via weighted stochastic differential equations using the Feynman--Kac representation. Our study provides a preliminary but rigorous investigation of WFR-based sampling dynamics, and aims to clarify their geometric and operator-theoretic structure as a foundation for future theoretical and algorithmic developments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>加权随机微分方程实现Wasserstein-Fisher-Rao梯度流</div>
<div class="mono" style="margin-top:8px">基于分数的扩散模型目前是连续生成建模的最先进方法。这些方法通常通过阻尼或非阻尼的Ornstein--Uhlenbeck型随机微分方程进行表述，其中采样由确定性漂移和布朗扩散的组合驱动，从而在环境空间中产生连续的粒子轨迹。尽管此类动力学在强对数凹目标分布下具有指数收敛保证，但众所周知，在非凸或多模态景观（如双井势能）存在的情况下，其混合率会指数级下降。由于许多实际的生成建模任务涉及高度非对数凹的目标分布，因此近期大量研究致力于开发能够超越经典扩散动力学的采样方案。
  一个有前景的研究方向利用信息几何的工具，通过引入可控的质量重加权机制来增强基于扩散的采样器。这种视角自然地引导出Wasserstein--Fisher--Rao（WFR）几何结构，它将样本空间中的传输与概率测度空间中的垂直（反应）动力学耦合在一起。在本工作中，我们通过引入显式的校正项来表述这种重加权机制，并展示如何利用Feynman--Kac表示法通过加权随机微分方程实现它们。我们的研究提供了对基于WFR的采样动力学的初步但严谨的探讨，并旨在阐明其几何和算子理论结构，为未来理论和算法的发展奠定基础。</div>
</details>
</div>
<div class="card">
<div class="title">Learning vertical coordinates via automatic differentiation of a dynamical core</div>
<div class="meta-line">Authors: Tim Whittaker, Seth Taylor, Elsa Cardoso-Bihlo, Alejandro Di Luca, Alex Bihlo</div>
<div class="meta-line">First: 2025-12-19T18:31:07+00:00 · Latest: 2025-12-19T18:31:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17877v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17877v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Terrain-following coordinates in atmospheric models often imprint their grid structure onto the solution, particularly over steep topography, where distorted coordinate layers can generate spurious horizontal and vertical motion. Standard formulations, such as hybrid or SLEVE coordinates, mitigate these errors by using analytic decay functions controlled by heuristic scale parameters that are typically tuned by hand and fixed a priori. In this work, we propose a framework to define a parametric vertical coordinate system as a learnable component within a differentiable dynamical core. We develop an end-to-end differentiable numerical solver for the two-dimensional non-hydrostatic Euler equations on an Arakawa C-grid, and introduce a NEUral Vertical Enhancement (NEUVE) terrain-following coordinate based on an integral transformed neural network that guarantees monotonicity. A key feature of our approach is the use of automatic differentiation to compute exact geometric metric terms, thereby eliminating truncation errors associated with finite-difference coordinate derivatives. By coupling simulation errors through the time integration to the parameterization, our formulation finds a grid structure optimized for both the underlying physics and numerics. Using several standard tests, we demonstrate that these learned coordinates reduce the mean squared error by a factor of 1.4 to 2 in non-linear statistical benchmarks, and eliminate spurious vertical velocity striations over steep topography.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过动力核心的自动微分学习垂直坐标</div>
<div class="mono" style="margin-top:8px">大气模型中的地形跟随坐标常常会将其网格结构印在解中，特别是在陡峭地形上，扭曲的坐标层会产生虚假的水平和垂直运动。标准的公式，如混合坐标或SLEVE坐标，通过使用由启发式尺度参数控制的解析衰减函数来缓解这些误差，而这些参数通常手动调整并预先固定。在本工作中，我们提出了一种框架，将参数化垂直坐标系统定义为可学习组件，嵌入到可微分动力核心中。我们开发了一个端到端的可微分数值求解器，用于在Arakawa C网格上求解二维非静力欧拉方程，并引入了一种基于积分变换神经网络的NEUral Vertical Enhancement（NEUVE）地形跟随坐标，以保证单调性。我们方法的一个关键特征是使用自动微分来计算精确的几何度量项，从而消除有限差分坐标导数所带来的截断误差。通过将模拟误差通过时间积分耦合到参数化中，我们的公式找到了一个在基础物理和数值计算方面都优化的网格结构。通过多个标准测试，我们证明这些学习到的坐标在非线性统计基准中将均方误差降低了1.4到2倍，并消除了陡峭地形上的虚假垂直速度条纹。</div>
</details>
</div>
<div class="card">
<div class="title">mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</div>
<div class="meta-line">Authors: Jonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees, Elvis Nava</div>
<div class="meta-line">First: 2025-12-17T18:47:31+00:00 · Latest: 2025-12-19T18:30:30+00:00</div>
<div class="meta-line">Comments: Revised Introduction, Related Work, and Appendix. Additional minor notational and grammatical fixes</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15692v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.15692v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce mimic-video, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>mimic-video: 超越视觉-语言-动作模型（VLAs）的视频-动作模型用于可泛化的机器人控制</div>
<div class="mono" style="margin-top:8px">目前用于机器人操作的视觉-语言-动作模型（VLAs）基于在大规模、不连贯的静态网络数据上预训练的视觉-语言主干网络。因此，尽管在语义泛化方面有所提升，策略仍必须隐式地从机器人轨迹中推断复杂的物理动态和时间依赖性。这种依赖性导致了不可持续的数据负担，需要持续的大规模专家数据收集来弥补缺乏固有的物理理解。我们认为，虽然视觉-语言预训练可以有效捕捉语义先验，但它对物理因果关系是盲目的。一种更有效的范式是在预训练过程中利用视频同时捕捉语义和视觉动态，从而将剩余的低级控制任务孤立出来。为此，我们引入了mimic-video，这是一种新颖的视频-动作模型（VAM），它将一个在互联网规模视频数据上预训练的模型与一个基于流匹配的动作解码器结合，该解码器根据其潜在表示进行条件化。解码器作为逆动力学模型（IDM），从视频空间动作计划的潜在表示中生成低级机器人动作。我们的广泛评估表明，我们的方法在模拟和现实世界机器人操作任务中达到了最先进的性能，与传统VLA架构相比，提高了10倍的样本效率和2倍的收敛速度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data.</div>
</details>
</div>
<div class="card">
<div class="title">Visually Prompted Benchmarks Are Surprisingly Fragile</div>
<div class="meta-line">Authors: Haiwen Feng, Long Lian, Lisa Dunlap, Jiahao Shu, XuDong Wang, Renhao Wang, Trevor Darrell, Alane Suhr, Angjoo Kanazawa</div>
<div class="meta-line">First: 2025-12-19T18:26:58+00:00 · Latest: 2025-12-19T18:26:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17875v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17875v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://lisadunlap.github.io/vpbench/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A key challenge in evaluating VLMs is testing models&#x27; ability to analyze visual content independently from their textual priors. Recent benchmarks such as BLINK probe visual perception through visual prompting, where questions about visual content are paired with coordinates to which the question refers, with the coordinates explicitly marked in the image itself. While these benchmarks are an important part of VLM evaluation, we find that existing models are surprisingly fragile to seemingly irrelevant details of visual prompting: simply changing a visual marker from red to blue can completely change rankings among models on a leaderboard. By evaluating nine commonly-used open- and closed-source VLMs on two visually prompted tasks, we demonstrate how details in benchmark setup, including visual marker design and dataset size, have a significant influence on model performance and leaderboard rankings. These effects can even be exploited to lift weaker models above stronger ones; for instance, slightly increasing the size of the visual marker results in open-source InternVL3-8B ranking alongside or better than much larger proprietary models like Gemini 2.5 Pro. We further show that low-level inference choices that are often ignored in benchmarking, such as JPEG compression levels in API calls, can also cause model lineup changes. These details have substantially larger impacts on visually prompted benchmarks than on conventional semantic VLM evaluations. To mitigate this instability, we curate existing datasets to create VPBench, a larger visually prompted benchmark with 16 visual marker variants. VPBench and additional analysis tools are released at https://lisadunlap.github.io/vpbench/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉提示基准测试出人意料地脆弱</div>
<div class="mono" style="margin-top:8px">评估视觉语言模型（VLMs）的一个关键挑战是测试模型独立分析视觉内容的能力，而不依赖其文本先验知识。最近的基准测试，如BLINK，通过视觉提示来探测视觉感知，其中关于视觉内容的问题会与问题所指的坐标配对，这些坐标在图像本身中明确标记。尽管这些基准测试是VLM评估的重要组成部分，但我们发现现有的模型对视觉提示中看似无关的细节异常敏感：仅仅将视觉标记从红色改为蓝色，就可能完全改变模型在排行榜上的排名。通过在两个视觉提示任务上评估九个常用的开源和闭源VLMs，我们展示了基准测试设置中的细节，包括视觉标记的设计和数据集大小，对模型性能和排行榜排名有显著影响。这些影响甚至可以被用来提升较弱模型的排名；例如，略微增加视觉标记的大小，使得开源的InternVL3-8B模型能够与更大规模的专有模型如Gemini 2.5 Pro并列或表现更优。我们进一步表明，一些在基准测试中常被忽略的低级推理选择，如API调用中的JPEG压缩级别，也可能导致模型表现的变化。这些细节对视觉提示基准测试的影响远大于对传统语义VLM评估的影响。为缓解这种不稳定性，我们整理现有数据集，创建了包含16种视觉标记变体的更大规模的视觉提示基准测试VPBench。VPBench及额外分析工具已发布在https://lisadunlap.github.io/vpbench/。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">A key challenge in evaluating VLMs is testing models&#x27; ability to analyze visual content independently from their textual priors.</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Focus Memory for Language Models</div>
<div class="meta-line">Authors: Christopher Cruz</div>
<div class="meta-line">First: 2025-11-16T17:52:32+00:00 · Latest: 2025-12-19T18:24:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12712v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.12712v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, yet their behavior remains bottlenecked by naive history management strategies. Replaying the full conversation at every turn is simple but costly, while recency-based truncation or static summarization often causes early, high-impact user constraints to drift out of effective context. As a result, models may retain text without reliably applying it when it matters.
  We present Adaptive Focus Memory (AFM), a lightweight context management system that dynamically assigns each past message one of three fidelity levels: Full, Compressed, or Placeholder, based on semantic relevance, temporal decay, and importance classification. AFM packs messages chronologically under a fixed token budget, preserving critical constraints at high fidelity while allowing low-importance context to degrade gracefully.
  We evaluate AFM on two multi-turn dialogue benchmarks designed to stress long-horizon constraint preservation: a safety-critical travel scenario involving a user with a severe peanut allergy, and a policy-critical tax compliance scenario involving an illegal evasion request. Under strict grading that requires both explicit constraint recall and appropriately conditioned generation, AFM succeeds in 83.3 percent of allergy runs where all baseline strategies fail, and preserves correct refusal behavior on the tax benchmark.
  These results demonstrate that effective dialogue memory requires more than retaining prior text. Selectively allocating fidelity across past messages enables reliable constraint preservation under bounded context growth, without modifying model weights or introducing external retrieval infrastructure. We release an open-source implementation of AFM compatible with OpenAI-style chat APIs to support reproducible research and practical deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言模型的自适应焦点记忆</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越多地部署在多轮对话场景中，但其行为仍受限于简单的对话历史管理策略。每次回复都重放完整对话是简单但代价高昂的，而基于最近性的截断或静态摘要则常常导致早期、高影响的用户约束脱离有效上下文。因此，模型可能保留文本，但在关键时候无法可靠地应用。
我们提出了自适应焦点记忆（AFM），这是一种轻量级的上下文管理系统，根据语义相关性、时间衰减和重要性分类，动态地为每条历史消息分配三种保真度级别之一：完整、压缩或占位符。AFM在固定令牌预算下按时间顺序打包消息，保持关键约束的高保真度，同时允许低重要性上下文优雅地退化。
我们在两个多轮对话基准测试中评估AFM，这两个基准测试旨在测试长跨度约束保留能力：一个涉及严重花生过敏用户的高安全要求旅行场景，另一个涉及非法避税请求的高政策要求税务合规场景。在严格评分标准下，要求模型同时具备显式约束回忆和适当条件生成，AFM在所有基线策略失败的情况下，在过敏场景中成功率达到83.3%，并在税务基准测试中保留正确的拒绝行为。
这些结果表明，有效的对话记忆不仅需要保留先前文本。在有限的上下文增长范围内，选择性地分配保真度可以实现可靠的约束保留，而无需修改模型权重或引入外部检索基础设施。我们发布了与OpenAI风格聊天API兼容的AFM开源实现，以支持可重复的研究和实际部署。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) are increasingly deployed in multi-turn dialogue settings, yet their behavior remains bottlenecked by naive history management strategies.</div>
</details>
</div>
<div class="card">
<div class="title">InSPECT: Invariant Spectral Features Preservation of Diffusion Models</div>
<div class="meta-line">Authors: Baohua Yan, Qingyuan Liu, Jennifer Kava, Xuan Di</div>
<div class="meta-line">First: 2025-12-19T18:24:02+00:00 · Latest: 2025-12-19T18:24:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17873v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17873v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern diffusion models (DMs) have achieved state-of-the-art image generation. However, the fundamental design choice of diffusing data all the way to white noise and then reconstructing it leads to an extremely difficult and computationally intractable prediction task. To overcome this limitation, we propose InSPECT (Invariant Spectral Feature-Preserving Diffusion Model), a novel diffusion model that keeps invariant spectral features during both the forward and backward processes. At the end of the forward process, the Fourier coefficients smoothly converge to a specified random noise, enabling features preservation while maintaining diversity and randomness. By preserving invariant features, InSPECT demonstrates enhanced visual diversity, faster convergence rate, and a smoother diffusion process. Experiments on CIFAR-10, Celeb-A, and LSUN demonstrate that InSPECT achieves on average a 39.23% reduction in FID and 45.80% improvement in IS against DDPM for 10K iterations under specified parameter settings, which demonstrates the significant advantages of preserving invariant features: achieving superior generation quality and diversity, while enhancing computational efficiency and enabling faster convergence rate. To the best of our knowledge, this is the first attempt to analyze and preserve invariant spectral features in diffusion models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InSPECT：扩散模型中不变谱特征的保持</div>
<div class="mono" style="margin-top:8px">现代扩散模型（DMs）在图像生成方面已达到最先进的水平。然而，将数据全部扩散到白噪声然后再进行重建的基本设计选择导致了极其困难且计算不可行的预测任务。为克服这一限制，我们提出了InSPECT（不变谱特征保持扩散模型），这是一种在正向和反向过程中保持不变谱特征的新型扩散模型。在正向过程结束时，傅里叶系数平滑地收敛到指定的随机噪声，从而在保持特征的同时维持多样性和随机性。通过保持不变特征，InSPECT展示了增强的视觉多样性、更快的收敛速度和更平滑的扩散过程。在CIFAR-10、Celeb-A和LSUN数据集上的实验表明，在指定参数设置下，与DDPM相比，InSPECT在10,000次迭代中平均将FID降低了39.23%，IS提升了45.80%，这证明了保持不变谱特征的显著优势：实现更优的生成质量和多样性，同时提升计算效率并实现更快的收敛速度。据我们所知，这是首次在扩散模型中分析并保持不变谱特征。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Gaussian Process Proximal Policy Optimization</div>
<div class="meta-line">Authors: Matthijs van der Lende, Juan Cardenas-Cartagena</div>
<div class="meta-line">First: 2025-11-22T23:13:04+00:00 · Latest: 2025-12-19T18:23:00+00:00</div>
<div class="meta-line">Comments: Withdrawn by the authors as the manuscript is not yet complete; no updated version is available at this time</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.18214v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.18214v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Uncertainty estimation for Reinforcement Learning (RL) is a critical component in control tasks where agents must balance safe exploration and efficient learning. While deep neural networks have enabled breakthroughs in RL, they often lack calibrated uncertainty estimates. We introduce Deep Gaussian Process Proximal Policy Optimization (GPPO), a scalable, model-free actor-critic algorithm that leverages Deep Gaussian Processes (DGPs) to approximate both the policy and value function. GPPO maintains competitive performance with respect to Proximal Policy Optimization on standard high-dimensional continuous control benchmarks while providing well-calibrated uncertainty estimates that can inform safer and more effective exploration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度高斯过程近端策略优化</div>
<div class="mono" style="margin-top:8px">不确定性估计在强化学习（RL）中是控制任务的关键组成部分，其中智能体必须在安全探索和高效学习之间取得平衡。尽管深度神经网络在RL中实现了突破，但它们通常缺乏校准的不确定性估计。我们引入了深度高斯过程近端策略优化（GPPO），这是一种可扩展的无模型演员-评论家算法，利用深度高斯过程（DGPs）来近似策略和价值函数。GPPO在标准的高维连续控制基准测试中保持与近端策略优化（PPO）相当的性能，同时提供校准良好的不确定性估计，从而支持更安全和有效的探索。</div>
</details>
</div>
<div class="card">
<div class="title">Data for Mathematical Copilots: Better Ways of Presenting Proofs for Machine Learning</div>
<div class="meta-line">Authors: Simon Frieder, Jonas Bayer, Sam Looi, Jacob Loader, Julius Berner, Katherine M. Collins, András Juhász, Fabian Ruehle, Sean Welleck, Gabriel Poesia, Ryan-Rhys Griffiths, Adrian Weller, Anirudh Goyal, Cameron Freer, Thomas Lukasiewicz, Timothy Gowers</div>
<div class="meta-line">First: 2024-12-19T18:55:17+00:00 · Latest: 2025-12-19T18:17:28+00:00</div>
<div class="meta-line">Comments: 59 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.15184v2">Abs</a> · <a href="https://arxiv.org/pdf/2412.15184v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The datasets and benchmarks commonly used to train and evaluate the mathematical capabilities of AI-based mathematical copilots (primarily large language models) exhibit several shortcomings and misdirections. These range from a restricted scope of mathematical complexity to limited fidelity in capturing aspects beyond the final, written proof (e.g. motivating the proof, or representing the thought processes leading to a proof). These issues are compounded by a dynamic reminiscent of Goodhart&#x27;s law: as benchmark performance becomes the primary target for model development, the benchmarks themselves become less reliable indicators of genuine mathematical capability. We systematically explore these limitations and contend that enhancing the capabilities of large language models, or any forthcoming advancements in AI-based mathematical assistants (copilots or ``thought partners&#x27;&#x27;), necessitates a course correction both in the design of mathematical datasets and the evaluation criteria of the models&#x27; mathematical ability. In particular, it is necessary for benchmarks to move beyond the existing result-based datasets that map theorem statements directly to proofs, and instead focus on datasets that translate the richer facets of mathematical research practice into data that LLMs can learn from. This includes benchmarks that supervise the proving process and the proof discovery process itself, and we advocate for mathematical dataset developers to consider the concept of &quot;motivated proof&quot;, introduced by G. Pólya in 1949, which can serve as a blueprint for datasets that offer a better proof learning signal, alleviating some of the mentioned limitations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>数学助手的数据：为机器学习提供更好的证明呈现方式</div>
<div class="mono" style="margin-top:8px">用于训练和评估基于人工智能的数学助手（主要是大语言模型）的数学数据集和基准测试存在多个缺陷和误导。这些问题包括数学复杂度范围受限，以及在捕捉证明之外的要素（如证明动机或证明过程中的思维）时缺乏保真度。这些问题在某种程度上被“好哈特定律”（Goodhart&#x27;s law）所加剧：随着基准测试性能成为模型开发的主要目标，这些基准测试本身逐渐成为衡量真实数学能力的不可靠指标。我们系统地探讨了这些限制，并认为提升大语言模型的能力，或任何未来人工智能数学助手（助手或“思维伙伴”）的发展，都需要在数学数据集的设计和模型数学能力的评估标准上进行调整。具体而言，基准测试需要超越现有的基于结果的数据集，这些数据集直接将定理陈述映射到证明，而应关注能够将数学研究实践的更丰富方面转化为LLMs可学习数据的基准测试。这包括监督证明过程和发现过程的基准测试，我们倡导数学数据集开发者考虑1949年G. Pólya提出的“动机证明”概念，该概念可作为提供更优证明学习信号的数据集蓝图，从而缓解一些提到的限制。</div>
</details>
</div>
<div class="card">
<div class="title">Interpretable Plant Leaf Disease Detection Using Attention-Enhanced CNN</div>
<div class="meta-line">Authors: Balram Singh, Ram Prakash Sharma, Somnath Dey</div>
<div class="meta-line">First: 2025-12-19T18:11:15+00:00 · Latest: 2025-12-19T18:11:15+00:00</div>
<div class="meta-line">Comments: 27 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17864v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17864v1">PDF</a> · <a href="https://github.com/BS0111/PlantAttentionCBAM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Plant diseases pose a significant threat to global food security, necessitating accurate and interpretable disease detection methods. This study introduces an interpretable attention-guided Convolutional Neural Network (CNN), CBAM-VGG16, for plant leaf disease detection. By integrating Convolution Block Attention Module (CBAM) at each convolutional stage, the model enhances feature extraction and disease localization. Trained on five diverse plant disease datasets, our approach outperforms recent techniques, achieving high accuracy (up to 98.87%) and demonstrating robust generalization. Here, we show the effectiveness of our method through comprehensive evaluation and interpretability analysis using CBAM attention maps, Grad-CAM, Grad-CAM++, and Layer-wise Relevance Propagation (LRP). This study advances the application of explainable AI in agricultural diagnostics, offering a transparent and reliable system for smart farming. The code of our proposed work is available at https://github.com/BS0111/PlantAttentionCBAM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于注意力增强卷积神经网络的可解释植物叶片病害检测</div>
<div class="mono" style="margin-top:8px">植物病害对全球粮食安全构成重大威胁，因此需要准确且可解释的病害检测方法。本研究提出了一种可解释的注意力引导卷积神经网络（CNN）CBAM-VGG16，用于植物叶片病害检测。通过在每个卷积阶段集成卷积块注意力模块（CBAM），该模型增强了特征提取和病害定位能力。在五个多样化的植物病害数据集上进行训练，我们的方法优于近期技术，实现了高达98.87%的高准确率，并展示了强大的泛化能力。通过使用CBAM注意力图、Grad-CAM、Grad-CAM++和层间相关性传播（LRP）进行综合评估和可解释性分析，我们展示了该方法的有效性。本研究推动了可解释AI在农业诊断中的应用，为智能农业提供了一个透明且可靠系统。我们提出的代码可在https://github.com/BS0111/PlantAttentionCBAM获取。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Human-Guided, Data-Centric LLM Co-Pilots</div>
<div class="meta-line">Authors: Evgeny Saveliev, Jiashuo Liu, Nabeel Seedat, Anders Boyd, Mihaela van der Schaar</div>
<div class="meta-line">First: 2025-01-17T17:51:22+00:00 · Latest: 2025-12-19T18:08:16+00:00</div>
<div class="meta-line">Comments: Saveliev, Liu &amp; Seedat contributed equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.10321v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.10321v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning (ML) has the potential to revolutionize various domains, but its adoption is often hindered by the disconnect between the needs of domain experts and translating these needs into robust and valid ML tools. Despite recent advances in LLM-based co-pilots to democratize ML for non-technical domain experts, these systems remain predominantly focused on model-centric aspects while overlooking critical data-centric challenges. This limitation is problematic in complex real-world settings where raw data often contains complex issues, such as missing values, label noise, and domain-specific nuances requiring tailored handling. To address this we introduce CliMB-DC, a human-guided, data-centric framework for LLM co-pilots that combines advanced data-centric tools with LLM-driven reasoning to enable robust, context-aware data processing. At its core, CliMB-DC introduces a novel, multi-agent reasoning system that combines a strategic coordinator for dynamic planning and adaptation with a specialized worker agent for precise execution. Domain expertise is then systematically incorporated to guide the reasoning process using a human-in-the-loop approach. To guide development, we formalize a taxonomy of key data-centric challenges that co-pilots must address. Thereafter, to address the dimensions of the taxonomy, we integrate state-of-the-art data-centric tools into an extensible, open-source architecture, facilitating the addition of new tools from the research community. Empirically, using real-world healthcare datasets we demonstrate CliMB-DC&#x27;s ability to transform uncurated datasets into ML-ready formats, significantly outperforming existing co-pilot baselines for handling data-centric challenges. CliMB-DC promises to empower domain experts from diverse domains -- healthcare, finance, social sciences and more -- to actively participate in driving real-world impact using ML.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向人类引导的数据中心化LLM协作者</div>
<div class="mono" style="margin-top:8px">机器学习（ML）有潜力革新多个领域，但其应用常因领域专家需求与将这些需求转化为稳健且有效的ML工具之间的脱节而受阻。尽管近期基于LLM的协作者在使非技术领域的专家能够民主化使用ML方面取得了进展，但这些系统仍主要关注模型中心化方面，而忽视了关键的数据中心化挑战。这种局限性在复杂的真实世界场景中尤为突出，因为原始数据通常包含复杂问题，如缺失值、标签噪声和需要定制处理的领域特定细节。为了解决这一问题，我们引入了CliMB-DC，这是一个面向人类引导的数据中心化框架，用于LLM协作者，结合了先进的数据中心化工具与LLM驱动的推理，以实现稳健且上下文感知的数据处理。其核心是一个新颖的多智能体推理系统，结合了用于动态规划和适应的战略协调者与用于精确执行的专业工作者智能体。随后，通过人机协同的方式，系统性地将领域专业知识融入推理过程。为了指导开发，我们正式化了一种关键数据中心化挑战的分类体系，该体系涵盖了协作者必须应对的各个方面。接着，为了应对该分类体系的各个维度，我们将最先进的数据中心化工具整合到一个可扩展的开源架构中，便于研究社区添加新的工具。实证研究表明，使用真实世界医疗数据集，我们展示了CliMB-DC将未整理数据集转化为ML就绪格式的能力，其在处理数据中心化挑战方面的表现显著优于现有协作者基线。CliMB-DC有望赋能来自不同领域的领域专家——包括医疗、金融、社会科学等——积极利用ML推动实际应用。</div>
</details>
</div>
<div class="card">
<div class="title">Low-Rank Filtering and Smoothing for Sequential Deep Learning</div>
<div class="meta-line">Authors: Joanna Sliwa, Frank Schneider, Nathanael Bosch, Agustinus Kristiadi, Philipp Hennig</div>
<div class="meta-line">First: 2024-10-09T11:54:33+00:00 · Latest: 2025-12-19T18:07:37+00:00</div>
<div class="meta-line">Comments: Revised version: improved presentation and added experiments</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.06800v2">Abs</a> · <a href="https://arxiv.org/pdf/2410.06800v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning multiple tasks sequentially requires neural networks to balance retaining knowledge, yet being flexible enough to adapt to new tasks. Regularizing network parameters is a common approach, but it rarely incorporates prior knowledge about task relationships, and limits information flow to future tasks only. We propose a Bayesian framework that treats the network&#x27;s parameters as the state space of a nonlinear Gaussian model, unlocking two key capabilities: (1) A principled way to encode domain knowledge about task relationships, allowing, e.g., control over which layers should adapt between tasks. (2) A novel application of Bayesian smoothing, allowing task-specific models to also incorporate knowledge from models learned later. This does not require direct access to their data, which is crucial, e.g., for privacy-critical applications. These capabilities rely on efficient filtering and smoothing operations, for which we propose diagonal plus low-rank approximations of the precision matrix in the Laplace approximation (LR-LGF). Empirical results demonstrate the efficiency of LR-LGF and the benefits of the unlocked capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于序列深度学习的低秩滤波与平滑</div>
<div class="mono" style="margin-top:8px">学习多个任务时，神经网络需要在保留已有知识的同时保持足够的灵活性以适应新任务。常规的参数正则化方法很少考虑任务间关系的先验知识，并且仅限制信息流向未来任务。我们提出了一种贝叶斯框架，将网络参数视为非线性高斯模型的状态空间，从而解锁了两项关键能力：(1) 一种原理化的方法来编码任务关系的领域知识，例如控制哪些层应在任务之间进行适应；(2) 贝叶斯平滑的一种新应用，使任务特定模型也能结合后续学习模型的知识。这种方法不需要直接访问后续模型的数据，这对于隐私关键型应用至关重要。这些能力依赖于高效的滤波与平滑操作，为此我们提出在拉普拉斯近似中使用对角线加低秩的精度矩阵近似（LR-LGF）。实验证明了LR-LGF的效率以及解锁能力的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Probing electroweak pair production of heavy neutral leptons with displaced vertices at the LHC</div>
<div class="meta-line">Authors: Stéphane Lavignac, Anibal D. Medina, Nicolás I. Mileo, Santiago Tanco</div>
<div class="meta-line">First: 2025-12-19T17:58:48+00:00 · Latest: 2025-12-19T17:58:48+00:00</div>
<div class="meta-line">Comments: 22 pages, 9 pdf figures and 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17857v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17857v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the sensitivity of displaced vertex searches at the LHC to heavy neutral leptons (also known as sterile neutrinos) that are produced in pairs with an electroweak-size cross section. We work within the context of a supersymmetric model in which the sterile neutrino is produced along with Standard Model particles in higgsino decays. By making use of model-independent reconstruction efficiencies provided by the ATLAS collaboration in their search for displaced vertices with multiple jets, we obtain constraints on this model from $139$ fb$^{-1}$ of data collected by ATLAS during the LHC Run~2, and assess the discovery reach of Run~3 and of the high-luminosity LHC (HL-LHC). Depending on the higgsino mass parameter, sterile neutrino masses between $20~\mathrm{GeV}$ and $230~\mathrm{GeV}$ and active-sterile neutrino mixings in the range $4 \times 10^{-14} \lesssim V^2_N \lesssim 3 \times 10^{-10}$ can be excluded. At the HL-LHC, discovery-level significances could be reached for sterile neutrinos masses up to $295~\mathrm{GeV}$ and values of $V^2_N$ down to $3 \times 10^{-14}$. Finally, moving away from the supersymmetric scenario, we study to which extent these results can be generalized to a broader class of models in which the sterile neutrinos are produced in the decays of heavier particles that are themselves pair-produced with an electroweak-size cross section.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用LHC上位移顶点探测重中性轻子的电弱对产生</div>
<div class="mono" style="margin-top:8px">我们研究了LHC上位移顶点搜寻对通过电弱截面产生的重中性轻子（也称为无质量中微子）的灵敏度。我们基于一个超对称模型，在该模型中无质量中微子与标准模型粒子一起在higgsino衰变中产生。通过使用ATLAS合作在多喷注位移顶点搜寻中提供的模型无关的重建效率，我们利用LHC Run~2期间ATLAS收集的$139$ fb$^{-1}$数据对该模型进行约束，并评估Run~3和高亮度LHC（HL-LHC）的发现潜力。根据higgsino质量参数的不同，可排除的无质量中微子质量范围为$20~\mathrm{GeV}$至$230~\mathrm{GeV}$，以及$4 \times 10^{-14} \lesssim V^2_N \lesssim 3 \times 10^{-10}$范围内的活性-无质量中微子混合。在HL-LHC上，对于无质量中微子质量高达$295~\mathrm{GeV}$和$V^2_N$低至$3 \times 10^{-14}$的情况，可以达到发现级别的显著性。最后，脱离超对称情景，我们研究这些结果在更广泛模型中的适用性，这些模型中无质量中微子通过更重粒子的衰变产生，而这些更重粒子本身也是通过电弱截面成对产生。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We study the sensitivity of displaced vertex searches at the LHC to heavy neutral leptons (also known as sterile neutrinos) that are produced in pairs with an electroweak-size cross section.</div>
</details>
</div>
<div class="card">
<div class="title">AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning</div>
<div class="meta-line">Authors: Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel, Valerio Pepe, Riedana Yan, Ahmet Gundogdu, Ivan Kapelyukh, Ali Abbas, Xiaoqiang Yan, Harsh Patel, Laura Herlant, Karl Schmeckpeper</div>
<div class="meta-line">First: 2025-12-19T17:55:48+00:00 · Latest: 2025-12-19T17:55:48+00:00</div>
<div class="meta-line">Comments: 28 pages, 25 figures. The first four authors contributed equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17853v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.17853v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data. We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards. We train behavior cloning policies on generated data, validate them in simulation, and deploy them directly on real robot hardware. The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks. Our project website is at https://anytask.rai-inst.com .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnyTask：一种用于推进模拟到现实政策学习的自动化任务与数据生成框架</div>
<div class="mono" style="margin-top:8px">通用型机器人学习仍受限于数据：大规模、多样且高质量的交互数据在现实世界中收集成本高昂。尽管模拟已成为扩展数据收集的有前景方法，但相关任务，包括模拟任务设计、任务感知场景生成、专家演示合成以及模拟到现实的迁移，仍需要大量的人工努力。我们提出了AnyTask，一个结合大规模并行GPU模拟与基础模型的自动化框架，用于设计多样化的操作任务并合成机器人数据。我们引入了三个AnyTask代理，旨在生成能够解决尽可能多任务的专家演示：1）ViPR，一种具有VLM-in-the-loop并行细化的新型任务与运动规划代理；2）ViPR-Eureka，一种具有生成密集奖励和LLM引导接触采样的强化学习代理；3）ViPR-RL，一种混合规划与学习方法，仅使用稀疏奖励即可联合生成高质量的演示。我们在生成的数据上训练行为克隆策略，验证其在模拟中的表现，并直接部署到真实机器人硬件上。这些策略能够泛化到新的物体姿态，在一系列现实世界的抓取与放置、抽屉开启、接触密集的推搡以及长时序操作任务中，平均成功率达到44%。我们的项目网站为https://anytask.rai-inst.com。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
