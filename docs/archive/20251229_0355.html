<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-29 03:55</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251229_0355</div>
    <div class="row"><div class="card">
<div class="title">HiStream: Efficient High-Resolution Video Generation via Redundancy-Eliminated Streaming</div>
<div class="meta-line">Authors: Haonan Qiu, Shikun Liu, Zijian Zhou, Zhaochong An, Weiming Ren, Zhiheng Liu, Jonas Schult, Sen He, Shoufa Chen, Yuren Cong, Tao Xiang, Ziwei Liu, Juan-Manuel Perez-Rua</div>
<div class="meta-line">First: 2025-12-24T18:59:58+00:00 · Latest: 2025-12-24T18:59:58+00:00</div>
<div class="meta-line">Comments: Project Page: http://haonanqiu.com/projects/HiStream.html</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21338v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21338v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="http://haonanqiu.com/projects/HiStream.html">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible. To address this, we introduce HiStream, an efficient autoregressive framework that systematically reduces redundancy across three axes: i) Spatial Compression: denoising at low resolution before refining at high resolution with cached features; ii) Temporal Compression: a chunk-by-chunk strategy with a fixed-size anchor cache, ensuring stable inference speed; and iii) Timestep Compression: applying fewer denoising steps to subsequent, cache-conditioned chunks. On 1080p benchmarks, our primary HiStream model (i+ii) achieves state-of-the-art visual quality while demonstrating up to 76.2x faster denoising compared to the Wan2.1 baseline and negligible quality loss. Our faster variant, HiStream+, applies all three optimizations (i+ii+iii), achieving a 107.5x acceleration over the baseline, offering a compelling trade-off between speed and quality, thereby making high-resolution video generation both practical and scalable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HiStream：通过冗余消除流式处理实现高效高分辨率视频生成</div>
<div class="mono" style="margin-top:8px">高分辨率视频生成对于数字媒体和电影至关重要，但扩散模型的二次复杂度使其计算上存在瓶颈，导致实际推理不可行。为了解决这一问题，我们引入了HiStream，一种高效的自回归框架，通过三个维度系统性地减少冗余：i) 空间压缩：在低分辨率下进行去噪，然后利用缓存特征进行高分辨率细化；ii) 时间压缩：采用固定大小的锚点缓存的分块策略，确保推理速度稳定；iii) 时间步压缩：对后续的缓存条件分块应用较少的去噪步骤。在1080p基准测试中，我们的主要HiStream模型（i+ii）在视觉质量上达到最先进水平，同时比Wan2.1基线快76.2倍，且质量损失可忽略。我们的更快版本HiStream+应用了所有三项优化（i+ii+iii），相比基线实现了107.5倍的加速，提供了速度与质量之间的良好平衡，从而使高分辨率视频生成既实用又可扩展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">High-resolution video generation, while crucial for digital media and film, is computationally bottlenecked by the quadratic complexity of diffusion models, making practical inference infeasible.</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Memorization: A Multi-Modal Ordinal Regression Benchmark to Expose Popularity Bias in Vision-Language Models</div>
<div class="meta-line">Authors: Li-Zhong Szu-Tu, Ting-Lin Wu, Chia-Jui Chang, He Syu, Yu-Lun Liu</div>
<div class="meta-line">First: 2025-12-24T18:59:54+00:00 · Latest: 2025-12-24T18:59:54+00:00</div>
<div class="meta-line">Comments: Project page: https://sytwu.github.io/BeyondMemo/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21337v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21337v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sytwu.github.io/BeyondMemo/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We expose a significant popularity bias in state-of-the-art vision-language models (VLMs), which achieve up to 34% higher accuracy on famous buildings compared to ordinary ones, indicating a reliance on memorization over generalizable understanding. To systematically investigate this, we introduce the largest open benchmark for this task: the YearGuessr dataset, a collection of 55,546 building images with multi-modal attributes from 157 countries, annotated with continuous ordinal labels of their construction year (1001-2024), GPS data, and page-view counts as a proxy for popularity. Using this dataset, we frame the construction year prediction task as ordinal regression and introduce popularity-aware interval accuracy metrics to quantify this bias. Our resulting benchmark of 30+ models, including our YearCLIP model, confirms that VLMs excel on popular, memorized items but struggle significantly with unrecognized subjects, exposing a critical flaw in their reasoning capabilities. Project page: https://sytwu.github.io/BeyondMemo/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越记忆：揭示视觉-语言模型中流行性偏差的多模态序数回归基准测试</div>
<div class="mono" style="margin-top:8px">我们揭示了当前最先进的视觉-语言模型（VLMs）中存在显著的流行性偏差，这些模型在著名建筑上的准确率比普通建筑高出高达34%，表明其依赖于记忆而非可泛化的理解。为系统地研究这一现象，我们引入了目前最大的开放基准数据集：YearGuessr数据集，包含来自157个国家的55,546张建筑图像，标注了建筑建造年份的连续序数标签（1001-2024）、GPS数据以及页面浏览次数作为流行度的代理指标。利用该数据集，我们将建造年份预测任务框架为序数回归问题，并引入了流行性感知的区间准确率度量指标以量化这种偏差。我们对30多个模型（包括我们的YearCLIP模型）的基准测试结果表明，VLMs在流行且被记忆的项目上表现优异，但在未被识别的主题上则显著表现不佳，暴露出其推理能力的关键缺陷。项目页面：https://sytwu.github.io/BeyondMemo/</div>
</details>
</div>
<div class="card">
<div class="title">Optimizing Decoding Paths in Masked Diffusion Models by Quantifying Uncertainty</div>
<div class="meta-line">Authors: Ziyu Chen, Xinbei Jiang, Peng Sun, Tao Lin</div>
<div class="meta-line">First: 2025-12-24T18:59:51+00:00 · Latest: 2025-12-24T18:59:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21336v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21336v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Masked Diffusion Models (MDMs) offer flexible, non-autoregressive generation, but this freedom introduces a challenge: final output quality is highly sensitive to the decoding order. We are the first to formalize this issue, attributing the variability in output quality to the cumulative predictive uncertainty along a generative path. To quantify this uncertainty, we introduce Denoising Entropy, a computable metric that serves as an internal signal for evaluating generative process. Leveraging this metric, we propose two algorithms designed to optimize the decoding path: a post-hoc selection method and a real-time guidance strategy. Experiments demonstrate that our entropy-guided methods significantly improve generation quality, consistently boosting accuracy on challenging reasoning, planning, and code benchmarks. Our work establishes Denoising Entropy as a principled tool for understanding and controlling generation, effectively turning the uncertainty in MDMs from a liability into a key advantage for discovering high-quality solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过量化不确定性优化掩码扩散模型的解码路径</div>
<div class="mono" style="margin-top:8px">掩码扩散模型（MDMs）提供了灵活的非自回归生成能力，但这种自由也带来了挑战：最终输出质量高度依赖于解码顺序。我们首次正式提出这一问题，将输出质量的差异归因于生成路径上累积的预测不确定性。为量化这种不确定性，我们引入了去噪熵（Denoising Entropy），这是一种可计算的度量指标，可作为评估生成过程的内部信号。利用这一度量，我们提出了两种优化解码路径的算法：一种是后处理选择方法，另一种是实时引导策略。实验表明，我们的熵引导方法显著提升了生成质量，在具有挑战性的推理、规划和代码基准测试中持续提高准确性。我们的工作确立了去噪熵作为理解和控制生成过程的原理性工具，有效地将MDMs中的不确定性从劣势转化为发现高质量解的有利条件。</div>
</details>
</div>
<div class="card">
<div class="title">Autonomous Uncertainty Quantification for Computational Point-of-care Sensors</div>
<div class="meta-line">Authors: Artem Goncharov, Rajesh Ghosh, Hyou-Arm Joung, Dino Di Carlo, Aydogan Ozcan</div>
<div class="meta-line">First: 2025-12-24T18:59:47+00:00 · Latest: 2025-12-24T18:59:47+00:00</div>
<div class="meta-line">Comments: 18 Pages, 5 Figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21335v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21335v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Computational point-of-care (POC) sensors enable rapid, low-cost, and accessible diagnostics in emergency, remote and resource-limited areas that lack access to centralized medical facilities. These systems can utilize neural network-based algorithms to accurately infer a diagnosis from the signals generated by rapid diagnostic tests or sensors. However, neural network-based diagnostic models are subject to hallucinations and can produce erroneous predictions, posing a risk of misdiagnosis and inaccurate clinical decisions. To address this challenge, here we present an autonomous uncertainty quantification technique developed for POC diagnostics. As our testbed, we used a paper-based, computational vertical flow assay (xVFA) platform developed for rapid POC diagnosis of Lyme disease, the most prevalent tick-borne disease globally. The xVFA platform integrates a disposable paper-based assay, a handheld optical reader and a neural network-based inference algorithm, providing rapid and cost-effective Lyme disease diagnostics in under 20 min using only 20 uL of patient serum. By incorporating a Monte Carlo dropout (MCDO)-based uncertainty quantification approach into the diagnostics pipeline, we identified and excluded erroneous predictions with high uncertainty, significantly improving the sensitivity and reliability of the xVFA in an autonomous manner, without access to the ground truth diagnostic information of patients. Blinded testing using new patient samples demonstrated an increase in diagnostic sensitivity from 88.2% to 95.7%, indicating the effectiveness of MCDO-based uncertainty quantification in enhancing the robustness of neural network-driven computational POC sensing systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于计算型即时检测传感器的自主不确定性量化</div>
<div class="mono" style="margin-top:8px">计算型即时检测（POC）传感器能够在缺乏集中医疗设施的紧急、偏远和资源有限地区，实现快速、低成本且易于获取的诊断。这些系统可以利用基于神经网络的算法，从快速诊断测试或传感器生成的信号中准确推断诊断结果。然而，基于神经网络的诊断模型容易产生幻觉，导致错误预测，从而带来误诊和临床决策不准确的风险。为了解决这一挑战，我们提出了一种用于POC诊断的自主不确定性量化技术。作为测试平台，我们使用了一种用于快速诊断莱姆病（全球最常见的蜱传疾病）的纸基计算型垂直流动分析（xVFA）平台。该xVFA平台集成了可丢弃的纸基检测模块、手持光学读数器和基于神经网络的推理算法，仅需20微升患者血清即可在20分钟内提供快速且经济的莱姆病诊断。通过将基于蒙特卡洛丢弃（MCDO）的不确定性量化方法整合到诊断流程中，我们识别并排除了高不确定性的错误预测，显著提高了xVFA的敏感性和可靠性，且无需访问患者的实际诊断信息。使用新的患者样本进行盲测显示诊断敏感性从88.2%提升至95.7%，表明基于MCDO的不确定性量化在增强神经网络驱动的计算型POC传感系统的鲁棒性方面具有有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Computational point-of-care (POC) sensors enable rapid, low-cost, and accessible diagnostics in emergency, remote and resource-limited areas that lack access to centralized medical facilities.</div>
</details>
</div>
<div class="card">
<div class="title">Streaming Video Instruction Tuning</div>
<div class="meta-line">Authors: Jiaer Xia, Peixian Chen, Mengdan Zhang, Xing Sun, Kaiyang Zhou</div>
<div class="meta-line">First: 2025-12-24T18:59:36+00:00 · Latest: 2025-12-24T18:59:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21334v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21334v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant. Unlike existing online video models that focus narrowly on question answering or captioning, Streamo performs a broad spectrum of streaming video tasks, including real-time narration, action understanding, event captioning, temporal event grounding, and time-sensitive question answering. To develop such versatility, we construct Streamo-Instruct-465K, a large-scale instruction-following dataset tailored for streaming video understanding. The dataset covers diverse temporal contexts and multi-task supervision, enabling unified training across heterogeneous streaming tasks. After training end-to-end on the instruction-following dataset through a streamlined pipeline, Streamo exhibits strong temporal reasoning, responsive interaction, and broad generalization across a variety of streaming benchmarks. Extensive experiments show that Streamo bridges the gap between offline video perception models and real-time multimodal assistants, making a step toward unified, intelligent video understanding in continuous video streams.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>流媒体视频指令微调</div>
<div class="mono" style="margin-top:8px">我们提出了Streamo，这是一个实时流媒体视频大语言模型，作为通用的交互式助手。与现有专注于问答或字幕生成的在线视频模型不同，Streamo能够执行广泛的流媒体视频任务，包括实时解说、动作理解、事件字幕生成、时间事件定位以及时间敏感问题回答。为了实现这种多功能性，我们构建了Streamo-Instruct-465K，一个专门用于流媒体视频理解的大规模指令遵循数据集。该数据集涵盖了多样化的时序上下文和多任务监督，使得异构流媒体任务能够统一训练。在通过简化流程对指令遵循数据集进行端到端训练后，Streamo展现出强大的时序推理能力、响应式交互以及在多种流媒体基准测试中的广泛泛化能力。大量实验表明，Streamo弥合了离线视频感知模型与实时多模态助手之间的差距，朝着统一、智能的连续视频流理解迈出了重要一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present Streamo, a real-time streaming video LLM that serves as a general-purpose interactive assistant.</div>
</details>
</div>
<div class="card">
<div class="title">Fast SAM2 with Text-Driven Token Pruning</div>
<div class="meta-line">Authors: Avilasha Mandal, Chaoning Zhang, Fachrina Dewi Puspitasari, Xudong Wang, Jiaquan Zhang, Caiyan Qin, Guoqing Wang, Yang Yang, Heng Tao Shen</div>
<div class="meta-line">First: 2025-12-24T18:59:05+00:00 · Latest: 2025-12-24T18:59:05+00:00</div>
<div class="meta-line">Comments: 28 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21333v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21333v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Segment Anything Model 2 (SAM2), a vision foundation model has significantly advanced in prompt-driven video object segmentation, yet their practical deployment remains limited by the high computational and memory cost of processing dense visual tokens across time. The SAM2 pipelines typically propagate all visual tokens produced by the image encoder through downstream temporal reasoning modules, regardless of their relevance to the target object, resulting in reduced scalability due to quadratic memory attention overhead. In this work, we introduce a text-guided token pruning framework that improves inference efficiency by selectively reducing token density prior to temporal propagation, without modifying the underlying segmentation architecture. Operating after visual encoding and before memory based propagation, our method ranks tokens using a lightweight routing mechanism that integrates local visual context, semantic relevance derived from object-centric textual descriptions (either user-provided or automatically generated), and uncertainty cues that help preserve ambiguous or boundary critical regions. By retaining only the most informative tokens for downstream processing, the proposed approach reduces redundant computation while maintaining segmentation fidelity. Extensive experiments across multiple challenging video segmentation benchmarks demonstrate that post-encoder token pruning provides a practical and effective pathway to efficient, prompt-aware video segmentation, achieving up to 42.50 percent faster inference and 37.41 percent lower GPU memory usage compared to the unpruned baseline SAM2, while preserving competitive J and F performance. These results highlight the potential of early token selection to improve the scalability of transformer-based video segmentation systems for real-time and resource-constrained applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于文本驱动的标记剪枝的快速SAM2</div>
<div class="mono" style="margin-top:8px">Segment Anything Model 2 (SAM2) 是一种视觉基础模型，在基于提示的视频对象分割领域取得了显著进展。然而，其实际部署仍受限于处理密集视觉标记所带来的高计算和内存成本。SAM2 管道通常会将图像编码器生成的所有视觉标记传播到下游的时间推理模块中，无论这些标记是否与目标对象相关，导致由于二次内存注意力开销而降低了可扩展性。在本工作中，我们引入了一种文本引导的标记剪枝框架，通过在时间传播前选择性地降低标记密度来提高推理效率，而无需修改底层的分割架构。该方法在视觉编码之后、基于内存的传播之前运行，利用一种轻量级路由机制对标记进行排序，该机制结合了局部视觉上下文、从以对象为中心的文本描述中提取的语义相关性（无论是用户提供的还是自动生成的），以及有助于保留模糊或边界关键区域的不确定性提示。通过仅保留最具有信息量的标记用于下游处理，所提出的方法减少了冗余计算，同时保持了分割的保真度。在多个具有挑战性的视频分割基准上进行的广泛实验表明，编码器后的标记剪枝为高效且提示感知的视频分割提供了一条实用且有效的方法路径，相比未剪枝的基线SAM2，推理速度提高了最高42.50%，GPU内存使用降低了37.41%，同时保持了具有竞争力的J和F性能。这些结果突显了早期标记选择在提升基于Transformer的视频分割系统可扩展性方面的潜力，特别是在实时和资源受限的应用中。</div>
</details>
</div>
<div class="card">
<div class="title">C2LLM Technical Report: A New Frontier in Code Retrieval via Adaptive Cross-Attention Pooling</div>
<div class="meta-line">Authors: Jin Qin, Zihan Liao, Ziyin Zhang, Hang Yu, Peng Di, Rui Wang</div>
<div class="meta-line">First: 2025-12-24T18:59:01+00:00 · Latest: 2025-12-24T18:59:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21332v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21332v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes. Building upon Qwen-2.5-Coder backbones, C2LLM adopts a Pooling by Multihead Attention (PMA) module for generating sequence embedding from token embeddings, effectively 1) utilizing the LLM&#x27;s causal representations acquired during pretraining, while also 2) being able to aggregate information from all tokens in the sequence, breaking the information bottleneck in EOS-based sequence embeddings, and 3) supporting flexible adaptation of embedding dimension, serving as an alternative to MRL. Trained on three million publicly available data, C2LLM models set new records on MTEB-Code among models of similar sizes, with C2LLM-7B ranking 1st on the overall leaderboard.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>C2LLM技术报告：通过自适应跨注意力池化在代码检索中开辟新前沿</div>
<div class="mono" style="margin-top:8px">我们提出了C2LLM - 对比代码大语言模型，这是一个包含0.5B和7B规模的代码嵌入模型家族。基于Qwen-2.5-Coder主干，C2LLM采用多头注意力池化（PMA）模块，从token嵌入生成序列嵌入，有效实现1) 利用预训练过程中获得的LLM因果表示，2) 能够聚合序列中所有token的信息，突破基于EOS的序列嵌入的信息瓶颈，3) 支持嵌入维度的灵活调整，作为MRL的替代方案。在三个百万条公开数据上训练，C2LLM模型在MTEB-Code中为相似规模的模型设立了新纪录，其中C2LLM-7B在总排行榜上排名第一。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present C2LLM - Contrastive Code Large Language Models, a family of code embedding models in both 0.5B and 7B sizes.</div>
</details>
</div>
<div class="card">
<div class="title">TICON: A Slide-Level Tile Contextualizer for Histopathology Representation Learning</div>
<div class="meta-line">Authors: Varun Belagali, Saarthak Kapse, Pierre Marza, Srijan Das, Zilinghan Li, Sofiène Boutaj, Pushpak Pati, Srikar Yellapragada, Tarak Nath Nandi, Ravi K Madduri, Joel Saltz, Prateek Prasanna, Stergios Christodoulidis Maria Vakalopoulou, Dimitris Samaras</div>
<div class="meta-line">First: 2025-12-24T18:58:16+00:00 · Latest: 2025-12-24T18:58:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21331v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21331v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The interpretation of small tiles in large whole slide images (WSI) often needs a larger image context. We introduce TICON, a transformer-based tile representation contextualizer that produces rich, contextualized embeddings for &#x27;&#x27;any&#x27;&#x27; application in computational pathology. Standard tile encoder-based pipelines, which extract embeddings of tiles stripped from their context, fail to model the rich slide-level information essential for both local and global tasks. Furthermore, different tile-encoders excel at different downstream tasks. Therefore, a unified model is needed to contextualize embeddings derived from &#x27;&#x27;any&#x27;&#x27; tile-level foundation model. TICON addresses this need with a single, shared encoder, pretrained using a masked modeling objective to simultaneously unify and contextualize representations from diverse tile-level pathology foundation models. Our experiments demonstrate that TICON-contextualized embeddings significantly improve performance across many different tasks, establishing new state-of-the-art results on tile-level benchmarks (i.e., HEST-Bench, THUNDER, CATCH) and slide-level benchmarks (i.e., Patho-Bench). Finally, we pretrain an aggregator on TICON to form a slide-level foundation model, using only 11K WSIs, outperforming SoTA slide-level foundation models pretrained with up to 350K WSIs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TICON：一种用于数字病理学表示学习的滑片级瓷砖上下文化模型</div>
<div class="mono" style="margin-top:8px">在大尺寸全切片图像（WSI）中，对小瓷砖的解释通常需要更大的图像上下文。我们引入了TICON，这是一种基于Transformer的瓷砖表示上下文化模型，能够为计算病理学中的任何应用生成丰富的上下文化嵌入。标准的基于瓷砖编码器的流程，从上下文中剥离瓷砖并提取嵌入，无法建模对局部和全局任务都至关重要的丰富滑片级信息。此外，不同的瓷砖编码器在不同的下游任务中表现优异。因此，需要一个统一的模型来上下文化来自任何瓷砖级基础模型的嵌入。TICON通过一个共享的编码器来满足这一需求，该编码器使用掩码建模目标进行预训练，以同时统一和上下文化来自多种瓷砖级病理学基础模型的表示。我们的实验表明，TICON上下文化的嵌入在许多不同任务中显著提升了性能，并在瓷砖级基准（如HEST-Bench、THUNDER、CATCH）和滑片级基准（如Patho-Bench）上建立了新的最先进结果。最后，我们使用TICON预训练一个聚合器，形成一个滑片级基础模型，仅需11,000个WSI即可超越使用多达350,000个WSI预训练的最先进滑片级基础模型。</div>
</details>
</div>
<div class="card">
<div class="title">Your Reasoning Benchmark May Not Test Reasoning: Revealing Perception Bottleneck in Abstract Reasoning Benchmarks</div>
<div class="meta-line">Authors: Xinhe Wang, Jin Huang, Xingjian Zhang, Tianhao Wang, Jiaqi W. Ma</div>
<div class="meta-line">First: 2025-12-24T18:58:04+00:00 · Latest: 2025-12-24T18:58:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21329v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21329v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning benchmarks such as the Abstraction and Reasoning Corpus (ARC) and ARC-AGI are widely used to assess progress in artificial intelligence and are often interpreted as probes of core, so-called ``fluid&#x27;&#x27; reasoning abilities. Despite their apparent simplicity for humans, these tasks remain challenging for frontier vision-language models (VLMs), a gap commonly attributed to deficiencies in machine reasoning. We challenge this interpretation and hypothesize that the gap arises primarily from limitations in visual perception rather than from shortcomings in inductive reasoning.
  To verify this hypothesis, we introduce a two-stage experimental pipeline that explicitly separates perception and reasoning. In the perception stage, each image is independently converted into a natural-language description, while in the reasoning stage a model induces and applies rules using these descriptions. This design prevents leakage of cross-image inductive signals and isolates reasoning from perception bottlenecks. Across three ARC-style datasets, Mini-ARC, ACRE, and Bongard-LOGO, we show that the perception capability is the dominant factor underlying the observed performance gap by comparing the two-stage pipeline with against standard end-to-end one-stage evaluation. Manual inspection of reasoning traces in the VLM outputs further reveals that approximately 80 percent of model failures stem from perception errors. Together, these results demonstrate that ARC-style benchmarks conflate perceptual and reasoning challenges and that observed performance gaps may overstate deficiencies in machine reasoning. Our findings underscore the need for evaluation protocols that disentangle perception from reasoning when assessing progress in machine intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>你的推理基准可能并未测试推理：揭示抽象推理基准中的感知瓶颈</div>
<div class="mono" style="margin-top:8px">诸如抽象与推理语料库（ARC）和ARC-AGI这样的推理基准被广泛用于评估人工智能的进步，并通常被解释为对核心、所谓的『流体』推理能力的探测。尽管这些任务对人类来说看似简单，但它们对前沿的视觉-语言模型（VLMs）仍然具有挑战性，这一差距通常归因于机器推理的不足。我们质疑这一解释，并假设该差距主要源于视觉感知的局限，而非归纳推理的缺陷。
为验证这一假设，我们引入了一个两阶段的实验流程，明确地将感知与推理分离。在感知阶段，每张图像被独立地转换为自然语言描述；在推理阶段，模型使用这些描述归纳并应用规则。这种设计防止了跨图像归纳信号的泄漏，并将推理与感知瓶颈隔离开来。在三个ARC风格数据集——Mini-ARC、ACRE和Bongard-LOGO中，我们通过将两阶段流程与标准的端到端单阶段评估进行比较，表明感知能力是观察到的性能差距的主要因素。对VLM输出中推理轨迹的进一步人工检查还发现，大约80%的模型失败源于感知错误。这些结果共同表明，ARC风格的基准将感知和推理挑战混为一谈，观察到的性能差距可能夸大了机器推理的不足。我们的发现强调了在评估机器智能进展时，需要分离感知与推理的评估协议的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reasoning benchmarks such as the Abstraction and Reasoning Corpus (ARC) and ARC-AGI are widely used to assess progress in artificial intelligence and are often interpreted as probes of core, so-called ``fluid&#x27;&#x27; reasoning abilities.</div>
</details>
</div>
<div class="card">
<div class="title">Measuring all the noises of LLM Evals</div>
<div class="meta-line">Authors: Sida Wang</div>
<div class="meta-line">First: 2025-12-24T18:54:37+00:00 · Latest: 2025-12-24T18:54:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21326v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21326v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Separating signal from noise is central to experimental science. Applying well-established statistical method effectively to LLM evals requires consideration of their unique noise characteristics. We clearly define and measure three types of noise: prediction noise from generating different answers on a given question, data noise from sampling questions, and their combined total noise following the law of total variance. To emphasize relative comparisons and gain statistical power, we propose the all-pairs paired method, which applies the paired analysis to all pairs of LLMs and measures all the noise components based on millions of question-level predictions across many evals and settings. These measurements revealed clear patterns. First, each eval exhibits a characteristic and highly predictable total noise level across all model pairs. Second, paired prediction noise typically exceeds paired data noise, which means reducing prediction noise by averaging can significantly increase statistical power. These findings enable practitioners to assess significance without custom testing and to detect much smaller effects in controlled experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>测量LLM评估中的所有噪声</div>
<div class="mono" style="margin-top:8px">从噪声中分离出信号是实验科学的核心。将已建立的统计方法有效应用于LLM评估需要考虑其独特的噪声特性。我们明确定义并测量了三种类型的噪声：给定问题生成不同答案的预测噪声、从采样问题中产生的数据噪声，以及根据总方差定律得出的综合总噪声。为了强调相对比较并提高统计功效，我们提出了全对配对方法，该方法对所有LLM配对应用配对分析，并基于大量评估和设置中的百万级问题级预测来测量所有噪声成分。这些测量揭示了清晰的模式。首先，每个评估在所有模型配对中表现出特征性和高度可预测的总噪声水平。其次，配对预测噪声通常超过配对数据噪声，这意味着通过平均来减少预测噪声可以显著提高统计功效。这些发现使从业者能够在不进行定制测试的情况下评估显著性，并在受控实验中检测到更小的效果。</div>
</details>
</div>
<div class="card">
<div class="title">Circuit-based characterization of finite-temperature quantum phases and self-correcting quantum memory</div>
<div class="meta-line">Authors: Ruochen Ma, Vedika Khemani, Shengqi Sang</div>
<div class="meta-line">First: 2025-09-18T17:55:15+00:00 · Latest: 2025-12-24T18:51:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.15204v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.15204v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quantum phases at zero temperature can be characterized as equivalence classes under local unitary transformations: two ground states within a gapped phase can be transformed into each other via a local unitary circuit. We generalize this circuit-based characterization of phases to systems at finite-temperature thermal equilibrium described by Gibbs states. We construct a channel circuit that approximately transforms one Gibbs state into another provided the two are connected by a path in parameter space along which a certain correlation-decay condition holds. For finite-dimensional systems of linear size $L$ and approximation error $ε$, the locality of the circuit is ${\rm polylog}({\rm poly}(L)/ε)$. The correlation-decay condition, which we specify, is expected to be satisfied in the interior of many noncritical thermal phases, including those displaying discrete symmetry breaking and topological order. As an application, we show that any system in the same thermal phase as a zero-temperature topological code coherently preserves quantum information for a macroscopically long time, establishing self-correction as a universal property of thermal phases. As part of the proof, we provide explicit encoding and decoding channel circuits to encode information into, and decode it from, a system in thermal equilibrium.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于电路的有限温度量子相与自校正量子记忆特性研究</div>
<div class="mono" style="margin-top:8px">零温度下的量子相可以被描述为在局部幺正变换下的等价类：两个处于能隙相中的基态可以通过一个局部幺正电路相互转换。我们将这种基于电路的相描述方法推广到处于有限温度热平衡状态的系统，这些系统由吉布斯态描述。我们构造了一个信道电路，该电路在参数空间中存在满足特定相关衰减条件的路径时，可以近似地将一个吉布斯态转换为另一个吉布斯态。对于线性尺寸为 $L$ 的有限维系统和近似误差 $ε$，该电路的局域性为 ${\rm polylog}({\rm poly}(L)/ε)$。我们指定的相关衰减条件预计在许多非临界热相的内部都成立，包括表现出离散对称性破缺和拓扑序的相。作为应用，我们证明了任何与零温度拓扑码处于同一热相的系统都能在宏观长时间尺度内相干地保持量子信息，从而确立自校正作为热相的普遍性质。在证明过程中，我们提供了显式的编码和解码信道电路，用于将信息编码到处于热平衡状态的系统中，并从系统中解码信息。</div>
</details>
</div>
<div class="card">
<div class="title">Parallel Token Prediction for Language Models</div>
<div class="meta-line">Authors: Felix Draxler, Justus Will, Farrin Marouf Sofian, Theofanis Karaletsos, Sameer Singh, Stephan Mandt</div>
<div class="meta-line">First: 2025-12-24T18:46:55+00:00 · Latest: 2025-12-24T18:46:55+00:00</div>
<div class="meta-line">Comments: Preprint. Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21323v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21323v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose Parallel Token Prediction (PTP), a universal framework for parallel sequence generation in language models. PTP jointly predicts multiple dependent tokens in a single transformer call by incorporating the sampling procedure into the model. This reduces the latency bottleneck of autoregressive decoding, and avoids the restrictive independence assumptions common in existing multi-token prediction methods. We prove that PTP can represent arbitrary autoregressive sequence distributions. PTP is trained either by distilling an existing model or through inverse autoregressive training without a teacher. Experimentally, we achieve state-of-the-art speculative decoding performance on Vicuna-7B by accepting over four tokens per step on Spec-Bench. The universality of our framework indicates that parallel generation of long sequences is feasible without loss of modeling power.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言模型中的并行标记预测</div>
<div class="mono" style="margin-top:8px">我们提出了并行标记预测（PTP），这是一个用于语言模型并行序列生成的通用框架。PTP 通过将采样过程整合到模型中，在单次 transformer 调用中联合预测多个依赖标记。这减少了自回归解码的延迟瓶颈，并避免了现有多种标记预测方法中常见的严格独立性假设。我们证明 PTP 可以表示任意的自回归序列分布。PTP 可以通过蒸馏现有模型或在无教师的情况下进行逆自回归训练来训练。实验表明，在 Spec-Bench 上接受每步超过四个标记的情况下，我们在 Vicuna-7B 上实现了最先进的推测解码性能。我们的框架的通用性表明，可以在不损失建模能力的前提下实现长序列的并行生成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We propose Parallel Token Prediction (PTP), a universal framework for parallel sequence generation in language models.</div>
</details>
</div>
<div class="card">
<div class="title">Complex variational autoencoders admit Kähler structure</div>
<div class="meta-line">Authors: Andrew Gracyk</div>
<div class="meta-line">First: 2025-11-19T06:51:03+00:00 · Latest: 2025-12-24T18:38:44+00:00</div>
<div class="meta-line">Comments: Corrections and improvements</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.15172v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.15172v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">It has been discovered that latent-Euclidean variational autoencoders (VAEs) admit, in various capacities, Riemannian structure. We adapt these arguments but for complex VAEs with a complex latent stage. We show that complex VAEs reveal to some level Kähler geometric structure. Our methods will be tailored for decoder geometry. We derive the Fisher information metric in the complex case under a latent complex Gaussian with trivial relation matrix. It is well known from statistical information theory that the Fisher information coincides with the Hessian of the Kullback-Leibler (KL) divergence. Thus, the metric Kähler potential relation is exactly achieved under relative entropy. We propose a Kähler potential derivative of complex Gaussian mixtures that acts as a rough proxy to the Fisher information metric while still being faithful to the underlying Kähler geometry. Computation of the metric via this potential is efficient, and through our potential, valid as a plurisubharmonic (PSH) function, large scale computational burden of automatic differentiation is displaced to small scale. Our methods leverage the law of total covariance to bridge behavior between our potential and the Fisher metric. We show that we can regularize the latent space with decoder geometry, and that we can sample in accordance with a weighted complex volume element. We demonstrate these strategies, at the exchange of sample variation, yield consistently smoother representations and fewer semantic outliers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>复数变分自编码器具有凯勒结构</div>
<div class="mono" style="margin-top:8px">已发现，具有欧几里得潜在空间的变分自编码器（VAEs）在不同方面具有黎曼结构。我们借鉴这些论点，将其应用于具有复数潜在空间的复数VAEs。我们证明复数VAEs在某种程度上揭示了凯勒几何结构。我们的方法专门针对解码器几何进行设计。我们推导了在潜在空间为复数高斯分布且关系矩阵为平凡矩阵的情况下，复数情况下的费舍尔信息度量。根据统计信息论，费舍尔信息与Kullback-Leibler（KL）散度的Hessian矩阵一致。因此，在相对熵下，度量凯勒势的关系恰好成立。我们提出了一种复数高斯混合的凯勒势导数，它作为费舍尔信息度量的粗略代理，同时仍忠实于底层的凯勒几何。通过该势计算度量是高效的，并且由于该势本身是一个全纯亚和谐函数（PSH），大规模自动微分的计算负担被转移到了小规模。我们的方法利用总协方差定律来连接我们的势与费舍尔度量之间的行为。我们展示了可以通过解码器几何对潜在空间进行正则化，并且可以按照加权复数体积元素进行采样。我们证明这些策略在牺牲样本变化的情况下，能够产生更平滑的表示并减少语义异常值。</div>
</details>
</div>
<div class="card">
<div class="title">Variationally correct operator learning: Reduced basis neural operator with a posteriori error estimation</div>
<div class="meta-line">Authors: Yuan Qiu, Wolfgang Dahmen, Peng Chen</div>
<div class="meta-line">First: 2025-12-24T18:37:59+00:00 · Latest: 2025-12-24T18:37:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21319v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21319v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Minimizing PDE-residual losses is a common strategy to promote physical consistency in neural operators. However, standard formulations often lack variational correctness, meaning that small residuals do not guarantee small solution errors due to the use of non-compliant norms or ad hoc penalty terms for boundary conditions. This work develops a variationally correct operator learning framework by constructing first-order system least-squares (FOSLS) objectives whose values are provably equivalent to the solution error in PDE-induced norms. We demonstrate this framework on stationary diffusion and linear elasticity, incorporating mixed Dirichlet-Neumann boundary conditions via variational lifts to preserve norm equivalence without inconsistent penalties. To ensure the function space conformity required by the FOSLS loss, we propose a Reduced Basis Neural Operator (RBNO). The RBNO predicts coefficients for a pre-computed, conforming reduced basis, thereby ensuring variational stability by design while enabling efficient training. We provide a rigorous convergence analysis that bounds the total error by the sum of finite element discretization bias, reduced basis truncation error, neural network approximation error, and statistical estimation errors arising from finite sampling and optimization. Numerical benchmarks validate these theoretical bounds and demonstrate that the proposed approach achieves superior accuracy in PDE-compliant norms compared to standard baselines, while the residual loss serves as a reliable, computable a posteriori error estimator.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>变分正确的算子学习：带有后验误差估计的降阶基神经算子</div>
<div class="mono" style="margin-top:8px">最小化偏微分方程残差损失是促进神经算子物理一致性的常见策略。然而，标准公式通常缺乏变分正确性，这意味着由于使用了不合规的范数或人为设计的惩罚项来处理边界条件，小的残差并不能保证小的解误差。本文通过构建第一阶系统最小二乘（FOSLS）目标函数，开发了一个变分正确的算子学习框架，其值可以被证明与偏微分方程诱导范数下的解误差等价。我们展示了该框架在稳态扩散和线性弹性问题上的应用，通过变分提升来结合混合狄利克雷-诺伊曼边界条件，以保持范数等价性而不引入不一致的惩罚项。为确保FOSLS损失所需的函数空间一致性，我们提出了一种降阶基神经算子（RBNO）。RBNO通过预测预计算的、符合要求的降阶基的系数，从而在设计上保证变分稳定性，同时实现高效的训练。我们提供了严格的收敛性分析，将总误差上界限定为有限元离散偏差、降阶基截断误差、神经网络逼近误差以及有限采样和优化带来的统计估计误差之和。数值基准验证了这些理论界限，并展示了所提出方法在偏微分方程合规范数下相比标准基线具有更高的精度，同时残差损失可作为可靠的、可计算的后验误差估计器。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Minimizing PDE-residual losses is a common strategy to promote physical consistency in neural operators.</div>
</details>
</div>
<div class="card">
<div class="title">View-aware Cross-modal Distillation for Multi-view Action Recognition</div>
<div class="meta-line">Authors: Trung Thanh Nguyen, Yasutomo Kawanishi, Vijay John, Takahiro Komamizu, Ichiro Ide</div>
<div class="meta-line">Venue: WACV</div>
<div class="meta-line">First: 2025-11-17T02:00:22+00:00 · Latest: 2025-12-24T18:29:21+00:00</div>
<div class="meta-line">Comments: IEEE/CVF Winter Conference on Applications of Computer Vision (WACV) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12870v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.12870v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The widespread use of multi-sensor systems has increased research in multi-view action recognition. While existing approaches in multi-view setups with fully overlapping sensors benefit from consistent view coverage, partially overlapping settings where actions are visible in only a subset of views remain underexplored. This challenge becomes more severe in real-world scenarios, as many systems provide only limited input modalities and rely on sequence-level annotations instead of dense frame-level labels. In this study, we propose View-aware Cross-modal Knowledge Distillation (ViCoKD), a framework that distills knowledge from a fully supervised multi-modal teacher to a modality- and annotation-limited student. ViCoKD employs a cross-modal adapter with cross-modal attention, allowing the student to exploit multi-modal correlations while operating with incomplete modalities. Moreover, we propose a View-aware Consistency module to address view misalignment, where the same action may appear differently or only partially across viewpoints. It enforces prediction alignment when the action is co-visible across views, guided by human-detection masks and confidence-weighted Jensen-Shannon divergence between their predicted class distributions. Experiments on the real-world MultiSensor-Home dataset show that ViCoKD consistently outperforms competitive distillation methods across multiple backbones and environments, delivering significant gains and surpassing the teacher model under limited conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向视角的跨模态知识蒸馏用于多视角动作识别</div>
<div class="mono" style="margin-top:8px">多传感器系统的广泛应用促进了多视角动作识别的研究。尽管现有方法在完全重叠传感器的多视角设置中受益于一致的视角覆盖，但在动作仅在部分视角中可见的设置中仍被忽视。这一挑战在现实场景中更加严重，因为许多系统仅提供有限的输入模态，并依赖序列级标注而非密集的帧级标签。在本研究中，我们提出了一种面向视角的跨模态知识蒸馏框架（ViCoKD），该框架从完全监督的多模态教师模型中蒸馏知识到模态和标注受限的学生模型。ViCoKD采用带有跨模态注意力的跨模态适配器，使学生模型能够在不完整的模态下利用多模态相关性。此外，我们提出了一种面向视角的一致性模块，以解决视角错位问题，其中同一动作可能在不同视角中以不同方式或仅部分出现。该模块在动作在多个视角中同时可见时强制执行预测一致性，通过人类检测掩码和预测类别分布之间的置信度加权Jensen-Shannon散度进行引导。在现实世界的MultiSensor-Home数据集上的实验表明，ViCoKD在多个主干网络和环境中均优于竞争蒸馏方法，取得了显著的性能提升，并在受限条件下超越了教师模型。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Laws for Economic Productivity: Experimental Evidence in LLM-Assisted Consulting, Data Analyst, and Management Tasks</div>
<div class="meta-line">Authors: Ali Merali</div>
<div class="meta-line">First: 2025-12-24T18:24:29+00:00 · Latest: 2025-12-24T18:24:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21316v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21316v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper derives `Scaling Laws for Economic Impacts&#x27; -- empirical relationships between the training compute of Large Language Models (LLMs) and professional productivity. In a preregistered experiment, over 500 consultants, data analysts, and managers completed professional tasks using one of 13 LLMs. We find that each year of AI model progress reduced task time by 8%, with 56% of gains driven by increased compute and 44% by algorithmic progress. However, productivity gains were significantly larger for non-agentic analytical tasks compared to agentic workflows requiring tool use. These findings suggest continued model scaling could boost U.S. productivity by approximately 20% over the next decade.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>经济生产率的扩展定律：在LLM辅助咨询、数据分析和管理任务中的实验证据</div>
<div class="mono" style="margin-top:8px">本文推导出&#x27;经济影响的扩展定律&#x27;——大型语言模型（LLMs）训练计算量与专业生产率之间的经验关系。在一项预先注册的实验中，超过500名顾问、数据分析师和管理者使用13种不同的LLM完成了专业任务。我们发现，每一年的AI模型进展使任务时间减少了8%，其中56%的收益来自于计算量的增加，44%来自于算法进展。然而，非代理型分析任务的生产率提升显著高于需要工具使用的代理型工作流程。这些发现表明，持续的模型扩展可能在未来十年内将美国的生产率提高约20%。</div>
</details>
</div>
<div class="card">
<div class="title">Does the Data Processing Inequality Reflect Practice? On the Utility of Low-Level Tasks</div>
<div class="meta-line">Authors: Roy Turgeman, Tom Tirer</div>
<div class="meta-line">First: 2025-12-24T18:21:01+00:00 · Latest: 2025-12-24T18:21:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21315v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21315v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The data processing inequality is an information-theoretic principle stating that the information content of a signal cannot be increased by processing the observations. In particular, it suggests that there is no benefit in enhancing the signal or encoding it before addressing a classification problem. This assertion can be proven to be true for the case of the optimal Bayes classifier. However, in practice, it is common to perform &quot;low-level&quot; tasks before &quot;high-level&quot; downstream tasks despite the overwhelming capabilities of modern deep neural networks. In this paper, we aim to understand when and why low-level processing can be beneficial for classification. We present a comprehensive theoretical study of a binary classification setup, where we consider a classifier that is tightly connected to the optimal Bayes classifier and converges to it as the number of training samples increases. We prove that for any finite number of training samples, there exists a pre-classification processing that improves the classification accuracy. We also explore the effect of class separation, training set size, and class balance on the relative gain from this procedure. We support our theory with an empirical investigation of the theoretical setup. Finally, we conduct an empirical study where we investigate the effect of denoising and encoding on the performance of practical deep classifiers on benchmark datasets. Specifically, we vary the size and class distribution of the training set, and the noise level, and demonstrate trends that are consistent with our theoretical results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>数据处理不等式是否反映实践？关于低级任务的效用</div>
<div class="mono" style="margin-top:8px">数据处理不等式是一个信息论原理，指出信号的信息内容不能通过处理观测值来增加。特别是，它表明在解决分类问题时，增强信号或对其进行编码并无益处。这一断言可以被证明在最优贝叶斯分类器的情况下是正确的。然而，在实践中，即使现代深度神经网络具有强大的能力，人们仍然常常在进行高级下游任务之前执行&quot;低级&quot;任务。本文旨在理解在什么情况下以及为什么低级处理对分类是有益的。我们对二元分类设置进行了全面的理论研究，其中我们考虑了一个与最优贝叶斯分类器紧密相连的分类器，并且随着训练样本数量的增加，该分类器会收敛到最优贝叶斯分类器。我们证明，对于任何有限数量的训练样本，都存在一种预分类处理可以提高分类准确率。我们还探讨了类别分离度、训练集大小和类别平衡对这种处理相对增益的影响。我们通过对该理论设置的实证研究来支持我们的理论。最后，我们进行了一项实证研究，探讨去噪和编码对实际深度分类器在基准数据集上性能的影响。具体来说，我们变化了训练集的大小和类别分布，以及噪声水平，并展示了与我们的理论结果一致的趋势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The data processing inequality is an information-theoretic principle stating that the information content of a signal cannot be increased by processing the observations.</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Solve PDEs on Neural Shape Representations</div>
<div class="meta-line">Authors: Lilian Welschinger, Yilin Liu, Zican Wang, Niloy Mitra</div>
<div class="meta-line">First: 2025-12-24T18:14:02+00:00 · Latest: 2025-12-24T18:14:02+00:00</div>
<div class="meta-line">Comments: Article webpage link: https://welschinger.github.io/Learning-to-Solve-PDEs-on-Neural-Shape-Representations/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21311v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21311v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://welschinger.github.io/Learning-to-Solve-PDEs-on-Neural-Shape-Representations/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Solving partial differential equations (PDEs) on shapes underpins many shape analysis and engineering tasks; yet, prevailing PDE solvers operate on polygonal/triangle meshes while modern 3D assets increasingly live as neural representations. This mismatch leaves no suitable method to solve surface PDEs directly within the neural domain, forcing explicit mesh extraction or per-instance residual training, preventing end-to-end workflows. We present a novel, mesh-free formulation that learns a local update operator conditioned on neural (local) shape attributes, enabling surface PDEs to be solved directly where the (neural) data lives. The operator integrates naturally with prevalent neural surface representations, is trained once on a single representative shape, and generalizes across shape and topology variations, enabling accurate, fast inference without explicit meshing or per-instance optimization while preserving differentiability. Across analytic benchmarks (heat equation and Poisson solve on sphere) and real neural assets across different representations, our method slightly outperforms CPM while remaining reasonably close to FEM, and, to our knowledge, delivers the first end-to-end pipeline that solves surface PDEs on both neural and classical surface representations. Code will be released on acceptance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在神经形状表示上学习求解偏微分方程</div>
<div class="mono" style="margin-top:8px">求解形状上的偏微分方程（PDEs）是许多形状分析和工程任务的基础；然而，现有的PDE求解器通常作用于多边形/三角形网格，而现代3D资产越来越多地以神经表示形式存在。这种不匹配使得无法在神经域内直接求解表面PDEs，迫使显式网格提取或逐实例残差训练，从而阻碍了端到端的工作流程。我们提出了一种新颖的无网格公式，该公式通过神经（局部）形状属性学习一个局部更新算子，使得表面PDEs可以直接在（神经）数据所在域中求解。该算子能够自然地与主流的神经表面表示集成，仅需在一个代表性形状上进行一次训练，即可泛化到不同形状和拓扑结构上，从而实现无需显式网格化或逐实例优化的准确且快速推理，同时保持可微性。在分析基准（球面上的热方程和泊松求解）和不同表示形式的真实神经资产上，我们的方法在性能上略优于CPM，同时与FEM保持合理接近，并据我们所知，这是首个能够在神经和经典表面表示上求解表面PDEs的端到端流程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Solving partial differential equations (PDEs) on shapes underpins many shape analysis and engineering tasks; yet, prevailing PDE solvers operate on polygonal/triangle meshes while modern 3D assets increasingly live as neural representations.</div>
</details>
</div>
<div class="card">
<div class="title">Predicting Metabolic Dysfunction-Associated Steatotic Liver Disease using Machine Learning Methods</div>
<div class="meta-line">Authors: Mary E. An, Paul Griffin, Jonathan G. Stine, Ramakrishna Balakrishnan, Soundar Kumara</div>
<div class="meta-line">First: 2025-10-25T13:36:18+00:00 · Latest: 2025-12-24T18:06:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.22293v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.22293v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Background: Metabolic Dysfunction-Associated Steatotic Liver Disease (MASLD) affects ~33% of U.S. adults and is the most common chronic liver disease. Although often asymptomatic, progression can lead to cirrhosis. Early detection is important, as lifestyle interventions can prevent disease progression. We developed a fair, rigorous, and reproducible MASLD prediction model and compared it to prior methods using a large electronic health record database.
  Methods: We evaluated LASSO logistic regression, random forest, XGBoost, and a neural network for MASLD prediction using clinical feature subsets, including the top 10 SHAP-ranked features. To reduce disparities in true positive rates across racial and ethnic subgroups, we applied an equal opportunity postprocessing method.
  Results: This study included 59,492 patients in the training data, 24,198 in the validating data, and 25,188 in the testing data. The LASSO logistic regression model with the top 10 features was selected for its interpretability and comparable performance. Before fairness adjustment, the model achieved AUROC of 0.84, accuracy of 78%, sensitivity of 72%, specificity of 79%, and F1-score of 0.617. After equal opportunity postprocessing, accuracy modestly increased to 81% and specificity to 94%, while sensitivity decreased to 41% and F1-score to 0.515, reflecting the fairness trade-off.
  Conclusions: We developed the MASER prediction model (MASLD Static EHR Risk Prediction), a LASSO logistic regression model which achieved competitive performance for MASLD prediction (AUROC 0.836, accuracy 77.6%), comparable to previously reported ensemble and tree-based models. Overall, this approach demonstrates that interpretable models can achieve a balance of predictive performance and fairness in diverse patient populations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用机器学习方法预测代谢功能障碍相关脂肪肝疾病</div>
<div class="mono" style="margin-top:8px">背景：代谢功能障碍相关脂肪肝疾病（MASLD）影响约33%的美国成年人，是目前最常见的慢性肝病。尽管通常无症状，但其进展可能导致肝硬化。早期检测非常重要，因为生活方式干预可以防止疾病进展。我们开发了一个公平、严谨且可重复的MASLD预测模型，并利用大型电子健康记录数据库将其与以往方法进行了比较。
方法：我们评估了LASSO逻辑回归、随机森林、XGBoost和神经网络在使用临床特征子集（包括前10个SHAP排名特征）进行MASLD预测中的表现。为减少不同种族和民族亚群的真实阳性率差异，我们应用了等机会后处理方法。
结果：本研究的训练数据包含59,492名患者，验证数据24,198名，测试数据25,188名。选择前10个特征的LASSO逻辑回归模型，因其可解释性和可比的性能。在公平性调整前，模型的AUROC为0.84，准确率为78%，灵敏度为72%，特异度为79%，F1分数为0.617。经过等机会后处理后，准确率适度提高至81%，特异度提高至94%，但灵敏度下降至41%，F1分数下降至0.515，反映了公平性与性能之间的权衡。
结论：我们开发了MASER预测模型（MASLD静态EHR风险预测），这是一个LASSO逻辑回归模型，其在MASLD预测中表现出具有竞争力的性能（AUROC 0.836，准确率77.6%），与之前报道的集成和树模型相当。总体而言，这种方法表明可解释模型可以在不同患者群体中实现预测性能与公平性的平衡。</div>
</details>
</div>
<div class="card">
<div class="title">When F1 Fails: Granularity-Aware Evaluation for Dialogue Topic Segmentation</div>
<div class="meta-line">Authors: Michael H. Coen</div>
<div class="meta-line">First: 2025-12-18T21:29:43+00:00 · Latest: 2025-12-24T18:05:57+00:00</div>
<div class="meta-line">Comments: 32 pages, 4 figures. Evaluation and methodology study on dialogue topic segmentation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17083v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.17083v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dialogue topic segmentation supports summarization, retrieval, memory management, and conversational continuity. Despite decades of work, evaluation practice remains dominated by strict boundary matching and F1-based metrics. Modern large language model (LLM) based conversational systems increasingly rely on segmentation to manage conversation history beyond fixed context windows. In such systems, unstructured context accumulation degrades efficiency and coherence.
  This paper introduces an evaluation framework that reports boundary density and segment alignment diagnostics (purity and coverage) alongside window-tolerant F1 (W-F1). By separating boundary scoring from boundary selection, we evaluate segmentation quality across density regimes rather than at a single operating point. Cross-dataset evaluation shows that reported performance differences often reflect annotation granularity mismatch rather than boundary placement quality alone.
  We evaluate structurally distinct segmentation strategies across eight dialogue datasets spanning task-oriented, open-domain, meeting-style, and synthetic interactions. Boundary-based metrics are strongly coupled to boundary density: threshold sweeps produce larger W-F1 changes than switching between methods. These findings support viewing topic segmentation as a granularity selection problem rather than prediction of a single correct boundary set. This motivates separating boundary scoring from boundary selection for analyzing and tuning segmentation under varying annotation granularities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当F1失效时：面向对话主题分割的粒度感知评估</div>
<div class="mono" style="margin-top:8px">对话主题分割支持摘要生成、检索、记忆管理和对话连贯性。尽管已有数十年的研究，评估实践仍主要依赖严格的边界匹配和基于F1的指标。现代基于大语言模型（LLM）的对话系统越来越多地依赖分割来管理超出固定上下文窗口的对话历史。在这些系统中，非结构化的上下文累积会降低效率和连贯性。
  本文介绍了一个评估框架，该框架在报告窗口容忍F1（W-F1）的同时，还提供边界密度和段对齐诊断（纯度和覆盖率）。通过将边界评分与边界选择分离，我们可以在不同密度范围内评估分割质量，而不仅仅是在单一操作点上。跨数据集的评估表明，报告的性能差异往往反映了标注粒度的不匹配，而不仅仅是边界位置的质量。
  我们在八个涵盖任务导向型、开放域、会议风格和合成交互的对话数据集上评估了结构上不同的分割策略。基于边界的指标与边界密度紧密相关：阈值扫描产生的W-F1变化比更换方法更大。这些发现支持将主题分割视为一个粒度选择问题，而不是预测一个单一正确的边界集合。这促使我们将边界评分与边界选择分离，以便在不同标注粒度下分析和调整分割策略。</div>
</details>
</div>
<div class="card">
<div class="title">Intrinsic Benefits of Categorical Distributional Loss: Uncertainty-aware Regularized Exploration in Reinforcement Learning</div>
<div class="meta-line">Authors: Ke Sun, Yingnan Zhao, Enze Shi, Yafei Wang, Xiaodong Yan, Bei Jiang, Linglong Kong</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2021-10-07T03:14:46+00:00 · Latest: 2025-12-24T17:53:45+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025; Previous Version in ICML Workshop: Exploration in AI Today (EXAIT) 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2110.03155v8">Abs</a> · <a href="https://arxiv.org/pdf/2110.03155v8">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The remarkable empirical performance of distributional reinforcement learning (RL) has garnered increasing attention to understanding its theoretical advantages over classical RL. By decomposing the categorical distributional loss commonly employed in distributional RL, we find that the potential superiority of distributional RL can be attributed to a derived distribution-matching entropy regularization. This less-studied entropy regularization aims to capture additional knowledge of return distribution beyond only its expectation, contributing to an augmented reward signal in policy optimization. In contrast to the vanilla entropy regularization in MaxEnt RL, which explicitly encourages exploration by promoting diverse actions, the novel entropy regularization derived from categorical distributional loss implicitly updates policies to align the learned policy with (estimated) environmental uncertainty. Finally, extensive experiments verify the significance of this uncertainty-aware regularization from distributional RL on the empirical benefits over classical RL. Our study offers an innovative exploration perspective to explain the intrinsic benefits of distributional learning in RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>类别分布损失的固有优势：强化学习中的不确定性感知正则化探索</div>
<div class="mono" style="margin-top:8px">分布强化学习（RL）显著的实证表现引发了对其相对于经典RL理论优势的广泛关注。通过分解分布RL中常用的类别分布损失，我们发现分布RL的潜在优势可以归因于一种派生的分布匹配熵正则化。这种较少被研究的熵正则化旨在捕捉回报分布除期望之外的额外信息，从而在策略优化中增强奖励信号。与MaxEnt RL中的常规熵正则化不同，后者通过鼓励多样化动作显式促进探索，而从类别分布损失派生的新熵正则化则隐式地更新策略，使学习策略与（估计的）环境不确定性对齐。最后，大量实验验证了这种不确定性感知正则化在分布RL中相对于经典RL所带来的实证优势。我们的研究提供了一种创新的探索视角，以解释RL中分布学习的固有优势。</div>
</details>
</div>
<div class="card">
<div class="title">The moduli spaces of left-invariant statistical structures on Lie groups</div>
<div class="meta-line">Authors: Hikozo Kobayashi, Yu Ohno, Takayuki Okuda, Hiroshi Tamaru</div>
<div class="meta-line">First: 2025-10-06T02:17:42+00:00 · Latest: 2025-12-24T17:52:02+00:00</div>
<div class="meta-line">Comments: 33 pages. Comments are welcome!</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.04442v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.04442v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the context of information geometry, the concept known as left-invariant statistical structure on Lie groups is defined by Furuhata--Inoguchi--Kobayashi [Inf. Geom. (2021)]. In this paper, we introduce the notion of the moduli space of left-invariant statistical structures on a Lie group. We study the moduli spaces for three particular Lie groups, each of which has a moduli space of left-invariant Riemannian metrics that is a singleton. As applications, we classify left-invariant conjugate symmetric statistical structures and left-invariant dually flat structures (which are equivalently left-invariant Hessian structures) on these three Lie groups. A characterization of the Amari--Chentsov $α$-connections on the Takano Gaussian space is also given.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>李群上左不变统计结构的模空间</div>
<div class="mono" style="margin-top:8px">在信息几何的背景下，Furuhata--Inoguchi--Kobayashi [Inf. Geom. (2021)] 定义了李群上的左不变统计结构。本文引入了李群上左不变统计结构的模空间概念。我们研究了三个特定李群的模空间，每个李群的左不变黎曼度量模空间都是单元素集合。作为应用，我们对这三个李群上的左不变共轭对称统计结构和左不变对偶平坦结构（等价于左不变Hessian结构）进行了分类。此外，还给出了Takano高斯空间上Amari--Chentsov $α$-联络的特征描述。</div>
</details>
</div>
<div class="card">
<div class="title">FORCE-$α$ Numerical Fluxes within the Arbitrary High Order Semidiscrete WENO-DeC Framework: A Competitive Alternative to Upwind Fluxes</div>
<div class="meta-line">Authors: Lorenzo Micalizzi, Eleuterio Toro</div>
<div class="meta-line">First: 2025-12-24T17:50:07+00:00 · Latest: 2025-12-24T17:50:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21306v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21306v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work systematically investigates the performance of FORCE--$α$ numerical fluxes within an arbitrary high order semidiscrete finite volume (FV) framework for hyperbolic partial differential equations (PDEs). Such numerical fluxes have been recently introduced by Toro, Saggiorato, Tokareva, and Hidalgo (Journal of Computational Physics, 416, 2020), and constitute a family of centred fluxes obtained from a suitable modification of First--Order Centred (FORCE) numerical fluxes. In contrast with upwind fluxes, such as Rusanov, Harten--Lax--van Leer (HLL) or the exact Riemann solver (RS) numerical flux, centred ones do not consider in any way the structure of the Riemann problem at cell interfaces. Adopting centred numerical fluxes leads to a high level of flexibility of the resulting numerical schemes, for example in the context of complicated hyperbolic systems, for which RSs may be impossible to construct or computationally expensive.
  The baseline framework adopted in this investigation is a FV semidiscrete approach with Weighted Essentially Non--Oscillatory (WENO) spatial reconstruction and Deferred Correction (DeC) time discretization, and results are reported up to order 7. Previous investigations involving the same framework have established that increasing the order of accuracy tends to decrease the differences in the results obtained through different numerical fluxes. The goal of this paper is to show that the employment of FORCE--$α$ numerical fluxes within such a framework is a competitive alternative to the adoption of more classical upwind fluxes. The hyperbolic system considered for this investigation is the ideal Euler equations in one and two space dimensions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FORCE-α数值通量在任意高阶半离散WENO-DeC框架中的应用：一种与迎风通量竞争的替代方案</div>
<div class="mono" style="margin-top:8px">本文系统地研究了FORCE-α数值通量在任意高阶半离散有限体积（FV）框架中对双曲偏微分方程（PDEs）的性能。此类数值通量最近由Toro、Saggiorato、Tokareva和Hidalgo在《计算物理杂志》第416卷（2020年）中提出，是一类通过适当修改第一阶中心（FORCE）数值通量而得到的中心通量族。与迎风通量（如Rusanov、Harten-Lax-van Leer（HLL）或精确黎曼求解器（RS）数值通量）不同，中心通量不考虑黎曼问题在单元界面的结构。采用中心数值通量可以显著提高数值格式的灵活性，例如在处理复杂双曲系统时，RS可能难以构造或计算成本过高。本文采用的基准框架是有限体积半离散方法，结合加权本质非振荡（WENO）空间重构和延迟修正（DeC）时间离散化，结果报告到七阶精度。此前基于相同框架的研究表明，随着精度阶数的增加，不同数值通量所得结果之间的差异会减小。本文旨在展示，在该框架下采用FORCE-α数值通量是一种与更传统的迎风通量相竞争的替代方案。本文研究的双曲系统为一维和二维的理想欧拉方程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work systematically investigates the performance of FORCE--$α$ numerical fluxes within an arbitrary high order semidiscrete finite volume (FV) framework for hyperbolic partial differential equations (PDEs).</div>
</details>
</div>
<div class="card">
<div class="title">A Note on Publicly Verifiable Quantum Money with Low Quantum Computational Resources</div>
<div class="meta-line">Authors: Fabrizio Genovese, Lev Stambler</div>
<div class="meta-line">First: 2025-12-24T17:48:03+00:00 · Latest: 2025-12-24T17:48:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21304v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21304v1">PDF</a> · <a href="https://github.com/neverlocal/otm_billz">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work we present a publicly verifiable quantum money protocol which assumes close to no quantum computational capabilities. We rely on one-time memories which in turn can be built from quantum conjugate coding and hardware-based assumptions. Specifically, our scheme allows for a limited number of verifications and also allows for quantum tokens for digital signatures. Double spending is prevented by the no-cloning principle of conjugate coding states. An implementation of the concepts presented in this work can be found at https://github.com/neverlocal/otm_billz.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于低量子计算资源的可公开验证量子货币的注记</div>
<div class="mono" style="margin-top:8px">在这项工作中，我们提出了一种可公开验证的量子货币协议，该协议几乎不需要任何量子计算能力。我们依赖一次性记忆（one-time memories），而这些记忆可以通过量子共轭编码和基于硬件的假设构建。具体而言，我们的方案允许有限次数的验证，并支持用于数字签名的量子令牌。通过共轭编码态的不可克隆原理，可以防止双重消费。本工作的概念实现可在 https://github.com/neverlocal/otm_billz 找到。</div>
</details>
</div>
<div class="card">
<div class="title">AndroidLens: Long-latency Evaluation with Nested Sub-targets for Android GUI Agents</div>
<div class="meta-line">Authors: Yue Cao, Yingyao Wang, Pi Bu, Jingxuan Xing, Wei Jiang, Zekun Zhu, Junpeng Ma, Sashuai Zhou, Tong Lu, Jun Song, Yu Cheng, Yuning Jiang, Bo Zheng</div>
<div class="meta-line">First: 2025-12-24T17:40:42+00:00 · Latest: 2025-12-24T17:40:42+00:00</div>
<div class="meta-line">Comments: 23 pages, 13 figures, 8 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21302v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21302v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graphical user interface (GUI) agents can substantially improve productivity by automating frequently executed long-latency tasks on mobile devices. However, existing evaluation benchmarks are still constrained to limited applications, simple tasks, and coarse-grained metrics. To address this, we introduce AndroidLens, a challenging evaluation framework for mobile GUI agents, comprising 571 long-latency tasks in both Chinese and English environments, each requiring an average of more than 26 steps to complete. The framework features: (1) tasks derived from real-world user scenarios across 38 domains, covering complex types such as multi-constraint, multi-goal, and domain-specific tasks; (2) static evaluation that preserves real-world anomalies and allows multiple valid paths to reduce bias; and (3) dynamic evaluation that employs a milestone-based scheme for fine-grained progress measurement via Average Task Progress (ATP). Our evaluation indicates that even the best models reach only a 12.7% task success rate and 50.47% ATP. We also underscore key challenges in real-world environments, including environmental anomalies, adaptive exploration, and long-term memory retention.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AndroidLens：面向Android GUI代理的嵌套子目标长延迟评估框架</div>
<div class="mono" style="margin-top:8px">图形用户界面（GUI）代理可以通过自动化移动设备上频繁执行的长延迟任务来显著提高生产力。然而，现有的评估基准仍受限于有限的应用、简单的任务和粗粒度的指标。为了解决这一问题，我们引入了AndroidLens，这是一个针对移动GUI代理的具有挑战性的评估框架，包含571个长延迟任务，涵盖中文和英文环境，每个任务平均需要超过26步才能完成。该框架具有以下特点：(1) 任务来源于38个领域的现实用户场景，涵盖多约束、多目标和领域特定任务等复杂类型；(2) 静态评估保留现实世界中的异常情况，并允许多种有效路径以减少偏差；(3) 动态评估采用基于里程碑的方案，通过平均任务进度（ATP）实现细粒度的进度测量。我们的评估结果显示，即使是最优模型也只能达到12.7%的任务成功率和50.47%的ATP。我们还强调了现实环境中的一些关键挑战，包括环境异常、自适应探索和长期记忆保持。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Graphical user interface (GUI) agents can substantially improve productivity by automating frequently executed long-latency tasks on mobile devices.</div>
</details>
</div>
<div class="card">
<div class="title">Transcriptome-Conditioned Personalized De Novo Drug Generation for AML Using Metaheuristic Assembly and Target-Driven Filtering</div>
<div class="meta-line">Authors: Abdullah G. Elafifi, Basma Mamdouh, Mariam Hanafy, Muhammed Alaa Eldin, Yosef Khaled, Nesma Mohamed El-Gelany, Tarek H. M. Abou-El-Enien</div>
<div class="meta-line">First: 2025-12-24T17:39:37+00:00 · Latest: 2025-12-24T17:39:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21301v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.21301v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Acute Myeloid Leukemia (AML) remains a clinical challenge due to its extreme molecular heterogeneity and high relapse rates. While precision medicine has introduced mutation-specific therapies, many patients still lack effective, personalized options. This paper presents a novel, end-to-end computational framework that bridges the gap between patient-specific transcriptomics and de novo drug discovery. By analyzing bulk RNA sequencing data from the TCGA-LAML cohort, the study utilized Weighted Gene Co-expression Network Analysis (WGCNA) to prioritize 20 high-value biomarkers, including metabolic transporters like HK3 and immune-modulatory receptors such as SIGLEC9. The physical structures of these targets were modeled using AlphaFold3, and druggable hotspots were quantitatively mapped via the DOGSiteScorer engine. Then developed a novel, reaction-first evolutionary metaheuristic algorithm as well as multi-objective optimization programming that assembles novel ligands from fragment libraries, guided by spatial alignment to these identified hotspots. The generative model produced structurally unique chemical entities with a strong bias toward drug-like space, as evidenced by QED scores peaking between 0.5 and 0.7. Validation through ADMET profiling and SwissDock molecular docking identified high-confidence candidates, such as Ligand L1, which achieved a binding free energy of -6.571 kcal/mol against the A08A96 biomarker. These results demonstrate that integrating systems biology with metaheuristic molecular assembly can produce pharmacologically viable, patient tailored leads, offering a scalable blueprint for precision oncology in AML and beyond</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于转录组调控的AML新型药物生成方法：利用元启发式组装和靶向驱动筛选</div>
<div class="mono" style="margin-top:8px">由于AML具有极端的分子异质性和高复发率，其仍是临床挑战。尽管精准医学引入了针对特定突变的治疗方案，但许多患者仍缺乏有效的个性化选择。本文提出了一种新颖的端到端计算框架，弥合了患者特异性转录组学与新型药物发现之间的差距。通过分析TCGA-LAML队列的批量RNA测序数据，研究利用加权基因共表达网络分析（WGCNA）优先选择20个高价值生物标志物，包括代谢转运蛋白如HK3和免疫调节受体如SIGLEC9。利用AlphaFold3对这些靶点的物理结构进行建模，并通过DOGSiteScorer引擎定量映射可药用热点区域。随后开发了一种新颖的以反应为导向的进化元启发式算法以及多目标优化编程，从片段库中组装新型配体，以这些识别出的热点区域的空间对齐为指导。生成模型产生了结构独特的化学实体，且具有显著的类药物空间偏向性，如QED分数在0.5至0.7之间达到峰值。通过ADMET分析和SwissDock分子对接验证，识别出高置信度候选物，如针对A08A96生物标志物的配体L1，其结合自由能为-6.571 kcal/mol。这些结果表明，将系统生物学与元启发式分子组装相结合，可以生成药理学可行的、针对患者的先导化合物，为AML及其他癌症的精准医学提供了可扩展的蓝图。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Acute Myeloid Leukemia (AML) remains a clinical challenge due to its extreme molecular heterogeneity and high relapse rates.</div>
</details>
</div>
<div class="card">
<div class="title">Alternating Gradient Flows: A Theory of Feature Learning in Two-layer Neural Networks</div>
<div class="meta-line">Authors: Daniel Kunin, Giovanni Luca Marchetti, Feng Chen, Dhruva Karkada, James B. Simon, Michael R. DeWeese, Surya Ganguli, Nina Miolane</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-06T19:29:13+00:00 · Latest: 2025-12-24T17:26:35+00:00</div>
<div class="meta-line">Comments: 40 pages, 8 figures, NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.06489v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.06489v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">What features neural networks learn, and how, remains an open question. In this paper, we introduce Alternating Gradient Flows (AGF), an algorithmic framework that describes the dynamics of feature learning in two-layer networks trained from small initialization. Prior works have shown that gradient flow in this regime exhibits a staircase-like loss curve, alternating between plateaus where neurons slowly align to useful directions and sharp drops where neurons rapidly grow in norm. AGF approximates this behavior as an alternating two-step process: maximizing a utility function over dormant neurons and minimizing a cost function over active ones. AGF begins with all neurons dormant. At each iteration, a dormant neuron activates, triggering the acquisition of a feature and a drop in the loss. AGF quantifies the order, timing, and magnitude of these drops, matching experiments across several commonly studied architectures. We show that AGF unifies and extends existing saddle-to-saddle analyses in fully connected linear networks and attention-only linear transformers, where the learned features are singular modes and principal components, respectively. In diagonal linear networks, we prove AGF converges to gradient flow in the limit of vanishing initialization. Applying AGF to quadratic networks trained to perform modular addition, we give the first complete characterization of the training dynamics, revealing that networks learn Fourier features in decreasing order of coefficient magnitude. Altogether, AGF offers a promising step towards understanding feature learning in neural networks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>交替梯度流：两层神经网络中特征学习的理论</div>
<div class="mono" style="margin-top:8px">神经网络学习了哪些特征以及如何学习，仍然是一个开放性问题。在本文中，我们引入了交替梯度流（AGF），这是一种描述从微小初始化训练的两层网络中特征学习动态的算法框架。先前的研究表明，在这种情况下，梯度流呈现出类似阶梯的损失曲线，交替在神经元缓慢对齐到有用方向的平台期和神经元快速增长的下降期之间。AGF将这种行为近似为一个交替的两步过程：在休眠神经元上最大化效用函数，在活跃神经元上最小化成本函数。AGF初始时所有神经元都处于休眠状态。在每次迭代中，一个休眠神经元被激活，从而获取特征并导致损失下降。AGF量化了这些下降的顺序、时间点和幅度，并在多个常用网络架构的实验中得到验证。我们展示了AGF统一并扩展了现有完全连接线性网络和仅注意力机制的线性Transformer网络中的鞍点到鞍点分析，其中学习的特征分别是奇异模态和主成分。在对角线性网络中，我们证明AGF在初始化趋近于零时收敛于梯度流。将AGF应用于执行模运算加法的二次网络，我们首次完整地描述了训练动态，揭示了网络按系数幅度递减的顺序学习傅里叶特征。总体而言，AGF为理解神经网络中的特征学习提供了一个有前景的步骤。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">What features neural networks learn, and how, remains an open question.</div>
</details>
</div>
<div class="card">
<div class="title">Post-detection inference for sequential changepoint localization</div>
<div class="meta-line">Authors: Aytijhya Saha, Aaditya Ramdas</div>
<div class="meta-line">First: 2025-02-10T02:01:30+00:00 · Latest: 2025-12-24T17:17:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.06096v4">Abs</a> · <a href="https://arxiv.org/pdf/2502.06096v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper addresses a fundamental but largely unexplored challenge in sequential changepoint analysis: conducting inference following a detected change. We develop a very general framework to construct confidence sets for the unknown changepoint using only the data observed up to a data-dependent stopping time at which an arbitrary sequential detection algorithm declares a change. Our framework is nonparametric, making no assumption on the composite post-change class, the observation space, or the sequential detection procedure used, and is non-asymptotically valid. We also extend it to handle composite pre-change classes under a suitable assumption, and also derive confidence sets for the change magnitude in parametric settings. We provide theoretical guarantees on the width of our confidence intervals. Extensive simulations demonstrate that the produced sets have reasonable size, and slightly conservative coverage. In summary, we present the first general method for sequential changepoint localization, which is theoretically sound and broadly applicable in practice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>检测后推理用于序列变化点定位</div>
<div class="mono" style="margin-top:8px">本文探讨了序列变化点分析中的一个基本但尚未被广泛研究的挑战：在检测到变化后进行推理。我们提出了一种非常通用的框架，仅利用数据在数据依赖的停止时间点处的观测值，构建未知变化点的置信集。该框架是非参数的，不对变化后类、观测空间或使用的序列检测过程做出任何假设，并且是非渐近有效的。我们还将其扩展以处理合适的假设下的复合变化前类，并推导出参数设置下的变化幅度置信集。我们提供了对置信区间宽度的理论保证。大量模拟实验表明，所生成的置信集具有合理的大小和略微保守的覆盖率。总之，我们提出了首个用于序列变化点定位的通用方法，该方法在理论上可靠且在实践中具有广泛的应用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses a fundamental but largely unexplored challenge in sequential changepoint analysis: conducting inference following a detected change.</div>
</details>
</div>
<div class="card">
<div class="title">DiTSinger: Scaling Singing Voice Synthesis with Diffusion Transformer and Implicit Alignment</div>
<div class="meta-line">Authors: Zongcai Du, Guilin Deng, Xiaofeng Guo, Xin Gao, Linke Li, Kaichang Cheng, Fubo Han, Siyu Yang, Peng Liu, Pan Zhong, Qiang Fu</div>
<div class="meta-line">First: 2025-10-10T05:39:45+00:00 · Latest: 2025-12-24T17:16:37+00:00</div>
<div class="meta-line">Comments: ICASSP26 under review. Demo page: https://nju-jet.github.io/DiTSinger</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.09016v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.09016v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://nju-jet.github.io/DiTSinger">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in diffusion-based Singing Voice Synthesis (SVS) demonstrates strong expressiveness but remains limited by data scarcity and model scalability. We introduce a two-stage pipeline: a compact seed set of human-sung recordings is constructed by pairing fixed melodies with diverse LLM-generated lyrics, and melody-specific models are trained to synthesize over 500 hours of high-quality Chinese singing data. Building on this corpus, we propose DiTSinger, a Diffusion Transformer with RoPE and qk-norm, systematically scaled in depth, width, and resolution for enhanced fidelity. Furthermore, we design an implicit alignment mechanism that obviates phoneme-level duration labels by constraining phoneme-to-acoustic attention within character-level spans, thereby improving robustness under noisy or uncertain alignments. Extensive experiments validate that our approach enables scalable, alignment-free, and high-fidelity SVS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiTSinger: 通过扩散变压器和隐式对齐扩展歌唱语音合成</div>
<div class="mono" style="margin-top:8px">基于扩散模型的歌唱语音合成（SVS）最近取得了显著进展，展现出强大的表现力，但受限于数据稀缺和模型扩展性。我们提出一个两阶段流程：通过将固定旋律与多样化的LLM生成歌词配对，构建一个紧凑的人声演唱种子集，并训练旋律特定模型以合成超过500小时的高质量中文歌唱数据。在此语料库基础上，我们提出了DiTSinger，这是一种带有RoPE和qk-norm的扩散变压器，其深度、宽度和分辨率被系统性地扩展以提高合成保真度。此外，我们设计了一种隐式对齐机制，通过在字符级范围内约束音素到声学的注意力，从而避免使用音素级时长标签，提高了在噪声或不确定对齐情况下的鲁棒性。大量实验验证了我们的方法能够实现可扩展、无需对齐且高保真的SVS。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent progress in diffusion-based Singing Voice Synthesis (SVS) demonstrates strong expressiveness but remains limited by data scarcity and model scalability.</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking Popularity Bias in Collaborative Filtering via Analytical Vector Decomposition</div>
<div class="meta-line">Authors: Lingfeng Liu, Yixin Song, Dazhong Shen, Bing Yin, Hao Li, Yanyong Zhang, Chao Wang</div>
<div class="meta-line">First: 2025-12-11T14:35:13+00:00 · Latest: 2025-12-24T17:12:02+00:00</div>
<div class="meta-line">Comments: Accepted by SIGKDD 2026(First Cycle)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10688v5">Abs</a> · <a href="https://arxiv.org/pdf/2512.10688v5">PDF</a> · <a href="https://github.com/LingFeng-Liu-AI/DDC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Popularity bias fundamentally undermines the personalization capabilities of collaborative filtering (CF) models, causing them to disproportionately recommend popular items while neglecting users&#x27; genuine preferences for niche content. While existing approaches treat this as an external confounding factor, we reveal that popularity bias is an intrinsic geometric artifact of Bayesian Pairwise Ranking (BPR) optimization in CF models. Through rigorous mathematical analysis, we prove that BPR systematically organizes item embeddings along a dominant &quot;popularity direction&quot; where embedding magnitudes directly correlate with interaction frequency. This geometric distortion forces user embeddings to simultaneously handle two conflicting tasks-expressing genuine preference and calibrating against global popularity-trapping them in suboptimal configurations that favor popular items regardless of individual tastes. We propose Directional Decomposition and Correction (DDC), a universally applicable framework that surgically corrects this embedding geometry through asymmetric directional updates. DDC guides positive interactions along personalized preference directions while steering negative interactions away from the global popularity direction, disentangling preference from popularity at the geometric source. Extensive experiments across multiple BPR-based architectures demonstrate that DDC significantly outperforms state-of-the-art debiasing methods, reducing training loss to less than 5% of heavily-tuned baselines while achieving superior recommendation quality and fairness. Code is available in https://github.com/LingFeng-Liu-AI/DDC.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过分析向量分解重新思考协同过滤中的流行度偏差</div>
<div class="mono" style="margin-top:8px">流行度偏差从根本上削弱了协同过滤（CF）模型的个性化能力，导致其倾向于推荐热门物品而忽视用户对小众内容的真实偏好。尽管现有方法将其视为外部混杂因素，我们揭示流行度偏差实际上是CF模型中贝叶斯成对排序（BPR）优化的内在几何产物。通过严格的数学分析，我们证明BPR系统地将物品嵌入沿着主导的&quot;流行度方向&quot;排列，其中嵌入向量的大小直接与交互频率相关。这种几何扭曲迫使用户嵌入同时处理两个冲突的任务——表达真实偏好和校准全局流行度，从而陷入无论个体偏好如何都偏向热门物品的次优配置。我们提出了一种通用框架Directional Decomposition and Correction（DDC），通过不对称的方向更新精确修正这种嵌入几何结构。DDC引导正向交互沿着个性化偏好方向进行，同时将负向交互远离全局流行度方向，从而在几何层面分离偏好与流行度。在多种基于BPR的架构上进行的大量实验表明，DDC显著优于现有的去偏差方法，将训练损失降低至高度调优基线的5%以下，同时实现更优的推荐质量和公平性。代码可在https://github.com/LingFeng-Liu-AI/DDC获取。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
