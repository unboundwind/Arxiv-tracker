<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-15 03:56</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260115_0356</div>
    <div class="row"><div class="card">
<div class="title">RAVEN: Erasing Invisible Watermarks via Novel View Synthesis</div>
<div class="meta-line">Authors: Fahad Shamshad, Nils Lukas, Karthik Nandakumar</div>
<div class="meta-line">First: 2026-01-13T18:59:58+00:00 · Latest: 2026-01-13T18:59:58+00:00</div>
<div class="meta-line">Comments: 13 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08832v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08832v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Invisible watermarking has become a critical mechanism for authenticating AI-generated image content, with major platforms deploying watermarking schemes at scale. However, evaluating the vulnerability of these schemes against sophisticated removal attacks remains essential to assess their reliability and guide robust design. In this work, we expose a fundamental vulnerability in invisible watermarks by reformulating watermark removal as a view synthesis problem. Our key insight is that generating a perceptually consistent alternative view of the same semantic content, akin to re-observing a scene from a shifted perspective, naturally removes the embedded watermark while preserving visual fidelity. This reveals a critical gap: watermarks robust to pixel-space and frequency-domain attacks remain vulnerable to semantic-preserving viewpoint transformations. We introduce a zero-shot diffusion-based framework that applies controlled geometric transformations in latent space, augmented with view-guided correspondence attention to maintain structural consistency during reconstruction. Operating on frozen pre-trained models without detector access or watermark knowledge, our method achieves state-of-the-art watermark suppression across 15 watermarking methods--outperforming 14 baseline attacks while maintaining superior perceptual quality across multiple datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RAVEN：通过新颖视角合成消除不可见水印</div>
<div class="mono" style="margin-top:8px">不可见水印已成为验证AI生成图像内容的关键机制，主要平台已大规模部署水印方案。然而，评估这些方案对复杂移除攻击的脆弱性对于判断其可靠性并指导稳健设计仍至关重要。在本工作中，我们将水印移除重新表述为视角合成问题，揭示了不可见水印的一个根本性漏洞。我们的核心洞察是：生成与相同语义内容一致的感知上连贯的替代视角，类似于从不同视角重新观察场景，可以自然地去除嵌入的水印，同时保持视觉保真度。这暴露了一个关键的缺陷：对像素空间和频率域攻击具有鲁棒性的水印仍可能受到语义保持的视角变换攻击。我们提出了一种零样本扩散框架，通过在潜在空间中应用受控的几何变换，并结合视角引导的对应注意力机制以保持结构一致性。我们的方法在不使用检测器或水印知识的情况下，直接在冻结的预训练模型上运行，实现了对15种水印方案的最先进水印抑制效果，在多个数据集上优于14种基线攻击方法，同时保持了卓越的感知质量。</div>
</details>
</div>
<div class="card">
<div class="title">3AM: Segment Anything with Geometric Consistency in Videos</div>
<div class="meta-line">Authors: Yang-Che Sun, Cheng Sun, Chin-Yang Lin, Fu-En Yang, Min-Hung Chen, Yen-Yu Lin, Yu-Lun Liu</div>
<div class="meta-line">First: 2026-01-13T18:59:54+00:00 · Latest: 2026-01-13T18:59:54+00:00</div>
<div class="meta-line">Comments: Project page: https://jayisaking.github.io/3AM-Page/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08831v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08831v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://jayisaking.github.io/3AM-Page/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video object segmentation methods like SAM2 achieve strong performance through memory-based architectures but struggle under large viewpoint changes due to reliance on appearance features. Traditional 3D instance segmentation methods address viewpoint consistency but require camera poses, depth maps, and expensive preprocessing. We introduce 3AM, a training-time enhancement that integrates 3D-aware features from MUSt3R into SAM2. Our lightweight Feature Merger fuses multi-level MUSt3R features that encode implicit geometric correspondence. Combined with SAM2&#x27;s appearance features, the model achieves geometry-consistent recognition grounded in both spatial position and visual similarity. We propose a field-of-view aware sampling strategy ensuring frames observe spatially consistent object regions for reliable 3D correspondence learning. Critically, our method requires only RGB input at inference, with no camera poses or preprocessing. On challenging datasets with wide-baseline motion (ScanNet++, Replica), 3AM substantially outperforms SAM2 and extensions, achieving 90.6% IoU and 71.7% Positive IoU on ScanNet++&#x27;s Selected Subset, improving over state-of-the-art VOS methods by +15.9 and +30.4 points. Project page: https://jayisaking.github.io/3AM-Page/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>3AM：在视频中实现具有几何一致性的Segment Anything</div>
<div class="mono" style="margin-top:8px">像SAM2这样的视频对象分割方法通过基于内存的架构实现了强大的性能，但由于依赖外观特征，在大视角变化下表现不佳。传统的3D实例分割方法解决了视角一致性问题，但需要相机姿态、深度图和昂贵的预处理。我们引入了3AM，这是一种训练时的增强方法，将MUSt3R的3D感知特征集成到SAM2中。我们的轻量级Feature Merger融合了多级MUSt3R特征，这些特征编码了隐式的几何对应关系。结合SAM2的外观特征，该模型实现了基于空间位置和视觉相似性的几何一致性识别。我们提出了一种视野感知的采样策略，确保帧观察到空间一致的对象区域，从而实现可靠的3D对应学习。关键的是，我们的方法在推理时仅需要RGB输入，无需相机姿态或预处理。在具有宽基线运动的挑战性数据集（如ScanNet++和Replica）上，3AM显著优于SAM2及其扩展方法，在ScanNet++的Selected Subset上实现了90.6%的IoU和71.7%的Positive IoU，分别比最先进的VOS方法提升了15.9和30.4个百分点。</div>
</details>
</div>
<div class="card">
<div class="title">Modeling LLM Agent Reviewer Dynamics in Elo-Ranked Review System</div>
<div class="meta-line">Authors: Hsiang-Wei Huang, Junbin Lu, Kuang-Ming Chen, Jenq-Neng Hwang</div>
<div class="meta-line">First: 2026-01-13T18:59:17+00:00 · Latest: 2026-01-13T18:59:17+00:00</div>
<div class="meta-line">Comments: In submission. The first two authors contributed equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08829v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08829v1">PDF</a> · <a href="https://github.com/hsiangwei0903/EloReview">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we explore the Large Language Model (LLM) agent reviewer dynamics in an Elo-ranked review system using real-world conference paper submissions. Multiple LLM agent reviewers with different personas are engage in multi round review interactions moderated by an Area Chair. We compare a baseline setting with conditions that incorporate Elo ratings and reviewer memory. Our simulation results showcase several interesting findings, including how incorporating Elo improves Area Chair decision accuracy, as well as reviewers&#x27; adaptive review strategy that exploits our Elo system without improving review effort. Our code is available at https://github.com/hsiangwei0903/EloReview.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在Elo评分评审系统中建模LLM代理评审员动态</div>
<div class="mono" style="margin-top:8px">在本工作中，我们利用现实中的会议论文提交数据，探索在Elo评分系统中大型语言模型（LLM）代理评审员的动态。多个具有不同人设的LLM代理评审员通过Area Chair主持的多轮评审互动进行交流。我们比较了基线设置与结合Elo评分和评审员记忆条件的设置。模拟结果展示了几个有趣的发现，包括如何通过Elo评分提高Area Chair的决策准确性，以及评审员如何利用我们的Elo系统而无需增加评审努力来采取适应性评审策略。我们的代码可在https://github.com/hsiangwei0903/EloReview获取。</div>
</details>
</div>
<div class="card">
<div class="title">Motion Attribution for Video Generation</div>
<div class="meta-line">Authors: Xindi Wu, Despoina Paschalidou, Jun Gao, Antonio Torralba, Laura Leal-Taixé, Olga Russakovsky, Sanja Fidler, Jonathan Lorraine</div>
<div class="meta-line">First: 2026-01-13T18:59:09+00:00 · Latest: 2026-01-13T18:59:09+00:00</div>
<div class="meta-line">Comments: See the project website at https://research.nvidia.com/labs/sil/projects/MOTIVE/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08828v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08828v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://research.nvidia.com/labs/sil/projects/MOTIVE/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood. We present Motive (MOTIon attribution for Video gEneration), a motion-centric, gradient-based data attribution framework that scales to modern, large, high-quality video datasets and models. We use this to study which fine-tuning clips improve or degrade temporal dynamics. Motive isolates temporal dynamics from static appearance via motion-weighted loss masks, yielding efficient and scalable motion-specific influence computation. On text-to-video models, Motive identifies clips that strongly affect motion and guides data curation that improves temporal consistency and physical plausibility. With Motive-selected high-influence data, our method improves both motion smoothness and dynamic degree on VBench, achieving a 74.1% human preference win rate compared with the pretrained base model. To our knowledge, this is the first framework to attribute motion rather than visual appearance in video generative models and to use it to curate fine-tuning data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频生成中的运动归因</div>
<div class="mono" style="margin-top:8px">尽管视频生成模型取得了快速进展，但数据如何影响运动的作用仍不明确。我们提出了Motive（MOTIon attribution for Video gEneration），这是一个以运动为中心、基于梯度的数据归因框架，能够扩展到现代、大规模、高质量的视频数据集和模型。我们利用该框架研究哪些微调片段能够改善或损害时间动态。Motive通过运动加权损失掩码将时间动态与静态外观分离，从而实现高效且可扩展的运动特定影响计算。在文本到视频模型上，Motive识别出对运动有显著影响的片段，并指导数据整理以提升时间一致性与物理合理性。使用Motive选出的高影响数据，我们的方法在VBench上提升了运动平滑度和动态程度，相较于预训练基模型，实现了74.1%的人类偏好胜率。据我们所知，这是首个在视频生成模型中归因运动而非视觉外观，并利用其整理微调数据的框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Despite the rapid progress of video generation models, the role of data in influencing motion is poorly understood.</div>
</details>
</div>
<div class="card">
<div class="title">SemiETPicker: Fast and Label-Efficient Particle Picking for CryoET Tomography Using Semi-Supervised Learning</div>
<div class="meta-line">Authors: Linhan Wang, Jianwen Dou, Wang Li, Shengkun Wang, Zhiwu Xie, Chang-Tien Lu, Yinlin Chen</div>
<div class="meta-line">Venue: ISBI</div>
<div class="meta-line">First: 2025-10-25T23:09:22+00:00 · Latest: 2026-01-13T18:57:24+00:00</div>
<div class="meta-line">Comments: IEEE International Symposium on Biomedical Imaging (ISBI) 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.22454v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.22454v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cryogenic Electron Tomography (CryoET) combined with sub-volume averaging (SVA) is the only imaging modality capable of resolving protein structures inside cells at molecular resolution. Particle picking, the task of localizing and classifying target proteins in 3D CryoET volumes, remains the main bottleneck. Due to the reliance on time-consuming manual labels, the vast reserve of unlabeled tomograms remains underutilized. In this work, we present a fast, label-efficient semi-supervised framework that exploits this untapped data. Our framework consists of two components: (i) an end-to-end heatmap-supervised detection model inspired by keypoint detection, and (ii) a teacher-student co-training mechanism that enhances performance under sparse labeling conditions. Furthermore, we introduce multi-view pseudo-labeling and a CryoET-specific DropBlock augmentation strategy to further boost performance. Extensive evaluations on the large-scale CZII dataset show that our approach improves F1 by 10% over supervised baselines, underscoring the promise of semi-supervised learning for leveraging unlabeled CryoET data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SemiETPicker：用于冷冻电子断层扫描的快速且标签高效的粒子挑选方法，结合半监督学习</div>
<div class="mono" style="margin-top:8px">结合子体积平均（SVA）的冷冻电子断层扫描（CryoET）是唯一能够以分子分辨率解析细胞内蛋白质结构的成像技术。粒子挑选，即在三维CryoET数据中定位和分类目标蛋白质的任务，仍然是主要瓶颈。由于依赖耗时的手动标签，大量未标记的断层扫描数据仍未被充分利用。在本工作中，我们提出了一种快速且标签高效的半监督框架，以利用这些未开发的数据。我们的框架包含两个部分：(i) 一种受关键点检测启发的端到端热图监督检测模型；(ii) 一种教师-学生协同训练机制，可在稀疏标签条件下提升性能。此外，我们还引入了多视角伪标签和一种针对CryoET的DropBlock增强策略，以进一步提升性能。在大规模CZII数据集上的广泛评估表明，我们的方法在监督基线之上将F1分数提升了10%，突显了半监督学习在利用未标记CryoET数据方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">FilmSceneDesigner: Chaining Set Design for Procedural Film Scene Generation</div>
<div class="meta-line">Authors: Zhifeng Xie, Keyi Zhang, Yiye Yan, Yuling Guo, Fan Yang, Jiting Zhou, Mengtian Li</div>
<div class="meta-line">First: 2025-11-24T14:00:40+00:00 · Latest: 2026-01-13T18:51:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.19137v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.19137v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Film set design plays a pivotal role in cinematic storytelling and shaping the visual atmosphere. However, the traditional process depends on expert-driven manual modeling, which is labor-intensive and time-consuming. To address this issue, we introduce FilmSceneDesigner, an automated scene generation system that emulates professional film set design workflow. Given a natural language description, including scene type, historical period, and style, we design an agent-based chaining framework to generate structured parameters aligned with film set design workflow, guided by prompt strategies that ensure parameter accuracy and coherence. On the other hand, we propose a procedural generation pipeline which executes a series of dedicated functions with the structured parameters for floorplan and structure generation, material assignment, door and window placement, and object retrieval and layout, ultimately constructing a complete film scene from scratch. Moreover, to enhance cinematic realism and asset diversity, we construct SetDepot-Pro, a curated dataset of 6,862 film-specific 3D assets and 733 materials. Experimental results and human evaluations demonstrate that our system produces structurally sound scenes with strong cinematic fidelity, supporting downstream tasks such as virtual previs, construction drawing and mood board creation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FilmSceneDesigner：用于程序化电影场景生成的场景设计链式方法</div>
<div class="mono" style="margin-top:8px">电影布景设计在电影叙事和视觉氛围塑造中起着关键作用。然而，传统流程依赖于专家驱动的手动建模，这既耗时又耗力。为了解决这一问题，我们引入了FilmSceneDesigner，一个自动化场景生成系统，模拟专业电影布景设计流程。基于自然语言描述，包括场景类型、历史时期和风格，我们设计了一个基于代理的链式框架，生成与电影布景设计流程一致的结构化参数，并通过提示策略确保参数的准确性和连贯性。另一方面，我们提出了一种程序化生成流程，利用结构化参数执行一系列专用函数，用于平面图和结构生成、材质分配、门窗布置以及物体检索和布局，最终从零构建完整的电影场景。此外，为了增强电影的真实感和资产多样性，我们构建了SetDepot-Pro，一个包含6,862个电影专用3D资产和733种材质的精选数据集。实验结果和人工评估表明，我们的系统能够生成结构合理且具有强电影真实感的场景，支持虚拟预可视化、施工图纸制作和氛围板创建等下游任务。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Film set design plays a pivotal role in cinematic storytelling and shaping the visual atmosphere.</div>
</details>
</div>
<div class="card">
<div class="title">MemRec: Collaborative Memory-Augmented Agentic Recommender System</div>
<div class="meta-line">Authors: Weixin Chen, Yuhan Zhao, Jingyuan Huang, Zihe Ye, Clark Mingxuan Ju, Tong Zhao, Neil Shah, Li Chen, Yongfeng Zhang</div>
<div class="meta-line">First: 2026-01-13T18:51:16+00:00 · Latest: 2026-01-13T18:51:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08816v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08816v1">PDF</a> · <a href="https://github.com/rutgerswiselab/memrec">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The evolution of recommender systems has shifted preference storage from rating matrices and dense embeddings to semantic memory in the agentic era. Yet existing agents rely on isolated memory, overlooking crucial collaborative signals. Bridging this gap is hindered by the dual challenges of distilling vast graph contexts without overwhelming reasoning agents with cognitive load, and evolving the collaborative memory efficiently without incurring prohibitive computational costs. To address this, we propose MemRec, a framework that architecturally decouples reasoning from memory management to enable efficient collaborative augmentation. MemRec introduces a dedicated, cost-effective LM_Mem to manage a dynamic collaborative memory graph, serving synthesized, high-signal context to a downstream LLM_Rec. The framework operates via a practical pipeline featuring efficient retrieval and cost-effective asynchronous graph propagation that evolves memory in the background. Extensive experiments on four benchmarks demonstrate that MemRec achieves state-of-the-art performance. Furthermore, architectural analysis confirms its flexibility, establishing a new Pareto frontier that balances reasoning quality, cost, and privacy through support for diverse deployments, including local open-source models. Code:https://github.com/rutgerswiselab/memrec and Homepage: https://memrec.weixinchen.com</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MemRec：协作记忆增强的智能体推荐系统</div>
<div class="mono" style="margin-top:8px">推荐系统的发展在智能体时代将偏好存储从评分矩阵和密集嵌入转移到了语义记忆。然而，现有智能体依赖孤立的记忆，忽略了关键的协作信号。弥合这一差距面临双重挑战：一方面需要在不给推理智能体带来认知负担的情况下提炼庞大的图上下文；另一方面需要在不产生高昂计算成本的情况下高效演进协作记忆。为了解决这一问题，我们提出了MemRec框架，通过架构上将推理与记忆管理解耦，实现高效的协作增强。MemRec引入了一个专用且成本效益高的LM_Mem来管理动态的协作记忆图，为下游的LLM_Rec提供合成的、高信号的上下文。该框架通过一个实用的流程运行，包括高效的检索和低成本的异步图传播，从而在后台演进记忆。在四个基准上的大量实验表明，MemRec实现了最先进的性能。此外，架构分析证实了其灵活性，通过支持多种部署方式（包括本地开源模型），建立了新的帕累托前沿，平衡了推理质量、成本和隐私。代码：https://github.com/rutgerswiselab/memrec 和主页：https://memrec.weixinchen.com</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning Matters for 3D Visual Grounding</div>
<div class="meta-line">Authors: Hsiang-Wei Huang, Kuang-Ming Chen, Wenhao Chai, Cheng-Yen Yang, Jen-Hao Cheng, Jenq-Neng Hwang</div>
<div class="meta-line">Venue: CVPR</div>
<div class="meta-line">First: 2026-01-13T18:48:41+00:00 · Latest: 2026-01-13T18:48:41+00:00</div>
<div class="meta-line">Comments: 2025 CVPR Workshop on 3D-LLM/VLA: Bridging Language, Vision and Action in 3D Environments</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08811v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08811v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The recent development of Large Language Models (LLMs) with strong reasoning ability has driven research in various domains such as mathematics, coding, and scientific discovery. Meanwhile, 3D visual grounding, as a fundamental task in 3D understanding, still remains challenging due to the limited reasoning ability of recent 3D visual grounding models. Most of the current methods incorporate a text encoder and visual feature encoder to generate cross-modal fuse features and predict the referring object. These models often require supervised training on extensive 3D annotation data. On the other hand, recent research also focus on scaling synthetic data to train stronger 3D visual grounding LLM, however, the performance gain remains limited and non-proportional to the data collection cost. In this work, we propose a 3D visual grounding data pipeline, which is capable of automatically synthesizing 3D visual grounding data along with corresponding reasoning process. Additionally, we leverage the generated data for LLM fine-tuning and introduce Reason3DVG-8B, a strong 3D visual grounding LLM that outperforms previous LLM-based method 3D-GRAND using only 1.6% of their training data, demonstrating the effectiveness of our data and the importance of reasoning in 3D visual grounding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推理对3D视觉定位至关重要</div>
<div class="mono" style="margin-top:8px">近年来，具有强大推理能力的大型语言模型（LLMs）的发展推动了数学、编程和科学发现等多个领域的研究。然而，作为3D理解基础任务的3D视觉定位，由于现有3D视觉定位模型推理能力有限，仍然面临挑战。当前大多数方法结合文本编码器和视觉特征编码器，生成跨模态融合特征并预测所指对象。这些模型通常需要在大量3D标注数据上进行监督训练。另一方面，近期研究也关注通过扩展合成数据来训练更强大的3D视觉定位LLMs，但性能提升有限，且与数据收集成本不成正比。在本工作中，我们提出了一种3D视觉定位数据生成管道，能够自动合成3D视觉定位数据及其对应的推理过程。此外，我们利用生成的数据进行LLM微调，并引入了Reason3DVG-8B，这是一种性能优于先前基于LLM的方法3D-GRAND的3D视觉定位LLM，仅使用其训练数据的1.6%即可实现这一效果，展示了我们数据的有效性以及推理在3D视觉定位中的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Multiplex Thinking: Reasoning via Token-wise Branch-and-Merge</div>
<div class="meta-line">Authors: Yao Tang, Li Dong, Yaru Hao, Qingxiu Dong, Furu Wei, Jiatao Gu</div>
<div class="meta-line">First: 2026-01-13T18:48:00+00:00 · Latest: 2026-01-13T18:48:00+00:00</div>
<div class="meta-line">Comments: 21 pages. Code available at https://github.com/GMLR-Penn/Multiplex-Thinking</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08808v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08808v1">PDF</a> · <a href="https://github.com/GMLR-Penn/Multiplex-Thinking">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models often solve complex reasoning tasks more effectively with Chain-of-Thought (CoT), but at the cost of long, low-bandwidth token sequences. Humans, by contrast, often reason softly by maintaining a distribution over plausible next steps. Motivated by this, we propose Multiplex Thinking, a stochastic soft reasoning mechanism that, at each thinking step, samples K candidate tokens and aggregates their embeddings into a single continuous multiplex token. This preserves the vocabulary embedding prior and the sampling dynamics of standard discrete generation, while inducing a tractable probability distribution over multiplex rollouts. Consequently, multiplex trajectories can be directly optimized with on-policy reinforcement learning (RL). Importantly, Multiplex Thinking is self-adaptive: when the model is confident, the multiplex token is nearly discrete and behaves like standard CoT; when it is uncertain, it compactly represents multiple plausible next steps without increasing sequence length. Across challenging math reasoning benchmarks, Multiplex Thinking consistently outperforms strong discrete CoT and RL baselines from Pass@1 through Pass@1024, while producing shorter sequences. The code and checkpoints are available at https://github.com/GMLR-Penn/Multiplex-Thinking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多路思维：通过逐标记分支合并进行推理</div>
<div class="mono" style="margin-top:8px">大型语言模型通常通过思维链（Chain-of-Thought, CoT）更有效地解决复杂推理任务，但代价是产生长而低带宽的标记序列。相比之下，人类往往通过维护可能下一步的分布来进行软性推理。受此启发，我们提出了多路思维（Multiplex Thinking），一种随机的软性推理机制。在每一步推理中，该机制会采样K个候选标记，并将它们的嵌入聚合为一个连续的多路标记。这种方法保留了标准离散生成的词汇嵌入先验和采样动态，同时在多路生成路径上诱导出一个可处理的概率分布。因此，多路轨迹可以直接通过基于策略的强化学习（RL）进行优化。重要的是，多路思维是自适应的：当模型信心较高时，多路标记几乎接近离散，行为类似于标准的CoT；当模型不确定时，它能紧凑地表示多个可能的下一步，而不会增加序列长度。在具有挑战性的数学推理基准测试中，多路思维在Pass@1到Pass@1024的指标上持续优于强大的离散CoT和RL基线，同时生成更短的序列。代码和检查点可在https://github.com/GMLR-Penn/Multiplex-Thinking获取。</div>
</details>
</div>
<div class="card">
<div class="title">S3-CLIP: Video Super Resolution for Person-ReID</div>
<div class="meta-line">Authors: Tamas Endrei, Gyorgy Cserey</div>
<div class="meta-line">First: 2026-01-13T18:46:37+00:00 · Latest: 2026-01-13T18:46:37+00:00</div>
<div class="meta-line">Comments: Accepted to the 2026 IEEE/CVF Winter Conference on Applications of Computer Vision Workshops (WACVW), VReID-XFD Challenge</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08807v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08807v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tracklet quality is often treated as an afterthought in most person re-identification (ReID) methods, with the majority of research presenting architectural modifications to foundational models. Such approaches neglect an important limitation, posing challenges when deploying ReID systems in real-world, difficult scenarios. In this paper, we introduce S3-CLIP, a video super-resolution-based CLIP-ReID framework developed for the VReID-XFD challenge at WACV 2026. The proposed method integrates recent advances in super-resolution networks with task-driven super-resolution pipelines, adapting them to the video-based person re-identification setting. To the best of our knowledge, this work represents the first systematic investigation of video super-resolution as a means of enhancing tracklet quality for person ReID, particularly under challenging cross-view conditions. Experimental results demonstrate performance competitive with the baseline, achieving 37.52% mAP in aerial-to-ground and 29.16% mAP in ground-to-aerial scenarios. In the ground-to-aerial setting, S3-CLIP achieves substantial gains in ranking accuracy, improving Rank-1, Rank-5, and Rank-10 performance by 11.24%, 13.48%, and 17.98%, respectively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>S3-CLIP：面向人重识别的视频超分辨率技术</div>
<div class="mono" style="margin-top:8px">在大多数行人重识别（ReID）方法中，轨迹片段质量往往被忽视，大部分研究仅对基础模型进行架构修改。此类方法忽略了重要的限制，给ReID系统在现实世界复杂场景中的部署带来了挑战。本文提出S3-CLIP，这是一个基于视频超分辨率的CLIP-ReID框架，专为2026年WACV会议上的VReID-XFD挑战赛设计。所提出的方法结合了超分辨率网络的最新进展与任务驱动的超分辨率流程，将其适配到基于视频的行人重识别场景中。据我们所知，这是首次系统性地研究视频超分辨率作为提升轨迹片段质量的手段，特别是在具有挑战性的跨视角条件下。实验结果表明，该方法的性能与基线相当，在空地到地面场景中达到37.52%的mAP，在地面到空地场景中达到29.16%的mAP。在地面到空地设置中，S3-CLIP在排名准确率上取得了显著提升，分别提高了Rank-1、Rank-5和Rank-10的性能11.24%、13.48%和17.98%。</div>
</details>
</div>
<div class="card">
<div class="title">LocalSearchBench: Benchmarking Agentic Search in Real-World Local Life Services</div>
<div class="meta-line">Authors: Hang He, Chuhuai Yue, Chengqi Dong, Mingxue Tian, Hao Chen, Zhenfeng Liu, Jiajun Chai, Xiaohan Wang, Yufei Zhang, Qun Liao, Guojun Yin, Wei Lin, Chengcheng Wan, Haiying Sun, Ting Su</div>
<div class="meta-line">First: 2025-12-08T11:12:39+00:00 · Latest: 2026-01-13T18:44:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.07436v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.07436v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://localsearchbench.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large reasoning models LRMs have enabled agentic search systems to perform complex multi-step reasoning across multiple sources. However, most studies focus on general information retrieval and rarely explores vertical domains with unique challenges. In this work, we focus on local life services and introduce LocalSearchBench, which encompass diverse and complex business scenarios. Real-world queries in this domain are often ambiguous and require multi-hop reasoning across merchants and products, remaining challenging and not fully addressed. As the first comprehensive benchmark for agentic search in local life services, LocalSearchBench comprises a database of over 1.3M merchant entries across 6 service categories and 9 major cities, and 900 multi-hop QA tasks from real user queries that require multi-step reasoning. We also developed LocalPlayground, a unified environment integrating multiple tools for LRMs interaction. Experiments show that even state-of-the-art LRMs struggle on LocalSearchBench: the best model (DeepSeek-V3.2) achieves only 35.60% correctness, and most models have issues with completeness (average 60.32%) and faithfulness (average 30.72%). This highlights the need for specialized benchmarks and domain-specific agent training in local life services. Code, Benchmark, and Leaderboard are available at https://localsearchbench.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LocalSearchBench：面向现实本地生活服务的代理搜索基准测试</div>
<div class="mono" style="margin-top:8px">近年来，大型推理模型（LRMs）的进步使得代理搜索系统能够在多个来源之间执行复杂的多步推理。然而，大多数研究集中在通用信息检索上，很少涉及具有独特挑战的垂直领域。在本工作中，我们专注于本地生活服务，并引入了LocalSearchBench，涵盖多样且复杂的商业场景。该领域的现实查询通常具有歧义性，需要跨商家和产品的多跳推理，仍然具有挑战性且未被充分解决。作为首个针对本地生活服务代理搜索的综合性基准，LocalSearchBench包含超过130万个商家条目，涵盖6个服务类别和9个主要城市，以及来自真实用户查询的900个多跳问答任务，需要多步推理。我们还开发了LocalPlayground，这是一个集成多种工具的统一环境，用于LRMs的交互。实验表明，即使是最先进的LRMs在LocalSearchBench上也面临困难：最佳模型（DeepSeek-V3.2）仅达到35.60%的正确率，而大多数模型在完整性和忠实性方面存在明显问题（平均分别为60.32%和30.72%）。这突显了在本地生活服务领域需要专门的基准和领域特定的代理训练。代码、基准和排行榜可在https://localsearchbench.github.io/获取。</div>
</details>
</div>
<div class="card">
<div class="title">APEX-SWE</div>
<div class="meta-line">Authors: Abhi Kottamasu, Akul Datta, Aakash Barthwal, Chirag Mahapatra, Ajay Arun, Adarsh Hiremath, Brendan Foody, Bertie Vidgen</div>
<div class="meta-line">First: 2026-01-13T18:44:08+00:00 · Latest: 2026-01-13T18:44:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08806v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08806v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce the AI Productivity Index for Software Engineering (APEX-SWE), a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work. Unlike existing evaluations that focus on narrow, well-defined tasks, APEX-SWE assesses two novel task types that reflect real-world software engineering work: (1) Integration tasks (n=100), which require constructing end-to-end systems across heterogeneous cloud primitives, business applications, and infrastructure-as-code services, and (2) Observability tasks (n=100), which require debugging production failures using telemetry signals such as logs and dashboards, as well as unstructured context. We evaluated eight frontier models on APEX-SWE. Gemini 3 Pro (Thinking = High) performs best, with a Pass@1 score of 25\%. Our analysis shows that strong performance is primarily driven by epistemic reasoning, defined as the ability to distinguish between assumptions and verified facts, combined with agency to resolve uncertainty prior to acting. We open-source the APEX-SWE evaluation harness and a dev set (n=50).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>APEX-SWE</div>
<div class="mono" style="margin-top:8px">我们引入了用于软件工程的AI生产力指数（APEX-SWE），这是一个评估前沿AI模型是否能够执行具有经济价值的软件工程工作的基准。与现有专注于狭窄、明确定义任务的评估不同，APEX-SWE评估了两种新颖的任务类型，反映了现实世界中的软件工程工作：(1) 集成任务（n=100），需要在异构的云原生组件、业务应用和基础设施即代码服务之间构建端到端系统；(2) 可观测性任务（n=100），需要利用日志、仪表盘等遥测信号以及非结构化上下文来调试生产环境中的故障。我们在APEX-SWE上评估了八个前沿模型。Gemini 3 Pro（Thinking = High）表现最佳，Pass@1得分为25\%。我们的分析表明，优异表现主要由知识推理（epistemic reasoning）驱动，即区分假设与验证事实的能力，以及在行动前解决不确定性的能力（agency）。我们开源了APEX-SWE评估框架和一个开发数据集（n=50）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We introduce the AI Productivity Index for Software Engineering (APEX-SWE), a benchmark for assessing whether frontier AI models can execute economically valuable software engineering work.</div>
</details>
</div>
<div class="card">
<div class="title">Unstable synthetic deformations I: Malcev theories</div>
<div class="meta-line">Authors: William Balderrama, Piotr Pstrągowski</div>
<div class="meta-line">First: 2026-01-13T18:39:45+00:00 · Latest: 2026-01-13T18:39:45+00:00</div>
<div class="meta-line">Comments: 85 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08802v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08802v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper is the first in a series of articles devoted to the construction and study of synthetic deformations of $\infty$-categories in the unstable context: that is, deformations of $\infty$-categories that categorify spectral sequence or obstruction-theoretic information.
  This paper sets up the foundations of our study. We introduce and study various classes of $\infty$-categorical and infinitary algebraic theories. We establish many basic properties of the $\infty$-categories of the models of different classes of theories, as well as recognition theorems identifying the $\infty$-categories that arise this way.
  We give an intrinsic definition of a Malcev theory in higher universal algebra. We establish that the $\infty$-category of models of a Malcev theory may be characterized as freely adjoining geometric realizations to the theory. This leads to the notion of a derived functor between $\infty$-categories of models of Malcev theories, and we study the behavior of these derived functors with respect to connectivity and limits.
  We recall the notion of a loop theory and study in detail the interaction between functors and derived functors of $\infty$-categories of loop models and models, establishing that a large class of comonads on the $\infty$-category of loop models deform canonically to the $\infty$-category of all models.
  In the last part of the paper, we show that by considering the coalgebras for these deformed comonads over $\infty$-categories of models, one can recover various stable deformations considered in the literature, such as filtered models or Postnikov-complete synthetic spectra. We then expand on these results by constructing $\infty$-categories of synthetic spaces and synthetic $\mathbf{E}_k$-rings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>不稳定合成变形 I: Malcev 理论</div>
<div class="mono" style="margin-top:8px">本文是关于在不稳定背景下构造和研究 $\infty$-范畴的合成变形的一系列文章中的第一篇：即对 $\infty$-范畴进行变形，以范畴化谱序列或障碍理论信息。
本文奠定了我们研究的基础。我们引入并研究了各种 $\infty$-范畴和无限代数理论的类。我们建立了不同类理论模型的 $\infty$-范畴的基本性质，以及识别这些 $\infty$-范畴的识别定理。
我们给出了一个更高阶普遍代数中 Malcev 理论的内在定义。我们证明了 Malcev 理论模型的 $\infty$-范畴可以被描述为在理论上自由地附加几何实现。这引出了在 Malcev 理论模型的 $\infty$-范畴之间的导出函子的概念，我们研究了这些导出函子在连通性和极限下的行为。
我们回顾了环理论的概念，并详细研究了环模型的 $\infty$-范畴及其导出函子之间的相互作用，证明了一个大类的共变子在环模型的 $\infty$-范畴上可以规范地变形到所有模型的 $\infty$-范畴。
在本文的最后一部分，我们展示了通过考虑这些变形共变子在模型 $\infty$-范畴上的煤代数，可以恢复文献中各种稳定的变形，例如滤过模型或 Postnikov 完全的合成谱。随后，我们通过构造合成空间和合成 $\mathbf{E}_k$-环的 $\infty$-范畴，扩展了这些结果。</div>
</details>
</div>
<div class="card">
<div class="title">Free-RBF-KAN: Kolmogorov-Arnold Networks with Adaptive Radial Basis Functions for Efficient Function Learning</div>
<div class="meta-line">Authors: Shao-Ting Chiu, Siu Wun Cheung, Ulisses Braga-Neto, Chak Shing Lee, Rui Peng Li</div>
<div class="meta-line">First: 2026-01-12T17:45:31+00:00 · Latest: 2026-01-13T18:39:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07760v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07760v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Kolmogorov-Arnold Networks (KANs) have shown strong potential for efficiently approximating complex nonlinear functions. However, the original KAN formulation relies on B-spline basis functions, which incur substantial computational overhead due to De Boor&#x27;s algorithm. To address this limitation, recent work has explored alternative basis functions such as radial basis functions (RBFs) that can improve computational efficiency and flexibility. Yet, standard RBF-KANs often sacrifice accuracy relative to the original KAN design. In this work, we propose Free-RBF-KAN, a RBF-based KAN architecture that incorporates adaptive learning grids and trainable smoothness to close this performance gap. Our method employs freely learnable RBF shapes that dynamically align grid representations with activation patterns, enabling expressive and adaptive function approximation. Additionally, we treat smoothness as a kernel parameter optimized jointly with network weights, without increasing computational complexity. We provide a general universality proof for RBF-KANs, which encompasses our Free-RBF-KAN formulation. Through a broad set of experiments, including multiscale function approximation, physics-informed machine learning, and PDE solution operator learning, Free-RBF-KAN achieves accuracy comparable to the original B-spline-based KAN while delivering faster training and inference. These results highlight Free-RBF-KAN as a compelling balance between computational efficiency and adaptive resolution, particularly for high-dimensional structured modeling tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Free-RBF-KAN：基于自适应径向基函数的Kolmogorov-Arnold网络，用于高效函数学习</div>
<div class="mono" style="margin-top:8px">Kolmogorov-Arnold网络（KANs）在高效逼近复杂非线性函数方面展现出强大潜力。然而，原始KAN结构依赖于B样条基函数，由于De Boor算法导致了显著的计算开销。为了解决这一限制，近期研究探索了替代基函数，如径向基函数（RBFs），以提高计算效率和灵活性。然而，标准的RBF-KAN通常在精度上不如原始KAN设计。在本文中，我们提出Free-RBF-KAN，一种基于RBF的KAN架构，结合自适应学习网格和可训练的平滑度，以弥合这一性能差距。我们的方法采用可自由学习的RBF形状，动态对齐网格表示与激活模式，从而实现表达力强且自适应的函数逼近。此外，我们将平滑度视为一个内核参数，与网络权重联合优化，而不会增加计算复杂度。我们为RBF-KAN提供了一个通用的普适性证明，涵盖了我们的Free-RBF-KAN公式。通过一系列广泛的实验，包括多尺度函数逼近、物理信息机器学习以及偏微分方程解算子学习，Free-RBF-KAN在保持与原始B样条KAN相当精度的同时，实现了更快的训练和推理速度。这些结果突显了Free-RBF-KAN在计算效率与自适应分辨率之间的良好平衡，特别是在高维结构化建模任务中。</div>
</details>
</div>
<div class="card">
<div class="title">MixServe: An Automatic Distributed Serving System for MoE Models with Hybrid Parallelism Based on Fused Communication Algorithm</div>
<div class="meta-line">Authors: Bowen Zhou, Jinrui Jia, Wenhao He, Yong Zhang, Fang Dong</div>
<div class="meta-line">First: 2026-01-13T18:38:18+00:00 · Latest: 2026-01-13T18:38:18+00:00</div>
<div class="meta-line">Comments: Submitted to ICDCS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08800v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08800v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Mixture of Experts (MoE) models are emerging as the latest paradigm for Large Language Models (LLMs). However, due to memory constraints, MoE models with billions or even trillions of parameters can only be deployed in multi-GPU or even multi-node &amp; multi-GPU based serving systems. Thus, communication has became a major bottleneck in distributed serving systems, especially inter-node communication. Contemporary distributed MoE models are primarily implemented using all-reduce (AR) based tensor parallelism (TP) and all-to-all (A2A) based expert parallelism (EP). However, TP generally exhibits low inter-node efficiency and is thus confined to high-speed intra-node bandwidth. In contrast, EP tends to suffer from load imbalance, especially when the parallel degree is high.
  In this work, we introduce MixServe, a novel automatic distributed serving system for efficient deployment of MoE models by a novel TP-EP hybrid parallelism based on fused AR-A2A communication algorithm. MixServe begins by evaluating the communication overhead associated with various parallel strategies, taking into account the model hyperparameters and the configurations of network and hardware resources, and then automatically selects the most efficient parallel strategy. Then, we propose the TP-EP hybrid parallelism based on fused AR-A2A communication algorithm that overlaps intra-node AR communication and inter-node A2A communication. Extensive experiments on DeepSeek-R1 and Qwen3 models demonstrate that MixServe achieves superior inference performance, with 1.08~3.80x acceleration in time to first token (TTFT), 1.03~1.66x acceleration in inter-token latency (ITL), and 5.2%~50.3% throughput improvement compared to existing approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MixServe：基于融合通信算法的混合并行自动分布式服务系统用于MoE模型</div>
<div class="mono" style="margin-top:8px">专家混合（MoE）模型正成为大语言模型（LLMs）的最新范式。然而，由于内存限制，拥有数十亿甚至数万亿参数的MoE模型只能部署在多GPU或甚至多节点与多GPU结合的服务系统中。因此，通信成为分布式服务系统中的主要瓶颈，尤其是节点间通信。当前的分布式MoE模型主要通过基于全归约（AR）的张量并行（TP）和基于全对全（A2A）的专家并行（EP）实现。然而，TP通常表现出较低的节点间效率，因此受限于高速节点内带宽。相比之下，EP容易出现负载不平衡，尤其是在并行度较高的情况下。在本工作中，我们引入了MixServe，这是一种基于融合AR-A2A通信算法的新型TP-EP混合并行方法的自动分布式服务系统，用于高效部署MoE模型。MixServe首先评估不同并行策略的通信开销，考虑模型超参数以及网络和硬件资源的配置，然后自动选择最有效的并行策略。随后，我们提出了基于融合AR-A2A通信算法的TP-EP混合并行方法，该方法重叠了节点内AR通信和节点间A2A通信。在DeepSeek-R1和Qwen3模型上的大量实验表明，MixServe在推理性能上表现优异，相较于现有方法，其首次标记生成时间（TTFT）加速了1.08~3.80倍，标记间延迟（ITL）加速了1.03~1.66倍，吞吐量提升了5.2%~50.3%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The Mixture of Experts (MoE) models are emerging as the latest paradigm for Large Language Models (LLMs).</div>
</details>
</div>
<div class="card">
<div class="title">Near-perfect photo-ID of the Hula painted frog with zero-shot deep local-feature matching</div>
<div class="meta-line">Authors: Maayan Yesharim, R. G. Bina Perl, Uri Roll, Sarig Gafny, Eli Geffen, Yoav Ram</div>
<div class="meta-line">First: 2026-01-13T18:32:43+00:00 · Latest: 2026-01-13T18:32:43+00:00</div>
<div class="meta-line">Comments: 18 pages, 4 figures,</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08798v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08798v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate individual identification is essential for monitoring rare amphibians, yet invasive marking is often unsuitable for critically endangered species. We evaluate state-of-the-art computer-vision methods for photographic re-identification of the Hula painted frog (Latonia nigriventer) using 1,233 ventral images from 191 individuals collected during 2013-2020 capture-recapture surveys. We compare deep local-feature matching in a zero-shot setting with deep global-feature embedding models. The local-feature pipeline achieves 98% top-1 closed-set identification accuracy, outperforming all global-feature models; fine-tuning improves the best global-feature model to 60% top-1 (91% top-10) but remains below local matching. To combine scalability with accuracy, we implement a two-stage workflow in which a fine-tuned global-feature model retrieves a short candidate list that is re-ranked by local-feature matching, reducing end-to-end runtime from 6.5-7.8 hours to ~38 minutes while maintaining ~96% top-1 closed-set accuracy on the labeled dataset. Separation of match scores between same- and different-individual pairs supports thresholding for open-set identification, enabling practical handling of novel individuals. We deploy this pipeline as a web application for routine field use, providing rapid, standardized, non-invasive identification to support conservation monitoring and capture-recapture analyses. Overall, in this species, zero-shot deep local-feature matching outperformed global-feature embedding and provides a strong default for photo-identification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用零样本深度局部特征匹配对胡拉彩蛙进行近乎完美的照片身份识别</div>
<div class="mono" style="margin-top:8px">准确的个体识别对于监测稀有两栖动物至关重要，但对濒危物种而言，侵入性标记方法往往不适用。我们利用2013-2020年间捕获-再捕获调查中收集的191个个体的1,233张腹部图像，评估了最先进的计算机视觉方法在胡拉彩蛙（Latonia nigriventer）照片再识别中的表现。我们比较了零样本设置下的深度局部特征匹配与深度全局特征嵌入模型。局部特征流程在闭集识别中达到98%的top-1准确率，优于所有全局特征模型；对最佳全局特征模型进行微调后，其top-1准确率提升至60%（top-10为91%），但仍低于局部匹配。为了兼顾可扩展性和准确性，我们实现了一个两阶段工作流程，其中微调后的全局特征模型用于检索一个较短的候选列表，再由局部特征匹配重新排序，将端到端运行时间从6.5-7.8小时减少到约38分钟，同时在标记数据集上保持约96%的top-1闭集识别准确率。通过区分同个体与异个体对的匹配得分，支持开放集识别的阈值设定，从而实现对新个体的实用处理。我们将该流程部署为一个网络应用程序，用于常规野外作业，提供快速、标准化、非侵入性的个体识别，以支持保护监测和捕获-再捕获分析。总体而言，在该物种中，零样本深度局部特征匹配优于全局特征嵌入，并为照片识别提供了强大的默认方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Accurate individual identification is essential for monitoring rare amphibians, yet invasive marking is often unsuitable for critically endangered species.</div>
</details>
</div>
<div class="card">
<div class="title">DentalX: Context-Aware Dental Disease Detection with Radiographs</div>
<div class="meta-line">Authors: Zhi Qin Tan, Xiatian Zhu, Owen Addison, Yunpeng Li</div>
<div class="meta-line">Venue: ISBI 2026</div>
<div class="meta-line">First: 2026-01-13T18:32:28+00:00 · Latest: 2026-01-13T18:32:28+00:00</div>
<div class="meta-line">Comments: Accepted at ISBI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08797v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08797v1">PDF</a> · <a href="https://github.com/zhiqin1998/DentYOLOX">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diagnosing dental diseases from radiographs is time-consuming and challenging due to the subtle nature of diagnostic evidence. Existing methods, which rely on object detection models designed for natural images with more distinct target patterns, struggle to detect dental diseases that present with far less visual support. To address this challenge, we propose {\bf DentalX}, a novel context-aware dental disease detection approach that leverages oral structure information to mitigate the visual ambiguity inherent in radiographs. Specifically, we introduce a structural context extraction module that learns an auxiliary task: semantic segmentation of dental anatomy. The module extracts meaningful structural context and integrates it into the primary disease detection task to enhance the detection of subtle dental diseases. Extensive experiments on a dedicated benchmark demonstrate that DentalX significantly outperforms prior methods in both tasks. This mutual benefit arises naturally during model optimization, as the correlation between the two tasks is effectively captured. Our code is available at https://github.com/zhiqin1998/DentYOLOX.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DentalX：基于牙科影像的上下文感知牙科疾病检测</div>
<div class="mono" style="margin-top:8px">由于诊断证据较为微妙，从牙科影像中诊断牙科疾病既耗时又具有挑战性。现有方法依赖于为自然图像设计的对象检测模型，这些模型通常针对具有更明显目标模式的图像，难以检测到视觉支持较少的牙科疾病。为了解决这一挑战，我们提出了DentalX，一种新颖的上下文感知牙科疾病检测方法，利用口腔结构信息来缓解牙科影像中固有的视觉模糊性。具体而言，我们引入了一个结构上下文提取模块，该模块学习一个辅助任务：牙科解剖结构的语义分割。该模块提取有意义的结构上下文，并将其整合到主要的疾病检测任务中，以增强对细微牙科疾病的检测能力。在专门的基准数据集上的大量实验表明，DentalX在两个任务中均显著优于现有方法。这种相互促进的效果在模型优化过程中自然产生，因为两个任务之间的相关性被有效捕捉。</div>
</details>
</div>
<div class="card">
<div class="title">Black hole entropy from the quantum atmosphere of bound gravitational fluctuations</div>
<div class="meta-line">Authors: Seth Major, Daniel Rodriguez, Thomas Takis</div>
<div class="meta-line">First: 2026-01-13T18:27:50+00:00 · Latest: 2026-01-13T18:27:50+00:00</div>
<div class="meta-line">Comments: 17 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08794v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08794v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Black hole entropy is identified with the counting of the dynamical degrees of freedom of trapped gravitational modes continually sourced by the Hawking-Unruh process. In the context of linear perturbations of Schwarzschild spacetime the density of states is derived from the orthogonality of states in the solution space of the Regge-Wheeler-Zerilli equation. The otherwise divergent energy and entropy is cutoff by the Planck scale closest approach of constantly accelerating observers near the horizon. The thermal distribution of the trapped modes, which represent shape fluctuations in the near horizon geometry, store a significant fraction of the spacetime mass as observed from far away. Unlike quasi-normal modes the modes are not directly observable outside of $\sim 3 M$ but, being external to the horizon, they affect the propagation of null rays near the black hole. The characteristic frequencies, around 100 Hz for solar mass black holes, are discussed in relation to possible observations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从束缚引力涨落的量子大气中导出黑洞熵</div>
<div class="mono" style="margin-top:8px">黑洞熵被识别为通过霍金-Unruh过程持续产生的被捕获引力模式的动态自由度计数。在Schwarzschild时空线性扰动的背景下，状态密度来源于Regge-Wheeler-Zerilli方程解空间中状态的正交性。否则发散的能量和熵被接近视界处不断加速观测者的普朗克尺度最近接近所截断。这些被捕获模式的热分布，代表视界附近几何形状的涨落，从远处观测储存了显著的时空质量。与准规范模式不同，这些模式不能直接在约3M的外部观测到，但由于位于视界之外，它们影响了黑洞附近光子的传播。对于太阳质量黑洞，其特征频率约为100Hz，与可能的观测相关。</div>
</details>
</div>
<div class="card">
<div class="title">DGAE: Diffusion-Guided Autoencoder for Efficient Latent Representation Learning</div>
<div class="meta-line">Authors: Dongxu Liu, Jiahui Zhu, Yuang Peng, Haomiao Tang, Yuwei Chen, Chunrui Han, Zheng Ge, Daxin Jiang, Mingxue Liao</div>
<div class="meta-line">First: 2025-06-11T12:01:03+00:00 · Latest: 2026-01-13T18:26:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.09644v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.09644v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoencoders empower state-of-the-art image and video generative models by compressing pixels into a latent space through visual tokenization. Although recent advances have alleviated the performance degradation of autoencoders under high compression ratios, addressing the training instability caused by GAN remains an open challenge. While improving spatial compression, we also aim to minimize the latent space dimensionality, enabling more efficient and compact representations. To tackle these challenges, we focus on improving the decoder&#x27;s expressiveness. Concretely, we propose DGAE, which employs a diffusion model to guide the decoder in recovering informative signals that are not fully decoded from the latent representation. With this design, DGAE effectively mitigates the performance degradation under high spatial compression rates. At the same time, DGAE achieves state-of-the-art performance with a 2x smaller latent space. When integrated with Diffusion Models, DGAE demonstrates competitive performance on image generation for ImageNet-1K and shows that this compact latent representation facilitates faster convergence of the diffusion model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DGAE：用于高效潜在表示学习的扩散引导自编码器</div>
<div class="mono" style="margin-top:8px">自编码器通过视觉分词将像素压缩到潜在空间，从而赋能最先进的图像和视频生成模型。尽管近期进展在一定程度上缓解了高压缩比下自编码器的性能退化问题，但由GAN引起的训练不稳定性仍然是一个开放性挑战。在提升空间压缩的同时，我们还旨在最小化潜在空间的维度，从而实现更高效且紧凑的表示。为了解决这些挑战，我们专注于提升解码器的表达能力。具体而言，我们提出了DGAE，它利用扩散模型引导解码器恢复潜在表示中未被完全解码的信息信号。通过这种设计，DGAE有效缓解了高空间压缩率下的性能退化。同时，DGAE在潜在空间规模缩小一半的情况下实现了最先进的性能。当与扩散模型结合时，DGAE在ImageNet-1K图像生成任务中表现出竞争力，并表明这种紧凑的潜在表示有助于扩散模型更快收敛。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Autoencoders empower state-of-the-art image and video generative models by compressing pixels into a latent space through visual tokenization.</div>
</details>
</div>
<div class="card">
<div class="title">Stability of Primal-Dual Gradient Flow Dynamics for Multi-Block Convex Optimization Problems</div>
<div class="meta-line">Authors: Ibrahim K. Ozaslan, Panagiotis Patrinos, Mihailo R. Jovanović</div>
<div class="meta-line">First: 2024-08-28T17:43:18+00:00 · Latest: 2026-01-13T18:25:29+00:00</div>
<div class="meta-line">Comments: 32 pages; 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2408.15969v3">Abs</a> · <a href="https://arxiv.org/pdf/2408.15969v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We examine stability properties of primal-dual gradient flow dynamics for composite convex optimization problems with multiple, possibly nonsmooth, terms in the objective function under the generalized consensus constraint. The proposed dynamics are based on the proximal augmented Lagrangian and they provide a viable alternative to ADMM which faces significant challenges from both analysis and implementation viewpoints in large-scale multi-block scenarios. In contrast to customized algorithms with individualized convergence guarantees, we develop a systematic approach for solving a broad class of challenging composite optimization problems. We leverage various structural properties to establish global (exponential) convergence guarantees for the proposed dynamics. Our assumptions are much weaker than those required to prove (exponential) stability of primal-dual dynamics as well as (linear) convergence of discrete-time methods such as standard two-block and multi-block ADMM and EXTRA algorithms. Finally, we show necessity of some of our structural assumptions for exponential stability and provide computational experiments to demonstrate the convenience of the proposed approach for parallel and distributed computing applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多块凸优化问题中原始-对偶梯度流动力学的稳定性</div>
<div class="mono" style="margin-top:8px">我们研究了在广义共识约束下，具有多个可能非光滑项的复合凸优化问题中原始-对偶梯度流动力学的稳定性性质。所提出的动力学基于近端增广拉格朗日方法，为ADMM提供了一个可行的替代方案，后者在大规模多块场景中从分析和实现角度来看都面临显著挑战。与具有个性化收敛保证的定制化算法不同，我们开发了一种系统的方法来解决广泛的一类具有挑战性的复合优化问题。我们利用各种结构特性，为所提出的动力学建立了全局（指数级）收敛保证。我们的假设条件比证明原始-对偶动力学的（指数级）稳定性以及标准两块和多块ADMM和EXTRA等离散时间方法的（线性）收敛所需的假设要弱得多。最后，我们展示了某些结构假设对于指数级稳定性的必要性，并通过计算实验说明所提出方法在并行和分布式计算应用中的便捷性。</div>
</details>
</div>
<div class="card">
<div class="title">A Vision for Multisensory Intelligence: Sensing, Science, and Synergy</div>
<div class="meta-line">Authors: Paul Pu Liang</div>
<div class="meta-line">First: 2026-01-08T03:46:20+00:00 · Latest: 2026-01-13T18:24:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.04563v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.04563v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://mit-mi.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Our experience of the world is multisensory, spanning a synthesis of language, sight, sound, touch, taste, and smell. Yet, artificial intelligence has primarily advanced in digital modalities like text, vision, and audio. This paper outlines a research vision for multisensory artificial intelligence over the next decade. This new set of technologies can change how humans and AI experience and interact with one another, by connecting AI to the human senses and a rich spectrum of signals from physiological and tactile cues on the body, to physical and social signals in homes, cities, and the environment. We outline how this field must advance through three interrelated themes of sensing, science, and synergy. Firstly, research in sensing should extend how AI captures the world in richer ways beyond the digital medium. Secondly, developing a principled science for quantifying multimodal heterogeneity and interactions, developing unified modeling architectures and representations, and understanding cross-modal transfer. Finally, we present new technical challenges to learn synergy between modalities and between humans and AI, covering multisensory integration, alignment, reasoning, generation, generalization, and experience. Accompanying this vision paper are a series of projects, resources, and demos of latest advances from the Multisensory Intelligence group at the MIT Media Lab, see https://mit-mi.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多感官智能的愿景：感知、科学与协同</div>
<div class="mono" style="margin-top:8px">我们对世界的体验是多感官的，涵盖语言、视觉、听觉、触觉、味觉和嗅觉的综合。然而，人工智能主要在文本、视觉和音频等数字模态上取得了进展。本文提出了一种未来十年多感官人工智能的研究愿景。这一新技术集合能够通过将人工智能与人类感官以及从身体生理和触觉信号到家庭、城市和环境中的物理和社会信号的丰富谱系连接起来，改变人类与人工智能的体验和互动方式。我们阐述了该领域必须通过三个相互关联的主题——感知、科学和协同——来推进。首先，感知研究应拓展人工智能以更丰富的方式捕捉世界，超越数字媒介的限制。其次，发展一种基于原则的科学，用于量化多模态异质性和交互，构建统一的建模架构和表示方法，并理解跨模态迁移。最后，我们提出了新的技术挑战，以学习模态之间以及人类与人工智能之间的协同，涵盖多感官整合、对齐、推理、生成、泛化和体验。本文附有麻省理工学院媒体实验室多感官智能小组最新进展的一系列项目、资源和演示，详见 https://mit-mi.github.io/。</div>
</details>
</div>
<div class="card">
<div class="title">Aggregating Diverse Cue Experts for AI-Generated Image Detection</div>
<div class="meta-line">Authors: Lei Tan, Shuwei Li, Mohan Kankanhalli, Robby T. Tan</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2026-01-13T18:23:42+00:00 · Latest: 2026-01-13T18:23:42+00:00</div>
<div class="meta-line">Comments: Accepted by AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08790v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08790v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid emergence of image synthesis models poses challenges to the generalization of AI-generated image detectors. However, existing methods often rely on model-specific features, leading to overfitting and poor generalization. In this paper, we introduce the Multi-Cue Aggregation Network (MCAN), a novel framework that integrates different yet complementary cues in a unified network. MCAN employs a mixture-of-encoders adapter to dynamically process these cues, enabling more adaptive and robust feature representation. Our cues include the input image itself, which represents the overall content, and high-frequency components that emphasize edge details. Additionally, we introduce a Chromatic Inconsistency (CI) cue, which normalizes intensity values and captures noise information introduced during the image acquisition process in real images, making these noise patterns more distinguishable from those in AI-generated content. Unlike prior methods, MCAN&#x27;s novelty lies in its unified multi-cue aggregation framework, which integrates spatial, frequency-domain, and chromaticity-based information for enhanced representation learning. These cues are intrinsically more indicative of real images, enhancing cross-model generalization. Extensive experiments on the GenImage, Chameleon, and UniversalFakeDetect benchmark validate the state-of-the-art performance of MCAN. In the GenImage dataset, MCAN outperforms the best state-of-the-art method by up to 7.4% in average ACC across eight different image generators.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向AI生成图像检测的多线索聚合网络</div>
<div class="mono" style="margin-top:8px">图像合成模型的快速涌现对AI生成图像检测器的泛化能力提出了挑战。然而，现有方法通常依赖于模型特定的特征，导致过拟合和泛化能力差。本文提出了一种名为Multi-Cue Aggregation Network (MCAN)的新框架，该框架在一个统一的网络中整合了不同但互补的线索。MCAN采用混合编码器适配器动态处理这些线索，从而实现更自适应和鲁棒的特征表示。我们的线索包括输入图像本身，代表整体内容，以及强调边缘细节的高频成分。此外，我们引入了色度不一致性（Chromatic Inconsistency, CI）线索，该线索通过归一化强度值并捕捉真实图像在图像采集过程中引入的噪声信息，使这些噪声模式更易于与AI生成内容中的噪声模式区分开。与以往方法不同，MCAN的创新之处在于其统一的多线索聚合框架，该框架整合了空间、频率域和色度信息，以增强表示学习。这些线索本质上更能反映真实图像的特征，从而提升跨模型的泛化能力。我们在GenImage、Chameleon和UniversalFakeDetect基准数据集上进行了大量实验，验证了MCAN的最先进性能。在GenImage数据集上，MCAN在八种不同图像生成器的平均ACC指标上，优于当前最佳方法最高达7.4%。</div>
</details>
</div>
<div class="card">
<div class="title">The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning</div>
<div class="meta-line">Authors: Qiguang Chen, Yantao Du, Ziniu Li, Jinhao Liu, Songyao Duan, Jiarui Guo, Minghao Liu, Jiaheng Liu, Tong Yang, Ge Zhang, Libo Qin, Wanxiang Che, Wenhao Huang</div>
<div class="meta-line">First: 2026-01-09T18:39:01+00:00 · Latest: 2026-01-13T18:21:01+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06002v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.06002v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>思维的分子结构：长链式思维推理的拓扑映射</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）通常无法从人类或非长链式思维（Long CoT）LLMs的模仿中学习到有效的长链式思维推理。为理解这一现象，我们提出有效的、可学习的长链式思维轨迹在统一视角下具有稳定的类似分子的结构，这些结构由三种交互类型构成：深度推理（类似共价键）、自我反思（类似氢键）和自我探索（类似范德华力）。对蒸馏轨迹的分析表明，这些结构源于长链式思维微调，而非关键词模仿。我们引入了有效语义异构体，并展示只有促进快速熵收敛的键才能支持稳定的长链式思维学习，而结构竞争则会损害训练效果。基于这些发现，我们提出了Mole-Syn，一种分布迁移图方法，用于指导有效长链式思维结构的合成，从而在多个基准测试中提升性能和强化学习的稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">SafePro: Evaluating the Safety of Professional-Level AI Agents</div>
<div class="meta-line">Authors: Kaiwen Zhou, Shreedhar Jangam, Ashwin Nagarajan, Tejas Polu, Suhas Oruganti, Chengzhi Liu, Ching-Chen Kuo, Yuting Zheng, Sravana Narayanaraju, Xin Eric Wang</div>
<div class="meta-line">First: 2026-01-10T19:53:09+00:00 · Latest: 2026-01-13T18:20:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06663v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.06663v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model-based agents are rapidly evolving from simple conversational assistants into autonomous systems capable of performing complex, professional-level tasks in various domains. While these advancements promise significant productivity gains, they also introduce critical safety risks that remain under-explored. Existing safety evaluations primarily focus on simple, daily assistance tasks, failing to capture the intricate decision-making processes and potential consequences of misaligned behaviors in professional settings. To address this gap, we introduce \textbf{SafePro}, a comprehensive benchmark designed to evaluate the safety alignment of AI agents performing professional activities. SafePro features a dataset of high-complexity tasks across diverse professional domains with safety risks, developed through a rigorous iterative creation and review process. Our evaluation of state-of-the-art AI models reveals significant safety vulnerabilities and uncovers new unsafe behaviors in professional contexts. We further show that these models exhibit both insufficient safety judgment and weak safety alignment when executing complex professional tasks. In addition, we investigate safety mitigation strategies for improving agent safety in these scenarios and observe encouraging improvements. Together, our findings highlight the urgent need for robust safety mechanisms tailored to the next generation of professional AI agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SafePro：评估专业级AI代理的安全性</div>
<div class="mono" style="margin-top:8px">基于大型语言模型的代理正在迅速从简单的对话助手演变为能够在多个领域执行复杂、专业级任务的自主系统。尽管这些进展有望带来显著的生产力提升，但它们也引入了尚未充分研究的关键安全风险。现有的安全评估主要集中在简单的日常辅助任务上，未能捕捉到专业环境中复杂决策过程和行为偏差的潜在后果。为了解决这一问题，我们引入了\textbf{SafePro}，这是一个全面的基准，旨在评估执行专业活动的AI代理的安全对齐情况。SafePro包含来自多个专业领域的高复杂度任务数据集，这些数据集通过严格而迭代的创建和审核流程开发而成。我们对最先进的AI模型进行评估，发现其存在显著的安全漏洞，并揭示了专业场景中的新安全隐患。此外，我们还展示了这些模型在执行复杂专业任务时表现出的安全判断不足和安全对齐薄弱。我们进一步研究了提高这些场景下代理安全性的安全缓解策略，并观察到了积极的改进。综上所述，我们的研究结果突显了为下一代专业AI代理量身定制的稳健安全机制的迫切需求。</div>
</details>
</div>
<div class="card">
<div class="title">FastFLUX: Pruning FLUX with Block-wise Replacement and Sandwich Training</div>
<div class="meta-line">Authors: Fuhan Cai, Yong Guo, Jie Li, Wenbo Li, Jian Chen, Xiangzhong Fang</div>
<div class="meta-line">First: 2025-06-10T20:48:30+00:00 · Latest: 2026-01-13T18:20:18+00:00</div>
<div class="meta-line">Comments: 14 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.10035v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.10035v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in text-to-image (T2I) generation have led to the emergence of highly expressive models such as diffusion transformers (DiTs), exemplified by FLUX. However, their massive parameter sizes lead to slow inference, high memory usage, and poor deployability. Existing acceleration methods (e.g., single-step distillation and attention pruning) often suffer from significant performance degradation and incur substantial training costs. To address these limitations, we propose FastFLUX, an architecture-level pruning framework designed to enhance the inference efficiency of FLUX. At its core is the Block-wise Replacement with Linear Layers (BRLL) method, which replaces structurally complex residual branches in ResBlocks with lightweight linear layers while preserving the original shortcut connections for stability. Furthermore, we introduce Sandwich Training (ST), a localized fine-tuning strategy that leverages LoRA to supervise neighboring blocks, mitigating performance drops caused by structural replacement. Experiments show that our FastFLUX maintains high image quality under both qualitative and quantitative evaluations, while significantly improving inference speed, even with 20\% of the hierarchy pruned. Our code will be available soon.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FastFLUX：通过块级替换和三明治训练对FLUX进行剪枝</div>
<div class="mono" style="margin-top:8px">近年来，文本到图像（T2I）生成技术取得了显著进展，涌现出如FLUX这样的高度表达性模型，例如扩散变压器（DiTs）。然而，这些模型的庞大参数量导致推理速度慢、内存占用高以及部署困难。现有的加速方法（如单步蒸馏和注意力剪枝）通常会导致显著的性能下降，并产生较高的训练成本。为了解决这些问题，我们提出了FastFLUX，这是一种架构级的剪枝框架，旨在提升FLUX的推理效率。其核心是块级替换与线性层（BRLL）方法，该方法通过用轻量级线性层替换ResBlock中的结构复杂的残差分支，同时保留原始的快捷连接以确保稳定性。此外，我们引入了三明治训练（ST），这是一种局部微调策略，利用LoRA监督相邻块，从而缓解因结构替换导致的性能下降。实验表明，我们的FastFLUX在定性和定量评估中均能保持高质量的图像生成，同时显著提升推理速度，即使在剪枝了20\%的层级后也是如此。我们的代码将很快发布。</div>
</details>
</div>
<div class="card">
<div class="title">Uncovering Political Bias in Large Language Models using Parliamentary Voting Records</div>
<div class="meta-line">Authors: Jieying Chen, Karen de Jong, Andreas Poole, Jan Burakowski, Elena Elderson Nosti, Joep Windt, Chendi Wang</div>
<div class="meta-line">First: 2026-01-13T18:18:25+00:00 · Latest: 2026-01-13T18:18:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08785v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08785v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) become deeply embedded in digital platforms and decision-making systems, concerns about their political biases have grown. While substantial work has examined social biases such as gender and race, systematic studies of political bias remain limited, despite their direct societal impact. This paper introduces a general methodology for constructing political bias benchmarks by aligning model-generated voting predictions with verified parliamentary voting records. We instantiate this methodology in three national case studies: PoliBiasNL (2,701 Dutch parliamentary motions and votes from 15 political parties), PoliBiasNO (10,584 motions and votes from 9 Norwegian parties), and PoliBiasES (2,480 motions and votes from 10 Spanish parties). Across these benchmarks, we assess ideological tendencies and political entity bias in LLM behavior. As part of our evaluation framework, we also propose a method to visualize the ideology of LLMs and political parties in a shared two-dimensional CHES (Chapel Hill Expert Survey) space by linking their voting-based positions to the CHES dimensions, enabling direct and interpretable comparisons between models and real-world political actors. Our experiments reveal fine-grained ideological distinctions: state-of-the-art LLMs consistently display left-leaning or centrist tendencies, alongside clear negative biases toward right-conservative parties. These findings highlight the value of transparent, cross-national evaluation grounded in real parliamentary behavior for understanding and auditing political bias in modern LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用议会投票记录揭示大型语言模型中的政治偏见</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）在数字平台和决策系统中日益深入应用，对其政治偏见的担忧不断增长。尽管已有大量研究探讨了性别和种族等社会偏见，但系统性研究政治偏见仍较为有限，尽管其对社会有直接影响。本文提出了一种构建政治偏见基准的通用方法，通过将模型生成的投票预测与已验证的议会投票记录进行对齐。我们以三个国家案例研究为例：PoliBiasNL（涵盖15个政党2,701项荷兰议会动议和投票）、PoliBiasNO（涵盖9个挪威政党10,584项动议和投票）和PoliBiasES（涵盖10个西班牙政党2,480项动议和投票）。在这些基准上，我们评估了LLMs的行为中的意识形态倾向和政治实体偏见。作为我们评估框架的一部分，我们还提出了一种方法，通过将基于投票的立场与CHES（夏洛特山专家调查）维度相联系，在共享的二维CHES空间中可视化LLMs和政党的意识形态，从而实现模型与现实政治行为者之间直接且可解释的比较。我们的实验揭示了细致的意识形态差异：最先进的LLMs通常表现出左倾或中间派倾向，同时对右翼保守政党有明显的负面偏见。这些发现突显了基于真实议会行为进行透明、跨国评估的价值，有助于理解和审计现代LLMs中的政治偏见。</div>
</details>
</div>
<div class="card">
<div class="title">On the use of graph models to achieve individual and group fairness</div>
<div class="meta-line">Authors: Arturo Pérez-Peralta, Sandra Benítez-Peña, Rosa E. Lillo</div>
<div class="meta-line">First: 2026-01-13T18:17:43+00:00 · Latest: 2026-01-13T18:17:43+00:00</div>
<div class="meta-line">Comments: 75 pages, 46 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08784v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08784v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine Learning algorithms are ubiquitous in key decision-making contexts such as justice, healthcare and finance, which has spawned a great demand for fairness in these procedures. However, the theoretical properties of such models in relation with fairness are still poorly understood, and the intuition behind the relationship between group and individual fairness is still lacking. In this paper, we provide a theoretical framework based on Sheaf Diffusion to leverage tools based on dynamical systems and homology to model fairness. Concretely, the proposed method projects input data into a bias-free space that encodes fairness constrains, resulting in fair solutions. Furthermore, we present a collection of network topologies handling different fairness metrics, leading to a unified method capable of dealing with both individual and group bias. The resulting models have a layer of interpretability in the form of closed-form expressions for their SHAP values, consolidating their place in the responsible Artificial Intelligence landscape. Finally, these intuitions are tested on a simulation study and standard fairness benchmarks, where the proposed methods achieve satisfactory results. More concretely, the paper showcases the performance of the proposed models in terms of accuracy and fairness, studying available trade-offs on the Pareto frontier, checking the effects of changing the different hyper-parameters, and delving into the interpretation of its outputs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于使用图模型实现个体与群体公平性的研究</div>
<div class="mono" style="margin-top:8px">机器学习算法在司法、医疗和金融等关键决策场景中广泛应用，这催生了对这些流程中公平性的巨大需求。然而，这些模型与公平性相关的理论性质仍不明确，且个体公平与群体公平之间的关系缺乏直观理解。本文基于Sheaf Diffusion提出一个理论框架，利用动力系统和同调理论的工具来建模公平性。具体而言，所提出的方法将输入数据投影到一个无偏空间中，该空间编码了公平性约束，从而产生公平的解决方案。此外，我们还提出了一组处理不同公平性度量的网络拓扑结构，实现了一种统一的方法，能够同时处理个体和群体偏差。所得到的模型具有可解释性，其SHAP值以闭式表达式呈现，巩固了其在负责任人工智能领域中的地位。最后，这些直觉在模拟研究和标准公平性基准测试中进行了验证，所提出的方法取得了令人满意的结果。更具体地说，本文展示了所提出模型在准确性和公平性方面的性能，研究了帕累托前沿上的可用权衡，检查了不同超参数变化的影响，并深入探讨了其输出的解释。</div>
</details>
</div>
<div class="card">
<div class="title">Incentivizing Multi-Tenant Split Federated Learning for Foundation Models at the Network Edge</div>
<div class="meta-line">Authors: Songyuan Li, Jia Hu, Geyong Min, Haojun Huang</div>
<div class="meta-line">First: 2025-03-06T21:06:27+00:00 · Latest: 2026-01-13T18:16:48+00:00</div>
<div class="meta-line">Comments: Accepted for publication in IEEE/ACM Transactions on Networking. Index Terms: Foundation models, Edge computing, Split federated learning, Multi-tenant system, Incentive mechanism</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.04971v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.04971v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Foundation models (FMs) such as GPT-4 exhibit exceptional generative capabilities across diverse downstream tasks through fine-tuning. Split Federated Learning (SFL) facilitates privacy-preserving FM fine-tuning on resource-constrained local devices by offloading partial FM computations to edge servers, enabling device-edge synergistic fine-tuning. Practical edge networks often host multiple SFL tenants to support diversified downstream tasks. However, existing research primarily focuses on single-tenant SFL scenarios, and lacks tailored incentive mechanisms for multi-tenant settings, which are essential to effectively coordinate self-interested local devices for participation in various downstream tasks, ensuring that each SFL tenant&#x27;s distinct FM fine-tuning requirements (e.g., FM types, performance targets, and fine-tuning deadlines) are met. To address this gap, we propose a novel Price-Incentive Mechanism (PRINCE) that guides multiple SFL tenants to offer strategic price incentives, which solicit high-quality device participation for efficient FM fine-tuning. Specifically, we first develop a bias-resilient global SFL model aggregation scheme to eliminate model biases caused by independent device participation. We then derive a rigorous SFL convergence bound to evaluate the contributions of heterogeneous devices to FM performance improvements, guiding the incentive strategies of SFL tenants. Furthermore, we model inter-tenant device competition as a congestion game for Stackelberg equilibrium (SE) analysis, deriving each SFL tenant&#x27;s optimal incentive strategy. Extensive simulations involving four representative SFL tenant types (ViT, BERT, Whisper, and LLaMA) across diverse data modalities (text, images, and audio) demonstrate that PRINCE accelerates FM fine-tuning by up to 3.07x compared to state-of-the-art approaches, while consistently meeting fine-tuning performance targets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过多租户分割联邦学习激励基础模型在网络边缘的优化</div>
<div class="mono" style="margin-top:8px">基础模型（FMs）如GPT-4通过微调在各种下游任务中展现出卓越的生成能力。分割联邦学习（SFL）通过将部分FM计算卸载到边缘服务器，使资源受限的本地设备能够在保护隐私的前提下进行FM微调，实现设备与边缘的协同微调。实际的边缘网络通常托管多个SFL租户以支持多样化的下游任务。然而，现有研究主要关注单租户SFL场景，缺乏针对多租户设置的定制化激励机制，这对于有效协调自利的本地设备参与不同下游任务至关重要，确保每个SFL租户独特的FM微调需求（如FM类型、性能目标和微调截止时间）得到满足。为解决这一问题，我们提出了一种新颖的价格激励机制（PRINCE），引导多个SFL租户提供策略性价格激励，以吸引高质量的设备参与，从而实现高效的FM微调。具体而言，我们首先开发了一种抗偏差的全局SFL模型聚合方案，以消除由独立设备参与导致的模型偏差。随后，我们推导出一个严格的SFL收敛界限，用于评估异构设备对FM性能提升的贡献，指导SFL租户的激励策略。此外，我们将租户间的设备竞争建模为拥塞博弈，以进行斯塔克尔伯格均衡（SE）分析，推导出每个SFL租户的最优激励策略。在涉及四种代表性SFL租户类型（ViT、BERT、Whisper和LLaMA）以及多种数据模态（文本、图像和音频）的广泛模拟中，结果表明PRINCE相比现有最先进的方法，可将FM微调速度提升高达3.07倍，同时持续满足微调性能目标。</div>
</details>
</div>
<div class="card">
<div class="title">Simultaneous bosonic and fermionic T-dualization of the type II superstring theory -- Buscher approach and double space representation</div>
<div class="meta-line">Authors: Bojan Nikolic</div>
<div class="meta-line">First: 2025-10-22T12:41:09+00:00 · Latest: 2026-01-13T18:16:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.19536v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.19536v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this article I consider type II superstring in the pure spinor formulation with constant background fields in the context of T-dualization. First I prove that bosonic and fermionic T-dualization commute using already known T-dual transformation laws for bosonic and fermionic T-dualization. Consequently, the T-dual transformation laws of the full T-dualization are obtained. At the end the full T-dualization is realized in double space and it is showed that Buscher procedure and double space approach are equivalent in this specific case.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>II型超弦理论中玻色子和费米子T对偶化的同时进行——Buscher方法与双空间表示</div>
<div class="mono" style="margin-top:8px">本文在T对偶化背景下，考虑了具有常数背景场的纯旋矢量形式II型超弦。首先，我利用已知的玻色子和费米子T对偶化变换规律证明了它们的对偶化操作是可交换的。因此，得到了完整T对偶化的变换规律。最后，在双空间中实现了完整的T对偶化，并展示了Buscher方法与双空间方法在此特定情况下的等价性。</div>
</details>
</div>
<div class="card">
<div class="title">Fast and explainable clustering in the Manhattan and Tanimoto distance</div>
<div class="meta-line">Authors: Stefan Güttel, Kaustubh Roy</div>
<div class="meta-line">First: 2026-01-13T18:14:03+00:00 · Latest: 2026-01-13T18:14:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08781v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.08781v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The CLASSIX algorithm is a fast and explainable approach to data clustering. In its original form, this algorithm exploits the sorting of the data points by their first principal component to truncate the search for nearby data points, with nearness being defined in terms of the Euclidean distance. Here we extend CLASSIX to other distance metrics, including the Manhattan distance and the Tanimoto distance. Instead of principal components, we use an appropriate norm of the data vectors as the sorting criterion, combined with the triangle inequality for search termination. In the case of Tanimoto distance, a provably sharper intersection inequality is used to further boost the performance of the new algorithm. On a real-world chemical fingerprint benchmark, CLASSIX Tanimoto is about 30 times faster than the Taylor--Butina algorithm, and about 80 times faster than DBSCAN, while computing higher-quality clusters in both cases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>曼哈顿距离和Tanimoto距离下的快速可解释聚类</div>
<div class="mono" style="margin-top:8px">CLASSIX算法是一种快速且可解释的数据聚类方法。在原始版本中，该算法通过数据点按其第一主成分排序来截断搜索附近数据点的过程，其中近邻是基于欧几里得距离定义的。本文将CLASSIX扩展到其他距离度量，包括曼哈顿距离和Tanimoto距离。我们使用数据向量的适当范数作为排序标准，并结合三角不等式来终止搜索。在Tanimoto距离的情况下，使用了一个可证明更精确的交集不等式来进一步提升新算法的性能。在现实世界的化学指纹基准测试中，CLASSIX Tanimoto比Taylor--Butina算法快约30倍，比DBSCAN快约80倍，同时在两种情况下都能计算出更高质量的聚类。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
