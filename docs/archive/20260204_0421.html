<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-04 04:21</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260204_0421</div>
    <div class="row"><div class="card">
<div class="title">Reward-free Alignment for Conflicting Objectives</div>
<div class="meta-line">Authors: Peter Chen, Xiaopeng Li, Xi Chen, Tianyi Lin</div>
<div class="meta-line">First: 2026-02-02T18:59:52+00:00 · Latest: 2026-02-02T18:59:52+00:00</div>
<div class="meta-line">Comments: 27 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02495v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02495v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Direct alignment methods are increasingly used to align large language models (LLMs) with human preferences. However, many real-world alignment problems involve multiple conflicting objectives, where naive aggregation of preferences can lead to unstable training and poor trade-offs. In particular, weighted loss methods may fail to identify update directions that simultaneously improve all objectives, and existing multi-objective approaches often rely on explicit reward models, introducing additional complexity and distorting user-specified preferences. The contributions of this paper are two-fold. First, we propose a Reward-free Alignment framework for Conflicted Objectives (RACO) that directly leverages pairwise preference data and resolves gradient conflicts via a novel clipped variant of conflict-averse gradient descent. We provide convergence guarantees to Pareto-critical points that respect user-specified objective weights, and further show that clipping can strictly improve convergence rate in the two-objective setting. Second, we improve our method using some heuristics and conduct experiments to demonstrate the compatibility of the proposed framework for LLM alignment. Both qualitative and quantitative evaluations on multi-objective summarization and safety alignment tasks across multiple LLM families (Qwen 3, Llama 3, Gemma 3) show that our method consistently achieves better Pareto trade-offs compared to existing multi-objective alignment baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无奖励对齐：针对冲突目标的对齐方法</div>
<div class="mono" style="margin-top:8px">直接对齐方法越来越多地用于将大型语言模型（LLMs）与人类偏好对齐。然而，许多现实世界中的对齐问题涉及多个冲突目标，其中简单地聚合偏好可能导致训练不稳定和较差的权衡。特别是，加权损失方法可能无法识别同时提升所有目标的更新方向，而现有的多目标方法通常依赖于显式的奖励模型，增加了额外的复杂性并扭曲了用户指定的偏好。本文的贡献有两个方面。首先，我们提出了一种针对冲突目标的无奖励对齐框架（RACO），该框架直接利用成对偏好数据，并通过一种新颖的剪切变体冲突规避梯度下降来解决梯度冲突。我们提供了收敛到尊重用户指定目标权重的帕累托临界点的保证，并进一步表明在双目标设置中，剪切可以严格提高收敛速度。其次，我们利用一些启发式方法改进了我们的方法，并进行了实验以展示所提出的框架在LLM对齐中的兼容性。在多个LLM家族（Qwen 3、Llama 3、Gemma 3）的多目标摘要和安全对齐任务上的定性和定量评估表明，我们的方法在帕累托权衡上优于现有的多目标对齐基线。</div>
</details>
</div>
<div class="card">
<div class="title">MEG-XL: Data-Efficient Brain-to-Text via Long-Context Pre-Training</div>
<div class="meta-line">Authors: Dulhan Jayalath, Oiwi Parker Jones</div>
<div class="meta-line">First: 2026-02-02T18:59:50+00:00 · Latest: 2026-02-02T18:59:50+00:00</div>
<div class="meta-line">Comments: 19 pages, 8 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02494v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02494v1">PDF</a> · <a href="https://github.com/neural-processing-lab/MEG-XL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Clinical brain-to-text interfaces are designed for paralysed patients who cannot provide extensive training recordings. Pre-training improves data-efficient generalisation by learning statistical priors across subjects, but these priors critically depend on context. While natural speech might unfold gradually over minutes, most methods pre-train with only a few seconds of context. Thus, we propose MEG-XL, a model pre-trained with 2.5 minutes of MEG context per sample, 5-300x longer than prior work, and equivalent to 191k tokens, capturing extended neural context. Fine-tuning on the task of word decoding from brain data, MEG-XL matches supervised performance with a fraction of the data (e.g. 1hr vs 50hrs) and outperforms brain foundation models. We find that models pre-trained with longer contexts learn representations that transfer better to word decoding. Our results indicate that long-context pre-training helps exploit extended neural context that other methods unnecessarily discard. Code, model weights, and instructions are available at https://github.com/neural-processing-lab/MEG-XL .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MEG-XL：通过长上下文预训练实现高效的数据驱动脑到文本接口</div>
<div class="mono" style="margin-top:8px">临床脑到文本接口旨在为无法提供大量训练记录的瘫痪患者服务。预训练通过在不同受试者间学习统计先验知识，提高数据效率下的泛化能力，但这些先验知识高度依赖上下文。虽然自然语言可能在数分钟内逐步展开，但大多数方法仅使用几秒钟的上下文进行预训练。因此，我们提出了MEG-XL，该模型每个样本使用2.5分钟的MEG上下文进行预训练，比以往方法长5-300倍，相当于191,000个token，能够捕捉更长的神经上下文。在从脑数据解码单词的任务上进行微调，MEG-XL仅需少量数据（例如1小时 vs 50小时）即可达到监督学习的性能，并优于其他脑基础模型。我们发现，使用更长上下文预训练的模型在单词解码任务中具有更好的迁移能力。我们的结果表明，长上下文预训练有助于利用其他方法无意中丢弃的扩展神经上下文。代码、模型权重和使用说明可在https://github.com/neural-processing-lab/MEG-XL获取。</div>
</details>
</div>
<div class="card">
<div class="title">PixelGen: Pixel Diffusion Beats Latent Diffusion with Perceptual Loss</div>
<div class="meta-line">Authors: Zehong Ma, Ruihan Xu, Shiliang Zhang</div>
<div class="meta-line">First: 2026-02-02T18:59:42+00:00 · Latest: 2026-02-02T18:59:42+00:00</div>
<div class="meta-line">Comments: Project Pages: https://zehong-ma.github.io/PixelGen/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02493v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02493v1">PDF</a> · <a href="https://github.com/Zehong-Ma/PixelGen">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://zehong-ma.github.io/PixelGen/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pixel diffusion generates images directly in pixel space in an end-to-end manner, avoiding the artifacts and bottlenecks introduced by VAEs in two-stage latent diffusion. However, it is challenging to optimize high-dimensional pixel manifolds that contain many perceptually irrelevant signals, leaving existing pixel diffusion methods lagging behind latent diffusion models. We propose PixelGen, a simple pixel diffusion framework with perceptual supervision. Instead of modeling the full image manifold, PixelGen introduces two complementary perceptual losses to guide diffusion model towards learning a more meaningful perceptual manifold. An LPIPS loss facilitates learning better local patterns, while a DINO-based perceptual loss strengthens global semantics. With perceptual supervision, PixelGen surpasses strong latent diffusion baselines. It achieves an FID of 5.11 on ImageNet-256 without classifier-free guidance using only 80 training epochs, and demonstrates favorable scaling performance on large-scale text-to-image generation with a GenEval score of 0.79. PixelGen requires no VAEs, no latent representations, and no auxiliary stages, providing a simpler yet more powerful generative paradigm. Codes are publicly available at https://github.com/Zehong-Ma/PixelGen.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PixelGen：使用感知损失超越潜在扩散的像素扩散</div>
<div class="mono" style="margin-top:8px">像素扩散以端到端的方式直接在像素空间中生成图像，避免了两阶段潜在扩散中由VAEs引入的伪影和瓶颈。然而，优化高维像素流形（包含大量感知无关信号）具有挑战性，使得现有的像素扩散方法在性能上落后于潜在扩散模型。我们提出PixelGen，这是一个带有感知监督的简单像素扩散框架。与建模完整图像流形不同，PixelGen引入了两个互补的感知损失，引导扩散模型学习更具意义的感知流形。LPIPS损失有助于学习更好的局部模式，而基于DINO的感知损失则增强了全局语义。借助感知监督，PixelGen超越了强大的潜在扩散基线。在无需分类器自由引导的情况下，仅使用80个训练周期，PixelGen在ImageNet-256上实现了5.11的FID分数，并在大规模文本到图像生成任务中展示了良好的扩展性能，GenEval得分为0.79。PixelGen不需要VAEs、潜在表示或辅助阶段，提供了一种更简单但更强大的生成范式。代码已公开在https://github.com/Zehong-Ma/PixelGen。</div>
</details>
</div>
<div class="card">
<div class="title">Secure Multi-User Linearly-Separable Distributed Computing</div>
<div class="meta-line">Authors: Amir Masoud Jafarpisheh, Ali Khalesi, Petros Elia</div>
<div class="meta-line">First: 2026-02-02T18:59:10+00:00 · Latest: 2026-02-02T18:59:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02489v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02489v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The introduction of the new multi-user linearly-separable distributed computing framework, has recently revealed how a parallel treatment of users can yield large parallelization gains with relatively low computation and communication costs. These gains stem from a new approach that converts the computing problem into a sparse matrix factorization problem; a matrix $F$ that describes the users&#x27; requests, is decomposed as \(F = DE\), where a \(γ\)-sparse \(E\) defines the task allocation across $N$ servers, and a \(δ\)-sparse \(D\) defines the connectivity between \(N\) servers and \(K\) users as well as the decoding process. While this approach provides near-optimal performance, its linear nature has raised data secrecy concerns.
  We here adopt an information-theoretic secrecy framework, seeking guarantees that each user can learn nothing more than its own requested function. In this context, our main result provides two necessary and sufficient secrecy criteria; (i) for each user \(k\) who observes $α_k$ server responses, the common randomness visible to that user must span a subspace of dimension exactly $α_k-1$,
  and (ii) for each user, removing from \(\mathbf{D}\) the columns corresponding to the servers it observes must leave a matrix of rank at least \(K-1\). With these conditions in place, we design a general scheme -- that applies to finite and non-finite fields alike -- which is based on appending to \(\mathbf{E}\) a basis of \(\mathrm{Null}(\mathbf{D})\) and by carefully injecting shared randomness. In many cases, this entails no additional costs. The scheme, while maintaining performance, guarantees perfect information-theoretic secrecy in the case of finite fields, while in the real case, the conditions yield an explicit mutual-information bound that can be made arbitrarily small by increasing the variance of Gaussian common randomness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>安全多用户线性可分分布式计算</div>
<div class="mono" style="margin-top:8px">新的多用户线性可分分布式计算框架的引入，最近揭示了如何通过并行处理用户来实现较大的并行化增益，同时计算和通信成本相对较低。这些增益来源于一种新方法，将计算问题转化为稀疏矩阵分解问题；一个描述用户请求的矩阵 $F$ 被分解为 \(F = DE\)，其中 $γ$-稀疏的 $E$ 定义了任务在 $N$ 个服务器上的分配，而 $δ$-稀疏的 $D$ 定义了 $N$ 个服务器与 $K$ 个用户之间的连接性以及解码过程。尽管这种方法提供了近似最优的性能，但其线性性质引发了数据保密性的担忧。
我们在此采用信息论保密框架，寻求保证每个用户只能了解其自身请求的函数。在此背景下，我们的主要结果提供了两个必要且充分的保密条件；(i) 对于每个观察到 $α_k$ 个服务器响应的用户 $k$，该用户可见的公共随机性必须覆盖一个维度恰好为 $α_k-1$ 的子空间，
(ii) 对于每个用户，从 $\mathbf{D}$ 中移除其观察到的服务器对应的列后，剩余矩阵的秩必须至少为 $K-1$。在满足这些条件的情况下，我们设计了一种通用方案——适用于有限域和无限域——该方案基于在 $\mathbf{E}$ 中附加 $\mathrm{Null}(\mathbf{D})$ 的一个基，并通过精心注入共享随机性。在许多情况下，这种方案不会带来额外的成本。该方案在保持性能的同时，在有限域情况下保证了完美的信息论保密性，而在实数域情况下，这些条件给出了一个显式的互信息界，通过增加高斯公共随机性的方差，可以使其任意小。</div>
</details>
</div>
<div class="card">
<div class="title">RLAnything: Forge Environment, Policy, and Reward Model in Completely Dynamic RL System</div>
<div class="meta-line">Authors: Yinjie Wang, Tianbao Xie, Ke Shen, Mengdi Wang, Ling Yang</div>
<div class="meta-line">First: 2026-02-02T18:59:04+00:00 · Latest: 2026-02-02T18:59:04+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/Gen-Verse/Open-AgentRL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02488v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02488v1">PDF</a> · <a href="https://github.com/Gen-Verse/Open-AgentRL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose RLAnything, a reinforcement learning framework that dynamically forges environment, policy, and reward models through closed-loop optimization, amplifying learning signals and strengthening the overall RL system for any LLM or agentic scenarios. Specifically, the policy is trained with integrated feedback from step-wise and outcome signals, while the reward model is jointly optimized via consistency feedback, which in turn further improves policy training. Moreover, our theory-motivated automatic environment adaptation improves training for both the reward and policy models by leveraging critic feedback from each, enabling learning from experience. Empirically, each added component consistently improves the overall system, and RLAnything yields substantial gains across various representative LLM and agentic tasks, boosting Qwen3-VL-8B-Thinking by 9.1% on OSWorld and Qwen2.5-7B-Instruct by 18.7% and 11.9% on AlfWorld and LiveBench, respectively. We also that optimized reward-model signals outperform outcomes that rely on human labels. Code: https://github.com/Gen-Verse/Open-AgentRL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RLAnything：在完全动态的强化学习系统中构建环境、策略和奖励模型</div>
<div class="mono" style="margin-top:8px">我们提出了RLAnything，这是一个通过闭环优化动态构建环境、策略和奖励模型的强化学习框架，增强学习信号并强化整体的强化学习系统，适用于任何大语言模型或代理场景。具体而言，策略通过逐步信号和结果信号的集成反馈进行训练，而奖励模型则通过一致性反馈联合优化，从而进一步提升策略训练。此外，我们基于理论的自动环境适应机制利用每个模型的批评反馈来提升奖励和策略模型的训练效果，实现从经验中学习。实证结果表明，每个新增的组件都持续提升整体系统性能，RLAnything在各种代表性的大语言模型和代理任务中均取得了显著提升，例如在OSWorld上使Qwen3-VL-8B-Thinking提升9.1%，在AlfWorld和LiveBench上分别使Qwen2.5-7B-Instruct提升18.7%和11.9%。我们还发现，优化后的奖励模型信号优于依赖人类标签的结果信号。代码：https://github.com/Gen-Verse/Open-AgentRL</div>
</details>
</div>
<div class="card">
<div class="title">RE-TRAC: REcursive TRAjectory Compression for Deep Search Agents</div>
<div class="meta-line">Authors: Jialiang Zhu, Gongrui Zhang, Xiaolong Ma, Lin Xu, Miaosen Zhang, Ruiqi Yang, Song Wang, Kai Qiu, Zhirong Wu, Qi Dai, Ruichun Ma, Bei Liu, Yifan Yang, Chong Luo, Zhengyuan Yang, Linjie Li, Lijuan Wang, Weizhu Chen, Xin Geng, Baining Guo</div>
<div class="meta-line">First: 2026-02-02T18:58:07+00:00 · Latest: 2026-02-02T18:58:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02486v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02486v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based deep research agents are largely built on the ReAct framework. This linear design makes it difficult to revisit earlier states, branch into alternative search directions, or maintain global awareness under long contexts, often leading to local optima, redundant exploration, and inefficient search. We propose Re-TRAC, an agentic framework that performs cross-trajectory exploration by generating a structured state representation after each trajectory to summarize evidence, uncertainties, failures, and future plans, and conditioning subsequent trajectories on this state representation. This enables iterative reflection and globally informed planning, reframing research as a progressive process. Empirical results show that Re-TRAC consistently outperforms ReAct by 15-20% on BrowseComp with frontier LLMs. For smaller models, we introduce Re-TRAC-aware supervised fine-tuning, achieving state-of-the-art performance at comparable scales. Notably, Re-TRAC shows a monotonic reduction in tool calls and token usage across rounds, indicating progressively targeted exploration driven by cross-trajectory reflection rather than redundant search.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RE-TRAC：用于深度搜索代理的递归轨迹压缩</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的深度研究代理主要建立在ReAct框架之上。这种线性设计使得回顾早期状态、转向替代搜索方向或在长上下文中保持全局意识变得困难，常常导致局部最优、冗余探索和低效搜索。我们提出了Re-TRAC，这是一种代理框架，通过在每次轨迹后生成结构化的状态表示，以总结证据、不确定性、失败和未来计划，并基于此状态表示进行后续轨迹的条件生成。这使得能够进行迭代反思和全局信息驱动的规划，将研究重新定义为一个渐进的过程。实证结果表明，Re-TRAC在前沿LLM上的BrowseComp任务中，性能比ReAct稳定提升15-20%。对于较小的模型，我们引入了Re-TRAC-aware监督微调，实现了在相似规模下的最先进性能。值得注意的是，Re-TRAC在每轮中工具调用和令牌使用量均呈单调递减趋势，表明其探索过程是通过跨轨迹反思逐步聚焦的，而非冗余搜索。</div>
</details>
</div>
<div class="card">
<div class="title">Expanding the Capabilities of Reinforcement Learning via Text Feedback</div>
<div class="meta-line">Authors: Yuda Song, Lili Chen, Fahim Tajwar, Remi Munos, Deepak Pathak, J. Andrew Bagnell, Aarti Singh, Andrea Zanette</div>
<div class="meta-line">First: 2026-02-02T18:56:56+00:00 · Latest: 2026-02-02T18:56:56+00:00</div>
<div class="meta-line">Comments: 43 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02482v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02482v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of RL for LLM post-training stems from an unreasonably uninformative source: a single bit of information per rollout as binary reward or preference label. At the other extreme, distillation offers dense supervision but requires demonstrations, which are costly and difficult to scale. We study text feedback as an intermediate signal: richer than scalar rewards, yet cheaper than complete demonstrations. Textual feedback is a natural mode of human interaction and is already abundant in many real-world settings, where users, annotators, and automated judges routinely critique LLM outputs. Towards leveraging text feedback at scale, we formalize a multi-turn RL setup, RL from Text Feedback (RLTF), where text feedback is available during training but not at inference. Therefore, models must learn to internalize the feedback in order to improve their test-time single-turn performance. To do this, we propose two methods: Self Distillation (RLTF-SD), which trains the single-turn policy to match its own feedback-conditioned second-turn generations; and Feedback Modeling (RLTF-FM), which predicts the feedback as an auxiliary objective. We provide theoretical analysis on both methods, and empirically evaluate on reasoning puzzles, competition math, and creative writing tasks. Our results show that both methods consistently outperform strong baselines across benchmarks, highlighting the potential of RL with an additional source of rich supervision at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过文本反馈扩展强化学习的能力</div>
<div class="mono" style="margin-top:8px">强化学习在大语言模型微调中的成功源于一个不合理的信息来源：每个rollout仅提供一个二进制奖励或偏好标签的单比特信息。在另一极端，蒸馏方法提供密集的监督，但需要示范数据，而示范数据的获取成本高且难以扩展。我们研究文本反馈作为一种中间信号：比标量奖励更丰富，但比完整示范更便宜。文本反馈是人类交互的自然方式，并且在许多现实场景中已经大量存在，用户、标注者和自动化评估者经常对大语言模型的输出进行评论。为了大规模利用文本反馈，我们形式化了一个多轮强化学习设置，称为RLTF（基于文本反馈的强化学习），其中在训练过程中提供文本反馈，但在推理时不提供。因此，模型必须学会将反馈内化以提升其在测试时的单轮表现。为此，我们提出了两种方法：自我蒸馏（RLTF-SD），它训练单轮策略以匹配其自身的反馈条件下的第二轮生成；以及反馈建模（RLTF-FM），它将反馈预测为一个辅助目标。我们对这两种方法进行了理论分析，并在推理谜题、竞赛数学和创意写作任务上进行了实证评估。我们的结果表明，这两种方法在多个基准测试中均优于强大的基线模型，突显了在大规模场景下引入丰富监督源的强化学习的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Flow Policy Gradients for Robot Control</div>
<div class="meta-line">Authors: Brent Yi, Hongsuk Choi, Himanshu Gaurav Singh, Xiaoyu Huang, Takara E. Truong, Carmelo Sferrazza, Yi Ma, Rocky Duan, Pieter Abbeel, Guanya Shi, Karen Liu, Angjoo Kanazawa</div>
<div class="meta-line">First: 2026-02-02T18:56:49+00:00 · Latest: 2026-02-02T18:56:49+00:00</div>
<div class="meta-line">Comments: Project webpage: https://hongsukchoi.github.io/fpo-control</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02481v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02481v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hongsukchoi.github.io/fpo-control">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Likelihood-based policy gradient methods are the dominant approach for training robot control policies from rewards. These methods rely on differentiable action likelihoods, which constrain policy outputs to simple distributions like Gaussians. In this work, we show how flow matching policy gradients -- a recent framework that bypasses likelihood computation -- can be made effective for training and fine-tuning more expressive policies in challenging robot control settings. We introduce an improved objective that enables success in legged locomotion, humanoid motion tracking, and manipulation tasks, as well as robust sim-to-real transfer on two humanoid robots. We then present ablations and analysis on training dynamics. Results show how policies can exploit the flow representation for exploration when training from scratch, as well as improved fine-tuning robustness over baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于流匹配策略梯度的机器人控制</div>
<div class="mono" style="margin-top:8px">基于似然的策略梯度方法是训练机器人控制策略的主要方法，这些方法依赖可微分的动作似然，将策略输出限制在简单的分布如高斯分布中。在本工作中，我们展示了如何使基于流匹配的策略梯度方法——一种绕过似然计算的最新框架——在具有挑战性的机器人控制环境中有效用于训练和微调更具表达力的策略。我们引入了一个改进的目标函数，使得在腿部运动、人形机器人运动跟踪和操作任务中取得成功，并在两个人形机器人上实现了稳健的仿真到现实的迁移。随后，我们对训练动态进行了消融实验和分析。结果表明，策略在从头训练时可以利用流表示进行探索，并且在微调方面比基线方法更具鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">AgentRx: Diagnosing AI Agent Failures from Execution Trajectories</div>
<div class="meta-line">Authors: Shraddha Barke, Arnav Goyal, Alind Khare, Avaljot Singh, Suman Nath, Chetan Bansal</div>
<div class="meta-line">First: 2026-02-02T18:54:07+00:00 · Latest: 2026-02-02T18:54:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02475v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02475v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI agents often fail in ways that are difficult to localize because executions are probabilistic, long-horizon, multi-agent, and mediated by noisy tool outputs. We address this gap by manually annotating failed agent runs and release a novel benchmark of 115 failed trajectories spanning structured API workflows, incident management, and open-ended web/file tasks. Each trajectory is annotated with a critical failure step and a category from a grounded-theory derived, cross-domain failure taxonomy. To mitigate the human cost of failure attribution, we present AGENTRX, an automated domain-agnostic diagnostic framework that pinpoints the critical failure step in a failed agent trajectory. It synthesizes constraints, evaluates them step-by-step, and produces an auditable validation log of constraint violations with associated evidence; an LLM-based judge uses this log to localize the critical step and category. Our framework improves step localization and failure attribution over existing baselines across three domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AgentRx：从执行轨迹诊断AI代理故障</div>
<div class="mono" style="margin-top:8px">AI代理常常以难以定位的方式失败，因为其执行过程具有概率性、长周期性、多代理性和由噪声工具输出中介的特性。我们通过手动标注失败的代理运行来解决这一问题，并发布了一个包含115条失败轨迹的新颖基准，涵盖结构化API工作流、事件管理以及开放式的网页/文件任务。每条轨迹均标注了关键失败步骤和一个基于扎根理论的跨领域失败分类中的类别。为降低故障归因的人力成本，我们提出了AGENTRX，一个自动化且领域无关的诊断框架，用于识别失败代理轨迹中的关键失败步骤。该框架综合约束条件，逐步评估这些约束，并生成可审计的约束违反验证日志及其相关证据；基于大语言模型的裁判使用该日志来定位关键步骤和类别。我们的框架在三个领域中均优于现有基线，提升了步骤定位和故障归因的准确性。</div>
</details>
</div>
<div class="card">
<div class="title">MemSkill: Learning and Evolving Memory Skills for Self-Evolving Agents</div>
<div class="meta-line">Authors: Haozhen Zhang, Quanyu Long, Jianzhu Bao, Tao Feng, Weizhi Zhang, Haodong Yue, Wenya Wang</div>
<div class="meta-line">First: 2026-02-02T18:53:28+00:00 · Latest: 2026-02-02T18:53:28+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/ViktorAxelsen/MemSkill</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02474v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02474v1">PDF</a> · <a href="https://github.com/ViktorAxelsen/MemSkill">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory. These fixed procedures hard-code human priors about what to store and how to revise memory, making them rigid under diverse interaction patterns and inefficient on long histories. To this end, we present \textbf{MemSkill}, which reframes these operations as learnable and evolvable memory skills, structured and reusable routines for extracting, consolidating, and pruning information from interaction traces. Inspired by the design philosophy of agent skills, MemSkill employs a \emph{controller} that learns to select a small set of relevant skills, paired with an LLM-based \emph{executor} that produces skill-guided memories. Beyond learning skill selection, MemSkill introduces a \emph{designer} that periodically reviews hard cases where selected skills yield incorrect or incomplete memories, and evolves the skill set by proposing refinements and new skills. Together, MemSkill forms a closed-loop procedure that improves both the skill-selection policy and the skill set itself. Experiments on LoCoMo, LongMemEval, HotpotQA, and ALFWorld demonstrate that MemSkill improves task performance over strong baselines and generalizes well across settings. Further analyses shed light on how skills evolve, offering insights toward more adaptive, self-evolving memory management for LLM agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MemSkill: 为自我演进智能体学习和演进记忆技能</div>
<div class="mono" style="margin-top:8px">大多数大型语言模型（LLM）代理的记忆系统依赖于少量静态、手工设计的操作来提取记忆。这些固定程序硬编码了人类关于存储什么以及如何修订记忆的先验知识，导致其在多样化的交互模式下表现僵化，并在长交互历史中效率低下。为此，我们提出了\textbf{MemSkill}，将这些操作重新定义为可学习和可演进的记忆技能，即用于从交互轨迹中提取、巩固和修剪信息的结构化且可复用的程序。受代理技能设计哲学的启发，MemSkill采用一个\emph{控制器}，学习选择一组相关技能，并搭配基于LLM的\emph{执行器}，生成技能引导的记忆。除了学习技能选择，MemSkill还引入了一个\emph{设计者}，定期审查所选技能在处理困难案例时产生错误或不完整记忆的情况，并通过提出改进和新技能来演进技能集。综上，MemSkill形成一个闭环过程，同时提升技能选择策略和技能集本身。在LoCoMo、LongMemEval、HotpotQA和ALFWorld上的实验表明，MemSkill在强基线之上提升了任务表现，并且在不同场景下具有良好的泛化能力。进一步的分析揭示了技能如何演进，为LLM代理实现更适应、自我演进的记忆管理提供了见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Most Large Language Model (LLM) agent memory systems rely on a small set of static, hand-designed operations for extracting memory.</div>
</details>
</div>
<div class="card">
<div class="title">HumanX: Toward Agile and Generalizable Humanoid Interaction Skills from Human Videos</div>
<div class="meta-line">Authors: Yinhuai Wang, Qihan Zhao, Yuen Fui Lau, Runyi Yu, Hok Wai Tsui, Qifeng Chen, Jingbo Wang, Jiangmiao Pang, Ping Tan</div>
<div class="meta-line">First: 2026-02-02T18:53:01+00:00 · Latest: 2026-02-02T18:53:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02473v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02473v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Enabling humanoid robots to perform agile and adaptive interactive tasks has long been a core challenge in robotics. Current approaches are bottlenecked by either the scarcity of realistic interaction data or the need for meticulous, task-specific reward engineering, which limits their scalability. To narrow this gap, we present HumanX, a full-stack framework that compiles human video into generalizable, real-world interaction skills for humanoids, without task-specific rewards. HumanX integrates two co-designed components: XGen, a data generation pipeline that synthesizes diverse and physically plausible robot interaction data from video while supporting scalable data augmentation; and XMimic, a unified imitation learning framework that learns generalizable interaction skills. Evaluated across five distinct domains--basketball, football, badminton, cargo pickup, and reactive fighting--HumanX successfully acquires 10 different skills and transfers them zero-shot to a physical Unitree G1 humanoid. The learned capabilities include complex maneuvers such as pump-fake turnaround fadeaway jumpshots without any external perception, as well as interactive tasks like sustained human-robot passing sequences over 10 consecutive cycles--learned from a single video demonstration. Our experiments show that HumanX achieves over 8 times higher generalization success than prior methods, demonstrating a scalable and task-agnostic pathway for learning versatile, real-world robot interactive skills.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HumanX：从人类视频中学习敏捷且可泛化的类人交互技能</div>
<div class="mono" style="margin-top:8px">使类人机器人能够执行敏捷且适应性的交互任务一直是机器人学中的核心挑战。当前方法受限于真实交互数据的稀缺或需要细致的任务特定奖励工程，这限制了其可扩展性。为缩小这一差距，我们提出了HumanX，一个全栈框架，能够将人类视频编译为可泛化、适用于现实场景的交互技能，无需任务特定奖励。HumanX集成了两个协同设计的组件：XGen，一个数据生成管道，能够从视频中合成多样且物理上合理的机器人交互数据，并支持可扩展的数据增强；以及XMimic，一个统一的模仿学习框架，用于学习可泛化的交互技能。我们在五个不同的领域——篮球、足球、羽毛球、货物拾取和反应式格斗中对HumanX进行了评估，成功获取了10种不同的技能，并将其零样本迁移至物理类人机器人Unitree G1。所学习的能力包括无需外部感知的复杂动作，如假动作转身跳投，以及从单个视频演示中学习的持续人机传球序列，连续执行超过10个周期。我们的实验表明，HumanX的泛化成功率比之前的方法高出8倍以上，展示了学习多样化、现实场景机器人交互技能的可扩展且任务无关的路径。</div>
</details>
</div>
<div class="card">
<div class="title">SPARKLING: Balancing Signal Preservation and Symmetry Breaking for Width-Progressive Learning</div>
<div class="meta-line">Authors: Qifan Yu, Xinyu Ma, Zhijian Zhuo, Minrui Wang, Deyi Liu, Shiyi Zhan, Yiyuan Ma, Liang Xiang, Xingyan Bin, Di He</div>
<div class="meta-line">First: 2026-02-02T18:52:52+00:00 · Latest: 2026-02-02T18:52:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02472v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02472v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Progressive Learning (PL) reduces pre-training computational overhead by gradually increasing model scale. While prior work has extensively explored depth expansion, width expansion remains significantly understudied, with the few existing methods limited to the early stages of training. However, expanding width during the mid-stage is essential for maximizing computational savings, yet it remains a formidable challenge due to severe training instabilities. Empirically, we show that naive initialization at this stage disrupts activation statistics, triggering loss spikes, while copy-based initialization introduces gradient symmetry that hinders feature diversity. To address these issues, we propose SPARKLING (balancing {S}ignal {P}reservation {A}nd symmet{R}y brea{K}ing for width-progressive {L}earn{ING}), a novel framework for mid-stage width expansion. Our method achieves signal preservation via RMS-scale consistency, stabilizing activation statistics during expansion. Symmetry breaking is ensured through asymmetric optimizer state resetting and learning rate re-warmup. Extensive experiments on Mixture-of-Experts (MoE) models demonstrate that, across multiple width axes and optimizer families, SPARKLING consistently outperforms training from scratch and reduces training cost by up to 35% under $2\times$ width expansion.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPARKLING：在保持信号与打破对称性之间平衡的宽度渐进学习</div>
<div class="mono" style="margin-top:8px">渐进学习（PL）通过逐步增加模型规模来减少预训练的计算开销。尽管已有大量研究探索深度扩展，但宽度扩展的研究仍显著不足，现有方法大多局限于训练初期。然而，在中期阶段进行宽度扩展对于最大化计算节省至关重要，但由于严重的训练不稳定性，这仍然是一个重大挑战。我们实证表明，在这一阶段使用简单的初始化会破坏激活统计特性，导致损失激增；而基于复制的初始化则引入梯度对称性，阻碍特征多样性。为了解决这些问题，我们提出了SPARKLING（用于宽度渐进学习的信号保持与对称性打破平衡），一种新型的中期宽度扩展框架。我们的方法通过RMS-scale一致性实现信号保持，在扩展过程中稳定激活统计特性。通过非对称优化器状态重置和学习率重新预热确保对称性打破。在专家混合（MoE）模型上的大量实验表明，SPARKLING在多个宽度轴和优化器家族中均优于从头训练，并在2倍宽度扩展下将训练成本降低了高达35%。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-head automated segmentation by incorporating detection head into the contextual layer neural network</div>
<div class="meta-line">Authors: Edwin Kys, Febian Febian</div>
<div class="meta-line">First: 2026-02-02T18:51:25+00:00 · Latest: 2026-02-02T18:51:25+00:00</div>
<div class="meta-line">Comments: 8 pages, 3 figures, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02471v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02471v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep learning based auto segmentation is increasingly used in radiotherapy, but conventional models often produce anatomically implausible false positives, or hallucinations, in slices lacking target structures. We propose a gated multi-head Transformer architecture based on Swin U-Net, augmented with inter-slice context integration and a parallel detection head, which jointly performs slice-level structure detection via a multi-layer perceptron and pixel-level segmentation through a context-enhanced stream. Detection outputs gate the segmentation predictions to suppress false positives in anatomically invalid slices, and training uses slice-wise Tversky loss to address class imbalance. Experiments on the Prostate-Anatomical-Edge-Cases dataset from The Cancer Imaging Archive demonstrate that the gated model substantially outperforms a non-gated segmentation-only baseline, achieving a mean Dice loss of $0.013 \pm 0.036$ versus $0.732 \pm 0.314$, with detection probabilities strongly correlated with anatomical presence, effectively eliminating spurious segmentations. In contrast, the non-gated model exhibited higher variability and persistent false positives across all slices. These results indicate that detection-based gating enhances robustness and anatomical plausibility in automated segmentation applications, reducing hallucinated predictions without compromising segmentation quality in valid slices, and offers a promising approach for improving the reliability of clinical radiotherapy auto-contouring workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过将检测头整合到上下文层神经网络中实现多头自动分割</div>
<div class="mono" style="margin-top:8px">基于深度学习的自动分割技术在放射治疗中应用日益广泛，但传统模型在缺乏目标结构的切片中常产生解剖学上不合理的假阳性结果或幻觉。我们提出了一种基于Swin U-Net的门控多头Transformer架构，增加了跨切片上下文整合和并行检测头，通过多层感知机联合执行切片级结构检测，通过上下文增强流进行像素级分割。检测输出用于门控分割预测，以抑制解剖学无效切片中的假阳性结果，训练过程中采用切片级Tversky损失来解决类别不平衡问题。在The Cancer Imaging Archive提供的前列腺解剖边缘案例数据集上的实验表明，门控模型显著优于非门控的仅分割基线，其平均Dice损失为0.013 ± 0.036，而非门控模型为0.732 ± 0.314。检测概率与解剖结构的存在高度相关，有效消除了虚假分割。相比之下，非门控模型在所有切片中表现出更高的变异性和持续的假阳性结果。这些结果表明，基于检测的门控机制在自动分割应用中提高了鲁棒性和解剖学合理性，减少了幻觉预测，而不会影响有效切片的分割质量，为提高临床放射治疗自动勾画工作流程的可靠性提供了一种有前景的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Breaking the Reversal Curse in Autoregressive Language Models via Identity Bridge</div>
<div class="meta-line">Authors: Xutao Ma, Yixiao Huang, Hanlin Zhu, Somayeh Sojoudi</div>
<div class="meta-line">First: 2026-02-02T18:50:57+00:00 · Latest: 2026-02-02T18:50:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02470v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02470v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the &quot;reversal curse&quot; -- when trained on forward knowledge data of the form &quot;$A \rightarrow B$&quot; (e.g., Alice&#x27;s husband is Bob), the model is unable to deduce the reversal knowledge &quot;$B \leftarrow A$&quot; (e.g., Bob&#x27;s wife is Alice) during test. Extensive prior research suggests that this failure is an inherent, fundamental limit of autoregressive causal LLMs, indicating that these models tend to memorize factual-level knowledge rather than capture higher-level rules. In this paper, we challenge this view by showing that this seemingly fundamental limit can be mitigated by slightly tweaking the training data with a simple regularization data recipe called the Identity Bridge of the form &quot;$A \to A$&quot; (e.g., The name of Alice is Alice). Theoretically, we prove that under this recipe, even a one-layer transformer can break the reversal curse by analyzing the implicit bias of gradient descent. Empirically, we show that a 1B pretrained language model finetuned with the proposed data recipe achieves a 40% success rate on reversal tasks, in stark contrast to a near-zero success rate when trained solely on forward-knowledge data. Our work provides a novel theoretical foundation for the reversal curse and offers a principled, low-cost path to encouraging LLMs to learn higher-level rules from data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过身份桥打破自回归语言模型中的逆转诅咒</div>
<div class="mono" style="margin-top:8px">自回归大语言模型（LLMs）在许多复杂任务中取得了显著成功，但在简单的逻辑推理任务中，如&quot;逆转诅咒&quot;，仍可能失败。例如，当模型在形如&quot;$A \rightarrow B$&quot;（如Alice的丈夫是Bob）的前向知识数据上训练时，无法在测试中推断出逆转知识&quot;$B \leftarrow A$&quot;（如Bob的妻子是Alice）。大量先前研究认为，这种失败是自回归因果LLMs的固有且根本性的限制，表明这些模型倾向于记忆事实性知识而非捕捉更高层次的规则。在本文中，我们通过引入一种简单的正则化数据配方——身份桥（形如&quot;$A \to A$&quot;，如Alice的名字是Alice）来调整训练数据，从而挑战这一观点。理论上，我们证明了在该配方下，即使仅使用单层Transformer，也能通过分析梯度下降的隐式偏差来打破逆转诅咒。实验上，我们展示了使用该数据配方微调的10亿参数预训练语言模型在逆转任务上的成功率达到40%，与仅使用前向知识数据训练的模型接近零的成功率形成鲜明对比。我们的工作为逆转诅咒提供了新的理论基础，并为鼓励LLMs从数据中学习更高层次规则提供了一种原理性且低成本的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Autoregressive large language models (LLMs) have achieved remarkable success in many complex tasks, yet they can still fail in very simple logical reasoning such as the &quot;reversal curse&quot; -- when trained on forward knowledge data of the form &quot;$A \rightarrow B$&quot; (e.g., Alice&#x27;s husband is Bob), the model is unable to deduce the reversal knowledge &quot;$B \leftarrow A$&quot; (e.g., Bob&#x27;s wife is Alice) during test.</div>
</details>
</div>
<div class="card">
<div class="title">Age-Aware Edge-Blind Federated Learning via Over-the-Air Aggregation</div>
<div class="meta-line">Authors: Ahmed M. Elshazly, Ahmed Arafa</div>
<div class="meta-line">First: 2026-02-02T18:50:51+00:00 · Latest: 2026-02-02T18:50:51+00:00</div>
<div class="meta-line">Comments: To appear in IEEE ICC 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02469v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02469v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study federated learning (FL) over wireless fading channels where multiple devices simultaneously send their model updates. We propose an efficient \emph{age-aware edge-blind over-the-air FL} approach that does not require channel state information (CSI) at the devices. Instead, the parameter server (PS) uses multiple antennas and applies maximum-ratio combining (MRC) based on its estimated sum of the channel gains to detect the parameter updates. A key challenge is that the number of orthogonal subcarriers is limited; thus, transmitting many parameters requires multiple Orthogonal Frequency Division Multiplexing (OFDM) symbols, which increases latency. To address this, the PS selects only a small subset of model coordinates each round using \emph{AgeTop-\(k\)}, which first picks the largest-magnitude entries and then chooses the \(k\) coordinates with the longest waiting times since they were last selected. This ensures that all selected parameters fit into a single OFDM symbol, reducing latency. We provide a convergence bound that highlights the advantages of using a higher number of antenna array elements and demonstrates a key trade-off: increasing \(k\) decreases compression error at the cost of increasing the effect of channel noise. Experimental results show that (i) more PS antennas greatly improve accuracy and convergence speed; (ii) AgeTop-\(k\) outperforms random selection under relatively good channel conditions; and (iii) the optimum \(k\) depends on the channel, with smaller \(k\) being better in noisy settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于空中聚合的年龄感知边缘盲联邦学习</div>
<div class="mono" style="margin-top:8px">我们研究了在无线衰落信道上进行的联邦学习（FL），其中多个设备同时发送其模型更新。我们提出了一种高效的\emph{年龄感知边缘盲空中联邦学习}方法，该方法不需要在设备上获取信道状态信息（CSI）。相反，参数服务器（PS）使用多天线，并基于其估计的信道增益总和应用最大比合并（MRC）来检测参数更新。一个关键挑战是正交子载波的数量有限，因此传输大量参数需要多个正交频分复用（OFDM）符号，从而增加延迟。为了解决这一问题，PS 每轮仅选择一小部分模型坐标，使用\emph{AgeTop-\$k\$}方法，该方法首先选择最大幅值的条目，然后选择自上次被选中以来等待时间最长的\$k\$个坐标。这确保了所有选定的参数可以放入一个OFDM符号中，从而减少延迟。我们提供了一个收敛性界限，突出了使用更多天线阵列元素的优势，并展示了关键权衡：增加\$k\$会降低压缩误差，但会增加信道噪声的影响。实验结果表明：(i) 更多的PS天线显著提高了准确性和收敛速度；(ii) 在信道条件相对较好的情况下，AgeTop-\$k\$优于随机选择；(iii) 最优的\$k\$取决于信道，在噪声较大的情况下较小的\$k\$更优。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We study federated learning (FL) over wireless fading channels where multiple devices simultaneously send their model updates.</div>
</details>
</div>
<div class="card">
<div class="title">Helios 2.0: A Robust, Ultra-Low Power Gesture Recognition System Optimised for Event-Sensor based Wearables</div>
<div class="meta-line">Authors: Prarthana Bhattacharyya, Joshua Mitton, Ryan Page, Owen Morgan, Oliver Powell, Benjamin Menzies, Gabriel Homewood, Kemi Jacobs, Paolo Baesso, Taru Muhonen, Richard Vigars, Louis Berridge</div>
<div class="meta-line">First: 2025-03-10T20:12:06+00:00 · Latest: 2026-02-02T18:50:47+00:00</div>
<div class="meta-line">Comments: 24 pages, 14 figures. Prarthana Bhattacharyya, Joshua Mitton, Ryan Page, Owen Morgan, and Oliver Powell contributed equally to this paper</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.07825v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.07825v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present an advance in wearable technology: a mobile-optimized, real-time, ultra-low-power event camera system that enables natural hand gesture control for smart glasses, dramatically improving user experience. While hand gesture recognition in computer vision has advanced significantly, critical challenges remain in creating systems that are intuitive, adaptable across diverse users and environments, and energy-efficient enough for practical wearable applications. Our approach tackles these challenges through carefully selected microgestures: lateral thumb swipes across the index finger (in both directions) and a double pinch between thumb and index fingertips. These human-centered interactions leverage natural hand movements, ensuring intuitive usability without requiring users to learn complex command sequences. To overcome variability in users and environments, we developed a novel simulation methodology that enables comprehensive domain sampling without extensive real-world data collection. Our power-optimised architecture maintains exceptional performance, achieving F1 scores above 80\% on benchmark datasets featuring diverse users and environments. The resulting models operate at just 6-8 mW when exploiting the Qualcomm Snapdragon Hexagon DSP, with our 2-channel implementation exceeding 70\% F1 accuracy and our 6-channel model surpassing 80\% F1 accuracy across all gesture classes in user studies. These results were achieved using only synthetic training data. This improves on the state-of-the-art for F1 accuracy by 20\% with a power reduction 25x when using DSP. This advancement brings deploying ultra-low-power vision systems in wearable devices closer and opens new possibilities for seamless human-computer interaction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Helios 2.0：一种面向事件传感器可穿戴设备的鲁棒、超低功耗手势识别系统</div>
<div class="mono" style="margin-top:8px">我们提出了可穿戴技术的一项进展：一种针对移动设备优化的实时、超低功耗事件相机系统，使智能眼镜能够实现自然的手势控制，显著提升用户体验。尽管计算机视觉中的手势识别技术取得了显著进步，但在创建直观、适用于多样用户和环境且足够节能的系统方面仍面临关键挑战。我们的方法通过精心选择的微手势来解决这些问题，包括在食指上横向滑动拇指（双向）以及拇指与食指指尖之间的双捏动作。这些以用户为中心的交互方式利用自然的手部动作，确保直观易用，无需用户学习复杂的命令序列。为克服用户和环境的差异性，我们开发了一种新颖的模拟方法，能够在不进行大量真实世界数据收集的情况下实现全面的领域采样。我们的功耗优化架构保持了卓越的性能，在包含多样用户和环境的基准数据集上实现了超过80\%的F1分数。所得到的模型在利用高通Snapdragon Hexagon DSP时仅消耗6-8 mW的功率，其中我们的双通道实现超过了70\%的F1准确率，六通道模型在用户研究中所有手势类别上均超过了80\%的F1准确率。这些结果仅使用合成训练数据实现。与使用DSP的现有最佳方法相比，F1准确率提高了20\%，功耗降低了25倍。这一进展使在可穿戴设备中部署超低功耗视觉系统更加可行，并为无缝的人机交互开辟了新的可能性。</div>
</details>
</div>
<div class="card">
<div class="title">Avenir-Web: Human-Experience-Imitating Multimodal Web Agents with Mixture of Grounding Experts</div>
<div class="meta-line">Authors: Aiden Yiliu Li, Xinyue Hao, Shilong Liu, Mengdi Wang</div>
<div class="meta-line">First: 2026-02-02T18:50:07+00:00 · Latest: 2026-02-02T18:50:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02468v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02468v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite advances in multimodal large language models, autonomous web agents still struggle to reliably execute long-horizon tasks on complex and dynamic web interfaces. Existing agents often suffer from inaccurate element grounding, the absence of site-specific procedural knowledge, and unstable long-term task tracking and memory, particularly when operating over complex Document Object Model structures. To address these limitations, we introduce Avenir-Web, a web agent that achieves a new open-source state of the art on the Online-Mind2Web benchmark in real-world deployment. Avenir-Web leverages a Mixture of Grounding Experts, Experience-Imitation Planning for incorporating procedural priors, and a task-tracking checklist combined with adaptive memory to enable robust and seamless interaction across diverse user interface paradigms. We evaluate Avenir-Web on Online-Mind2Web, a rigorous benchmark of live and user-centered web tasks. Our results demonstrate that Avenir-Web significantly surpasses prior open-source agents and attains performance parity with top-tier proprietary models, thereby establishing a new open-source state of the art for reliable web agents on live websites.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Avenir-Web：模仿人类体验的多模态网络代理，结合接地专家混合模型</div>
<div class="mono" style="margin-top:8px">尽管多模态大语言模型取得了进展，自主网络代理在复杂且动态的网络界面执行长期任务时仍面临挑战。现有代理常因元素接地不准确、缺乏特定网站的程序性知识以及长期任务跟踪和记忆不稳定而表现不佳，尤其是在处理复杂的文档对象模型结构时。为了解决这些限制，我们引入了Avenir-Web，这是一种在真实部署中达到Online-Mind2Web基准新开源最优性能的网络代理。Avenir-Web结合了接地专家混合模型、经验模仿规划以整合程序性先验知识，并采用任务跟踪清单与自适应记忆机制，从而实现跨多样化用户界面范式的稳健和无缝交互。我们在Online-Mind2Web基准上评估了Avenir-Web，该基准严格测试实时且以用户为中心的网络任务。实验结果表明，Avenir-Web显著超越了先前的开源代理，并在性能上与顶级专有模型相当，从而在实时网站上建立了新的开源最优标准。</div>
</details>
</div>
<div class="card">
<div class="title">MentisOculi: Revealing the Limits of Reasoning with Mental Imagery</div>
<div class="meta-line">Authors: Jana Zeller, Thaddäus Wiedemer, Fanfei Li, Thomas Klein, Prasanna Mayilvahanan, Matthias Bethge, Felix Wichmann, Ryan Cotterell, Wieland Brendel</div>
<div class="meta-line">First: 2026-02-02T18:49:06+00:00 · Latest: 2026-02-02T18:49:06+00:00</div>
<div class="meta-line">Comments: 9 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02465v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02465v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Frontier models are transitioning from multimodal large language models (MLLMs) that merely ingest visual information to unified multimodal models (UMMs) capable of native interleaved generation. This shift has sparked interest in using intermediate visualizations as a reasoning aid, akin to human mental imagery. Central to this idea is the ability to form, maintain, and manipulate visual representations in a goal-oriented manner. To evaluate and probe this capability, we develop MentisOculi, a procedural, stratified suite of multi-step reasoning problems amenable to visual solution, tuned to challenge frontier models. Evaluating visual strategies ranging from latent tokens to explicit generated imagery, we find they generally fail to improve performance. Analysis of UMMs specifically exposes a critical limitation: While they possess the textual reasoning capacity to solve a task and can sometimes generate correct visuals, they suffer from compounding generation errors and fail to leverage even ground-truth visualizations. Our findings suggest that despite their inherent appeal, visual thoughts do not yet benefit model reasoning. MentisOculi establishes the necessary foundation to analyze and close this gap across diverse model families.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MentisOculi：通过心理意象揭示推理的局限性</div>
<div class="mono" style="margin-top:8px">前沿模型正从仅摄入视觉信息的多模态大语言模型（MLLMs）转向能够原生交错生成的统一多模态模型（UMMs）。这种转变激发了人们使用中间可视化作为推理辅助工具的兴趣，类似于人类的心理意象。这一理念的核心在于以目标为导向地形成、维持和操作视觉表征。为了评估和探索这一能力，我们开发了MentisOculi，这是一个程序化、分层的多步骤推理问题集，适合通过视觉方式求解，并针对挑战前沿模型进行了优化。我们评估了从潜在标记到显式生成图像的多种视觉策略，发现它们通常无法提升模型性能。对UMMs的分析特别揭示了一个关键限制：尽管它们具备通过文本推理解决任务的能力，有时还能生成正确的图像，但它们会受到累积生成错误的影响，并且无法有效利用甚至真实视觉化信息。我们的研究结果表明，尽管视觉思维具有内在吸引力，但目前尚未对模型推理产生实际帮助。MentisOculi为分析和弥合不同模型家族之间的这一差距建立了必要的基础。</div>
</details>
</div>
<div class="card">
<div class="title">Abstract Activation Spaces for Content-Invariant Reasoning in Large Language Models</div>
<div class="meta-line">Authors: Gabriele Maraia, Marco Valentino, Fabio Massimo Zanzotto, Leonardo Ranaldi</div>
<div class="meta-line">First: 2026-02-02T18:48:44+00:00 · Latest: 2026-02-02T18:48:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02462v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02462v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) often struggle with deductive judgment in syllogistic reasoning, systematically conflating semantic plausibility with formal validity a phenomenon known as content effect. This bias persists even when models generate step-wise explanations, indicating that intermediate rationales may inherit the same semantic shortcuts that affect answers. Recent approaches propose mitigating this issue by increasing inference-time structural constraints, either by encouraging abstract intermediate representations or by intervening directly in the model&#x27;s internal computations; however, reliably suppressing semantic interference remains an open challenge. To make formal deduction less sensitive to semantic content, we introduce a framework for abstraction-guided reasoning that explicitly separates structural inference from lexical semantics. We construct paired content-laden and abstract syllogisms and use the model&#x27;s activations on abstract inputs to define an abstract reasoning space. We then learn lightweight Abstractors that, from content-conditioned residual-stream states, predict representations aligned with this space and integrate these predictions via multi-layer interventions during the forward pass. Using cross-lingual transfer as a test bed, we show that abstraction-aligned steering reduces content-driven errors and improves validity-sensitive performance. Our results position activation-level abstraction as a scalable mechanism for enhancing the robustness of formal reasoning in LLMs against semantic interference.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大型语言模型中用于内容不变推理的抽象激活空间</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在三段论推理中的演绎判断上常常遇到困难，系统性地将语义合理性与形式有效性混淆，这种现象称为内容效应。即使模型生成分步解释，这种偏差依然存在，表明中间推理可能继承了影响答案的相同语义捷径。最近的方法提出通过增加推理时的结构约束来缓解这一问题，例如鼓励抽象的中间表示或直接干预模型的内部计算；然而，可靠地抑制语义干扰仍然是一个开放性挑战。为了使形式演绎对语义内容不那么敏感，我们引入了一个基于抽象引导推理的框架，显式地将结构推理与词汇语义分离。我们构建了包含内容丰富和抽象三段论的配对数据集，并利用模型在抽象输入上的激活来定义一个抽象推理空间。随后，我们学习轻量级的Abstractors，从内容条件的残差流状态中预测与该空间对齐的表示，并在前向传递过程中通过多层干预整合这些预测。通过跨语言迁移作为测试平台，我们展示了与抽象对齐的引导如何减少内容驱动的错误并提升形式有效性敏感的性能。我们的结果表明，激活层面的抽象化是一种可扩展的机制，能够增强LLMs在语义干扰下的形式推理鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Conflict-Aware Client Selection for Multi-Server Federated Learning</div>
<div class="meta-line">Authors: Mingwei Hong, Zheng Lin, Zehang Lin, Lin Li, Miao Yang, Xia Du, Zihan Fang, Zhaolu Kang, Dianxin Luan, Shunzhi Zhu</div>
<div class="meta-line">First: 2026-02-02T18:47:16+00:00 · Latest: 2026-02-02T18:47:16+00:00</div>
<div class="meta-line">Comments: 6 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02458v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02458v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated learning (FL) has emerged as a promising distributed machine learning (ML) that enables collaborative model training across clients without exposing raw data, thereby preserving user privacy and reducing communication costs. Despite these benefits, traditional single-server FL suffers from high communication latency due to the aggregation of models from a large number of clients. While multi-server FL distributes workloads across edge servers, overlapping client coverage and uncoordinated selection often lead to resource contention, causing bandwidth conflicts and training failures. To address these limitations, we propose a decentralized reinforcement learning with conflict risk prediction, named RL CRP, to optimize client selection in multi-server FL systems. Specifically, each server estimates the likelihood of client selection conflicts using a categorical hidden Markov model based on its sparse historical client selection sequence. Then, a fairness-aware reward mechanism is incorporated to promote long-term client participation for minimizing training latency and resource contention. Extensive experiments demonstrate that the proposed RL-CRP framework effectively reduces inter-server conflicts and significantly improves training efficiency in terms of convergence speed and communication cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向多服务器联邦学习的冲突感知客户端选择</div>
<div class="mono" style="margin-top:8px">联邦学习（FL）作为一种有前景的分布式机器学习（ML）方法，通过在不暴露原始数据的情况下实现客户端间的协作模型训练，从而保护用户隐私并降低通信成本。尽管具有这些优势，传统的单服务器联邦学习由于需要从大量客户端聚合模型，导致通信延迟较高。而多服务器联邦学习虽然将工作负载分配到边缘服务器，但客户端覆盖重叠和非协调选择常常引发资源竞争，造成带宽冲突和训练失败。为了解决这些限制，我们提出了一种基于冲突风险预测的去中心化强化学习方法，称为RL CRP，以优化多服务器联邦学习系统中的客户端选择。具体而言，每个服务器基于其稀疏的历史客户端选择序列，利用分类隐藏马尔可夫模型估计客户端选择冲突的可能性。随后，引入一种公平性感知的奖励机制，以促进长期客户端参与，从而最小化训练延迟和资源竞争。大量实验表明，所提出的RL-CRP框架有效减少了服务器间的冲突，并在收敛速度和通信成本方面显著提升了训练效率。</div>
</details>
</div>
<div class="card">
<div class="title">MetaCLASS: Metacognitive Coaching for Learning with Adaptive Self-regulation Support</div>
<div class="meta-line">Authors: Naiming Liu, Richard Baraniuk, Shashank Sonkar</div>
<div class="meta-line">First: 2026-02-02T18:47:04+00:00 · Latest: 2026-02-02T18:47:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02457v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02457v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models can generate fluent explanations, but effective tutoring requires supporting the learner&#x27;s thought process, not just delivering content. Metacognitive tutoring targets this gap by prompting planning, monitoring, debugging, and evaluation, and crucially, deciding when to be active versus minimally present, based on learner signals and trajectory. We introduce MetaCLASS, a learning-science grounded framework that formulates metacognitive tutoring as move selection over 11 interpretable actions aligned to self-regulated learning processes. MetaCLASS uses a two-phase framework that first plans a pedagogical trajectory conditioned on learner profiles (calibration, help-seeking) and then generates natural dialogue consistent with that plan. This yields a dataset of 1,015 conversations (7,711 turns) annotated with turn-level metacognitive labels, and validated for pedagogical contingency and trajectory adherence. We benchmark nine LLMs on predicting the next coach move given the problem and dialogue context. The best model achieves only 43.2\% accuracy, and models exhibit compulsive intervention bias: in turns where effective metacognitive tutoring requires silent (41.7\% of cases), models predict `no intervention&#x27; only 4.2\% of the time, while severely over-predicting high-intervention moves. These results show that traditional content-based tutoring ability does not translate to metacognitive tutoring competence, positioning MetaCLASS as a testbed for developing intelligent tutors that promote self-regulated learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MetaCLASS：基于元认知的辅导学习框架，具有自适应自我调节支持</div>
<div class="mono" style="margin-top:8px">大型语言模型可以生成流畅的解释，但有效的辅导需要支持学习者的思维过程，而不仅仅是传递内容。元认知辅导通过提示计划、监控、调试和评估来弥补这一差距，并且关键在于根据学习者信号和学习轨迹决定何时积极介入或保持最小存在。我们引入了MetaCLASS，这是一个基于学习科学的框架，将元认知辅导建模为11个可解释动作的移动选择，这些动作与自我调节学习过程相一致。MetaCLASS采用两阶段框架，首先根据学习者资料（校准、求助）规划教学轨迹，然后生成与该计划一致的自然对话。这产生了一个包含1,015次对话（7,711轮）的数据集，每轮都标注了元认知标签，并经过教学情境依赖性和轨迹遵循性的验证。我们在九个大型语言模型上对预测下一个辅导动作进行了基准测试，最佳模型的准确率仅为43.2\%。模型表现出强制性干预偏差：在需要静默的41.7\%情况下，模型仅预测4.2\%的“无干预”，而严重高估了高干预动作的预测。这些结果表明，传统的基于内容的辅导能力并不能转化为元认知辅导能力，这使得MetaCLASS成为开发促进自我调节学习的智能辅导系统的一个测试平台。</div>
</details>
</div>
<div class="card">
<div class="title">Relationship-Aware Hierarchical 3D Scene Graph for Task Reasoning</div>
<div class="meta-line">Authors: Albert Gassol Puigjaner, Angelos Zacharia, Kostas Alexis</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-02T18:47:02+00:00 · Latest: 2026-02-02T18:47:02+00:00</div>
<div class="meta-line">Comments: ICRA 2026, 8 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02456v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02456v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Representing and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings. While traditional Simultaneous Localization and Mapping (SLAM) methods generate metric reconstructions and can be extended to metric-semantic mapping, they lack a higher level of abstraction and relational reasoning. To address this gap, 3D scene graphs have emerged as a powerful representation for capturing hierarchical structures and object relationships. In this work, we propose an enhanced hierarchical 3D scene graph that integrates open-vocabulary features across multiple abstraction levels and supports object-relational reasoning. Our approach leverages a Vision Language Model (VLM) to infer semantic relationships. Notably, we introduce a task reasoning module that combines Large Language Models (LLM) and a VLM to interpret the scene graph&#x27;s semantic and relational information, enabling agents to reason about tasks and interact with their environment more intelligently. We validate our method by deploying it on a quadruped robot in multiple environments and tasks, highlighting its ability to reason about them.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向任务推理的关系感知分层3D场景图</div>
<div class="mono" style="margin-top:8px">以结构化方式表示和理解3D环境对自主代理导航和环境推理至关重要。尽管传统的同时定位与地图构建（SLAM）方法可以生成度量重建并可扩展为度量-语义地图，但它们缺乏更高层次的抽象和关系推理能力。为了解决这一问题，3D场景图作为一种强大的表示方法，已被用于捕捉层次结构和物体关系。在本工作中，我们提出了一种增强的分层3D场景图，该图在多个抽象层次上整合开放词汇特征，并支持物体-关系推理。我们的方法利用视觉语言模型（VLM）推断语义关系。值得注意的是，我们引入了一个任务推理模块，结合大型语言模型（LLM）和VLM来解释场景图的语义和关系信息，使代理能够更智能地推理任务并与环境互动。我们通过在多种环境和任务中部署该方法来验证我们的方法，突显了其推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Representing and understanding 3D environments in a structured manner is crucial for autonomous agents to navigate and reason about their surroundings.</div>
</details>
</div>
<div class="card">
<div class="title">Drift-Bench: Diagnosing Cooperative Breakdowns in LLM Agents under Input Faults via Multi-Turn Interaction</div>
<div class="meta-line">Authors: Han Bao, Zheyuan Zhang, Pengcheng Jing, Zhengqing Yuan, Kaiwen Shi, Yanfang Ye</div>
<div class="meta-line">First: 2026-02-02T18:46:16+00:00 · Latest: 2026-02-02T18:46:16+00:00</div>
<div class="meta-line">Comments: 65 pages, 40 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02455v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02455v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As Large Language Models transition to autonomous agents, user inputs frequently violate cooperative assumptions (e.g., implicit intent, missing parameters, false presuppositions, or ambiguous expressions), creating execution risks that text-only evaluations do not capture. Existing benchmarks typically assume well-specified instructions or restrict evaluation to text-only, single-turn clarification, and thus do not measure multi-turn disambiguation under grounded execution risk. We introduce \textbf{Drift-Bench}, the first diagnostic benchmark that evaluates agentic pragmatics under input faults through multi-turn clarification across state-oriented and service-oriented execution environments. Grounded in classical theories of communication, \textbf{Drift-Bench} provides a unified taxonomy of cooperative breakdowns and employs a persona-driven user simulator with the \textbf{Rise} evaluation protocol. Experiments show substantial performance drops under these faults, with clarification effectiveness varying across user personas and fault types. \MethodName bridges clarification research and agent safety evaluation, enabling systematic diagnosis of failures that can lead to unsafe executions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Drift-Bench：通过多轮交互诊断LLM代理在输入故障下的协作失效</div>
<div class="mono" style="margin-top:8px">随着大型语言模型向自主代理演进，用户输入经常违反协作假设（例如隐含意图、缺失参数、错误预设或模糊表达），从而产生文本评估无法捕捉的执行风险。现有基准通常假设指令明确或仅限于文本单轮澄清，因此无法衡量基于实际执行风险的多轮澄清效果。我们引入\textbf{Drift-Bench}，这是首个通过多轮澄清在面向状态和面向服务的执行环境中评估代理语用的诊断基准。\textbf{Drift-Bench}基于经典沟通理论，提供协作失效的统一分类，并采用\textbf{Rise}评估协议的个性驱动用户模拟器。实验表明，在这些故障下性能有显著下降，澄清效果因用户个性和故障类型而异。\MethodName连接了澄清研究与代理安全性评估，实现了对可能导致不安全执行的故障进行系统性诊断。</div>
</details>
</div>
<div class="card">
<div class="title">World-Gymnast: Training Robots with Reinforcement Learning in a World Model</div>
<div class="meta-line">Authors: Ansh Kumar Sharma, Yixiang Sun, Ninghao Lu, Yunzhe Zhang, Jiarao Liu, Sherry Yang</div>
<div class="meta-line">First: 2026-02-02T18:44:45+00:00 · Latest: 2026-02-02T18:44:45+00:00</div>
<div class="meta-line">Comments: https://world-gymnast.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02454v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02454v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://world-gymnast.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction. The two alternatives, supervised finetuning (SFT) from expert demonstrations and reinforcement learning (RL) in a software-based simulator, are limited by the amount of expert data available and the sim-to-real gap for manipulation. With the recent emergence of world models learned from real-world video-action data, we ask the question of whether training a policy in a world model can be more effective than supervised learning or software simulation in achieving better real-robot performance. We propose World-Gymnast, which performs RL finetuning of a vision-language-action (VLA) policy by rolling out the policy in an action-conditioned video world model and rewarding the rollouts with a vision-language model (VLM). On the Bridge robot setup, World-Gymnast outperforms SFT by as much as 18x and outperforms software simulator by as much as 2x. More importantly, World-Gymnast demonstrates intriguing capabilities of RL with a world model, including training on diverse language instructions and novel scenes from the world model, test-time training in a novel scene, and online iterative world model and policy improvement. Our results suggest learning a world model and training robot policies in the cloud could be the key to bridging the gap between robots that work in demonstrations and robots that can work in anyone&#x27;s household.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>World-Gymnast：在世界模型中使用强化学习训练机器人</div>
<div class="mono" style="margin-top:8px">机器人通过与物理世界互动进行学习本质上受到物理交互成本的限制。监督微调（SFT）依赖于专家演示数据，而基于软件模拟器的强化学习（RL）则受限于模拟到现实的差距。随着从真实世界视频-动作数据中学习到的世界模型的出现，我们探讨了在世界模型中训练策略是否比监督学习或软件模拟更有效，以实现更好的真实机器人性能。我们提出了World-Gymnast，它通过在动作条件视频世界模型中展开策略，并使用视觉语言模型（VLM）对展开过程进行奖励，来对视觉-语言-动作（VLA）策略进行强化学习微调。在Bridge机器人设置中，World-Gymnast的表现比SFT高18倍，比软件模拟器高2倍。更重要的是，World-Gymnast展示了使用世界模型进行强化学习的引人注目的能力，包括在多样化的语言指令和世界模型中的新场景上进行训练、在新场景中进行测试时的训练，以及在线迭代的世界模型和策略改进。我们的结果表明，学习世界模型并在云端训练机器人策略可能是弥合演示中工作机器人与能在任何家庭中工作的机器人的差距的关键。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Robot learning from interacting with the physical world is fundamentally bottlenecked by the cost of physical interaction.</div>
</details>
</div>
<div class="card">
<div class="title">Thinking with Comics: Enhancing Multimodal Reasoning through Structured Visual Storytelling</div>
<div class="meta-line">Authors: Andong Chen, Wenxin Zhu, Qiuyu Ding, Yuchen Song, Muyun Yang, Tiejun Zhao</div>
<div class="meta-line">First: 2026-02-02T18:43:57+00:00 · Latest: 2026-02-02T18:43:57+00:00</div>
<div class="meta-line">Comments: Working paper</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02453v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02453v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos. However, different modalities still have clear limitations: static images struggle to represent temporal structure, while videos introduce substantial redundancy and computational cost. In this work, we propose Thinking with Comics, a visual reasoning paradigm that uses comics as a high information-density medium positioned between images and videos. Comics preserve temporal structure, embedded text, and narrative coherence while requiring significantly lower reasoning cost. We systematically study two reasoning paths based on comics and evaluate them on a range of reasoning tasks and long-context understanding tasks. Experimental results show that Thinking with Comics outperforms Thinking with Images on multi-step temporal and causal reasoning tasks, while remaining substantially more efficient than Thinking with Video. Further analysis indicates that different comic narrative structures and styles consistently affect performance across tasks, suggesting that comics serve as an effective intermediate visual representation for improving multimodal reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过漫画思考：通过结构化视觉叙事增强多模态推理</div>
<div class="mono" style="margin-top:8px">链式思维推理推动了大语言模型从基于文本的推理扩展到基于图像和视频的推理。然而，不同模态仍存在明显限制：静态图像难以表示时间结构，而视频则引入大量冗余和计算成本。在本工作中，我们提出了一种名为『通过漫画思考』的视觉推理范式，将漫画作为一种信息密度高且介于图像和视频之间的媒介。漫画保留了时间结构、嵌入文本和叙事连贯性，同时显著降低了推理成本。我们系统地研究了基于漫画的两种推理路径，并在多种推理任务和长上下文理解任务上进行了评估。实验结果表明，在多步骤的时间和因果推理任务中，『通过漫画思考』优于『通过图像思考』，同时仍显著优于『通过视频思考』。进一步分析表明，不同的漫画叙事结构和风格对任务表现有持续影响，这表明漫画可作为提升多模态推理的有效中间视觉表示。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Chain-of-Thought reasoning has driven large language models to extend from thinking with text to thinking with images and videos.</div>
</details>
</div>
<div class="card">
<div class="title">Active Causal Experimentalist (ACE): Learning Intervention Strategies via Direct Preference Optimization</div>
<div class="meta-line">Authors: Patrick Cooper, Alvaro Velasquez</div>
<div class="meta-line">First: 2026-02-02T18:43:52+00:00 · Latest: 2026-02-02T18:43:52+00:00</div>
<div class="meta-line">Comments: 9 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02451v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02451v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Discovering causal relationships requires controlled experiments, but experimentalists face a sequential decision problem: each intervention reveals information that should inform what to try next. Traditional approaches such as random sampling, greedy information maximization, and round-robin coverage treat each decision in isolation, unable to learn adaptive strategies from experience. We propose Active Causal Experimentalist (ACE), which learns experimental design as a sequential policy. Our key insight is that while absolute information gains diminish as knowledge accumulates (making value-based RL unstable), relative comparisons between candidate interventions remain meaningful throughout. ACE exploits this via Direct Preference Optimization, learning from pairwise intervention comparisons rather than non-stationary reward magnitudes. Across synthetic benchmarks, physics simulations, and economic data, ACE achieves 70-71% improvement over baselines at equal intervention budgets (p &lt; 0.001, Cohen&#x27;s d ~ 2). Notably, the learned policy autonomously discovers that collider mechanisms require concentrated interventions on parent variables, a theoretically-grounded strategy that emerges purely from experience. This suggests preference-based learning can recover principled experimental strategies, complementing theory with learned domain adaptation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>主动因果实验者（ACE）：通过直接偏好优化学习干预策略</div>
<div class="mono" style="margin-top:8px">发现因果关系需要受控实验，但实验者面临一个序列决策问题：每次干预都会提供信息，这些信息应指导下一步的干预选择。传统方法如随机采样、贪心信息最大化和轮询覆盖将每个决策孤立处理，无法从经验中学习适应性策略。我们提出主动因果实验者（ACE），将实验设计学习为一个序列策略。我们的关键洞察是，虽然绝对信息增益随着知识积累而减少（使得基于价值的强化学习不稳定），但候选干预之间的相对比较在整个过程中仍然有意义。ACE 通过直接偏好优化利用这一点，从成对的干预比较中学习，而非非平稳的奖励幅度。在合成基准、物理模拟和经济数据上，ACE 在相同干预预算下比基线方法提升了 70-71%（p &lt; 0.001，Cohen&#x27;s d ~ 2）。值得注意的是，学习到的策略自主发现碰撞机制需要对父变量进行集中干预，这是一种纯粹基于经验产生的理论策略。这表明基于偏好的学习可以恢复出有原则的实验策略，将理论与学习到的领域自适应相结合。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Discovering causal relationships requires controlled experiments, but experimentalists face a sequential decision problem: each intervention reveals information that should inform what to try next.</div>
</details>
</div>
<div class="card">
<div class="title">Painting a Family Portrait of the Yellow Super- and Hypergiants in the Milky Way I. Constraining the Distances and Luminosities</div>
<div class="meta-line">Authors: A. Kasikov, A. Mehner, I. Kolka, A. Aret</div>
<div class="meta-line">First: 2026-02-02T18:43:44+00:00 · Latest: 2026-02-02T18:43:44+00:00</div>
<div class="meta-line">Comments: 22 pages, 11 figures, accepted in A&amp;A</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02449v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02449v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Context. Distances to evolved massive stars in the Milky Way are not well constrained by Gaia parallaxes due to their brightness and variability. This makes it difficult to determine their fundamental stellar parameters, such as radius or luminosity, and infer their evolutionary states. Aims. We aim to improve the distance estimates of Yellow Hypergiants (YHGs) and Yellow Supergiants (YSGs) by identifying possible cluster and association memberships. Using these distances, we derived updated luminosities and revised their positions in the Hertzsprung-Russell diagram. Methods. We compiled from the literature a sample of 35 luminous yellow massive stars (YHGs and the most luminous YSGs). We used Gaia DR3 astrometry to identify possible membership in clusters and OB associations. We derived distances by combining the parallaxes of nearby co-moving stars. We independently validated these distances by comparing the stellar radial velocities to the Galactic H I kinematic map. We combined angular diameters and effective temperature values from the literature with the new distances to estimate luminosities. Results. We improved the distance estimates for 28 of the 35 stars through association with co-moving stellar groups. For an additional six stars, we provided distance estimates based on the H I kinematic map. For one star, the distance remains unclear. Most YSGs are members of young stellar populations, while the environments of the YHGs are more diverse, and for some of them, their origin populations remain unclear. We derived updated luminosities for a subset of 20 stars. Most YHGs have luminosities above log L/L = 5.4, while YSGs occupy a wider range of luminosities and the most luminous YSGs have luminosities similar to YHGs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>银河系黄超巨星和超超巨星家族画像绘制 I. 距离和光度的约束</div>
<div class="mono" style="margin-top:8px">背景。由于黄超巨星（YHGs）和黄超巨星（YSGs）的亮度和变异性，它们的距离难以通过盖亚视差精确确定。这使得确定其基本恒星参数（如半径或光度）变得困难，并影响对其演化状态的推断。目标。我们旨在通过识别可能的星团和OB星协成员关系，提高黄超巨星和黄超巨星的距离估计。利用这些距离，我们推导了更新的光度，并重新确定了它们在赫罗图中的位置。方法。我们从文献中收集了35颗明亮的黄质量恒星（YHGs和最亮的YSGs）样本。我们使用盖亚DR3天体测量数据来识别可能的星团和OB星协成员关系。通过结合附近共动恒星的视差，我们推导了距离。我们通过将恒星的径向速度与银河H I动力学图进行比较，独立验证了这些距离。我们结合文献中的角直径和有效温度数据，以及新的距离，来估计光度。结果。通过与共动恒星群关联，我们提高了其中28颗恒星的距离估计。对于另外6颗恒星，我们基于H I动力学图提供了距离估计。对于1颗恒星，其距离仍不明确。大多数YSGs属于年轻恒星群体，而YHGs的环境更为多样，其中一些恒星的起源群体仍不清楚。我们为20颗恒星子集推导了更新的光度。大多数YHGs的光度高于log L/L = 5.4，而YSGs的光度范围更广，最亮的YSGs的光度与YHGs相似。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Context.</div>
</details>
</div>
<div class="card">
<div class="title">Finite-Sample Wasserstein Error Bounds and Concentration Inequalities for Nonlinear Stochastic Approximation</div>
<div class="meta-line">Authors: Seo Taek Kong, R. Srikant</div>
<div class="meta-line">First: 2026-02-02T18:41:06+00:00 · Latest: 2026-02-02T18:41:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02445v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02445v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper derives non-asymptotic error bounds for nonlinear stochastic approximation algorithms in the Wasserstein-$p$ distance. To obtain explicit finite-sample guarantees for the last iterate, we develop a coupling argument that compares the discrete-time process to a limiting Ornstein-Uhlenbeck process. Our analysis applies to algorithms driven by general noise conditions, including martingale differences and functions of ergodic Markov chains. Complementing this result, we handle the convergence rate of the Polyak-Ruppert average through a direct analysis that applies under the same general setting.
  Assuming the driving noise satisfies a non-asymptotic central limit theorem, we show that the normalized last iterates converge to a Gaussian distribution in the $p$-Wasserstein distance at a rate of order $γ_n^{1/6}$, where $γ_n$ is the step size. Similarly, the Polyak-Ruppert average is shown to converge in the Wasserstein distance at a rate of order $n^{-1/6}$. These distributional guarantees imply high-probability concentration inequalities that improve upon those derived from moment bounds and Markov&#x27;s inequality. We demonstrate the utility of this approach by considering two applications: (1) linear stochastic approximation, where we explicitly quantify the transition from heavy-tailed to Gaussian behavior of the iterates, thereby bridging the gap between recent finite-sample analyses and asymptotic theory and (2) stochastic gradient descent, where we establish rate of convergence to the central limit theorem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非渐近的Wasserstein误差界与非线性随机逼近算法的集中不等式</div>
<div class="mono" style="margin-top:8px">本文推导了非线性随机逼近算法在Wasserstein-$p$距离下的非渐近误差界。为了获得最后一个迭代点的显式有限样本保证，我们开发了一种耦合论证方法，将离散时间过程与极限Ornstein-Uhlenbeck过程进行比较。我们的分析适用于由一般噪声条件驱动的算法，包括鞅差分和ergodic马尔可夫链的函数。作为补充，我们通过直接分析处理了Polyak-Ruppert平均的收敛速率，该分析适用于相同的通用设定。
假设驱动噪声满足非渐近中心极限定理，我们证明了归一化最后一个迭代点在$p$-Wasserstein距离下以$γ_n^{1/6}$的速率收敛到高斯分布，其中$γ_n$为步长。同样，Polyak-Ruppert平均在Wasserstein距离下以$n^{-1/6}$的速率收敛。这些分布保证意味着比基于矩界和Markov不等式推导出的更优的高概率集中不等式。我们通过两个应用来展示该方法的实用性：(1) 线性随机逼近，其中我们明确量化了迭代点从重尾分布向高斯分布的转变，从而弥合了近期有限样本分析与渐近理论之间的差距；(2) 随机梯度下降，其中我们建立了收敛到中心极限定理的速率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper derives non-asymptotic error bounds for nonlinear stochastic approximation algorithms in the Wasserstein-$p$ distance.</div>
</details>
</div>
<div class="card">
<div class="title">RANKVIDEO: Reasoning Reranking for Text-to-Video Retrieval</div>
<div class="meta-line">Authors: Tyler Skow, Alexander Martin, Benjamin Van Durme, Rama Chellappa, Reno Kriz</div>
<div class="meta-line">First: 2026-02-02T18:40:37+00:00 · Latest: 2026-02-02T18:40:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02444v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02444v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reranking is a critical component of modern retrieval systems, which typically pair an efficient first-stage retriever with a more expressive model to refine results. While large reasoning models have driven rapid progress in text-centric reranking, reasoning-based reranking for video retrieval remains underexplored. To address this gap, we introduce RANKVIDEO, a reasoning-based reranker for video retrieval that explicitly reasons over query-video pairs using video content to assess relevance. RANKVIDEO is trained using a two-stage curriculum consisting of perception-grounded supervised fine-tuning followed by reranking training that combines pointwise, pairwise, and teacher confidence distillation objectives, and is supported by a data synthesis pipeline for constructing reasoning-intensive query-video pairs. Experiments on the large-scale MultiVENT 2.0 benchmark demonstrate that RANKVIDEO consistently improves retrieval performance within a two-stage framework, yielding an average improvement of 31% on nDCG@10 and outperforming text-only and vision-language reranking alternatives, while more efficient.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RANKVIDEO：面向文本到视频检索的推理重排序</div>
<div class="mono" style="margin-top:8px">重排序是现代检索系统中的关键组件，通常由一个高效的初步检索器与一个更具表达力的模型组合，以优化检索结果。尽管大型推理模型在文本中心的重排序任务中推动了快速进展，但基于推理的视频检索重排序仍处于探索阶段。为了解决这一问题，我们引入了RANKVIDEO，这是一个面向视频检索的基于推理的重排序模型，通过视频内容显式地对查询-视频对进行推理以评估相关性。RANKVIDEO采用两阶段课程进行训练，包括基于感知的监督微调，随后是结合点wise、pairwise和教师置信度蒸馏目标的重排序训练，并通过数据合成流水线构建推理密集型的查询-视频对。在大规模MultiVENT 2.0基准上的实验表明，RANKVIDEO在两阶段框架内持续提升检索性能，其在nDCG@10上的平均提升达到31%，并优于仅文本和视觉-语言重排序的替代方案，同时更加高效。</div>
</details>
</div>
<div class="card">
<div class="title">Certain Head, Uncertain Tail: Expert-Sample for Test-Time Scaling in Fine-Grained MoE</div>
<div class="meta-line">Authors: Yuanteng Chen, Peisong Wang, Nanxin Zeng, Yuantian Shao, Gang Li, Jing Liu, Jian Cheng</div>
<div class="meta-line">First: 2026-02-02T18:39:33+00:00 · Latest: 2026-02-02T18:39:33+00:00</div>
<div class="meta-line">Comments: 24 pages, 13 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02443v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02443v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test-time scaling improves LLM performance by generating multiple candidate solutions, yet token-level sampling requires temperature tuning that trades off diversity against stability. Fine-grained MoE, featuring hundreds of well-trained experts per layer and multi-expert activation per token, offers an unexplored alternative through its rich routing space. We empirically characterize fine-grained MoE routing and uncover an informative pattern: router scores exhibit a certain head of high-confidence experts followed by an uncertain tail of low-confidence candidates. While single-run greedy accuracy remains stable when fewer experts are activated, multi-sample pass@n degrades significantly-suggesting that the certain head governs core reasoning capability while the uncertain tail correlates with reasoning diversity. Motivated by these findings, we propose Expert-Sample, a training-free method that preserves high-confidence selections while injecting controlled stochasticity into the uncertain tail, enabling diverse generation without destabilizing outputs. Evaluated on multiple fine-grained MoE models across math, knowledge reasoning, and code tasks, Expert-Sample consistently improves pass@n and verification-based accuracy. On Qwen3-30B-A3B-Instruct evaluated on GPQA-Diamond with 32 parallel samples, pass@32 rises from 85.4% to 91.9%, and accuracy improves from 59.1% to 62.6% with Best-of-N verification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>确定头部，不确定尾部：用于细粒度MoE测试时扩展的专家样本方法</div>
<div class="mono" style="margin-top:8px">测试时扩展通过生成多个候选解来提升大语言模型的性能，但基于token的采样需要温度调优，在多样性与稳定性之间进行权衡。细粒度MoE通过每层数百个训练良好的专家以及每个token多专家激活，提供了尚未被探索的替代方案，其丰富的路由空间具有潜力。我们通过实证分析细粒度MoE的路由机制，并发现了一个有信息量的模式：路由器得分呈现出一个高置信度专家的确定头部，随后是低置信度候选的不确定尾部。虽然当激活较少专家时单次运行的贪心准确率保持稳定，但多样本pass@n显著下降，这表明确定头部控制着核心推理能力，而不确定尾部则与推理多样性相关。基于这些发现，我们提出Expert-Sample，一种无需训练的方法，它在保持高置信度选择的同时，向不确定尾部注入可控的随机性，从而实现多样化的生成而不破坏输出的稳定性。在数学、知识推理和代码任务等多个细粒度MoE模型上进行评估，Expert-Sample在pass@n和基于验证的准确率方面均表现出持续提升。在GPQA-Diamond数据集上对Qwen3-30B-A3B-Instruct模型进行评估，使用32个并行样本时，pass@32从85.4%提升至91.9%，准确率从59.1%提升至62.6%（使用Best-of-N验证）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Test-time scaling improves LLM performance by generating multiple candidate solutions, yet token-level sampling requires temperature tuning that trades off diversity against stability.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260202_0402.html">20260202_0402</a>
<a href="archive/20260201_0354.html">20260201_0354</a>
<a href="archive/20260131_0408.html">20260131_0408</a>
<a href="archive/20260130_0406.html">20260130_0406</a>
<a href="archive/20260129_0407.html">20260129_0407</a>
<a href="archive/20260128_0407.html">20260128_0407</a>
<a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
