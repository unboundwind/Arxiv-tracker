<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-23 04:01</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260223_0401</div>
    <div class="row"><div class="card">
<div class="title">OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents</div>
<div class="meta-line">Authors: Akashah Shabbir, Muhammad Umer Sheikh, Muhammad Akhtar Munir, Hiyam Debary, Mustansar Fiaz, Muhammad Zaigham Zaheer, Paolo Fraccaro, Fahad Shahbaz Khan, Muhammad Haris Khan, Xiao Xiang Zhu, Salman Khan</div>
<div class="meta-line">First: 2026-02-19T18:59:54+00:00 · Latest: 2026-02-19T18:59:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17665v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17665v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks. Extending such capabilities to the remote sensing domain remains challenging, as models must reason over spatial scale, geographic structures, and multispectral indices while maintaining coherent multi-step logic. To bridge this gap, OpenEarthAgent introduces a unified framework for developing tool-augmented geospatial agents trained on satellite imagery, natural-language queries, and detailed reasoning traces. The training pipeline relies on supervised fine-tuning over structured reasoning trajectories, aligning the model with verified multistep tool interactions across diverse analytical contexts. The accompanying corpus comprises 14,538 training and 1,169 evaluation instances, with more than 100K reasoning steps in the training split and over 7K reasoning steps in the evaluation split. It spans urban, environmental, disaster, and infrastructure domains, and incorporates GIS-based operations alongside index analyses such as NDVI, NBR, and NDBI. Grounded in explicit reasoning traces, the learned agent demonstrates structured reasoning, stable spatial understanding, and interpretable behaviour through tool-driven geospatial interactions across diverse conditions. We report consistent improvements over a strong baseline and competitive performance relative to recent open and closed-source models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OpenEarthAgent：一种用于工具增强地理空间代理的统一框架</div>
<div class="mono" style="margin-top:8px">多模态推理的最新进展使得代理能够解释图像、将其与语言连接，并执行结构化的分析任务。将这些能力扩展到遥感领域仍然具有挑战性，因为模型必须在空间尺度、地理结构和多光谱指数上进行推理，同时保持连贯的多步逻辑。为弥合这一差距，OpenEarthAgent 引入了一种统一框架，用于开发基于卫星图像、自然语言查询和详细推理轨迹训练的工具增强型地理空间代理。训练流程依赖于结构化推理轨迹上的监督微调，使模型与多种分析情境下的经过验证的多步工具交互对齐。配套语料库包含14,538个训练实例和1,169个评估实例，训练集包含超过100,000个推理步骤，评估集包含超过7,000个推理步骤。该语料库涵盖城市、环境、灾害和基础设施等多个领域，并结合了基于GIS的操作以及NDVI、NBR和NDBI等指数分析。该学习代理基于显式的推理轨迹，通过工具驱动的地理空间交互，在各种条件下展现出结构化的推理能力、稳定的地理理解能力和可解释的行为。我们在一个强大的基线模型上报告了持续的性能提升，并在与近期开源和闭源模型的比较中表现出竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">Sink-Aware Pruning for Diffusion Language Models</div>
<div class="meta-line">Authors: Aidar Myrzakhan, Tianyi Li, Bowei Guo, Shengkun Tang, Zhiqiang Shen</div>
<div class="meta-line">First: 2026-02-19T18:59:50+00:00 · Latest: 2026-02-19T18:59:50+00:00</div>
<div class="meta-line">Comments: Code at: https://github.com/VILA-Lab/Sink-Aware-Pruning</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17664v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17664v1">PDF</a> · <a href="https://github.com/VILA-Lab/Sink-Aware-Pruning">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Language Models (DLMs) incur high inference cost due to iterative denoising, motivating efficient pruning. Existing pruning heuristics largely inherited from autoregressive (AR) LLMs, typically preserve attention sink tokens because AR sinks serve as stable global anchors. We show that this assumption does not hold for DLMs: the attention-sink position exhibits substantially higher variance over the full generation trajectory (measured by how the dominant sink locations shift across timesteps), indicating that sinks are often transient and less structurally essential than in AR models. Based on this observation, we propose ${\bf \texttt{Sink-Aware Pruning}}$, which automatically identifies and prunes unstable sinks in DLMs (prior studies usually keep sinks for AR LLMs). Without retraining, our method achieves a better quality-efficiency trade-off and outperforms strong prior pruning baselines under matched compute. Our code is available at https://github.com/VILA-Lab/Sink-Aware-Pruning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向扩散语言模型的Sink感知剪枝</div>
<div class="mono" style="margin-top:8px">由于迭代去噪过程，扩散语言模型（DLMs）的推理成本较高，这促使了高效剪枝方法的发展。现有的剪枝启发式方法大多继承自自回归（AR）语言模型，通常保留注意力sink标记，因为AR模型中的sink标记作为稳定的全局锚点。我们发现这一假设在DLMs中并不成立：注意力sink的位置在整个生成过程中表现出显著更高的方差（通过主导sink位置在时间步之间的变化来衡量），表明sink通常是瞬时的，不如自回归模型中那样结构上关键。基于这一观察，我们提出了${\bf \texttt{Sink-Aware Pruning}}$，该方法能够自动识别并剪枝DLMs中的不稳定sink（而以往研究通常保留sink用于自回归语言模型）。在无需重新训练的情况下，我们的方法实现了更好的质量与效率的平衡，并在计算资源匹配的情况下优于现有的强大剪枝基线。我们的代码可在https://github.com/VILA-Lab/Sink-Aware-Pruning获取。</div>
</details>
</div>
<div class="card">
<div class="title">CLEF HIPE-2026: Evaluating Accurate and Efficient Person-Place Relation Extraction from Multilingual Historical Texts</div>
<div class="meta-line">Authors: Juri Opitz, Corina Raclé, Emanuela Boros, Andrianos Michail, Matteo Romanello, Maud Ehrmann, Simon Clematide</div>
<div class="meta-line">First: 2026-02-19T18:59:44+00:00 · Latest: 2026-02-19T18:59:44+00:00</div>
<div class="meta-line">Comments: ECIR 2026. CLEF Evaluation Lab. Registration DL: 2026/04/23. Task Homepage at https://hipe-eval.github.io/HIPE-2026/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17663v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17663v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hipe-eval.github.io/HIPE-2026/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">HIPE-2026 is a CLEF evaluation lab dedicated to person-place relation extraction from noisy, multilingual historical texts. Building on the HIPE-2020 and HIPE-2022 campaigns, it extends the series toward semantic relation extraction by targeting the task of identifying person--place associations in multiple languages and time periods. Systems are asked to classify relations of two types - $at$ (&quot;Has the person ever been at this place?&quot;) and $isAt$ (&quot;Is the person located at this place around publication time?&quot;) - requiring reasoning over temporal and geographical cues. The lab introduces a three-fold evaluation profile that jointly assesses accuracy, computational efficiency, and domain generalization. By linking relation extraction to large-scale historical data processing, HIPE-2026 aims to support downstream applications in knowledge-graph construction, historical biography reconstruction, and spatial analysis in digital humanities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLEF HIPE-2026：从多语言历史文本中评估准确且高效的人员-地点关系抽取</div>
<div class="mono" style="margin-top:8px">HIPE-2026 是一个 CLEF 评估实验室，专注于从嘈杂的多语言历史文本中抽取人员-地点关系。该实验室延续了 HIPE-2020 和 HIPE-2022 的活动，通过聚焦于多语言和不同时期中识别人员与地点的关联，将系列研究扩展到语义关系抽取。系统需要对两种类型的关系进行分类：$at$（&quot;该人物是否曾在此地？&quot;）和 $isAt$（&quot;该人物在出版时是否位于此地？&quot;），这要求系统能够基于时间和地理线索进行推理。该实验室引入了一个三重评估框架，综合评估准确性、计算效率和领域泛化能力。通过将关系抽取与大规模历史数据处理相结合，HIPE-2026 旨在支持知识图谱构建、历史传记重建以及数字人文中的空间分析等下游应用。</div>
</details>
</div>
<div class="card">
<div class="title">When Vision Overrides Language: Evaluating and Mitigating Counterfactual Failures in VLAs</div>
<div class="meta-line">Authors: Yu Fang, Yuchun Feng, Dong Jing, Jiaqi Liu, Yue Yang, Zhenyu Wei, Daniel Szafir, Mingyu Ding</div>
<div class="meta-line">First: 2026-02-19T18:59:20+00:00 · Latest: 2026-02-19T18:59:20+00:00</div>
<div class="meta-line">Comments: Website: https://vla-va.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17659v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17659v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vla-va.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action models (VLAs) promise to ground language instructions in robot control, yet in practice often fail to faithfully follow language. When presented with instructions that lack strong scene-specific supervision, VLAs suffer from counterfactual failures: they act based on vision shortcuts induced by dataset biases, repeatedly executing well-learned behaviors and selecting objects frequently seen during training regardless of language intent. To systematically study it, we introduce LIBERO-CF, the first counterfactual benchmark for VLAs that evaluates language following capability by assigning alternative instructions under visually plausible LIBERO layouts. Our evaluation reveals that counterfactual failures are prevalent yet underexplored across state-of-the-art VLAs. We propose Counterfactual Action Guidance (CAG), a simple yet effective dual-branch inference scheme that explicitly regularizes language conditioning in VLAs. CAG combines a standard VLA policy with a language-unconditioned Vision-Action (VA) module, enabling counterfactual comparison during action selection. This design reduces reliance on visual shortcuts, improves robustness on under-observed tasks, and requires neither additional demonstrations nor modifications to existing architectures or pretrained models. Extensive experiments demonstrate its plug-and-play integration across diverse VLAs and consistent improvements. For example, on LIBERO-CF, CAG improves $π_{0.5}$ by 9.7% in language following accuracy and 3.6% in task success on under-observed tasks using a training-free strategy, with further gains of 15.5% and 8.5%, respectively, when paired with a VA model. In real-world evaluations, CAG reduces counterfactual failures of 9.4% and improves task success by 17.2% on average.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当视觉超越语言时：评估和缓解VLAs中的反事实失败</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作模型（VLAs）承诺将语言指令与机器人控制相结合，但在实践中往往无法忠实遵循语言。当面对缺乏强场景监督的指令时，VLAs会出现反事实失败：它们基于数据集偏差诱导的视觉捷径进行行动，反复执行已学好的行为，并选择在训练中频繁出现的对象，而不管语言意图如何。为系统性地研究这一问题，我们引入了LIBERO-CF，这是首个用于评估VLAs语言遵循能力的反事实基准，通过在视觉上合理的LIBERO布局下分配替代指令进行测试。我们的评估表明，反事实失败在最先进的VLAs中普遍存在但尚未被充分研究。我们提出了反事实动作引导（Counterfactual Action Guidance, CAG），这是一种简单而有效的双分支推理方案，显式地对VLAs中的语言条件进行正则化。CAG将标准的VLA策略与无语言条件的视觉-动作（Vision-Action, VA）模块相结合，使在动作选择过程中能够进行反事实比较。这种设计减少了对视觉捷径的依赖，提高了在低观测任务中的鲁棒性，且无需额外的演示或对现有架构或预训练模型进行修改。大量实验验证了其在多种VLAs中的即插即用集成能力以及持续的性能提升。例如，在LIBERO-CF上，CAG通过无训练策略在语言遵循准确率上提高了9.7%，在低观测任务中任务成功率提高了3.6%；当与VA模型结合时，分别进一步提升了15.5%和8.5%。在现实世界评估中，CAG平均减少了9.4%的反事实失败，并提高了17.2%的任务成功率。</div>
</details>
</div>
<div class="card">
<div class="title">MARS: Margin-Aware Reward-Modeling with Self-Refinement</div>
<div class="meta-line">Authors: Payel Bhattacharjee, Osvaldo Simeone, Ravi Tandon</div>
<div class="meta-line">First: 2026-02-19T18:59:03+00:00 · Latest: 2026-02-19T18:59:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17658v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17658v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward modeling is a core component of modern alignment pipelines including RLHF and RLAIF, underpinning policy optimization methods including PPO and TRPO. However, training reliable reward models relies heavily on human-labeled preference data, which is costly and limited, motivating the use of data augmentation. Existing augmentation approaches typically operate at the representation or semantic level and remain agnostic to the reward model&#x27;s estimation difficulty. In this paper, we propose MARS, an adaptive, margin-aware augmentation and sampling strategy that explicitly targets ambiguous and failure modes of the reward model. Our proposed framework, MARS, concentrates augmentation on low-margin (ambiguous) preference pairs where the reward model is most uncertain, and iteratively refines the training distribution via hard-sample augmentation. We provide theoretical guarantees showing that this strategy increases the average curvature of the loss function hence enhance information and improves conditioning, along with empirical results demonstrating consistent gains over uniform augmentation for robust reward modeling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MARS：基于边距感知的奖励模型自优化方法</div>
<div class="mono" style="margin-top:8px">奖励建模是现代对齐流程中的核心组件，包括RLHF和RLAIF，支撑着策略优化方法如PPO和TRPO。然而，训练可靠的奖励模型严重依赖于人工标注的偏好数据，这既昂贵又有限，因此推动了数据增强的应用。现有的增强方法通常在表示或语义层面操作，而忽视了奖励模型的估计难度。本文提出MARS，一种自适应、边距感知的增强与采样策略，明确针对奖励模型的模糊和失败模式。我们的框架MARS专注于奖励模型最不确定的低边距（模糊）偏好对进行增强，并通过硬样本增强迭代地优化训练分布。我们提供了理论保证，表明该策略可以提高损失函数的平均曲率，从而增强信息并改善条件，同时实证结果也显示其在鲁棒奖励建模中优于均匀增强。</div>
</details>
</div>
<div class="card">
<div class="title">Mine and Refine: Optimizing Graded Relevance in E-commerce Search Retrieval</div>
<div class="meta-line">Authors: Jiaqi Xi, Raghav Saboo, Luming Chen, Martin Wang, Sudeep Das</div>
<div class="meta-line">First: 2026-02-19T18:56:36+00:00 · Latest: 2026-02-19T18:56:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17654v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17654v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a two-stage &quot;Mine and Refine&quot; contrastive training framework for semantic text embeddings to enhance multi-category e-commerce search retrieval. Large scale e-commerce search demands embeddings that generalize to long tail, noisy queries while adhering to scalable supervision compatible with product and policy constraints. A practical challenge is that relevance is often graded: users accept substitutes or complements beyond exact matches, and production systems benefit from clear separation of similarity scores across these relevance strata for stable hybrid blending and thresholding. To obtain scalable policy consistent supervision, we fine-tune a lightweight LLM on human annotations under a three-level relevance guideline and further reduce residual noise via engagement driven auditing. In Stage 1, we train a multilingual Siamese two-tower retriever with a label aware supervised contrastive objective that shapes a robust global semantic space. In Stage 2, we mine hard samples via ANN and re-annotate them with the policy aligned LLM, and introduce a multi-class extension of circle loss that explicitly sharpens similarity boundaries between relevance levels, to further refine and enrich the embedding space. Robustness is additionally improved through additive spelling augmentation and synthetic query generation. Extensive offline evaluations and production A/B tests show that our framework improves retrieval relevance and delivers statistically significant gains in engagement and business impact.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>挖掘与精炼：优化电子商务搜索中的分级相关性</div>
<div class="mono" style="margin-top:8px">我们提出了一种两阶段的&quot;挖掘与精炼&quot;对比训练框架，用于语义文本嵌入，以提升多类别的电子商务搜索检索效果。大规模电子商务搜索需要能够泛化到长尾、噪声查询的嵌入，同时满足可扩展的监督机制，以符合产品和政策约束。一个实际挑战是相关性通常是分级的：用户接受精确匹配之外的替代品或互补品，而生产系统则受益于在这些相关性层级之间清晰区分相似度分数，以实现稳定的混合融合和阈值判断。为了获得可扩展且符合政策的监督信号，我们在三级相关性指南下对轻量级LLM进行微调，并通过以参与度为导向的审核进一步减少残余噪声。在第一阶段，我们训练一个多语言的Siamese双塔检索器，使用标签感知的监督对比目标来构建一个稳健的全局语义空间。在第二阶段，我们通过ANN挖掘困难样本，并使用与政策对齐的LLM重新标注这些样本，同时引入一种多类扩展的圆形损失，明确地增强不同相关性层级之间的相似度边界，以进一步精炼和丰富嵌入空间。通过添加拼写增强和合成查询生成，我们进一步提升了鲁棒性。广泛的离线评估和生产A/B测试表明，我们的框架提升了检索相关性，并在参与度和商业影响方面带来了统计显著的提升。</div>
</details>
</div>
<div class="card">
<div class="title">Human-level 3D shape perception emerges from multi-view learning</div>
<div class="meta-line">Authors: Tyler Bonnen, Jitendra Malik, Angjoo Kanazawa</div>
<div class="meta-line">First: 2026-02-19T18:56:05+00:00 · Latest: 2026-02-19T18:56:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17650v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17650v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans can infer the three-dimensional structure of objects from two-dimensional visual inputs. Modeling this ability has been a longstanding goal for the science and engineering of visual intelligence, yet decades of computational methods have fallen short of human performance. Here we develop a modeling framework that predicts human 3D shape inferences for arbitrary objects, directly from experimental stimuli. We achieve this with a novel class of neural networks trained using a visual-spatial objective over naturalistic sensory data; given a set of images taken from different locations within a natural scene, these models learn to predict spatial information related to these images, such as camera location and visual depth, without relying on any object-related inductive biases. Notably, these visual-spatial signals are analogous to sensory cues readily available to humans. We design a zero-shot evaluation approach to determine the performance of these `multi-view&#x27; models on a well established 3D perception task, then compare model and human behavior. Our modeling framework is the first to match human accuracy on 3D shape inferences, even without task-specific training or fine-tuning. Remarkably, independent readouts of model responses predict fine-grained measures of human behavior, including error patterns and reaction times, revealing a natural correspondence between model dynamics and human perception. Taken together, our findings indicate that human-level 3D perception can emerge from a simple, scalable learning objective over naturalistic visual-spatial data. All code, human behavioral data, and experimental stimuli needed to reproduce our findings can be found on our project page.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>人类级3D形状感知从多视角学习中涌现</div>
<div class="mono" style="margin-top:8px">人类可以从二维视觉输入中推断出物体的三维结构。对这种能力的建模一直是视觉智能科学与工程的长期目标，但几十年来的计算方法都未能达到人类的表现水平。在这里，我们开发了一个建模框架，能够直接从实验刺激中预测人类对任意物体的三维形状推断。我们通过在自然感官数据上使用视觉空间目标的新型神经网络实现这一目标；这些模型在给定一组来自自然场景中不同位置的图像时，能够学习预测与这些图像相关的空间信息，如摄像机位置和视觉深度，而无需依赖任何与物体相关的归纳偏置。值得注意的是，这些视觉空间信号与人类可直接获取的感官线索相似。我们设计了一种零样本评估方法，以确定这些`多视角&#x27;模型在一项已建立的三维感知任务中的表现，然后将模型行为与人类行为进行比较。我们的建模框架是首个在没有任务特定训练或微调的情况下达到人类在三维形状推断上的准确度。令人惊讶的是，模型响应的独立读出能够预测人类行为的细粒度指标，包括错误模式和反应时间，揭示了模型动态与人类感知之间的自然对应关系。综上所述，我们的研究结果表明，人类级的三维感知可以从对自然视觉空间数据的简单、可扩展的学习目标中涌现。所有用于复现我们研究结果的代码、人类行为数据和实验刺激都可以在我们的项目页面上找到。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Round Human-AI Collaboration with User-Specified Requirements</div>
<div class="meta-line">Authors: Sima Noorani, Shayan Kiyani, Hamed Hassani, George Pappas</div>
<div class="meta-line">First: 2026-02-19T18:54:34+00:00 · Latest: 2026-02-19T18:54:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17646v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17646v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As humans increasingly rely on multiround conversational AI for high stakes decisions, principled frameworks are needed to ensure such interactions reliably improve decision quality. We adopt a human centric view governed by two principles: counterfactual harm, ensuring the AI does not undermine human strengths, and complementarity, ensuring it adds value where the human is prone to err. We formalize these concepts via user defined rules, allowing users to specify exactly what harm and complementarity mean for their specific task. We then introduce an online, distribution free algorithm with finite sample guarantees that enforces the user-specified constraints over the collaboration dynamics. We evaluate our framework across two interactive settings: LLM simulated collaboration on a medical diagnostic task and a human crowdsourcing study on a pictorial reasoning task. We show that our online procedure maintains prescribed counterfactual harm and complementarity violation rates even under nonstationary interaction dynamics. Moreover, tightening or loosening these constraints produces predictable shifts in downstream human accuracy, confirming that the two principles serve as practical levers for steering multi-round collaboration toward better decision quality without the need to model or constrain human behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用户指定需求下的多轮人机协作</div>
<div class="mono" style="margin-top:8px">随着人类越来越多地依赖多轮对话AI进行高风险决策，需要有原则性的框架来确保这些互动能可靠地提升决策质量。我们采用以人类为中心的观点，遵循两个原则：反事实伤害，确保AI不会削弱人类的优势；互补性，确保AI在人类容易出错的领域增加价值。我们通过用户定义的规则形式化这些概念，使用户能够明确指定其特定任务中伤害和互补性的含义。随后，我们引入一种在线、无分布假设的算法，该算法具有有限样本保证，并能在协作动态中强制执行用户指定的约束。我们在两个交互场景中评估了我们的框架：在医疗诊断任务中使用大型语言模型模拟协作，以及在图像推理任务中进行的人类众包研究。我们展示了我们的在线过程即使在非平稳的交互动态下，也能维持指定的反事实伤害和互补性违规率。此外，收紧或放宽这些约束会导致下游人类准确率的可预测变化，证实了这两个原则可以作为实际杠杆，引导多轮协作向更高的决策质量发展，而无需建模或约束人类行为。</div>
</details>
</div>
<div class="card">
<div class="title">Pushing the Frontier of Black-Box LVLM Attacks via Fine-Grained Detail Targeting</div>
<div class="meta-line">Authors: Xiaohan Zhao, Zhaoyi Li, Yaxin Luo, Jiacheng Cui, Zhiqiang Shen</div>
<div class="meta-line">First: 2026-02-19T18:54:32+00:00 · Latest: 2026-02-19T18:54:32+00:00</div>
<div class="meta-line">Comments: Code at: https://github.com/vila-lab/M-Attack-V2</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17645v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17645v1">PDF</a> · <a href="https://github.com/vila-lab/M-Attack-V2">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Black-box adversarial attacks on Large Vision-Language Models (LVLMs) are challenging due to missing gradients and complex multimodal boundaries. While prior state-of-the-art transfer-based approaches like M-Attack perform well using local crop-level matching between source and target images, we find this induces high-variance, nearly orthogonal gradients across iterations, violating coherent local alignment and destabilizing optimization. We attribute this to (i) ViT translation sensitivity that yields spike-like gradients and (ii) structural asymmetry between source and target crops. We reformulate local matching as an asymmetric expectation over source transformations and target semantics, and build a gradient-denoising upgrade to M-Attack. On the source side, Multi-Crop Alignment (MCA) averages gradients from multiple independently sampled local views per iteration to reduce variance. On the target side, Auxiliary Target Alignment (ATA) replaces aggressive target augmentation with a small auxiliary set from a semantically correlated distribution, producing a smoother, lower-variance target manifold. We further reinterpret momentum as Patch Momentum, replaying historical crop gradients; combined with a refined patch-size ensemble (PE+), this strengthens transferable directions. Together these modules form M-Attack-V2, a simple, modular enhancement over M-Attack that substantially improves transfer-based black-box attacks on frontier LVLMs: boosting success rates on Claude-4.0 from 8% to 30%, Gemini-2.5-Pro from 83% to 97%, and GPT-5 from 98% to 100%, outperforming prior black-box LVLM attacks. Code and data are publicly available at: https://github.com/vila-lab/M-Attack-V2.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过细粒度细节目标定位推动黑盒大视觉语言模型攻击的前沿</div>
<div class="mono" style="margin-top:8px">由于缺乏梯度和复杂的多模态边界，对大视觉语言模型（LVLMs）进行黑盒对抗攻击具有挑战性。尽管基于迁移的先进方法如M-Attack在源图像和目标图像之间的局部裁剪匹配上表现良好，但我们发现这种方法在迭代过程中会导致高方差、几乎正交的梯度，违反了局部对齐的一致性并破坏了优化稳定性。我们将其归因于（i）ViT的平移敏感性，导致尖峰状梯度；以及（ii）源裁剪和目标裁剪之间的结构不对称性。我们将局部匹配重新表述为对源变换和目标语义的不对称期望，并构建了M-Attack的梯度降噪升级版本。在源端，多裁剪对齐（MCA）通过在每次迭代中平均多个独立采样的局部视图的梯度来降低方差。在目标端，辅助目标对齐（ATA）用一个语义相关分布的小型辅助集替代激进的目标增强，从而生成更平滑、方差更低的目标流形。我们进一步将动量重新解释为补丁动量，重放历史裁剪梯度；结合优化的补丁大小集成（PE+），这增强了可迁移的方向。这些模块共同构成了M-Attack-V2，这是对M-Attack的简单、模块化增强，显著提升了基于迁移的黑盒攻击在前沿LVLM上的效果：将Claude-4.0的成功率从8%提升至30%，Gemini-2.5-Pro从83%提升至97%，GPT-5从98%提升至100%，优于以往的黑盒LVLM攻击方法。代码和数据可在以下链接获取：https://github.com/vila-lab/M-Attack-V2。</div>
</details>
</div>
<div class="card">
<div class="title">A.R.I.S.: Automated Recycling Identification System for E-Waste Classification Using Deep Learning</div>
<div class="meta-line">Authors: Dhruv Talwar, Harsh Desai, Wendong Yin, Goutam Mohanty, Rafael Reveles</div>
<div class="meta-line">First: 2026-02-19T18:54:06+00:00 · Latest: 2026-02-19T18:54:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17642v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17642v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional electronic recycling processes suffer from significant resource loss due to inadequate material separation and identification capabilities, limiting material recovery. We present A.R.I.S. (Automated Recycling Identification System), a low-cost, portable sorter for shredded e-waste that addresses this efficiency gap. The system employs a YOLOx model to classify metals, plastics, and circuit boards in real time, achieving low inference latency with high detection accuracy. Experimental evaluation yielded 90% overall precision, 82.2% mean average precision (mAP), and 84% sortation purity. By integrating deep learning with established sorting methods, A.R.I.S. enhances material recovery efficiency and lowers barriers to advanced recycling adoption. This work complements broader initiatives in extending product life cycles, supporting trade-in and recycling programs, and reducing environmental impact across the supply chain.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>A.R.I.S.: 基于深度学习的电子废物分类自动回收识别系统</div>
<div class="mono" style="margin-top:8px">传统的电子废物回收流程由于材料分离和识别能力不足，导致资源损失严重，限制了材料回收率。我们提出了A.R.I.S.（自动回收识别系统），这是一种低成本、便携式的碎纸机电子废物分拣设备，旨在解决这一效率差距。该系统采用YOLOx模型对金属、塑料和电路板进行实时分类，实现了低推理延迟和高检测精度。实验评估结果显示整体精度为90%，平均精度均值（mAP）为82.2%，分拣纯度为84%。通过将深度学习与现有分拣方法相结合，A.R.I.S. 提高了材料回收效率，并降低了采用先进回收技术的门槛。这项工作补充了更广泛的产品生命周期延长、支持以旧换新和回收计划以及降低供应链环境影响的倡议。</div>
</details>
</div>
<div class="card">
<div class="title">FAMOSE: A ReAct Approach to Automated Feature Discovery</div>
<div class="meta-line">Authors: Keith Burghardt, Jienan Liu, Sadman Sakib, Yuning Hao, Bo Li</div>
<div class="meta-line">First: 2026-02-19T18:53:15+00:00 · Latest: 2026-02-19T18:53:15+00:00</div>
<div class="meta-line">Comments: 23 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17641v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17641v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Feature engineering remains a critical yet challenging bottleneck in machine learning, particularly for tabular data, as identifying optimal features from an exponentially large feature space traditionally demands substantial domain expertise. To address this challenge, we introduce FAMOSE (Feature AugMentation and Optimal Selection agEnt), a novel framework that leverages the ReAct paradigm to autonomously explore, generate, and refine features while integrating feature selection and evaluation tools within an agent architecture. To our knowledge, FAMOSE represents the first application of an agentic ReAct framework to automated feature engineering, especially for both regression and classification tasks. Extensive experiments demonstrate that FAMOSE is at or near the state-of-the-art on classification tasks (especially tasks with more than 10K instances, where ROC-AUC increases 0.23% on average), and achieves the state-of-the-art for regression tasks by reducing RMSE by 2.0% on average, while remaining more robust to errors than other algorithms. We hypothesize that FAMOSE&#x27;s strong performance is because ReAct allows the LLM context window to record (via iterative feature discovery and evaluation steps) what features did or did not work. This is similar to a few-shot prompt and guides the LLM to invent better, more innovative features. Our work offers evidence that AI agents are remarkably effective in solving problems that require highly inventive solutions, such as feature engineering.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FAMOSE：一种用于自动化特征发现的ReAct方法</div>
<div class="mono" style="margin-top:8px">特征工程仍然是机器学习中的关键但具有挑战性的瓶颈，尤其是在表格数据中，因为从指数级庞大的特征空间中识别最优特征通常需要大量领域专业知识。为了解决这一挑战，我们引入了FAMOSE（特征增强与最优选择代理），这是一种新颖的框架，利用ReAct范式自主探索、生成和优化特征，同时在代理架构中整合特征选择和评估工具。据我们所知，FAMOSE是首个将代理ReAct框架应用于自动化特征工程的尝试，特别是在回归和分类任务中。大量实验表明，FAMOSE在分类任务中表现达到或接近当前最优水平（尤其是在实例超过10K的任务中，ROC-AUC平均提升0.23%），并在回归任务中通过平均降低RMSE 2.0%达到当前最优水平，同时比其他算法更具鲁棒性。我们假设FAMOSE的优异表现是因为ReAct允许LLM上下文窗口通过迭代的特征发现和评估步骤记录哪些特征有效或无效。这类似于少样本提示，引导LLM发明更优、更具创新性的特征。我们的工作提供了证据，表明AI代理在需要高度创新性解决方案的问题上（如特征工程）表现出惊人的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">IntRec: Intent-based Retrieval with Contrastive Refinement</div>
<div class="meta-line">Authors: Pourya Shamsolmoali, Masoumeh Zareapoor, Eric Granger, Yue Lu</div>
<div class="meta-line">First: 2026-02-19T18:50:53+00:00 · Latest: 2026-02-19T18:50:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17639v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17639v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieving user-specified objects from complex scenes remains a challenging task, especially when queries are ambiguous or involve multiple similar objects. Existing open-vocabulary detectors operate in a one-shot manner, lacking the ability to refine predictions based on user feedback. To address this, we propose IntRec, an interactive object retrieval framework that refines predictions based on user feedback. At its core is an Intent State (IS) that maintains dual memory sets for positive anchors (confirmed cues) and negative constraints (rejected hypotheses). A contrastive alignment function ranks candidate objects by maximizing similarity to positive cues while penalizing rejected ones, enabling fine-grained disambiguation in cluttered scenes. Our interactive framework provides substantial improvements in retrieval accuracy without additional supervision. On LVIS, IntRec achieves 35.4 AP, outperforming OVMR, CoDet, and CAKE by +2.3, +3.7, and +0.5, respectively. On the challenging LVIS-Ambiguous benchmark, it improves performance by +7.9 AP over its one-shot baseline after a single corrective feedback, with less than 30 ms of added latency per interaction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IntRec：基于意图的检索与对比优化</div>
<div class="mono" style="margin-top:8px">从复杂场景中检索用户指定的对象仍然是一个具有挑战性的任务，尤其是在查询存在歧义或涉及多个相似对象时。现有的开放词汇检测器以单次方式运行，缺乏根据用户反馈优化预测的能力。为了解决这一问题，我们提出了IntRec，一个基于用户反馈的交互式对象检索框架。其核心是一个意图状态（IS），用于维护正向锚点（确认的线索）和负向约束（被拒绝的假设）的双重记忆集。一个对比对齐函数通过最大化与正向线索的相似性并惩罚被拒绝的对象，对候选对象进行排序，从而在杂乱场景中实现细粒度的歧义消解。我们的交互式框架在不增加额外监督的情况下显著提升了检索准确性。在LVIS数据集上，IntRec达到了35.4 AP，分别优于OVMR、CoDet和CAKE 2.3、3.7和0.5 AP。在具有挑战性的LVIS-Ambiguous基准测试中，IntRec在单次纠正反馈后，性能比其单次基线提升7.9 AP，且每次交互的延迟低于30毫秒。</div>
</details>
</div>
<div class="card">
<div class="title">CORAL: Correspondence Alignment for Improved Virtual Try-On</div>
<div class="meta-line">Authors: Jiyoung Kim, Youngjin Shin, Siyoon Jin, Dahyun Chung, Jisu Nam, Tongmin Kim, Jongjae Park, Hyeonwoo Kang, Seungryong Kim</div>
<div class="meta-line">First: 2026-02-19T18:50:12+00:00 · Latest: 2026-02-19T18:50:12+00:00</div>
<div class="meta-line">Comments: 32 pages, 25 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17636v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17636v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing methods for Virtual Try-On (VTON) often struggle to preserve fine garment details, especially in unpaired settings where accurate person-garment correspondence is required. These methods do not explicitly enforce person-garment alignment and fail to explain how correspondence emerges within Diffusion Transformers (DiTs). In this paper, we first analyze full 3D attention in DiT-based architecture and reveal that the person-garment correspondence critically depends on precise person-garment query-key matching within the full 3D attention. Building on this insight, we then introduce CORrespondence ALignment (CORAL), a DiT-based framework that explicitly aligns query-key matching with robust external correspondences. CORAL integrates two complementary components: a correspondence distillation loss that aligns reliable matches with person-garment attention, and an entropy minimization loss that sharpens the attention distribution. We further propose a VLM-based evaluation protocol to better reflect human preference. CORAL consistently improves over the baseline, enhancing both global shape transfer and local detail preservation. Extensive ablations validate our design choices.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CORAL：用于提升虚拟试穿的对应关系对齐</div>
<div class="mono" style="margin-top:8px">现有的虚拟试穿（VTON）方法在保持服装细节方面常常表现不佳，尤其是在需要准确人-衣对应关系的无配对设置中。这些方法并未显式地强制人-衣对齐，也未能解释对应关系如何在扩散Transformer（DiTs）中产生。本文首先分析了基于DiT架构的全3D注意力机制，并揭示了人-衣对应关系在全3D注意力中严重依赖于精确的人-衣查询-键匹配。基于这一洞察，我们引入了基于DiT的框架CORrespondence ALignment（CORAL），该框架显式地将查询-键匹配与稳健的外部对应关系对齐。CORAL集成了两个互补的组件：一个对应关系蒸馏损失，用于将可靠的匹配与人-衣注意力对齐；一个熵最小化损失，用于增强注意力分布。我们进一步提出了一种基于视觉语言模型（VLM）的评估协议，以更好地反映人类偏好。CORAL在基线方法上持续提升，增强了全局形状迁移和局部细节保持能力。广泛的消融实验验证了我们的设计选择。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing methods for Virtual Try-On (VTON) often struggle to preserve fine garment details, especially in unpaired settings where accurate person-garment correspondence is required.</div>
</details>
</div>
<div class="card">
<div class="title">Reverso: Efficient Time Series Foundation Models for Zero-shot Forecasting</div>
<div class="meta-line">Authors: Xinghong Fu, Yanhong Li, Georgios Papaioannou, Yoon Kim</div>
<div class="meta-line">First: 2026-02-19T18:48:08+00:00 · Latest: 2026-02-19T18:48:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17634v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17634v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning time series foundation models has been shown to be a promising approach for zero-shot time series forecasting across diverse time series domains. Insofar as scaling has been a critical driver of performance of foundation models in other modalities such as language and vision, much recent work on time series foundation modeling has focused on scaling. This has resulted in time series foundation models with hundreds of millions of parameters that are, while performant, inefficient and expensive to use in practice. This paper describes a simple recipe for learning efficient foundation models for zero-shot time series forecasting that are orders of magnitude smaller. We show that large-scale transformers are not necessary: small hybrid models that interleave long convolution and linear RNN layers (in particular DeltaNet layers) can match the performance of larger transformer-based models while being more than a hundred times smaller. We also describe several data augmentation and inference strategies that further improve performance. This recipe results in Reverso, a family of efficient time series foundation models for zero-shot forecasting that significantly push the performance-efficiency Pareto frontier.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Reverso：用于零样本时间序列预测的高效时间序列基础模型</div>
<div class="mono" style="margin-top:8px">学习时间序列基础模型已被证明是跨多领域零样本时间序列预测的一种有前景的方法。鉴于在语言和视觉等其他模态中，基础模型的性能很大程度上依赖于扩展规模，近期关于时间序列基础建模的大量工作都集中在扩展规模上。这导致了参数数量达到数亿甚至上百亿的时间序列基础模型，虽然性能良好，但在实际应用中效率低下且成本高昂。本文提出了一种简单的方法，用于学习更高效且规模小几个数量级的时间序列基础模型，以实现零样本预测。我们表明大规模Transformer并非必需：通过交错长卷积和线性RNN层（特别是DeltaNet层）的小型混合模型可以匹配基于Transformer的大模型性能，同时体积缩小超过一百倍。我们还描述了几种数据增强和推理策略，进一步提升模型性能。这种配方最终产生了Reverso，这是一个用于零样本预测的高效时间序列基础模型家族，显著推动了性能与效率的帕累托前沿。</div>
</details>
</div>
<div class="card">
<div class="title">When to Trust the Cheap Check: Weak and Strong Verification for Reasoning</div>
<div class="meta-line">Authors: Shayan Kiyani, Sima Noorani, George Pappas, Hamed Hassani</div>
<div class="meta-line">First: 2026-02-19T18:47:38+00:00 · Latest: 2026-02-19T18:47:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17633v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17633v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning with LLMs increasingly unfolds inside a broader verification loop. Internally, systems use cheap checks, such as self-consistency or proxy rewards, which we call weak verification. Externally, users inspect outputs and steer the model through feedback until results are trustworthy, which we call strong verification. These signals differ sharply in cost and reliability: strong verification can establish trust but is resource-intensive, while weak verification is fast and scalable but noisy and imperfect. We formalize this tension through weak--strong verification policies, which decide when to accept or reject based on weak verification and when to defer to strong verification. We introduce metrics capturing incorrect acceptance, incorrect rejection, and strong-verification frequency. Over population, we show that optimal policies admit a two-threshold structure and that calibration and sharpness govern the value of weak verifiers. Building on this, we develop an online algorithm that provably controls acceptance and rejection errors without assumptions on the query stream, the language model, or the weak verifier.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>何时相信廉价的检查：推理中的弱验证与强验证</div>
<div class="mono" style="margin-top:8px">使用大语言模型进行推理时，越来越多地嵌入更广泛的验证循环中。系统内部使用廉价检查，如自洽性或代理奖励，我们称之为弱验证。外部，用户检查输出并通过反馈引导模型，直到结果可信，我们称之为强验证。这些信号在成本和可靠性方面存在显著差异：强验证可以建立信任但资源消耗大，而弱验证则快速且可扩展但存在噪声和不完美。我们通过弱-强验证策略来形式化这种紧张关系，这些策略根据弱验证决定何时接受或拒绝，何时转而依赖强验证。我们引入了衡量错误接受、错误拒绝以及强验证频率的指标。在大规模数据集上，我们展示了最优策略具有双阈值结构，并且校准和锐度决定了弱验证器的价值。在此基础上，我们开发了一个在线算法，该算法在不依赖查询流、语言模型或弱验证器的假设下，可以证明地控制接受和拒绝错误。</div>
</details>
</div>
<div class="card">
<div class="title">SMAC: Score-Matched Actor-Critics for Robust Offline-to-Online Transfer</div>
<div class="meta-line">Authors: Nathan S. de Lara, Florian Shkurti</div>
<div class="meta-line">First: 2026-02-19T18:47:31+00:00 · Latest: 2026-02-19T18:47:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17632v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17632v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern offline Reinforcement Learning (RL) methods find performant actor-critics, however, fine-tuning these actor-critics online with value-based RL algorithms typically causes immediate drops in performance. We provide evidence consistent with the hypothesis that, in the loss landscape, offline maxima for prior algorithms and online maxima are separated by low-performance valleys that gradient-based fine-tuning traverses. Following this, we present Score Matched Actor-Critic (SMAC), an offline RL method designed to learn actor-critics that transition to online value-based RL algorithms with no drop in performance. SMAC avoids valleys between offline and online maxima by regularizing the Q-function during the offline phase to respect a first-order derivative equality between the score of the policy and action-gradient of the Q-function. We experimentally demonstrate that SMAC converges to offline maxima that are connected to better online maxima via paths with monotonically increasing reward found by first-order optimization. SMAC achieves smooth transfer to Soft Actor-Critic and TD3 in 6/6 D4RL tasks. In 4/6 environments, it reduces regret by 34-58% over the best baseline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SMAC：用于鲁棒离线到在线迁移的得分匹配演员-评论家方法</div>
<div class="mono" style="margin-top:8px">现代离线强化学习（RL）方法能够找到表现良好的演员-评论家，然而，使用基于价值的RL算法对这些演员-评论家进行在线微调通常会导致性能立即下降。我们提供了与假设一致的证据，即在损失景观中，先前算法的离线最大值和在线最大值之间存在低性能山谷，梯度微调会穿越这些山谷。基于此，我们提出了得分匹配演员-评论家（SMAC），一种离线RL方法，旨在学习能够平滑过渡到在线基于价值的RL算法的演员-评论家，而不会出现性能下降。SMAC通过在离线阶段对Q函数进行正则化，以尊重策略得分与Q函数动作梯度之间的第一阶导数等式，从而避免离线和在线最大值之间的山谷。我们通过实验表明，SMAC收敛于离线最大值，这些最大值通过基于一阶优化找到的奖励单调递增路径连接到更好的在线最大值。在6/6个D4RL任务中，SMAC实现了平滑的迁移至Soft Actor-Critic和TD3。在4/6个环境中，它将遗憾减少了34-58%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modern offline Reinforcement Learning (RL) methods find performant actor-critics, however, fine-tuning these actor-critics online with value-based RL algorithms typically causes immediate drops in performance.</div>
</details>
</div>
<div class="card">
<div class="title">The Hidden Nature of Non-Markovianity</div>
<div class="meta-line">Authors: Jihong Cai, Advith Govindarajan, Marius Junge</div>
<div class="meta-line">First: 2026-02-19T18:47:28+00:00 · Latest: 2026-02-19T18:47:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17631v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17631v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The theory of open quantum systems served as a tool to prepare entanglement at the beginning stage of quantum technology and more recently provides an important tool for state preparation. Dynamics given by time dependent Lindbladians are Markovian and lead to decoherence, decay of correlation and convergence to equilibrium. In contrast Non-Markovian evolutions can outperform their Markovian counterparts by enhancing memory. In this letter we compare the trajectories of Markovian and Non-Markovian evolutions starting from a fixed initial value. It turns out that under mild assumptions every trajectory can be obtained from a family of time dependent Lindbladians. Hence Non-Markovianity is invisible if single trajectories are concerned.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非马尔可夫性的隐性本质</div>
<div class="mono" style="margin-top:8px">开放量子系统的理论最初作为量子技术早期制备纠缠的工具，近年来则成为状态制备的重要手段。由时间依赖林德布拉德算符描述的动力学是马尔可夫性的，会导致退相干、相关性衰减以及向平衡态收敛。相比之下，非马尔可夫演化通过增强记忆效应，可以超越其马尔可夫对应过程。在本文中，我们比较了从固定初始值出发的马尔可夫性和非马尔可夫性演化的轨迹。结果表明，在较弱的假设下，每条轨迹都可以由一个时间依赖林德布拉德算符族产生。因此，从单条轨迹的角度来看，非马尔可夫性是不可见的。</div>
</details>
</div>
<div class="card">
<div class="title">New Kreutz Sungrazer C/2026 A1 (MAPS): Third Time&#x27;s the Charm?</div>
<div class="meta-line">Authors: Zdenek Sekanina</div>
<div class="meta-line">First: 2026-02-19T18:45:00+00:00 · Latest: 2026-02-19T18:45:00+00:00</div>
<div class="meta-line">Comments: 8 pages, 3 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17626v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17626v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper describes progress achieved in early investigations of the orbital motion and light curve of comet C/2026 A1 (MAPS), the third ground-based discovery of a Kreutz sungrazer in the 21st century. The highly unusual trait of the comet that has so far been ascertained is its extraordinarily long orbital period. The most recent orbital computations make it increasingly likely that the object is a fragment of one of the comets observed by Ammianus Marcellinus in AD 363, thereby strengthening evidence in support of the contact-binary hypothesis of the Kreutz system. In this context, the comet is the only second-generation fragment of Aristotle&#x27;s comet that we are aware of to appear after the 12th century. It does not look like a major fragment, but rather like an outlying fragment of a much larger sungrazer. In 363 it apparently separated from a parent different from the lineage of comet Pereyra. The light curve of comet MAPS has so far been fairly smooth, without outbursts. To reach the brightness of comet Ikeya-Seki, the comet would have to follow an r^(-17) law in the coming weeks, which is unlikely.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>新发现的克鲁兹日食彗星C/2026 A1 (MAPS)：第三次机会？</div>
<div class="mono" style="margin-top:8px">本文描述了对21世纪第三个地面观测发现的克鲁兹日食彗星C/2026 A1 (MAPS)的轨道运动和光变曲线的早期研究进展。目前已确认的彗星异常特征是其极长的轨道周期。最近的轨道计算表明，该天体更可能是公元363年阿米亚努斯·马尔切利努斯观测到的彗星碎片之一，从而加强了克鲁兹系统接触二元假说的证据。在此背景下，该彗星是自12世纪以来已知的第二代阿里士多德彗星碎片。它看起来不像主要碎片，而更像是一个更大日食彗星的边缘碎片。363年时，它似乎与佩雷亚彗星的谱系不同的母体分离。目前MAPS彗星的光变曲线相对平滑，没有爆发现象。若要达到伊基亚-塞基彗星的亮度，该彗星在未来几周内必须遵循r^(-17)定律，这似乎不太可能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper describes progress achieved in early investigations of the orbital motion and light curve of comet C/2026 A1 (MAPS), the third ground-based discovery of a Kreutz sungrazer in the 21st century.</div>
</details>
</div>
<div class="card">
<div class="title">Catastrophic Forgetting Resilient One-Shot Incremental Federated Learning</div>
<div class="meta-line">Authors: Obaidullah Zaland, Zulfiqar Ahmad Khan, Monowar Bhuyan</div>
<div class="meta-line">First: 2026-02-19T18:44:23+00:00 · Latest: 2026-02-19T18:44:23+00:00</div>
<div class="meta-line">Comments: Accepted for publication in the IEEE International Conference on Big Data (IEEE BigData) 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17625v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17625v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern big-data systems generate massive, heterogeneous, and geographically dispersed streams that are large-scale and privacy-sensitive, making centralization challenging. While federated learning (FL) provides a privacy-enhancing training mechanism, it assumes a static data flow and learns a collaborative model over multiple rounds, making learning with \textit{incremental} data challenging in limited-communication scenarios. This paper presents One-Shot Incremental Federated Learning (OSI-FL), the first FL framework that addresses the dual challenges of communication overhead and catastrophic forgetting. OSI-FL communicates category-specific embeddings, devised by a frozen vision-language model (VLM) from each client in a single communication round, which a pre-trained diffusion model at the server uses to synthesize new data similar to the client&#x27;s data distribution. The synthesized samples are used on the server for training. However, two challenges still persist: i) tasks arriving incrementally need to retrain the global model, and ii) as future tasks arrive, retraining the model introduces catastrophic forgetting. To this end, we augment training with Selective Sample Retention (SSR), which identifies and retains the top-p most informative samples per category and task pair based on sample loss. SSR bounds forgetting by ensuring that representative retained samples are incorporated into training in further iterations. The experimental results indicate that OSI-FL outperforms baselines, including traditional and one-shot FL approaches, in both class-incremental and domain-incremental scenarios across three benchmark datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有抗灾难性遗忘能力的一次性增量联邦学习</div>
<div class="mono" style="margin-top:8px">现代大数据系统生成大量、异构且地理分布广泛的数据流，这些数据具有大规模和隐私敏感的特性，使得集中处理变得困难。虽然联邦学习（FL）提供了一种增强隐私的训练机制，但它假设数据流是静态的，并且通过多轮学习协作构建模型，这使得在通信受限的场景下进行增量学习具有挑战性。本文提出了一次性增量联邦学习（OSI-FL），这是首个同时解决通信开销和灾难性遗忘双重挑战的联邦学习框架。OSI-FL在单次通信轮次中，由每个客户端使用一个冻结的视觉-语言模型（VLM）生成类别特定的嵌入，这些嵌入被预训练的扩散模型在服务器端用于合成与客户端数据分布相似的新数据。合成的样本用于服务器端的训练。然而，仍然存在两个挑战：i）逐步到达的任务需要重新训练全局模型；ii）随着未来任务的到来，重新训练模型会导致灾难性遗忘。为了解决这些问题，我们引入了选择性样本保留（Selective Sample Retention, SSR）机制，该机制基于样本损失，识别并保留每个类别和任务对中最具信息量的top-p样本。SSR通过确保代表性保留样本被纳入后续的训练过程中，从而限制遗忘的发生。实验结果表明，OSI-FL在三个基准数据集上的类增量和域增量场景中，均优于传统和一次性联邦学习方法等基线模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modern big-data systems generate massive, heterogeneous, and geographically dispersed streams that are large-scale and privacy-sensitive, making centralization challenging.</div>
</details>
</div>
<div class="card">
<div class="title">Unmasking the Factual-Conceptual Gap in Persian Language Models</div>
<div class="meta-line">Authors: Alireza Sakhaeirad, Ali Ma&#x27;manpoosh, Arshia Hemmat</div>
<div class="meta-line">First: 2026-02-19T18:42:46+00:00 · Latest: 2026-02-19T18:42:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17623v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17623v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While emerging Persian NLP benchmarks have expanded into pragmatics and politeness, they rarely distinguish between memorized cultural facts and the ability to reason about implicit social norms. We introduce DivanBench, a diagnostic benchmark focused on superstitions and customs, arbitrary, context-dependent rules that resist simple logical deduction. Through 315 questions across three task types (factual retrieval, paired scenario verification, and situational reasoning), we evaluate seven Persian LLMs and reveal three critical failures: most models exhibit severe acquiescence bias, correctly identifying appropriate behaviors but failing to reject clear violations; continuous Persian pretraining amplifies this bias rather than improving reasoning, often degrading the model&#x27;s ability to discern contradictions; and all models show a 21\% performance gap between retrieving factual knowledge and applying it in scenarios. These findings demonstrate that cultural competence requires more than scaling monolingual data, as current models learn to mimic cultural patterns without internalizing the underlying schemas.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>揭开波斯语言模型中的事实-概念鸿沟</div>
<div class="mono" style="margin-top:8px">尽管新兴的波斯自然语言处理基准测试已扩展到语用学和礼貌领域，但它们很少区分记忆中的文化事实与对隐含社会规范进行推理的能力。我们引入了DivanBench，这是一个专注于迷信和习俗的诊断基准测试，这些基准测试包含任意且依赖上下文的规则，难以通过简单的逻辑推导来解决。通过三种任务类型（事实检索、配对情境验证和情境推理）的315个问题，我们评估了七种波斯大型语言模型，并揭示了三个关键问题：大多数模型表现出严重的顺从偏见，能够正确识别适当行为，但无法拒绝明显的违规行为；连续的波斯预训练反而加剧了这种偏见，而非提升推理能力，常常会降低模型识别矛盾的能力；所有模型在事实知识检索与情境应用之间均存在21%的性能差距。这些发现表明，文化能力不仅仅依赖于单语数据的扩展，因为当前模型学习的是模仿文化模式，而未能内化其背后的结构框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">While emerging Persian NLP benchmarks have expanded into pragmatics and politeness, they rarely distinguish between memorized cultural facts and the ability to reason about implicit social norms.</div>
</details>
</div>
<div class="card">
<div class="title">What Makes a Good LLM Agent for Real-world Penetration Testing?</div>
<div class="meta-line">Authors: Gelei Deng, Yi Liu, Yuekang Li, Ruozhao Yang, Xiaofei Xie, Jie Zhang, Han Qiu, Tianwei Zhang</div>
<div class="meta-line">First: 2026-02-19T18:42:40+00:00 · Latest: 2026-02-19T18:42:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17622v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17622v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based agents show promise for automating penetration testing, yet reported performance varies widely across systems and benchmarks. We analyze 28 LLM-based penetration testing systems and evaluate five representative implementations across three benchmarks of increasing complexity. Our analysis reveals two distinct failure modes: Type A failures stem from capability gaps (missing tools, inadequate prompts) that engineering readily addresses, while Type B failures persist regardless of tooling due to planning and state management limitations. We show that Type B failures share a root cause that is largely invariant to the underlying LLM: agents lack real-time task difficulty estimation. As a result, agents misallocate effort, over-commit to low-value branches, and exhaust context before completing attack chains.
  Based on this insight, we present Excalibur, a penetration testing agent that couples strong tooling with difficulty-aware planning. A Tool and Skill Layer eliminates Type A failures through typed interfaces and retrieval-augmented knowledge. A Task Difficulty Assessment (TDA) mechanism addresses Type B failures by estimating tractability through four measurable dimensions (horizon estimation, evidence confidence, context load, and historical success) and uses these estimates to guide exploration-exploitation decisions within an Evidence-Guided Attack Tree Search (EGATS) framework. Excalibur achieves up to 91% task completion on CTF benchmarks with frontier models (39 to 49% relative improvement over baselines) and compromises 4 of 5 hosts on the GOAD Active Directory environment versus 2 by prior systems. These results show that difficulty-aware planning yields consistent end-to-end gains across models and addresses a limitation that model scaling alone does not eliminate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>什么是适用于现实世界渗透测试的好LLM代理？</div>
<div class="mono" style="margin-top:8px">基于LLM的代理在自动化渗透测试方面展现出潜力，但不同系统和基准的报告性能差异很大。我们分析了28个基于LLM的渗透测试系统，并在三个复杂度递增的基准上评估了五个代表性实现。我们的分析揭示了两种不同的失败模式：类型A失败源于能力差距（缺少工具、提示不足），这可以通过工程手段轻松解决；而类型B失败则由于规划和状态管理的限制，无论工具如何都难以消除。我们表明，类型B失败的根源主要与底层LLM无关，即代理缺乏实时任务难度估计。因此，代理会错误分配努力，过度承诺低价值分支，并在完成攻击链前耗尽上下文。
基于这一见解，我们提出了Excalibur，这是一种结合强大工具和难度感知规划的渗透测试代理。工具和技能层通过类型化接口和检索增强的知识消除了类型A失败。任务难度评估（TDA）机制通过四个可衡量的维度（地平线估计、证据置信度、上下文负载和历史成功率）来估计任务可处理性，并利用这些估计在证据引导的攻击树搜索（EGATS）框架内指导探索与利用决策。Excalibur在CTF基准上实现了高达91%的任务完成率（相对于基线有39到49%的相对提升），并在GOAD Active Directory环境中成功攻破了5台主机中的4台，而之前的系统仅攻破了2台。这些结果表明，难度感知规划在不同模型上都能带来一致的端到端提升，并解决了仅靠模型扩展无法消除的限制。</div>
</details>
</div>
<div class="card">
<div class="title">Light dark sector via thermal decays of Dark Matter: the case of a 17 MeV particle coupled to electrons</div>
<div class="meta-line">Authors: Marco Graziani</div>
<div class="meta-line">First: 2026-02-19T18:41:33+00:00 · Latest: 2026-02-19T18:41:33+00:00</div>
<div class="meta-line">Comments: 81 pages, Master thesis supervised by Profs. Ludovico Vittorio and Giacomo Landini. Sapienza University of Rome</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17620v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17620v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent experimental observations, most notably those reported by the ATOMKI and Positron Annihilation into Dark Matter Experiment (PADME) collaborations, have hinted anomalies that may indicate the presence of a new resonance with a mass around $17\,\text{MeV}$, potentially interacting with both nucleons and electrons. Since 2020, ATOMKI has observed this resonance in nuclear transitions from excited to ground states in ${}^{8}\mathrm{Be}$, ${}^{4}\mathrm{He}$, and ${}^{12}\mathrm{C}$. More recently, in 2025, PADME, operating at the Laboratori Nazionali di Frascati, has also hinted a similar excess, in this case in the $e^{+}e^{-}$ final-state events originating from positron annihilation on fixed-target atomic electrons of Carbonium. This concordance strengthens the case for a common underlying origin, potentially involving a new boson, conventionally referred to as $X_{17}$.
  Despite these intriguing developments, the global experimental landscape remains highly dynamic, particularly in light of recent MEG~II constraints, and a definitive confirmation or exclusion of the $X_{17}$ hypothesis is still lacking. Within this evolving and exciting context, this thesis investigates whether a hypothetical $17\,\text{MeV}$ particle, coupled to electrons as suggested by the PADME observations, could function as a mediator between the Standard Model and previously unexplored hidden sectors. Such a mediator could, in principle, offer a novel pathway toward addressing one of the principal outstanding inconsistencies of the Standard Model: the nature and origin of dark matter.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过暗物质热衰变实现的亮暗扇区：一种与电子耦合的17 MeV粒子案例</div>
<div class="mono" style="margin-top:8px">最近的实验观测，尤其是ATOMKI和正电子湮灭成暗物质实验（PADME）合作组的报告，暗示了一些异常现象，可能表明存在一个质量约为17 MeV的新共振态，可能同时与核子和电子相互作用。自2020年以来，ATOMKI在${}^{8}\mathrm{Be}$、${}^{4}\mathrm{He}$和${}^{12}\mathrm{C}$的核跃迁中观测到了这种共振态。更近期的2025年，PADME在拉瓦托里国家实验室运行时，也观察到了类似的过剩信号，该信号来源于碳化物原子电子上的正电子湮灭产生的$e^{+}e^{-}$末态事件。这种一致性加强了该现象可能具有共同的起源，可能涉及一种新玻色子，通常称为$X_{17}$。
尽管这些引人注目的进展，全球实验研究仍处于高度动态的发展中，特别是在考虑了最近MEG~II的限制后，对$X_{17}$假说的明确确认或排除仍缺乏。在这一不断演进且充满活力的背景下，本论文探讨了一种假设的17 MeV粒子，如果如PADME观测所暗示的那样与电子耦合，是否可以作为标准模型与之前未被探索的隐藏扇区之间的媒介。这种媒介在理论上可能提供一种新的途径，以解决标准模型中一个主要的未解矛盾：暗物质的性质和起源。</div>
</details>
</div>
<div class="card">
<div class="title">Stable Asynchrony: Variance-Controlled Off-Policy RL for LLMs</div>
<div class="meta-line">Authors: Luke Huang, Zhuoyang Zhang, Qinghao Hu, Shang Yang, Song Han</div>
<div class="meta-line">First: 2026-02-19T18:40:51+00:00 · Latest: 2026-02-19T18:40:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17616v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17616v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is widely used to improve large language models on reasoning tasks, and asynchronous RL training is attractive because it increases end-to-end throughput. However, for widely adopted critic-free policy-gradient methods such as REINFORCE and GRPO, high asynchrony makes the policy-gradient estimator markedly $\textbf{higher variance}$: training on stale rollouts creates heavy-tailed importance ratios, causing a small fraction of samples to dominate updates. This amplification makes gradients noisy and learning unstable relative to matched on-policy training. Across math and general reasoning benchmarks, we find collapse is reliably predicted by effective sample size (ESS) and unstable gradient norms. Motivated by this diagnosis, we propose $\textbf{V}$ariance $\textbf{C}$ontrolled $\textbf{P}$olicy $\textbf{O}$ptimization ($\textbf{VCPO}$), a general stabilization method for REINFORCE/GRPO-style algorithms that (i) scales learning rate based on effective sample size to dampen unreliable updates, and (ii) applies a closed-form minimum-variance baseline for the off-policy setting, avoiding an auxiliary value model and adding minimal overhead. Empirically, VCPO substantially improves robustness for asynchronous training across math, general reasoning, and tool-use tasks, outperforming a broad suite of baselines spanning masking/clipping stabilizers and algorithmic variants. This reduces long-context, multi-turn training time by 2.5$\times$ while matching synchronous performance, demonstrating that explicit control of policy-gradient variance is key for reliable asynchronous RL at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稳定异步性：用于大语言模型的方差控制离策略强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）被广泛用于提升大语言模型在推理任务上的表现，而异步RL训练因其能提高端到端吞吐量而具有吸引力。然而，对于广泛采用的无批评者策略梯度方法（如REINFORCE和GRPO），高异步性会导致策略梯度估计器的方差显著增加：使用过时的轨迹进行训练会产生重尾的重要性比率，导致少数样本主导更新。这种放大效应使得梯度变得嘈杂，学习过程相对于匹配的在线策略训练更加不稳定。在数学和通用推理基准测试中，我们发现崩溃现象可以可靠地通过有效样本量（ESS）和不稳定的梯度范数来预测。基于这一诊断，我们提出了方差控制策略优化（VCPO），这是一种适用于REINFORCE/GRPO风格算法的一般性稳定化方法，其特点包括：(i) 根据有效样本量调整学习率以抑制不可靠的更新；(ii) 在离策略设置中应用闭式最小方差基线，避免使用辅助价值模型并增加极少的计算开销。实验证明，VCPO在数学、通用推理和工具使用任务中显著提升了异步训练的鲁棒性，优于涵盖掩码/裁剪稳定器和算法变体的广泛基线方法。这使得长上下文、多轮次训练时间减少了2.5倍，同时保持了同步训练的性能，证明了对策略梯度方差的显式控制对于大规模可靠异步RL至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">Guarding the Middle: Protecting Intermediate Representations in Federated Split Learning</div>
<div class="meta-line">Authors: Obaidullah Zaland, Sajib Mistry, Monowar Bhuyan</div>
<div class="meta-line">First: 2026-02-19T18:40:12+00:00 · Latest: 2026-02-19T18:40:12+00:00</div>
<div class="meta-line">Comments: Accepted for Publication in IEEE International Conference on Big Data (IEEE BigData) 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17614v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17614v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Big data scenarios, where massive, heterogeneous datasets are distributed across clients, demand scalable, privacy-preserving learning methods. Federated learning (FL) enables decentralized training of machine learning (ML) models across clients without data centralization. Decentralized training, however, introduces a computational burden on client devices. U-shaped federated split learning (UFSL) offloads a fraction of the client computation to the server while keeping both data and labels on the clients&#x27; side. However, the intermediate representations (i.e., smashed data) shared by clients with the server are prone to exposing clients&#x27; private data. To reduce exposure of client data through intermediate data representations, this work proposes k-anonymous differentially private UFSL (KD-UFSL), which leverages privacy-enhancing techniques such as microaggregation and differential privacy to minimize data leakage from the smashed data transferred to the server. We first demonstrate that an adversary can access private client data from intermediate representations via a data-reconstruction attack, and then present a privacy-enhancing solution, KD-UFSL, to mitigate this risk. Our experiments indicate that, alongside increasing the mean squared error between the actual and reconstructed images by up to 50% in some cases, KD-UFSL also decreases the structural similarity between them by up to 40% on four benchmarking datasets. More importantly, KD-UFSL improves privacy while preserving the utility of the global model. This highlights its suitability for large-scale big data applications where privacy and utility must be balanced.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>守护中间层：在联邦分割学习中保护中间表示</div>
<div class="mono" style="margin-top:8px">在大数据场景中，大量异构数据集分布在客户端，需要可扩展且隐私保护的学习方法。联邦学习（FL）能够在不集中数据的情况下，使客户端实现机器学习（ML）模型的分布式训练。然而，分布式训练会给客户端设备带来计算负担。U型联邦分割学习（UFSL）将部分客户端计算任务卸载到服务器，同时保持数据和标签在客户端本地。然而，客户端与服务器共享的中间表示（即破碎数据）容易暴露客户端的隐私数据。为减少通过中间数据表示暴露客户端数据的风险，本文提出了一种k-匿名差分隐私U型联邦分割学习（KD-UFSL），利用微聚合和差分隐私等隐私增强技术，以减少传输到服务器的破碎数据带来的数据泄露。我们首先证明了攻击者可以通过数据重构攻击从中间表示中获取私有客户端数据，然后提出了KD-UFSL这一隐私增强方案以缓解该风险。实验结果表明，在某些情况下，KD-UFSL可使实际图像与重构图像之间的均方误差增加高达50%，同时在四个基准数据集上，其结构相似性也降低了高达40%。更重要的是，KD-UFSL在提升隐私保护的同时，保持了全局模型的实用性。这突显了其在需要平衡隐私与实用性的大规模大数据应用中的适用性。</div>
</details>
</div>
<div class="card">
<div class="title">Contrastive Diffusion Alignment: Learning Structured Latents for Controllable Generation</div>
<div class="meta-line">Authors: Ruchi Sandilya, Sumaira Perez, Charles Lynch, Lindsay Victoria, Benjamin Zebley, Derrick Matthew Buchanan, Mahendra T. Bhati, Nolan Williams, Timothy J. Spellman, Faith M. Gunning, Conor Liston, Logan Grosenick</div>
<div class="meta-line">First: 2025-10-16T00:48:05+00:00 · Latest: 2026-02-19T18:33:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.14190v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.14190v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models excel at generation, but their latent spaces are high dimensional and not explicitly organized for interpretation or control. We introduce ConDA (Contrastive Diffusion Alignment), a plug-and-play geometry layer that applies contrastive learning to pretrained diffusion latents using auxiliary variables (e.g., time, stimulation parameters, facial action units). ConDA learns a low-dimensional embedding whose directions align with underlying dynamical factors, consistent with recent contrastive learning results on structured and disentangled representations. In this embedding, simple nonlinear trajectories support smooth interpolation, extrapolation, and counterfactual editing while rendering remains in the original diffusion space. ConDA separates editing and rendering by lifting embedding trajectories back to diffusion latents with a neighborhood-preserving kNN decoder and is robust across inversion solvers. Across fluid dynamics, neural calcium imaging, therapeutic neurostimulation, facial expression dynamics, and monkey motor cortex activity, ConDA yields more interpretable and controllable latent structure than linear traversals and conditioning-based baselines, indicating that diffusion latents encode dynamics-relevant structure that can be exploited by an explicit contrastive geometry layer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对比扩散对齐：学习用于可控生成的结构化潜在空间</div>
<div class="mono" style="margin-top:8px">扩散模型在生成任务中表现出色，但其潜在空间是高维的，并未显式组织以便于解释或控制。我们引入了ConDA（对比扩散对齐），这是一种即插即用的几何层，利用辅助变量（如时间、刺激参数、面部动作单元）对预训练扩散模型的潜在空间应用对比学习。ConDA学习了一个低维嵌入，其方向与潜在的动态因素对齐，与近期在结构化和解耦表示上的对比学习结果一致。在该嵌入中，简单的非线性轨迹支持平滑的插值、外推和反事实编辑，同时渲染仍保留在原始扩散空间中。ConDA通过使用邻域保持的kNN解码器将嵌入轨迹提升回扩散潜在空间，从而分离编辑和渲染过程，并且在反演求解器中表现出鲁棒性。在流体动力学、神经钙成像、治疗性神经刺激、面部表情动态和猴子运动皮层活动等多个领域中，ConDA比线性遍历和基于条件的基线方法提供了更可解释且可控的潜在结构，表明扩散潜在空间编码了与动态相关的信息，这些信息可以通过显式的对比几何层加以利用。</div>
</details>
</div>
<div class="card">
<div class="title">Gradient Testing and Estimation by Comparisons</div>
<div class="meta-line">Authors: Xiwen Tao, Chenyi Zhang, Helin Wang, Yexin Zhang, Tongyang Li</div>
<div class="meta-line">First: 2024-05-19T05:39:46+00:00 · Latest: 2026-02-19T18:33:13+00:00</div>
<div class="meta-line">Comments: v2: Significant changes compared to v1. v2 focuses on the gradient testing and gradient estimation problems, with an improved bound on classical gradient estimation, a new result on classical gradient testing, as well as a new quantum algorithm and lower bound on gradient estimation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2405.11454v2">Abs</a> · <a href="https://arxiv.org/pdf/2405.11454v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study gradient testing and gradient estimation of smooth functions using only a comparison oracle that, given two points, indicates which one has the larger function value. For any smooth $f\colon\mathbb R^n\to\mathbb R$, $\mathbf{x}\in\mathbb R^n$, and $\varepsilon&gt;0$, we design a gradient testing algorithm that determines whether the normalized gradient $\nabla f(\mathbf{x})/\|\nabla f(\mathbf{x})\|$ is $\varepsilon$-close or $2\varepsilon$-far from a given unit vector $\mathbf{v}$ using $O(1)$ queries, as well as a gradient estimation algorithm that outputs an $\varepsilon$-estimate of $\nabla f(\mathbf{x})/\|\nabla f(\mathbf{x})\|$ using $O(n\log(1/\varepsilon))$ queries which we prove to be optimal. Furthermore, we study gradient estimation in the quantum comparison oracle model where queries can be made in superpositions, and develop a quantum algorithm using $O(\log (n/\varepsilon))$ queries.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过比较进行梯度测试与估计</div>
<div class="mono" style="margin-top:8px">我们研究使用仅能比较两个点函数值的比较预言机，对光滑函数进行梯度测试和梯度估计。对于任意光滑函数 $f\colon\mathbb R^n\to\mathbb R$，点 $\mathbf{x}\in\mathbb R^n$ 以及 $\varepsilon&gt;0$，我们设计了一个梯度测试算法，使用 $O(1)$ 次查询即可判断归一化梯度 $\nabla f(\mathbf{x})/\|\nabla f(\mathbf{x})\|$ 是否与给定的单位向量 $\mathbf{v}$ 在 $\varepsilon$ 范围内或 $2\varepsilon$ 范围外。此外，我们还设计了一个梯度估计算法，使用 $O(n\log(1/\varepsilon))$ 次查询输出归一化梯度的 $\varepsilon$ 估计，并证明该复杂度是最佳的。进一步地，我们研究了在量子比较预言机模型下的梯度估计问题，其中查询可以在叠加态中进行，并开发了一个使用 $O(\log (n/\varepsilon))$ 次查询的量子算法。</div>
</details>
</div>
<div class="card">
<div class="title">ReplaceMe: Network Simplification via Depth Pruning and Transformer Block Linearization</div>
<div class="meta-line">Authors: Dmitriy Shopkhoev, Ammar Ali, Magauiya Zhussip, Valentin Malykh, Stamatios Lefkimmiatis, Nikos Komodakis, Sergey Zagoruyko</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-05-05T17:47:42+00:00 · Latest: 2026-02-19T18:32:53+00:00</div>
<div class="meta-line">Comments: This work was accepted and presented at NeurIPS 2025. Code is available at https://github.com/mts-ai/replaceme Reviews at OpenReview: https://openreview.net/forum?id=zEj1FSYCRn NeurIPS 2025 Proceedings: https://openreview.net/pdf?id=zEj1FSYCRn</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.02819v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.02819v4">PDF</a> · <a href="https://github.com/mts-ai/replaceme">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce ReplaceMe, a generalized training-free depth pruning method that effectively replaces transformer blocks with a linear operation, while maintaining high performance for low compression ratios. In contrast to conventional pruning approaches that require additional training or fine-tuning, our approach requires only a small calibration dataset that is used to estimate a linear transformation, which approximates the pruned blocks. The estimated linear mapping can be seamlessly merged with the remaining transformer blocks, eliminating the need for any additional network parameters. Our experiments show that ReplaceMe consistently outperforms other training-free approaches and remains highly competitive with state-of-the-art pruning methods that involve extensive retraining/fine-tuning and architectural modifications. Applied to several large language models (LLMs), ReplaceMe achieves up to 25\% pruning while retaining approximately 90\% of the original model&#x27;s performance on open benchmarks - without any training or healing steps, resulting in minimal computational overhead. We provide an open-source library implementing ReplaceMe alongside several state-of-the-art depth pruning techniques, available at https://github.com/mts-ai/ReplaceMe</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReplaceMe: 通过深度剪枝和Transformer块线性化实现网络简化</div>
<div class="mono" style="margin-top:8px">我们引入了ReplaceMe，这是一种通用的无需训练的深度剪枝方法，能够有效用线性操作替代Transformer块，同时在低压缩比下保持高性能。与需要额外训练或微调的传统剪枝方法不同，我们的方法仅需一个小的校准数据集，用于估计一个线性变换，该变换可近似剪枝后的块。估计的线性映射可以无缝地与剩余的Transformer块合并，无需任何额外的网络参数。实验表明，ReplaceMe在其他无需训练的方法上表现一致优异，并且在涉及大量重新训练/微调和架构修改的最先进剪枝方法中仍保持高度竞争力。在多个大型语言模型（LLMs）上应用ReplaceMe，可以在不进行任何训练或修复步骤的情况下，实现高达25%的剪枝率，同时保留原始模型约90%的性能，且计算开销极小。我们提供了实现ReplaceMe及其多种最先进深度剪枝技术的开源库，可在https://github.com/mts-ai/ReplaceMe获取。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Anytime-Valid Statistical Watermarking</div>
<div class="meta-line">Authors: Baihe Huang, Eric Xu, Kannan Ramchandran, Jiantao Jiao, Michael I. Jordan</div>
<div class="meta-line">First: 2026-02-19T18:32:26+00:00 · Latest: 2026-02-19T18:32:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17608v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17608v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The proliferation of Large Language Models (LLMs) necessitates efficient mechanisms to distinguish machine-generated content from human text. While statistical watermarking has emerged as a promising solution, existing methods suffer from two critical limitations: the lack of a principled approach for selecting sampling distributions and the reliance on fixed-horizon hypothesis testing, which precludes valid early stopping. In this paper, we bridge this gap by developing the first e-value-based watermarking framework, Anchored E-Watermarking, that unifies optimal sampling with anytime-valid inference. Unlike traditional approaches where optional stopping invalidates Type-I error guarantees, our framework enables valid, anytime-inference by constructing a test supermartingale for the detection process. By leveraging an anchor distribution to approximate the target model, we characterize the optimal e-value with respect to the worst-case log-growth rate and derive the optimal expected stopping time. Our theoretical claims are substantiated by simulations and evaluations on established benchmarks, showing that our framework can significantly enhance sample efficiency, reducing the average token budget required for detection by 13-15% relative to state-of-the-art baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向任何时间有效的统计水印方法</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的普及需要高效的机制来区分机器生成内容和人类文本。尽管统计水印已成为一种有前景的解决方案，但现有方法存在两个关键限制：缺乏选择采样分布的原理性方法，以及依赖固定地平线假设检验，这阻碍了有效提前停止。本文通过开发首个基于e值的水印框架——Anchored E-Watermarking，弥合了这一差距，将最优采样与任何时间有效的推理统一起来。与传统方法不同，我们的框架通过构建检测过程的测试超鞅，实现了有效的任何时间推理。通过利用锚定分布来近似目标模型，我们根据最坏情况下的对数增长速率刻画了最优e值，并推导出最优的期望停止时间。我们的理论主张通过在现有基准上的模拟和评估得到了验证，表明我们的框架可以显著提高样本效率，相较于最先进的基线方法，检测所需的平均token预算减少了13-15%。</div>
</details>
</div>
<div class="card">
<div class="title">AutoNumerics: An Autonomous, PDE-Agnostic Multi-Agent Pipeline for Scientific Computing</div>
<div class="meta-line">Authors: Jianda Du, Youran Sun, Haizhao Yang</div>
<div class="meta-line">First: 2026-02-19T18:31:52+00:00 · Latest: 2026-02-19T18:31:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17607v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17607v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">PDEs are central to scientific and engineering modeling, yet designing accurate numerical solvers typically requires substantial mathematical expertise and manual tuning. Recent neural network-based approaches improve flexibility but often demand high computational cost and suffer from limited interpretability. We introduce \texttt{AutoNumerics}, a multi-agent framework that autonomously designs, implements, debugs, and verifies numerical solvers for general PDEs directly from natural language descriptions. Unlike black-box neural solvers, our framework generates transparent solvers grounded in classical numerical analysis. We introduce a coarse-to-fine execution strategy and a residual-based self-verification mechanism. Experiments on 24 canonical and real-world PDE problems demonstrate that \texttt{AutoNumerics} achieves competitive or superior accuracy compared to existing neural and LLM-based baselines, and correctly selects numerical schemes based on PDE structural properties, suggesting its viability as an accessible paradigm for automated PDE solving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AutoNumerics：一种自主的、与偏微分方程无关的多智能体流水线用于科学计算</div>
<div class="mono" style="margin-top:8px">偏微分方程（PDEs）是科学和工程建模的核心，但设计精确的数值求解器通常需要大量的数学专业知识和手动调优。最近的基于神经网络的方法提高了灵活性，但往往需要高昂的计算成本且可解释性有限。我们引入了\texttt{AutoNumerics}，这是一种多智能体框架，能够自主地为一般的PDEs设计、实现、调试和验证数值求解器，直接从自然语言描述中生成。与黑盒神经求解器不同，我们的框架基于经典数值分析生成透明的求解器。我们引入了一种从粗到细的执行策略和基于残差的自验证机制。在24个经典和现实世界的PDE问题上的实验表明，\texttt{AutoNumerics}在准确率上与现有的神经网络和LLM基线方法相比具有竞争力或更优的表现，并能根据PDE的结构特性正确选择数值方案，表明其作为自动化PDE求解的可访问范式的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">Adapting Actively on the Fly: Relevance-Guided Online Meta-Learning with Latent Concepts for Geospatial Discovery</div>
<div class="meta-line">Authors: Jowaria Khan, Anindya Sarkar, Yevgeniy Vorobeychik, Elizabeth Bondi-Kelly</div>
<div class="meta-line">First: 2026-02-19T18:30:18+00:00 · Latest: 2026-02-19T18:30:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17605v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.17605v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In many real-world settings, such as environmental monitoring, disaster response, or public health, with costly and difficult data collection and dynamic environments, strategically sampling from unobserved regions is essential for efficiently uncovering hidden targets under tight resource constraints. Yet, sparse and biased geospatial ground truth limits the applicability of existing learning-based methods, such as reinforcement learning. To address this, we propose a unified geospatial discovery framework that integrates active learning, online meta-learning, and concept-guided reasoning. Our approach introduces two key innovations built on a shared notion of *concept relevance*, which captures how domain-specific factors influence target presence: a *concept-weighted uncertainty sampling strategy*, where uncertainty is modulated by learned relevance based on readily-available domain-specific concepts (e.g., land cover, source proximity); and a *relevance-aware meta-batch formation strategy* that promotes semantic diversity during online-meta updates, improving generalization in dynamic environments. Our experiments include testing on a real-world dataset of cancer-causing PFAS (Per- and polyfluoroalkyl substances) contamination, showcasing our method&#x27;s reliability at uncovering targets with limited data and a varying environment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>主动适应：基于潜在概念的语义引导在线元学习用于地理空间发现</div>
<div class="mono" style="margin-top:8px">在许多现实场景中，如环境监测、灾害响应或公共卫生，由于数据收集成本高且困难，以及环境动态变化，从未观测区域中战略性地采样对于在资源受限条件下高效发现隐藏目标至关重要。然而，稀疏且有偏差的地理空间真实数据限制了现有基于学习的方法（如强化学习）的应用。为了解决这一问题，我们提出一个统一的地理空间发现框架，集成了主动学习、在线元学习和概念引导推理。我们的方法基于一个共享的概念相关性理念，引入了两个关键创新：一种基于可获取的领域特定概念（如土地覆盖、污染源距离）学习相关性的概念加权不确定性采样策略；以及一种在在线元更新过程中促进语义多样性的相关性感知元批次生成策略，从而提升在动态环境中的泛化能力。我们的实验基于一个真实世界的数据集，测试了该方法在有限数据和变化环境下的可靠性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260222_0402.html">20260222_0402</a>
<a href="archive/20260221_0415.html">20260221_0415</a>
<a href="archive/20260220_0410.html">20260220_0410</a>
<a href="archive/20260219_0419.html">20260219_0419</a>
<a href="archive/20260218_0416.html">20260218_0416</a>
<a href="archive/20260217_0410.html">20260217_0410</a>
<a href="archive/20260216_0403.html">20260216_0403</a>
<a href="archive/20260215_0401.html">20260215_0401</a>
<a href="archive/20260213_0421.html">20260213_0421</a>
<a href="archive/20260212_0425.html">20260212_0425</a>
<a href="archive/20260210_0435.html">20260210_0435</a>
<a href="archive/20260209_0404.html">20260209_0404</a>
<a href="archive/20260208_0353.html">20260208_0353</a>
<a href="archive/20260207_0410.html">20260207_0410</a>
<a href="archive/20260206_0412.html">20260206_0412</a>
<a href="archive/20260205_0417.html">20260205_0417</a>
<a href="archive/20260204_0421.html">20260204_0421</a>
<a href="archive/20260202_0402.html">20260202_0402</a>
<a href="archive/20260201_0354.html">20260201_0354</a>
<a href="archive/20260131_0408.html">20260131_0408</a>
<a href="archive/20260130_0406.html">20260130_0406</a>
<a href="archive/20260129_0407.html">20260129_0407</a>
<a href="archive/20260128_0407.html">20260128_0407</a>
<a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
