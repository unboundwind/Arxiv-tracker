<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-18 04:16</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260218_0416</div>
    <div class="row"><div class="card">
<div class="title">EditCtrl: Disentangled Local and Global Control for Real-Time Generative Video Editing</div>
<div class="meta-line">Authors: Yehonathan Litman, Shikun Liu, Dario Seyb, Nicholas Milef, Yang Zhou, Carl Marshall, Shubham Tulsiani, Caleb Leak</div>
<div class="meta-line">First: 2026-02-16T18:59:58+00:00 · Latest: 2026-02-16T18:59:58+00:00</div>
<div class="meta-line">Comments: Project page: https://yehonathanlitman.github.io/edit_ctrl</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15031v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15031v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://yehonathanlitman.github.io/edit_ctrl">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-fidelity generative video editing has seen significant quality improvements by leveraging pre-trained video foundation models. However, their computational cost is a major bottleneck, as they are often designed to inefficiently process the full video context regardless of the inpainting mask&#x27;s size, even for sparse, localized edits. In this paper, we introduce EditCtrl, an efficient video inpainting control framework that focuses computation only where it is needed. Our approach features a novel local video context module that operates solely on masked tokens, yielding a computational cost proportional to the edit size. This local-first generation is then guided by a lightweight temporal global context embedder that ensures video-wide context consistency with minimal overhead. Not only is EditCtrl 10 times more compute efficient than state-of-the-art generative editing methods, it even improves editing quality compared to methods designed with full-attention. Finally, we showcase how EditCtrl unlocks new capabilities, including multi-region editing with text prompts and autoregressive content propagation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EditCtrl: 分离局部与全局控制的实时生成视频编辑</div>
<div class="mono" style="margin-top:8px">通过利用预训练的视频基础模型，高保真生成视频编辑在质量上取得了显著提升。然而，其计算成本是一个主要瓶颈，因为这些模型通常设计为无论修复掩码的大小如何，都低效地处理整个视频上下文。在本文中，我们引入了EditCtrl，一个高效的视频修复控制框架，仅在需要的地方进行计算。我们的方法包含一个新颖的局部视频上下文模块，仅在被掩码的标记上运行，计算成本与编辑区域大小成比例。随后，通过一个轻量级的时间全局上下文嵌入器引导局部优先生成，以确保视频全局上下文的一致性，且额外开销极小。不仅EditCtrl比最先进的生成编辑方法在计算效率上高出10倍，甚至在编辑质量方面也优于采用全注意力机制设计的方法。最后，我们展示了EditCtrl如何解锁新的功能，包括使用文本提示进行多区域编辑和自回归内容传播。</div>
</details>
</div>
<div class="card">
<div class="title">Image Generation with a Sphere Encoder</div>
<div class="meta-line">Authors: Kaiyu Yue, Menglin Jia, Ji Hou, Tom Goldstein</div>
<div class="meta-line">First: 2026-02-16T18:59:57+00:00 · Latest: 2026-02-16T18:59:57+00:00</div>
<div class="meta-line">Comments: Technical report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15030v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15030v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sphere-encoder.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce the Sphere Encoder, an efficient generative framework capable of producing images in a single forward pass and competing with many-step diffusion models using fewer than five steps. Our approach works by learning an encoder that maps natural images uniformly onto a spherical latent space, and a decoder that maps random latent vectors back to the image space. Trained solely through image reconstruction losses, the model generates an image by simply decoding a random point on the sphere. Our architecture naturally supports conditional generation, and looping the encoder/decoder a few times can further enhance image quality. Across several datasets, the sphere encoder approach yields performance competitive with state of the art diffusions, but with a small fraction of the inference cost. Project page is available at https://sphere-encoder.github.io .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用球面编码器进行图像生成</div>
<div class="mono" style="margin-top:8px">我们引入了Sphere Encoder，这是一种高效的生成框架，能够在单次前向传递中生成图像，并且使用少于五步即可与多步扩散模型竞争。我们的方法通过学习一个编码器，将自然图像均匀地映射到球面潜在空间，并学习一个解码器将随机潜在向量映射回图像空间。该模型仅通过图像重建损失进行训练，生成图像只需对球面上的随机点进行解码。我们的架构天然支持条件生成，重复编码器/解码器几次可以进一步提升图像质量。在多个数据集上，Sphere Encoder的方法表现与最先进的扩散模型相当，但推理成本却显著降低。项目页面可在 https://sphere-encoder.github.io 查看。</div>
</details>
</div>
<div class="card">
<div class="title">Superposed parameterised quantum circuits</div>
<div class="meta-line">Authors: Viktoria Patapovich, Maniraman Periyasamy, Mo Kordzanganeh, Alexey Melnikov</div>
<div class="meta-line">First: 2025-06-10T12:44:11+00:00 · Latest: 2026-02-16T18:59:56+00:00</div>
<div class="meta-line">Comments: 20 pages, 6 figures, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.08749v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.08749v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quantum machine learning has shown promise for high-dimensional data analysis, yet many existing approaches rely on linear unitary operations and shared trainable parameters across outputs. These constraints limit expressivity and scalability relative to the multi-layered, non-linear architectures of classical deep networks. We introduce superposed parameterised quantum circuits to overcome these limitations. By combining flip-flop quantum random-access memory with repeat-until-success protocols, a superposed parameterised quantum circuit embeds an exponential number of parameterised sub-models in a single circuit and induces polynomial activation functions through amplitude transformations and post-selection. We provide an analytic description of the architecture, showing how multiple parameter sets are trained in parallel while non-linear amplitude transformations broaden representational power beyond conventional quantum kernels. Numerical experiments underscore these advantages: on a 1D step-function regression a two-qubit superposed parameterised quantum circuit cuts the mean-squared error by three orders of magnitude versus a parameter-matched variational baseline; on a 2D star-shaped two-dimensional classification task, introducing a quadratic activation lifts accuracy to 81.4\% and reduces run-to-run variance three-fold. These results position superposed parameterised quantum circuits as a hardware-efficient route toward deeper, more versatile parameterised quantum circuits capable of learning complex decision boundaries.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>叠加参数化量子电路</div>
<div class="mono" style="margin-top:8px">量子机器学习在高维数据分析中展现出潜力，但许多现有方法依赖于线性幺正操作和跨输出的共享可训练参数。这些限制使得其表达能力和可扩展性相较于经典深度网络的多层非线性架构有所不足。我们引入叠加参数化量子电路以克服这些限制。通过结合翻转-翻转量子随机存取存储器与重复直到成功协议，叠加参数化量子电路在一个电路中嵌入指数数量的参数化子模型，并通过振幅变换和后选择诱导出多项式激活函数。我们提供了该架构的分析描述，展示了如何并行训练多个参数集，而非线性振幅变换则扩展了表示能力，超越了传统量子核。数值实验突显了这些优势：在1D阶跃函数回归任务中，一个两量子比特的叠加参数化量子电路相较于参数匹配的变分基线将均方误差降低了三个数量级；在2D星形分类任务中，引入二次激活函数将准确率提升至81.4%，并将运行间方差降低了三倍。这些结果表明，叠加参数化量子电路是一种高效利用硬件的途径，能够实现更深层、更通用的参数化量子电路，从而学习复杂的决策边界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Quantum machine learning has shown promise for high-dimensional data analysis, yet many existing approaches rely on linear unitary operations and shared trainable parameters across outputs.</div>
</details>
</div>
<div class="card">
<div class="title">Symmetry in language statistics shapes the geometry of model representations</div>
<div class="meta-line">Authors: Dhruva Karkada, Daniel J. Korchinski, Andres Nava, Matthieu Wyart, Yasaman Bahri</div>
<div class="meta-line">First: 2026-02-16T18:59:55+00:00 · Latest: 2026-02-16T18:59:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15029v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15029v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although learned representations underlie neural networks&#x27; success, their fundamental properties remain poorly understood. A striking example is the emergence of simple geometric structures in LLM representations: for example, calendar months organize into a circle, years form a smooth one-dimensional manifold, and cities&#x27; latitudes and longitudes can be decoded by a linear probe. We show that the statistics of language exhibit a translation symmetry -- e.g., the co-occurrence probability of two months depends only on the time interval between them -- and we prove that the latter governs the aforementioned geometric structures in high-dimensional word embedding models. Moreover, we find that these structures persist even when the co-occurrence statistics are strongly perturbed (for example, by removing all sentences in which two months appear together) and at moderate embedding dimension. We show that this robustness naturally emerges if the co-occurrence statistics are collectively controlled by an underlying continuous latent variable. We empirically validate this theoretical framework in word embedding models, text embedding models, and large language models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言统计中的对称性塑造了模型表示的几何结构</div>
<div class="mono" style="margin-top:8px">尽管学习到的表示是神经网络成功的基础，但其基本性质仍不为人所知。一个显著的例子是大型语言模型（LLM）表示中简单几何结构的出现：例如，月份组织成一个圆圈，年份形成一个平滑的一维流形，而城市纬度和经度可以通过线性探针解码。我们证明语言的统计特性表现出平移对称性——例如，两个月份的共现概率仅取决于它们之间的时间间隔——并且这种对称性决定了高维词嵌入模型中上述几何结构。此外，我们发现即使共现统计特性受到强烈扰动（例如，删除所有包含两个月份的句子），这些结构仍然在中等嵌入维度下保持稳定。我们表明，如果共现统计特性由一个潜在的连续隐变量共同控制，这种鲁棒性自然会出现。我们在词嵌入模型、文本嵌入模型和大型语言模型中实证验证了这一理论框架。</div>
</details>
</div>
<div class="card">
<div class="title">Long Context, Less Focus: A Scaling Gap in LLMs Revealed through Privacy and Personalization</div>
<div class="meta-line">Authors: Shangding Gu</div>
<div class="meta-line">First: 2026-02-16T18:59:42+00:00 · Latest: 2026-02-16T18:59:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15028v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15028v1">PDF</a> · <a href="https://github.com/SafeRL-Lab/PAPerBench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly deployed in privacy-critical and personalization-oriented scenarios, yet the role of context length in shaping privacy leakage and personalization effectiveness remains largely unexplored. We introduce a large-scale benchmark, PAPerBench, to systematically study how increasing context length influences both personalization quality and privacy protection in LLMs. The benchmark comprises approximately 29,000 instances with context lengths ranging from 1K to 256K tokens, yielding a total of 377K evaluation questions. It jointly evaluates personalization performance and privacy risks across diverse scenarios, enabling controlled analysis of long-context model behavior. Extensive evaluations across state-of-the-art LLMs reveal consistent performance degradation in both personalization and privacy as context length increases. We further provide a theoretical analysis of attention dilution under context scaling, explaining this behavior as an inherent limitation of soft attention in fixed-capacity Transformers. The empirical and theoretical findings together suggest a general scaling gap in current models -- long context, less focus. We release the benchmark to support reproducible evaluation and future research on scalable privacy and personalization. Code and data are available at https://github.com/SafeRL-Lab/PAPerBench</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>长上下文，弱聚焦：通过隐私与个性化揭示LLMs的扩展差距</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越多地部署在隐私关键和个性化导向的场景中，但上下文长度在影响隐私泄露和个性化效果中的作用仍鲜有研究。我们引入了一个大规模基准测试PAPerBench，系统地研究增加上下文长度如何影响LLMs的个性化质量和隐私保护。该基准包含约29,000个实例，上下文长度从1K到256K token不等，总计377K个评估问题。它在多种场景中联合评估个性化表现和隐私风险，支持对长上下文模型行为的受控分析。在多个最先进的LLMs上的广泛评估表明，随着上下文长度的增加，个性化和隐私保护性能均出现一致的下降。我们进一步提供了在上下文扩展下注意力稀释的理论分析，将这种行为解释为固定容量Transformer中软注意力机制的固有局限。实证和理论发现共同表明当前模型存在普遍的扩展差距——长上下文，弱聚焦。我们发布该基准以支持可复现的评估和未来关于可扩展隐私与个性化的研究。代码和数据可在https://github.com/SafeRL-Lab/PAPerBench获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) are increasingly deployed in privacy-critical and personalization-oriented scenarios, yet the role of context length in shaping privacy leakage and personalization effectiveness remains largely unexplored.</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking Diffusion Models with Symmetries through Canonicalization with Applications to Molecular Graph Generation</div>
<div class="meta-line">Authors: Cai Zhou, Zijie Chen, Zian Li, Jike Wang, Kaiyi Jiang, Pan Li, Rose Yu, Muhan Zhang, Stephen Bates, Tommi Jaakkola</div>
<div class="meta-line">First: 2026-02-16T18:58:55+00:00 · Latest: 2026-02-16T18:58:55+00:00</div>
<div class="meta-line">Comments: 32 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15022v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15022v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many generative tasks in chemistry and science involve distributions invariant to group symmetries (e.g., permutation and rotation). A common strategy enforces invariance and equivariance through architectural constraints such as equivariant denoisers and invariant priors. In this paper, we challenge this tradition through the alternative canonicalization perspective: first map each sample to an orbit representative with a canonical pose or order, train an unconstrained (non-equivariant) diffusion or flow model on the canonical slice, and finally recover the invariant distribution by sampling a random symmetry transform at generation time. Building on a formal quotient-space perspective, our work provides a comprehensive theory of canonical diffusion by proving: (i) the correctness, universality and superior expressivity of canonical generative models over invariant targets; (ii) canonicalization accelerates training by removing diffusion score complexity induced by group mixtures and reducing conditional variance in flow matching. We then show that aligned priors and optimal transport act complementarily with canonicalization and further improves training efficiency. We instantiate the framework for molecular graph generation under $S_n \times SE(3)$ symmetries. By leveraging geometric spectra-based canonicalization and mild positional encodings, canonical diffusion significantly outperforms equivariant baselines in 3D molecule generation tasks, with similar or even less computation. Moreover, with a novel architecture Canon, CanonFlow achieves state-of-the-art performance on the challenging GEOM-DRUG dataset, and the advantage remains large in few-step generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过规范化的对称性重新思考扩散模型及其在分子图生成中的应用</div>
<div class="mono" style="margin-top:8px">化学和科学中的许多生成任务涉及对群对称性不变的分布（例如排列和旋转）。一种常见策略是通过架构约束（如对称去噪器和不变先验）来强制不变性和协变性。在本文中，我们通过规范化的视角挑战这一传统：首先将每个样本映射到一个轨道代表，具有规范姿态或顺序，然后在规范切片上训练无约束（非协变）的扩散或流模型，最后在生成时通过随机对称变换恢复不变分布。基于形式化的商空间视角，我们的工作提供了规范扩散的全面理论，并证明了以下几点：(i) 规范生成模型在生成不变目标时具有正确性、普适性和优越的表达能力；(ii) 规范化通过消除群混合引起的扩散得分复杂性并减少流匹配中的条件方差，从而加速训练。我们进一步展示了对齐先验和最优传输与规范化相辅相成，进一步提升训练效率。我们为在 $S_n \times SE(3)$ 对称性下进行分子图生成的框架进行了实例化。通过利用基于几何谱的规范化和温和的位置编码，规范扩散在3D分子生成任务中显著优于协变基线，且计算量相似或更少。此外，通过一种新颖的架构 Canon，CanonFlow 在具有挑战性的 GEOM-DRUG 数据集上实现了最先进的性能，且在几步生成时仍具有显著优势。</div>
</details>
</div>
<div class="card">
<div class="title">Generalization from Low- to Moderate-Resolution Spectra with Neural Networks for Stellar Parameter Estimation: A Case Study with DESI</div>
<div class="meta-line">Authors: Xiaosheng Zhao, Yuan-Sen Ting, Rosemary F. G. Wyse, Alexander S. Szalay, Yang Huang, László Dobos, Tamás Budavári, Viska Wei</div>
<div class="meta-line">First: 2026-02-16T18:58:47+00:00 · Latest: 2026-02-16T18:58:47+00:00</div>
<div class="meta-line">Comments: 20 pages, 13 figures, 4 tables. Submitted to AAS journals. Comments welcome</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15021v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15021v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cross-survey generalization is a critical challenge in stellar spectral analysis, particularly in cases such as transferring from low- to moderate-resolution surveys. We investigate this problem using pre-trained models, focusing on simple neural networks such as multilayer perceptrons (MLPs), with a case study transferring from LAMOST low-resolution spectra (LRS) to DESI medium-resolution spectra (MRS). Specifically, we pre-train MLPs on either LRS or their embeddings and fine-tune them for application to DESI stellar spectra. We compare MLPs trained directly on spectra with those trained on embeddings derived from transformer-based models (self-supervised foundation models pre-trained for multiple downstream tasks). We also evaluate different fine-tuning strategies, including residual-head adapters, LoRA, and full fine-tuning. We find that MLPs pre-trained on LAMOST LRS achieve strong performance, even without fine-tuning, and that modest fine-tuning with DESI spectra further improves the results. For iron abundance, embeddings from a transformer-based model yield advantages in the metal-rich ([Fe/H] &gt; -1.0) regime, but underperform in the metal-poor regime compared to MLPs trained directly on LRS. We also show that the optimal fine-tuning strategy depends on the specific stellar parameter under consideration. These results highlight that simple pre-trained MLPs can provide competitive cross-survey generalization, while the role of spectral foundation models for cross-survey stellar parameter estimation requires further exploration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用神经网络从低分辨率光谱到中等分辨率光谱的泛化：以DESI为例的案例研究</div>
<div class="mono" style="margin-top:8px">跨调查光谱泛化是恒星光谱分析中的关键挑战，特别是在从低分辨率到中等分辨率调查的转移场景中。我们使用预训练模型来研究这一问题，重点关注多层感知机（MLPs）等简单神经网络，并以从LAMOST低分辨率光谱（LRS）转移到DESI中等分辨率光谱（MRS）的案例进行分析。具体而言，我们分别在LRS或其嵌入表示上预训练MLPs，并对它们进行微调以应用于DESI恒星光谱。我们比较了直接在光谱上训练的MLPs与基于Transformer模型的嵌入表示训练的MLPs的性能。此外，我们评估了不同的微调策略，包括残差头适配器、LoRA和全微调。我们发现，使用LAMOST LRS预训练的MLPs即使不进行微调也能取得良好性能，而适度的DESI光谱微调进一步提升了结果。对于铁丰度，基于Transformer模型的嵌入表示在金属丰富（[Fe/H] &gt; -1.0）区域具有优势，但在金属贫乏区域的表现不如直接在LRS上训练的MLPs。我们还表明，最佳的微调策略取决于所考虑的具体恒星参数。这些结果表明，简单的预训练MLPs可以提供具有竞争力的跨调查泛化能力，而光谱基础模型在跨调查恒星参数估计中的作用仍需进一步探索。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Cross-survey generalization is a critical challenge in stellar spectral analysis, particularly in cases such as transferring from low- to moderate-resolution surveys.</div>
</details>
</div>
<div class="card">
<div class="title">Hunt Globally: Deep Research AI Agents for Drug Asset Scouting in Investing, Business Development, and Search &amp; Evaluation</div>
<div class="meta-line">Authors: Alisa Vinogradova, Vlad Vinogradov, Luba Greenwood, Ilya Yasny, Dmitry Kobyzev, Shoman Kasbekar, Kong Nguyen, Dmitrii Radkevich, Roman Doronin, Andrey Doronichev</div>
<div class="meta-line">First: 2026-02-16T18:57:49+00:00 · Latest: 2026-02-16T18:57:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15019v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15019v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels. Recent data suggests &gt;85% of patent filings originate outside the U.S., with China accounting for nearly half of the global total; a growing share of scholarly output is also non-U.S. Industry estimates put China at ~30% of global drug development, spanning 1,200+ novel candidates. In this high-stakes environment, failing to surface &quot;under-the-radar&quot; assets creates multi-billion-dollar risk for investors and business development teams, making asset scouting a coverage-critical competition where speed and completeness drive value. Yet today&#x27;s Deep Research AI agents still lag human experts in achieving high-recall discovery across heterogeneous, multilingual sources without hallucinations.
  We propose a benchmarking methodology for drug asset scouting and a tuned, tree-based self-learning Bioptic Agent aimed at complete, non-hallucinated scouting. We construct a challenging completeness benchmark using a multilingual multi-agent pipeline: complex user queries paired with ground-truth assets that are largely outside U.S.-centric radar. To reflect real deal complexity, we collected screening queries from expert investors, BD, and VC professionals and used them as priors to conditionally generate benchmark queries. For grading, we use LLM-as-judge evaluation calibrated to expert opinions. We compare Bioptic Agent against Claude Opus 4.6, OpenAI GPT-5.2 Pro, Perplexity Deep Research, Gemini 3 Pro + Deep Research, and Exa Websets. Bioptic Agent achieves 79.7% F1 versus 56.2% (Claude Opus 4.6), 50.6% (Gemini 3 Pro + Deep Research), 46.6% (GPT-5.2 Pro), 44.2% (Perplexity Deep Research), and 26.9% (Exa Websets). Performance improves steeply with additional compute, supporting the view that more compute yields better results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>全球搜寻：用于投资、业务拓展和搜寻评估的深度研究AI代理药物资产挖掘</div>
<div class="mono" style="margin-top:8px">生物制药创新已发生变化：许多新药资产现在源自美国以外，并主要通过区域性的非英语渠道披露。最新数据显示，超过85%的专利申请来自美国以外，其中中国占全球总量的近一半；同时，非美国的学术产出也在增长。行业估计显示，中国占全球药物开发的约30%，涵盖1200多个新型候选药物。在这一高风险环境中，未能发现“低调”资产会给投资者和业务拓展团队带来数十亿美元的风险，使资产搜寻成为一项关键的覆盖竞争，其中速度和完整性决定价值。然而，当前的深度研究AI代理在跨异构、多语言来源中实现高召回率的发现方面仍落后于人类专家，且容易产生幻觉。
我们提出了一种药物资产搜寻的基准方法，并设计了一种经过调优的基于树的自学习Bioptic代理，旨在实现完整且无幻觉的资产搜寻。我们使用多语言多代理流程构建了一个具有挑战性的完整性基准：复杂的用户查询与大量位于美国中心视野之外的真实资产相匹配。为了反映真实交易的复杂性，我们从专家投资者、业务拓展和风投专业人士中收集了筛选查询，并将其作为先验条件来生成基准查询。在评估方面，我们采用LLM作为评委的评估方法，并根据专家意见进行校准。我们将Bioptic代理与Claude Opus 4.6、OpenAI GPT-5.2 Pro、Perplexity Deep Research、Gemini 3 Pro + Deep Research以及Exa Websets进行了比较。Bioptic代理的F1得分为79.7%，分别高于Claude Opus 4.6的56.2%、Gemini 3 Pro + Deep Research的50.6%、GPT-5.2 Pro的46.6%、Perplexity Deep Research的44.2%和Exa Websets的26.9%。随着计算资源的增加，性能显著提升，支持了更多计算资源能带来更好结果的观点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels.</div>
</details>
</div>
<div class="card">
<div class="title">Privileged Information Distillation for Language Models</div>
<div class="meta-line">Authors: Emiliano Penaloza, Dheeraj Vattikonda, Nicolas Gontier, Alexandre Lacoste, Laurent Charlin, Massimo Caccia</div>
<div class="meta-line">First: 2026-02-04T18:46:17+00:00 · Latest: 2026-02-16T18:57:38+00:00</div>
<div class="meta-line">Comments: Abstract border should have been purple</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04942v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.04942v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training-time privileged information (PI) can enable language models to succeed on tasks they would otherwise fail, making it a powerful tool for reinforcement learning in hard, long-horizon settings. However, transferring capabilities learned with PI to policies that must act without it at inference time remains a fundamental challenge. We study this problem in the context of distilling frontier models for multi-turn agentic environments, which typically hide their internal reasoning and expose only action trajectories. This breaks standard distillation pipelines, since successful behavior is observable, but the reasoning process is not. For this, we introduce π-Distill, a joint teacher-student objective that trains a PI-conditioned teacher and an unconditioned student simultaneously using the same model. Additionally, we also introduce On-Policy Self-Distillation (OPSD), an alternative approach that trains using Reinforcement Learning (RL) with a reverse KL-penalty between the student and the PI-conditioned teacher. We show that both of these algorithms effectively distill frontier agents using action-only PI. Specifically, we find that π-Distill and, in some cases, OPSD, outperform industry standard practices (Supervised finetuning followed by RL) that assume access to full Chain-of-Thought supervision across multiple agentic benchmarks, models, and forms of PI. We complement our results with extensive analysis that characterizes the factors enabling effective learning with PI, focusing primarily on π-Distill and characterizing when OPSD is competitive.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于语言模型的特权信息蒸馏</div>
<div class="mono" style="margin-top:8px">训练时的特权信息（PI）可以使语言模型在原本无法完成的任务中取得成功，使其成为在复杂、长周期环境中强化学习的强大工具。然而，将使用PI学习到的能力转移到推理时无法使用PI的策略仍然是一个根本性挑战。我们在此背景下研究了这一问题，即在多轮代理环境中蒸馏前沿模型，这些环境通常隐藏其内部推理过程，仅暴露行动轨迹。这打破了标准的蒸馏流程，因为成功的行为是可观测的，但推理过程却不可见。为此，我们引入了π-Distill，一种联合教师-学生目标，使用同一模型同时训练一个基于PI的教师和一个无条件的学生。此外，我们还引入了基于策略的自我蒸馏（OPSD），这是一种替代方法，通过强化学习（RL）训练学生模型，并在学生与基于PI的教师之间引入反向KL惩罚。我们证明这两种算法都能有效利用仅行动的PI进行前沿代理的蒸馏。具体而言，我们发现π-Distill和在某些情况下OPSD的表现优于行业标准做法（监督微调后接强化学习），后者假设可以访问多个代理基准、模型和PI形式的完整思维链监督。我们还通过广泛的分析来补充这些结果，以刻画使PI有效学习的因素，主要聚焦于π-Distill，并探讨OPSD何时具有竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Training-time privileged information (PI) can enable language models to succeed on tasks they would otherwise fail, making it a powerful tool for reinforcement learning in hard, long-horizon settings.</div>
</details>
</div>
<div class="card">
<div class="title">Simulating the Real World: A Unified Survey of Multimodal Generative Models</div>
<div class="meta-line">Authors: Yuqi Hu, Longguang Wang, Xian Liu, Ling-Hao Chen, Yuwei Guo, Yukai Shi, Ce Liu, Anyi Rao, Zeyu Wang, Hui Xiong</div>
<div class="meta-line">First: 2025-03-06T17:31:43+00:00 · Latest: 2026-02-16T18:57:17+00:00</div>
<div class="meta-line">Comments: Repository for the related papers at https://github.com/ALEEEHU/World-Simulator</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.04641v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.04641v3">PDF</a> · <a href="https://github.com/ALEEEHU/World-Simulator">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding and replicating the real world is a critical challenge in Artificial General Intelligence (AGI) research. To achieve this, many existing approaches, such as world models, aim to capture the fundamental principles governing the physical world, enabling more accurate simulations and meaningful interactions. However, current methods often treat different modalities, including 2D (images), videos, 3D, and 4D representations, as independent domains, overlooking their interdependencies. Additionally, these methods typically focus on isolated dimensions of reality without systematically integrating their connections. In this survey, we present a unified survey for multimodal generative models that investigate the progression of data dimensionality in real-world simulation. Specifically, this survey starts from 2D generation (appearance), then moves to video (appearance+dynamics) and 3D generation (appearance+geometry), and finally culminates in 4D generation that integrate all dimensions. To the best of our knowledge, this is the first attempt to systematically unify the study of 2D, video, 3D and 4D generation within a single framework. To guide future research, we provide a comprehensive review of datasets, evaluation metrics and future directions, and fostering insights for newcomers. This survey serves as a bridge to advance the study of multimodal generative models and real-world simulation within a unified framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模拟真实世界：多模态生成模型的统一综述</div>
<div class="mono" style="margin-top:8px">理解并复制真实世界是人工通用智能（AGI）研究中的关键挑战。为实现这一目标，许多现有方法，如世界模型，旨在捕捉物理世界的根本原理，从而实现更准确的模拟和有意义的交互。然而，当前方法通常将不同模态（包括2D图像、视频、3D和4D表示）视为独立领域，忽视了它们之间的相互依赖关系。此外，这些方法通常只关注现实的孤立维度，而没有系统地整合它们之间的联系。在本综述中，我们提出了一个统一的多模态生成模型综述，探讨了真实世界模拟中数据维度的发展。具体而言，本综述从2D生成（外观）开始，然后过渡到视频（外观+动态）和3D生成（外观+几何），最终达到整合所有维度的4D生成。据我们所知，这是首次在单一框架内系统地统一研究2D、视频、3D和4D生成。为了指导未来的研究，我们提供了数据集、评估指标和未来方向的全面综述，并为新研究者提供启发。本综述旨在成为推进多模态生成模型和真实世界模拟研究的桥梁。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Understanding and replicating the real world is a critical challenge in Artificial General Intelligence (AGI) research.</div>
</details>
</div>
<div class="card">
<div class="title">Neurosim: A Fast Simulator for Neuromorphic Robot Perception</div>
<div class="meta-line">Authors: Richeek Das, Pratik Chaudhari</div>
<div class="meta-line">First: 2026-02-16T18:57:04+00:00 · Latest: 2026-02-16T18:57:04+00:00</div>
<div class="meta-line">Comments: 13 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15018v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15018v1">PDF</a> · <a href="https://github.com/grasp-lyrl/neurosim">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neurosim is a fast, real-time, high-performance library for simulating sensors such as dynamic vision sensors, RGB cameras, depth sensors, and inertial sensors. It can also simulate agile dynamics of multi-rotor vehicles in complex and dynamic environments. Neurosim can achieve frame rates as high as ~2700 FPS on a desktop GPU. Neurosim integrates with a ZeroMQ-based communication library called Cortex to facilitate seamless integration with machine learning and robotics workflows. Cortex provides a high-throughput, low-latency message-passing system for Python and C++ applications, with native support for NumPy arrays and PyTorch tensors. This paper discusses the design philosophy behind Neurosim and Cortex. It demonstrates how they can be used to (i) train neuromorphic perception and control algorithms, e.g., using self-supervised learning on time-synchronized multi-modal data, and (ii) test real-time implementations of these algorithms in closed-loop. Neurosim and Cortex are available at https://github.com/grasp-lyrl/neurosim .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Neurosim：一种用于神经形态机器人感知的快速模拟器</div>
<div class="mono" style="margin-top:8px">Neurosim 是一个快速、实时、高性能的库，用于模拟动态视觉传感器、RGB 相机、深度传感器和惯性传感器。它还可以在复杂和动态环境中模拟多旋翼飞行器的敏捷动力学。Neurosim 在桌面 GPU 上可以达到高达约 2700 FPS 的帧率。Neurosim 集成了一个基于 ZeroMQ 的通信库 Cortex，以促进与机器学习和机器人工作流的无缝集成。Cortex 为 Python 和 C++ 应用程序提供高吞吐量、低延迟的消息传递系统，并原生支持 NumPy 数组和 PyTorch 张量。本文讨论了 Neurosim 和 Cortex 的设计哲学，并展示了它们如何用于（i）训练神经形态感知和控制算法，例如在时间同步的多模态数据上使用自监督学习；（ii）在闭环中测试这些算法的实时实现。Neurosim 和 Cortex 可在 https://github.com/grasp-lyrl/neurosim 获取。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Beyond Masked Diffusion Language Models</div>
<div class="meta-line">Authors: Subham Sekhar Sahoo, Jean-Marie Lemercier, Zhihan Yang, Justin Deschenaux, Jingyu Liu, John Thickstun, Ante Jukic</div>
<div class="meta-line">First: 2026-02-16T18:54:47+00:00 · Latest: 2026-02-16T18:54:47+00:00</div>
<div class="meta-line">Comments: code: https://github.com/s-sahoo/scaling-dllms</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15014v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15014v1">PDF</a> · <a href="https://github.com/s-sahoo/scaling-dllms">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="http://s-sahoo.github.io/scaling-dllms">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion language models are a promising alternative to autoregressive models due to their potential for faster generation. Among discrete diffusion approaches, Masked diffusion currently dominates, largely driven by strong perplexity on language modeling benchmarks. In this work, we present the first scaling law study of uniform-state and interpolating discrete diffusion methods. We also show that Masked diffusion models can be made approximately 12% more FLOPs-efficient when trained with a simple cross-entropy objective. We find that perplexity is informative within a diffusion family but can be misleading across families, where models with worse likelihood scaling may be preferable due to faster and more practical sampling, as reflected by the speed-quality Pareto frontier. These results challenge the view that Masked diffusion is categorically the future of diffusion language modeling and that perplexity alone suffices for cross-algorithm comparison. Scaling all methods to 1.7B parameters, we show that uniform-state diffusion remains competitive on likelihood-based benchmarks and outperforms autoregressive and Masked diffusion models on GSM8K, despite worse validation perplexity. We provide the code, model checkpoints, and video tutorials on the project page: http://s-sahoo.github.io/scaling-dllms</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越掩码扩散语言模型的扩展研究</div>
<div class="mono" style="margin-top:8px">扩散语言模型由于其在生成速度上的潜力，成为自回归模型的有前景替代方案。在离散扩散方法中，掩码扩散目前占据主导地位，这主要得益于其在语言建模基准测试中表现出的出色困惑度。在本研究中，我们首次对统一状态和插值离散扩散方法进行了扩展定律分析。我们还表明，当使用简单的交叉熵目标进行训练时，掩码扩散模型可以实现大约12%的FLOPs效率提升。我们发现，困惑度在扩散模型家族内部具有信息价值，但在不同家族之间可能具有误导性，其中在采样速度和实用性方面表现更优的模型可能更受青睐，这体现在速度-质量帕累托前沿上。这些结果挑战了认为掩码扩散必将成为扩散语言建模未来，且仅凭困惑度即可进行跨算法比较的观点。我们将所有方法扩展到17亿参数规模，结果显示统一状态扩散模型在基于似然的基准测试中仍具有竞争力，并且在GSM8K数据集上优于自回归和掩码扩散模型，尽管其验证困惑度较差。项目页面提供了代码、模型检查点和视频教程：http://s-sahoo.github.io/scaling-dllms</div>
</details>
</div>
<div class="card">
<div class="title">Cold-Start Personalization via Training-Free Priors from Structured World Models</div>
<div class="meta-line">Authors: Avinandan Bose, Shuyue Stella Li, Faeze Brahman, Pang Wei Koh, Simon Shaolei Du, Yulia Tsvetkov, Maryam Fazel, Lin Xiao, Asli Celikyilmaz</div>
<div class="meta-line">First: 2026-02-16T18:52:13+00:00 · Latest: 2026-02-16T18:52:13+00:00</div>
<div class="meta-line">Comments: 24 pages, 4 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15012v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15012v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cold-start personalization requires inferring user preferences through interaction when no user-specific historical data is available. The core challenge is a routing problem: each task admits dozens of preference dimensions, yet individual users care about only a few, and which ones matter depends on who is asking. With a limited question budget, asking without structure will miss the dimensions that matter. Reinforcement learning is the natural formulation, but in multi-turn settings its terminal reward fails to exploit the factored, per-criterion structure of preference data, and in practice learned policies collapse to static question sequences that ignore user responses. We propose decomposing cold-start elicitation into offline structure learning and online Bayesian inference. Pep (Preference Elicitation with Priors) learns a structured world model of preference correlations offline from complete profiles, then performs training-free Bayesian inference online to select informative questions and predict complete preference profiles, including dimensions never asked about. The framework is modular across downstream solvers and requires only simple belief models. Across medical, mathematical, social, and commonsense reasoning, Pep achieves 80.8% alignment between generated responses and users&#x27; stated preferences versus 68.5% for RL, with 3-5x fewer interactions. When two users give different answers to the same question, Pep changes its follow-up 39-62% of the time versus 0-28% for RL. It does so with ~10K parameters versus 8B for RL, showing that the bottleneck in cold-start elicitation is the capability to exploit the factored structure of preference data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过无训练先验的结构化世界模型实现冷启动个性化</div>
<div class="mono" style="margin-top:8px">冷启动个性化需要在没有用户特定历史数据的情况下，通过交互推断用户偏好。核心挑战是一个路由问题：每个任务可能有几十个偏好维度，但个体用户只关注其中几个，而哪些维度重要取决于提问者是谁。在有限的问题预算下，无结构的提问会遗漏关键的维度。强化学习是自然的建模方式，但在多轮交互中，其终端奖励无法有效利用偏好数据的分层、按标准的结构，实际中学习到的策略会退化为忽略用户反馈的静态问题序列。我们提出将冷启动偏好获取分解为离线结构学习和在线贝叶斯推理。Pep（带先验的偏好获取）从完整用户资料中离线学习偏好相关性的结构化世界模型，然后在线进行无训练贝叶斯推理，以选择信息性问题并预测完整的偏好资料，包括从未被询问过的维度。该框架在下游求解器之间具有模块化特性，仅需简单的信念模型。在医疗、数学、社会和常识推理任务中，Pep生成的响应与用户声明偏好之间的对齐度达到80.8%，而强化学习仅为68.5%，且交互次数少3-5倍。当两位用户对同一问题给出不同答案时，Pep有39-62%的概率改变后续问题，而强化学习仅为0-28%。Pep仅需约10K参数，而强化学习需要80亿参数，这表明冷启动偏好获取的瓶颈在于能否有效利用偏好数据的分层结构。</div>
</details>
</div>
<div class="card">
<div class="title">Stretching Beyond the Obvious: A Gradient-Free Framework to Unveil the Hidden Landscape of Visual Invariance</div>
<div class="meta-line">Authors: Lorenzo Tausani, Paolo Muratore, Morgan B. Talbot, Giacomo Amerio, Gabriel Kreiman, Davide Zoccolan</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-06-20T14:49:35+00:00 · Latest: 2026-02-16T18:51:59+00:00</div>
<div class="meta-line">Comments: 33 pages, 15 figures, Accepted as a conference paper at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.17040v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.17040v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Uncovering which feature combinations are encoded by visual units is critical to understanding how images are transformed into representations that support recognition. While existing feature visualization approaches typically infer a unit&#x27;s most exciting images, this is insufficient to reveal the manifold of transformations under which responses remain invariant, which is critical to generalization in vision. Here we introduce Stretch-and-Squeeze (SnS), a model-agnostic, gradient-free framework to systematically characterize a unit&#x27;s maximally invariant stimuli, and its vulnerability to adversarial perturbations, in both biological and artificial visual systems. SnS frames these transformations as bi-objective optimization problems. To probe invariance, SnS seeks image perturbations that maximally alter (stretch) the representation of a reference stimulus in a given processing stage while preserving unit activation downstream (squeeze). To probe adversarial sensitivity, stretching and squeezing are reversed to maximally perturb unit activation while minimizing changes to the upstream representation. Applied to CNNs, SnS revealed invariant transformations that were farther from a reference image in pixel-space than those produced by affine transformations, while more strongly preserving the target unit&#x27;s response. The discovered invariant images differed depending on the stage of the image representation used for optimization: pixel-level changes primarily affected luminance and contrast, while stretching mid- and late-layer representations mainly altered texture and pose. By measuring how well the hierarchical invariant images obtained for L2 robust networks were classified by humans and other observer networks, we discovered a substantial drop in their interpretability when the representation was stretched in deep layers, while the opposite trend was found for standard models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越显而易见：一种无梯度框架揭示视觉不变性的隐藏景观</div>
<div class="mono" style="margin-top:8px">揭示哪些特征组合被视觉单元编码对于理解图像如何转化为支持识别的表示至关重要。尽管现有的特征可视化方法通常推断单元最兴奋的图像，但这不足以揭示在哪些变换下响应保持不变，而这对于视觉系统的泛化能力至关重要。本文提出Stretch-and-Squeeze（SnS），一种模型无关、无梯度的框架，用于系统地表征单元的最大不变刺激及其对对抗性扰动的脆弱性，适用于生物和人工视觉系统。SnS将这些变换框架为双目标优化问题。为了探测不变性，SnS寻找在特定处理阶段最大程度改变（拉伸）参考刺激的表示，同时保持下游单元激活不变（压缩）的图像扰动。为了探测对抗性敏感性，拉伸和压缩的方向被反转，以最大程度扰动单元激活，同时最小化上游表示的变化。当应用于卷积神经网络（CNNs）时，SnS揭示了在像素空间中与参考图像距离更远的不变变换，同时更强烈地保持目标单元的响应。发现的不变图像根据用于优化的图像表示阶段不同而有所差异：像素级的变化主要影响亮度和对比度，而拉伸中层和高层表示则主要改变纹理和姿态。通过测量L2鲁棒网络获得的层次化不变图像在人类和其他观察者网络中的可解释性，我们发现当表示在深层被拉伸时，其可解释性显著下降，而标准模型则呈现出相反的趋势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Uncovering which feature combinations are encoded by visual units is critical to understanding how images are transformed into representations that support recognition.</div>
</details>
</div>
<div class="card">
<div class="title">BPP: Long-Context Robot Imitation Learning by Focusing on Key History Frames</div>
<div class="meta-line">Authors: Max Sobol Mark, Jacky Liang, Maria Attarian, Chuyuan Fu, Debidatta Dwibedi, Dhruv Shah, Aviral Kumar</div>
<div class="meta-line">First: 2026-02-16T18:49:56+00:00 · Latest: 2026-02-16T18:49:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15010v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15010v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many robot tasks require attending to the history of past observations. For example, finding an item in a room requires remembering which places have already been searched. However, the best-performing robot policies typically condition only on the current observation, limiting their applicability to such tasks. Naively conditioning on past observations often fails due to spurious correlations: policies latch onto incidental features of training histories that do not generalize to out-of-distribution trajectories upon deployment. We analyze why policies latch onto these spurious correlations and find that this problem stems from limited coverage over the space of possible histories during training, which grows exponentially with horizon. Existing regularization techniques provide inconsistent benefits across tasks, as they do not fundamentally address this coverage problem. Motivated by these findings, we propose Big Picture Policies (BPP), an approach that conditions on a minimal set of meaningful keyframes detected by a vision-language model. By projecting diverse rollouts onto a compact set of task-relevant events, BPP substantially reduces distribution shift between training and deployment, without sacrificing expressivity. We evaluate BPP on four challenging real-world manipulation tasks and three simulation tasks, all requiring history conditioning. BPP achieves 70% higher success rates than the best comparison on real-world evaluations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BPP：通过关注关键历史帧实现长上下文机器人模仿学习</div>
<div class="mono" style="margin-top:8px">许多机器人任务需要关注过去的观察历史。例如，在房间中寻找物品需要记住哪些地方已经搜索过。然而，表现最好的机器人策略通常仅基于当前观察进行条件建模，这限制了它们在这些任务中的适用性。简单地基于过去观察进行条件建模往往因虚假相关性而失败：策略会依赖于训练历史中的偶然特征，这些特征在部署时无法泛化到分布外的轨迹。我们分析了为何策略会依赖这些虚假相关性，并发现该问题源于训练过程中对可能历史空间的覆盖有限，而这种覆盖随着时间跨度呈指数级增长。现有的正则化技术在不同任务中提供的效果不一致，因为它们并未从根本上解决这一覆盖问题。基于这些发现，我们提出了Big Picture Policies（BPP），一种通过视觉语言模型检测出的最小有意义关键帧集合进行条件建模的方法。通过将多样化的轨迹投影到一组任务相关的事件上，BPP在不牺牲表达能力的前提下显著减少了训练与部署之间的分布偏移。我们在四个具有挑战性的现实世界操作任务和三个模拟任务上评估了BPP，所有任务都需要历史条件建模。在现实世界评估中，BPP的成功率比最佳对比方法高出70%。</div>
</details>
</div>
<div class="card">
<div class="title">Growth conditions for freeness of the Furstenberg boundary action</div>
<div class="meta-line">Authors: Nazmul Alam, Joseph Gondek, Mehrdad Kalantar, Randy Pham</div>
<div class="meta-line">First: 2026-02-16T18:49:28+00:00 · Latest: 2026-02-16T18:49:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15009v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15009v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Given a finitely generated group $Γ$ and $g\inΓ$, we prove sufficient conditions in terms of various growth/decay functions for freeness of the action of $g$ on the Furstenberg boundary of $Γ$. In this context, we also give a description of the support of stationary states on the reduced C*-algebra of $Γ$.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Furstenberg 边界作用的自由性生长条件</div>
<div class="mono" style="margin-top:8px">给定一个有限生成群 $Γ$ 和 $g\inΓ$，我们证明了 $g$ 在 $Γ$ 的 Furstenberg 边界上的作用自由性的充分条件，这些条件以各种增长/衰减函数形式给出。在此背景下，我们还给出了 $Γ$ 的约化 C*-代数上平稳态的支持集描述。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Sampling with Discrete Diffusion Models: Sharp and Adaptive Guarantees</div>
<div class="meta-line">Authors: Daniil Dmitriev, Zhihan Huang, Yuting Wei</div>
<div class="meta-line">First: 2026-02-16T18:48:17+00:00 · Latest: 2026-02-16T18:48:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15008v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15008v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models over discrete spaces have recently shown striking empirical success, yet their theoretical foundations remain incomplete. In this paper, we study the sampling efficiency of score-based discrete diffusion models under a continuous-time Markov chain (CTMC) formulation, with a focus on $τ$-leaping-based samplers. We establish sharp convergence guarantees for attaining $\varepsilon$ accuracy in Kullback-Leibler (KL) divergence for both uniform and masking noising processes. For uniform discrete diffusion, we show that the $τ$-leaping algorithm achieves an iteration complexity of order $\tilde O(d/\varepsilon)$, with $d$ the ambient dimension of the target distribution, eliminating linear dependence on the vocabulary size $S$ and improving existing bounds by a factor of $d$; moreover, we establish a matching algorithmic lower bound showing that linear dependence on the ambient dimension is unavoidable in general. For masking discrete diffusion, we introduce a modified $τ$-leaping sampler whose convergence rate is governed by an intrinsic information-theoretic quantity, termed the effective total correlation, which is bounded by $d \log S$ but can be sublinear or even constant for structured data. As a consequence, the sampler provably adapts to low-dimensional structure without prior knowledge or algorithmic modification, yielding sublinear convergence rates for various practical examples (such as hidden Markov models, image data, and random graphs). Our analysis requires no boundedness or smoothness assumptions on the score estimator beyond control of the score entropy loss.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于离散扩散模型的高效采样：精确且自适应的保证</div>
<div class="mono" style="margin-top:8px">近年来，离散空间上的扩散模型在实证上取得了显著成功，但其理论基础仍不完善。本文在连续时间马尔可夫链（CTMC）框架下研究基于得分的离散扩散模型的采样效率，重点分析基于τ跳跃的采样器。我们建立了在Kullback-Leibler（KL）散度下达到ε精度的精确收敛保证。对于均匀离散扩散，我们证明τ跳跃算法的迭代复杂度为\tilde O(d/ε)，其中d为目标分布的环境维度，消除了对词汇量S的线性依赖，并将现有界提高了d倍；此外，我们还建立了与之匹配的算法下界，表明在一般情况下对环境维度的线性依赖是不可避免的。对于掩码离散扩散，我们引入了一种改进的τ跳跃采样器，其收敛速率由一个内在的信息论量——有效总相关性所决定，该量被限制在d log S以内，但对于结构化数据可以是次线性甚至常数。因此，该采样器无需先验知识或算法修改即可适应低维结构，从而在各种实际例子（如隐马尔可夫模型、图像数据和随机图）中实现次线性收敛速率。我们的分析不需要对得分估计器进行任何有界性或平滑性假设，仅需控制得分熵损失。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Rate Annealing Improves Tuning Robustness in Stochastic Optimization</div>
<div class="meta-line">Authors: Amit Attia, Tomer Koren</div>
<div class="meta-line">First: 2025-03-12T14:06:34+00:00 · Latest: 2026-02-16T18:47:51+00:00</div>
<div class="meta-line">Comments: 23 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.09411v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.09411v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The learning rate in stochastic gradient methods is a critical hyperparameter that is notoriously costly to tune via standard grid search, especially for training modern large-scale models with billions of parameters. We identify a theoretical advantage of learning rate annealing schemes that decay the learning rate to zero at a polynomial rate, such as the widely-used cosine schedule, by demonstrating their increased robustness to initial parameter misspecification due to a coarse grid search. We present an analysis in a stochastic convex optimization setup demonstrating that the convergence rate of stochastic gradient descent with annealed schedules depends sublinearly on the multiplicative misspecification factor $ρ$ (i.e., the grid resolution), achieving a rate of $O(ρ^{1/(2p+1)}/\sqrt{T})$ where $p$ is the degree of polynomial decay and $T$ is the number of steps. This is in contrast to the $O(ρ/\sqrt{T})$ rate obtained under the inverse-square-root and fixed stepsize schedules, which depend linearly on $ρ$. Experiments confirm the increased robustness compared to tuning with a fixed stepsize, that has significant implications for the computational overhead of hyperparameter search in practical training scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习率衰减在随机优化中提高调参鲁棒性</div>
<div class="mono" style="margin-top:8px">在随机梯度方法中，学习率是一个关键的超参数，使用标准网格搜索对其进行调参成本极高，尤其是在训练具有数十亿参数的现代大规模模型时。我们通过展示其在粗略网格搜索下对初始参数误设的更强鲁棒性，识别出学习率以多项式速率衰减到零的理论优势，例如广泛使用的余弦调度。我们在随机凸优化设置下进行分析，证明了使用衰减调度的随机梯度下降的收敛速率与乘法误设因子 $ρ$（即网格分辨率）呈次线性关系，达到 $O(ρ^{1/(2p+1)}/\sqrt{T})$ 的速率，其中 $p$ 是多项式衰减的次数，$T$ 是步数。这与在逆平方根和固定步长调度下获得的 $O(ρ/\sqrt{T})$ 速率形成对比，后者与 $ρ$ 呈线性关系。实验结果证实了与固定步长调参相比的更强鲁棒性，这对实际训练场景中超参数搜索的计算开销有重要影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The learning rate in stochastic gradient methods is a critical hyperparameter that is notoriously costly to tune via standard grid search, especially for training modern large-scale models with billions of parameters.</div>
</details>
</div>
<div class="card">
<div class="title">Distributed Quantum Gaussian Processes for Multi-Agent Systems</div>
<div class="meta-line">Authors: Meet Gandhi, George P. Kontoudis</div>
<div class="meta-line">Venue: 2026 International Conference on Autonomous Agents and Multiagent Systems</div>
<div class="meta-line">First: 2026-02-16T18:46:23+00:00 · Latest: 2026-02-16T18:46:23+00:00</div>
<div class="meta-line">Comments: 9 pages, 4 figures, accepted at AAMAS 2026 (International Conference on Autonomous Agents and Multiagent Systems)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15006v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15006v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Gaussian Processes (GPs) are a powerful tool for probabilistic modeling, but their performance is often constrained in complex, largescale real-world domains due to the limited expressivity of classical kernels. Quantum computing offers the potential to overcome this limitation by embedding data into exponentially large Hilbert spaces, capturing complex correlations that remain inaccessible to classical computing approaches. In this paper, we propose a Distributed Quantum Gaussian Process (DQGP) method in a multiagent setting to enhance modeling capabilities and scalability. To address the challenging non-Euclidean optimization problem, we develop a Distributed consensus Riemannian Alternating Direction Method of Multipliers (DR-ADMM) algorithm that aggregates local agent models into a global model. We evaluate the efficacy of our method through numerical experiments conducted on a quantum simulator in classical hardware. We use real-world, non-stationary elevation datasets of NASA&#x27;s Shuttle Radar Topography Mission and synthetic datasets generated by Quantum Gaussian Processes. Beyond modeling advantages, our framework highlights potential computational speedups that quantum hardware may provide, particularly in Gaussian processes and distributed optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于多智能体系统的分布式量子高斯过程</div>
<div class="mono" style="margin-top:8px">高斯过程（GPs）是概率建模的强大工具，但在复杂、大规模的现实领域中，其性能常受到经典核函数表达能力有限的制约。量子计算通过将数据嵌入指数级大的希尔伯特空间，提供了克服这一限制的潜力，能够捕捉经典计算方法无法实现的复杂相关性。本文提出了一种多智能体设置下的分布式量子高斯过程（DQGP）方法，以增强建模能力和可扩展性。为了解决具有挑战性的非欧几里得优化问题，我们开发了一种分布式共识黎曼交替方向乘子法（DR-ADMM）算法，将局部智能体模型聚合为全局模型。我们通过在经典硬件量子模拟器上进行数值实验来评估所提方法的有效性。实验数据包括NASA航天飞机雷达地形任务的真实非平稳高程数据集以及由量子高斯过程生成的合成数据集。除了建模优势外，我们的框架还突显了量子硬件可能提供的计算加速潜力，尤其是在高斯过程和分布式优化方面。</div>
</details>
</div>
<div class="card">
<div class="title">PDE foundation models are skillful AI weather emulators for the Martian atmosphere</div>
<div class="meta-line">Authors: Johannes Schmude, Sujit Roy, Liping Wang, Theodore van Kessel, Levente Klein, Marcus Freitag, Eloisa Bentivegna, Robert Manson-Sawko, Bjorn Lutjens, Manil Maskey, Campbell Watson, Rahul Ramachandran, Juan Bernabe-Moreno</div>
<div class="meta-line">First: 2026-02-16T18:44:46+00:00 · Latest: 2026-02-16T18:44:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15004v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15004v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We show that AI foundation models that are pretrained on numerical solutions to a diverse corpus of partial differential equations can be adapted and fine-tuned to obtain skillful predictive weather emulators for the Martian atmosphere. We base our work on the Poseidon PDE foundation model for two-dimensional systems. We develop a method to extend Poseidon from two to three dimensions while keeping the pretraining information. Moreover, we investigate the performance of the model in the presence of sparse initial conditions. Our results make use of four Martian years (approx.~34 GB) of training data and a median compute budget of 13 GPU hours. We find that the combination of pretraining and model extension yields a performance increase of 34.4\% on a held-out year. This shows that PDEs-FMs can not only approximate solutions to (other) PDEs but also anchor models for real-world problems with complex interactions that lack a sufficient amount of training data or a suitable compute budget.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PDE基础模型是火星大气的高效AI天气模拟器</div>
<div class="mono" style="margin-top:8px">我们展示了通过在多种偏微分方程的数值解上进行预训练的AI基础模型，可以被调整和微调以获得高效的火星大气预测天气模拟器。我们的工作基于Poseidon PDE基础模型，用于二维系统。我们开发了一种方法，将Poseidon从二维扩展到三维，同时保留预训练信息。此外，我们研究了在稀疏初始条件下模型的性能。我们的结果利用了四个火星年（约34GB）的训练数据和中位数13个GPU小时的计算预算。我们发现，预训练与模型扩展的结合在保留年份上的性能提升了34.4%。这表明PDE-FMs不仅能近似求解其他PDE，还能作为缺乏足够训练数据或合适计算预算的复杂现实问题模型的锚点。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring Collaboration Breakdowns Between Provider Teams and Patients in Post-Surgery Care</div>
<div class="meta-line">Authors: Bingsheng Yao, Menglin Zhao, Zhan Zhang, Pengqi Wang, Emma G Chester, Changchang Yin, Tianshi Li, Varun Mishra, Lace Padilla, Odysseas Chatzipanagiotou, Timothy Pawlik, Ping Zhang, Weidan Cao, Dakuo Wang</div>
<div class="meta-line">First: 2025-09-27T21:56:57+00:00 · Latest: 2026-02-16T18:31:28+00:00</div>
<div class="meta-line">Comments: Accepted at CHI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23509v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.23509v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-surgery care involves ongoing collaboration between provider teams and patients, which starts from post-surgery hospitalization through home recovery after discharge. While prior HCI research has primarily examined patients&#x27; challenges at home, less is known about how provider teams coordinate discharge preparation and care handoffs, and how breakdowns in communication and care pathways may affect patient recovery. To investigate this gap, we conducted semi-structured interviews with 13 healthcare providers and 4 patients in the context of gastrointestinal (GI) surgery. We found coordination boundaries between in- and out-patient teams, coupled with complex organizational structures within teams, impeded the &quot;invisible work&quot; of preparing patients&#x27; home care plans and triaging patient information. For patients, these breakdowns resulted in inadequate preparation for home transition and fragmented self-collected data, both of which undermine timely clinical decision-making. Based on these findings, we outline design opportunities to formalize task ownership and handoffs, contextualize co-temporal signals, and align care plans with home resources.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索术后护理中医疗团队与患者之间的协作失效</div>
<div class="mono" style="margin-top:8px">术后护理涉及医疗团队与患者之间的持续协作，从术后住院到出院后的居家恢复。尽管以往的人机交互（HCI）研究主要关注患者在家中的挑战，但关于医疗团队如何协调出院准备和护理交接，以及沟通和护理路径中的失效如何影响患者康复的了解仍较为有限。为探讨这一问题，我们在胃肠道（GI）手术背景下对13名医疗提供者和4名患者进行了半结构化访谈。我们发现，院内与院外医疗团队之间的协调边界，以及团队内部复杂的组织结构，阻碍了为患者制定居家护理计划和患者信息分诊的&quot;隐形工作&quot;。对患者而言，这些失效导致出院过渡准备不足和自收集数据碎片化，从而影响及时的临床决策。基于这些发现，我们提出了设计机会，包括明确任务归属和交接流程、情境化同步信号以及将护理计划与居家资源对齐。</div>
</details>
</div>
<div class="card">
<div class="title">Boundary Point Jailbreaking of Black-Box LLMs</div>
<div class="meta-line">Authors: Xander Davies, Giorgi Giglemiani, Edmund Lau, Eric Winsor, Geoffrey Irving, Yarin Gal</div>
<div class="meta-line">First: 2026-02-16T18:29:09+00:00 · Latest: 2026-02-16T18:29:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15001v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15001v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Frontier LLMs are safeguarded against attempts to extract harmful information via adversarial prompts known as &quot;jailbreaks&quot;. Recently, defenders have developed classifier-based systems that have survived thousands of hours of human red teaming. We introduce Boundary Point Jailbreaking (BPJ), a new class of automated jailbreak attacks that evade the strongest industry-deployed safeguards. Unlike previous attacks that rely on white/grey-box assumptions (such as classifier scores or gradients) or libraries of existing jailbreaks, BPJ is fully black-box and uses only a single bit of information per query: whether or not the classifier flags the interaction. To achieve this, BPJ addresses the core difficulty in optimising attacks against robust real-world defences: evaluating whether a proposed modification to an attack is an improvement. Instead of directly trying to learn an attack for a target harmful string, BPJ converts the string into a curriculum of intermediate attack targets and then actively selects evaluation points that best detect small changes in attack strength (&quot;boundary points&quot;). We believe BPJ is the first fully automated attack algorithm that succeeds in developing universal jailbreaks against Constitutional Classifiers, as well as the first automated attack algorithm that succeeds against GPT-5&#x27;s input classifier without relying on human attack seeds. BPJ is difficult to defend against in individual interactions but incurs many flags during optimisation, suggesting that effective defence requires supplementing single-interaction methods with batch-level monitoring.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>黑盒大语言模型的边界点越狱攻击</div>
<div class="mono" style="margin-top:8px">前沿大语言模型通过对抗性提示（称为&quot;越狱&quot;）来防止有害信息的提取。最近，防御者开发了基于分类器的系统，这些系统已经经受住了数千小时的人类红队测试。我们引入了边界点越狱（BPJ），这是一种新的自动化越狱攻击类别，能够规避当前行业部署的最强防护措施。与以往依赖白盒/灰盒假设（如分类器得分或梯度）或现有越狱攻击库的攻击不同，BPJ是完全黑盒的，每个查询仅使用一个比特的信息：分类器是否标记了交互。为实现这一点，BPJ解决了针对稳健现实防御优化攻击的核心难题：评估所提出的攻击修改是否为改进。BPJ不直接尝试学习针对目标有害字符串的攻击，而是将字符串转换为一个中间攻击目标的课程，然后主动选择最能检测攻击强度微小变化的评估点（&quot;边界点&quot;）。我们认为BPJ是首个成功开发针对宪法分类器的通用越狱攻击的完全自动化攻击算法，也是首个无需依赖人类攻击种子即可成功针对GPT-5输入分类器的自动化攻击算法。BPJ在单次交互中难以防御，但在优化过程中会引发大量标记，这表明有效的防御需要将单次交互方法与批量监控相结合。</div>
</details>
</div>
<div class="card">
<div class="title">Spectral Convolution on Orbifolds for Geometric Deep Learning</div>
<div class="meta-line">Authors: Tim Mangliers, Bernhard Mössner, Benjamin Himpel</div>
<div class="meta-line">First: 2026-02-16T18:28:38+00:00 · Latest: 2026-02-16T18:28:38+00:00</div>
<div class="meta-line">Comments: 17 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14997v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14997v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Geometric deep learning (GDL) deals with supervised learning on data domains that go beyond Euclidean structure, such as data with graph or manifold structure. Due to the demand that arises from application-related data, there is a need to identify further topological and geometric structures with which these use cases can be made accessible to machine learning. There are various techniques, such as spectral convolution, that form the basic building blocks for some convolutional neural network-like architectures on non-Euclidean data. In this paper, the concept of spectral convolution on orbifolds is introduced. This provides a building block for making learning on orbifold structured data accessible using GDL. The theory discussed is illustrated using an example from music theory.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在 orbifold 上的谱卷积用于几何深度学习</div>
<div class="mono" style="margin-top:8px">几何深度学习（GDL）处理超出欧几里得结构的数据域上的监督学习，例如具有图或流形结构的数据。由于应用相关数据的需求，需要识别更多的拓扑和几何结构，以使这些用例能够被机器学习所利用。诸如谱卷积等技术是构建非欧几里得数据上类似卷积神经网络的架构的基本组件。本文引入了在 orbifold 上的谱卷积概念，这为利用 GDL 对 orbifold 结构数据进行学习提供了基本构建块。所讨论的理论通过一个来自音乐理论的例子进行说明。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Generalization with Adaptive Optimal Transport Priors for Decision-Focused Learning</div>
<div class="meta-line">Authors: Haixiang Sun, Andrew L. Liu</div>
<div class="meta-line">Venue: The 29th International Conference on Artificial Intelligence and Statistics (AISTATS), 2026</div>
<div class="meta-line">First: 2026-02-01T20:22:41+00:00 · Latest: 2026-02-16T18:27:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01427v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01427v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Few-shot learning requires models to generalize under limited supervision while remaining robust to distribution shifts. Existing Sinkhorn Distributionally Robust Optimization (DRO) methods provide theoretical guarantees but rely on a fixed reference distribution, which limits their adaptability. We propose a Prototype-Guided Distributionally Robust Optimization (PG-DRO) framework that learns class-adaptive priors from abundant base data via hierarchical optimal transport and embeds them into the Sinkhorn DRO formulation. This design enables few-shot information to be organically integrated into producing class-specific robust decisions that are both theoretically grounded and efficient, and further aligns the uncertainty set with transferable structural knowledge. Experiments show that PG-DRO achieves stronger robust generalization in few-shot scenarios, outperforming both standard learners and DRO baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向决策的自适应最优传输先验的鲁棒泛化学习</div>
<div class="mono" style="margin-top:8px">少样本学习要求模型在有限监督下进行泛化，同时保持对分布变化的鲁棒性。现有的Sinkhorn分布鲁棒优化（DRO）方法提供了理论保证，但依赖固定的参考分布，限制了其适应性。我们提出了一种原型引导的分布鲁棒优化（PG-DRO）框架，通过分层最优传输从丰富的基础数据中学习类别自适应的先验，并将其嵌入到Sinkhorn DRO公式中。这种设计使少样本信息能够有机地整合到生成类别特定鲁棒决策的过程中，这些决策既具有理论依据又高效，并进一步使不确定性集与可迁移的结构知识对齐。实验表明，PG-DRO在少样本场景下实现了更强的鲁棒泛化，优于标准学习器和DRO基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">On the Semantics of Primary Cause in Hybrid Dynamic Domains</div>
<div class="meta-line">Authors: Shakil M. Khan, Asim Mehmood, Sandra Zilles</div>
<div class="meta-line">First: 2026-02-16T18:25:08+00:00 · Latest: 2026-02-16T18:25:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14994v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14994v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning about actual causes of observed effects is fundamental to the study of rationality. This important problem has been studied since the time of Aristotle, with formal mathematical accounts emerging recently. We live in a world where change due to actions can be both discrete and continuous, that is, hybrid. Yet, despite extensive research on actual causation, only few recent studies looked into causation with continuous change. Building on recent progress, in this paper we propose two definitions of primary cause in a hybrid action-theoretic framework, namely the hybrid temporal situation calculus. One of these is foundational in nature while the other formalizes causation through contributions, which can then be verified from a counterfactual perspective using a modified ``but-for&#x27;&#x27; test. We prove that these two definitions are indeed equivalent. We then show that our definitions of causation have some intuitively justifiable properties.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>论混合动态领域中主要原因的语义</div>
<div class="mono" style="margin-top:8px">对观察到的效果的实际原因进行推理是理性研究的核心。这一重要问题自亚里士多德时代以来便受到关注，最近才出现形式化的数学分析。我们生活在一个由行动引发的变化既可以是离散的也可以是连续的世界，即混合世界。然而，尽管对实际因果关系有大量研究，只有少数近期研究关注了连续变化中的因果关系。基于近期进展，本文在混合行动理论框架，即混合时态情境演算中，提出了主要原因的两个定义。其中一个定义具有基础性质，另一个则通过贡献来形式化因果关系，随后可以借助修改后的“but-for”检验从反事实视角进行验证。我们证明了这两个定义实际上是等价的。接着，我们展示了我们对因果关系的定义具有一些直观合理的性质。</div>
</details>
</div>
<div class="card">
<div class="title">ThermEval: A Structured Benchmark for Evaluation of Vision-Language Models on Thermal Imagery</div>
<div class="meta-line">Authors: Ayush Shrivastava, Kirtan Gangani, Laksh Jain, Mayank Goel, Nipun Batra</div>
<div class="meta-line">First: 2026-02-16T18:16:19+00:00 · Latest: 2026-02-16T18:16:19+00:00</div>
<div class="meta-line">Comments: 8 Pages with 2 figures of main content. 2 pages of References. 10 pages of appendix with 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14989v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14989v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision language models (VLMs) achieve strong performance on RGB imagery, but they do not generalize to thermal images. Thermal sensing plays a critical role in settings where visible light fails, including nighttime surveillance, search and rescue, autonomous driving, and medical screening. Unlike RGB imagery, thermal images encode physical temperature rather than color or texture, requiring perceptual and reasoning capabilities that existing RGB-centric benchmarks do not evaluate. We introduce ThermEval-B, a structured benchmark of approximately 55,000 thermal visual question answering pairs designed to assess the foundational primitives required for thermal vision language understanding. ThermEval-B integrates public datasets with our newly collected ThermEval-D, the first dataset to provide dense per-pixel temperature maps with semantic body-part annotations across diverse indoor and outdoor environments. Evaluating 25 open-source and closed-source VLMs, we find that models consistently fail at temperature-grounded reasoning, degrade under colormap transformations, and default to language priors or fixed responses, with only marginal gains from prompting or supervised fine-tuning. These results demonstrate that thermal understanding requires dedicated evaluation beyond RGB-centric assumptions, positioning ThermEval as a benchmark to drive progress in thermal vision language modeling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ThermEval：用于热成像视觉语言模型评估的结构化基准</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在RGB图像上表现出色，但无法推广到热成像。热感知在可见光失效的场景中起着关键作用，包括夜间监控、搜救、自动驾驶和医疗筛查。与RGB图像不同，热图像编码的是物理温度而非颜色或纹理，需要现有以RGB为中心的基准未评估的感知和推理能力。我们引入了ThermEval-B，这是一个包含约55,000个热视觉问答对的结构化基准，旨在评估热视觉语言理解所需的基础能力。ThermEval-B整合了公开数据集与我们新收集的ThermEval-D数据集，后者是首个提供密集的每像素温度图并包含语义身体部位标注的多样化室内和室外环境数据集。通过评估25个开源和闭源的VLMs，我们发现模型在基于温度的推理上普遍失败，在色图变换下性能下降，并倾向于依赖语言先验或固定响应，仅通过提示或监督微调获得有限提升。这些结果表明，热理解需要超越RGB中心假设的专门评估，ThermEval成为推动热视觉语言建模发展的基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision language models (VLMs) achieve strong performance on RGB imagery, but they do not generalize to thermal images.</div>
</details>
</div>
<div class="card">
<div class="title">Method for noise-induced regularization in quantum neural networks</div>
<div class="meta-line">Authors: Viacheslav Kuzmin, Wilfrid Somogyi, Ekaterina Pankovets, Alexey Melnikov</div>
<div class="meta-line">Venue: Adv. Quantum Technol. 8(12), e00603 (2025)</div>
<div class="meta-line">First: 2024-10-25T18:29:42+00:00 · Latest: 2026-02-16T18:12:59+00:00</div>
<div class="meta-line">Comments: 12 pages, 5 figures, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.19921v2">Abs</a> · <a href="https://arxiv.org/pdf/2410.19921v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the current quantum computing paradigm, significant focus is placed on the reduction or mitigation of quantum decoherence. When designing new quantum processing units, the general objective is to reduce the amount of noise qubits are subject to, and in algorithm design, a large effort is underway to provide scalable error correction or mitigation techniques. Yet some previous work has indicated that certain classes of quantum algorithms, such as quantum machine learning, may, in fact, be intrinsically robust to or even benefit from the presence of a small amount of noise. Here, we demonstrate that noise levels in quantum hardware can be effectively tuned to enhance the ability of quantum neural networks to generalize data, acting akin to regularisation in classical neural networks. As an example, we consider two regression tasks, where, by tuning the noise level in the circuit, we demonstrated improvement of the validation mean squared error loss. Moreover, we demonstrate the method&#x27;s effectiveness by numerically simulating quantum neural network training on a realistic model of a noisy superconducting quantum computer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>量子神经网络中噪声诱导正则化的方法</div>
<div class="mono" style="margin-top:8px">在当前的量子计算范式中，重点在于减少或缓解量子退相干。在设计新的量子处理单元时，普遍目标是减少量子比特所受的噪声量，在算法设计中，也正在努力提供可扩展的错误校正或缓解技术。然而，一些先前的研究表明，某些类别的量子算法，如量子机器学习，实际上可能对少量噪声具有内在鲁棒性，甚至可能从中受益。在此，我们展示通过调节量子硬件中的噪声水平，可以有效增强量子神经网络对数据的泛化能力，其作用类似于经典神经网络中的正则化。作为例子，我们考虑了两个回归任务，通过调节电路中的噪声水平，我们展示了验证均方误差损失的改善。此外，我们通过在现实噪声超导量子计算机的模型上进行数值模拟，展示了该方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Orthogonalized Multimodal Contrastive Learning with Asymmetric Masking for Structured Representations</div>
<div class="meta-line">Authors: Carolin Cissee, Raneen Younis, Zahra Ahmadi</div>
<div class="meta-line">First: 2026-02-16T18:06:53+00:00 · Latest: 2026-02-16T18:06:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.14983v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.14983v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal learning seeks to integrate information from heterogeneous sources, where signals may be shared across modalities, specific to individual modalities, or emerge only through their interaction. While self-supervised multimodal contrastive learning has achieved remarkable progress, most existing methods predominantly capture redundant cross-modal signals, often neglecting modality-specific (unique) and interaction-driven (synergistic) information. Recent extensions broaden this perspective, yet they either fail to explicitly model synergistic interactions or learn different information components in an entangled manner, leading to incomplete representations and potential information leakage. We introduce \textbf{COrAL}, a principled framework that explicitly and simultaneously preserves redundant, unique, and synergistic information within multimodal representations. COrAL employs a dual-path architecture with orthogonality constraints to disentangle shared and modality-specific features, ensuring a clean separation of information components. To promote synergy modeling, we introduce asymmetric masking with complementary view-specific patterns, compelling the model to infer cross-modal dependencies rather than rely solely on redundant cues. Extensive experiments on synthetic benchmarks and diverse MultiBench datasets demonstrate that COrAL consistently matches or outperforms state-of-the-art methods while exhibiting low performance variance across runs. These results indicate that explicitly modeling the full spectrum of multimodal information yields more stable, reliable, and comprehensive embeddings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于结构化表示的正交化多模态对比学习与非对称掩码</div>
<div class="mono" style="margin-top:8px">多模态学习旨在整合异构来源的信息，其中信号可能在模态间共享，特定于单个模态，或仅通过模态间的交互产生。尽管自监督多模态对比学习取得了显著进展，但大多数现有方法主要捕捉冗余的跨模态信号，往往忽略了模态特定（独特）和交互驱动（协同）的信息。最近的扩展方法拓宽了这一视角，但它们要么未能显式建模协同交互，要么以纠缠的方式学习不同的信息成分，导致表示不完整和潜在的信息泄露。我们引入\textbf{COrAL}，一个原理性的框架，能够在多模态表示中显式且同时保留冗余、独特和协同的信息。COrAL采用双路径架构并施加正交性约束，以分离共享特征和模态特定特征，确保信息成分的清晰划分。为了促进协同建模，我们引入了具有互补视图特定模式的非对称掩码，迫使模型推断跨模态依赖关系，而非仅依赖冗余线索。在合成基准和多样化的MultiBench数据集上的大量实验表明，COrAL在保持与现有最先进方法相当或更优性能的同时，其运行间的性能方差较低。这些结果表明，显式建模多模态信息的完整谱系可以产生更稳定、可靠和全面的嵌入。</div>
</details>
</div>
<div class="card">
<div class="title">Accelerating Scientific Research with Gemini: Case Studies and Common Techniques</div>
<div class="meta-line">Authors: David P. Woodruff, Vincent Cohen-Addad, Lalit Jain, Jieming Mao, Song Zuo, MohammadHossein Bateni, Simina Branzei, Michael P. Brenner, Lin Chen, Ying Feng, Lance Fortnow, Gang Fu, Ziyi Guan, Zahra Hadizadeh, Mohammad T. Hajiaghayi, Mahdi JafariRaviz, Adel Javanmard, Karthik C. S., Ken-ichi Kawarabayashi, Ravi Kumar, Silvio Lattanzi, Euiwoong Lee, Yi Li, Ioannis Panageas, Dimitris Paparas, Benjamin Przybocki, Bernardo Subercaseaux, Ola Svensson, Shayan Taherijam, Xuan Wu, Eylon Yogev, Morteza Zadimoghaddam, Samson Zhou, Yossi Matias, James Manyika, Vahab Mirrokni</div>
<div class="meta-line">First: 2026-02-03T18:56:17+00:00 · Latest: 2026-02-16T18:02:06+00:00</div>
<div class="meta-line">Comments: Author list now includes Yossi Matias and James Manyika. Acknowledgements also updated. Added more general discussion to sections 1, 9.1, and 9.5. Discussed related work of Gurvits in section 4.3. Clarified closed form in section 6.1 and gave finite sum expansions for coefficients. Other minor formatting fixes</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03837v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.03837v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google&#x27;s Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a &quot;neuro-symbolic&quot; loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用Gemini加速科学研究：案例分析与常用技术</div>
<div class="mono" style="margin-top:8px">近年来，大语言模型（LLMs）的进展为加速科学研究开辟了新的途径。虽然这些模型在协助完成常规任务方面越来越强大，但它们在促进新颖、专家级数学发现方面的能力仍不为人所知。我们展示了一系列案例研究，说明研究人员如何成功与先进的AI模型（特别是Google的Gemini模型，包括Gemini Deep Think及其高级变体）合作，解决理论计算机科学及其他领域（如经济学、优化和物理学）中的开放性问题、反驳猜想并生成新证明。基于这些经验，我们提炼出一些有效的人机协作技术，例如迭代优化、问题分解和跨学科知识迁移。尽管我们大多数成果源于这种互动、对话式的方法，但也强调了一些超越标准聊天界面的具体实例。这些实例包括将模型作为严格的对抗性审稿人，以检测现有证明中的细微缺陷，以及将其嵌入到一个“神经符号”循环中，自主编写和执行代码以验证复杂的推导过程。这些例子共同展示了AI不仅作为自动化工具的潜力，更作为科学发现创造性过程中的多功能、真诚的合作伙伴。</div>
</details>
</div>
<div class="card">
<div class="title">Evolution Strategies at the Hyperscale</div>
<div class="meta-line">Authors: Bidipta Sarkar, Mattie Fellows, Juan Agustin Duque, Alistair Letcher, Antonio León Villares, Anya Sims, Clarisse Wibault, Dmitry Samsonov, Dylan Cope, Jarek Liesen, Kang Li, Lukas Seier, Theo Wolf, Uljad Berdica, Valentin Mohl, Alexander David Goldie, Aaron Courville, Karin Sevegnani, Shimon Whiteson, Jakob Nicolaus Foerster</div>
<div class="meta-line">First: 2025-11-20T18:56:05+00:00 · Latest: 2026-02-16T18:01:18+00:00</div>
<div class="meta-line">Comments: 76 pages, 15 figures, Website at https://eshyperscale.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.16652v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.16652v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://eshyperscale.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evolution Strategies (ES) is a class of powerful black-box optimisation methods that are highly parallelisable and can handle non-differentiable and noisy objectives. However, naïve ES becomes prohibitively expensive at scale on GPUs due to the low arithmetic intensity of batched matrix multiplications with unstructured random perturbations. We introduce Evolution Guided GeneRal Optimisation via Low-rank Learning (EGGROLL), which improves arithmetic intensity by structuring individual perturbations as rank-$r$ matrices, resulting in a hundredfold increase in training speed for billion-parameter models at large population sizes, achieving up to 91% of the throughput of pure batch inference. We provide a rigorous theoretical analysis of Gaussian ES for high-dimensional parameter objectives, investigating conditions needed for ES updates to converge in high dimensions. Our results reveal a linearising effect, and proving consistency between EGGROLL and ES as parameter dimension increases. Our experiments show that EGGROLL: (1) enables the stable pretraining of nonlinear recurrent language models that operate purely in integer datatypes, (2) is competitive with GRPO for post-training LLMs on reasoning tasks, and (3) does not compromise performance compared to ES in tabula rasa RL settings, despite being faster.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大规模下的进化策略</div>
<div class="mono" style="margin-top:8px">进化策略（Evolution Strategies, ES）是一类强大的黑箱优化方法，具有高度并行化能力，能够处理不可微和噪声干扰的目标函数。然而，传统的ES在GPU上进行大规模训练时成本变得过高，这是由于无结构随机扰动下的批量矩阵乘法算术强度较低。我们引入了通过低秩学习实现的进化引导通用优化方法（EGGROLL），通过将个体扰动结构化为秩-r矩阵来提高算术强度。在大规模种群下，EGGROLL使十亿参数模型的训练速度提升了百倍，达到纯批量推理吞吐量的91%。我们对高维参数目标的高斯ES进行了严格的理论分析，探讨了在高维空间中ES更新收敛所需的条件。我们的结果揭示了线性化效应，并证明了随着参数维度的增加，EGGROLL与ES之间的一致性。实验表明，EGGROLL：(1) 能够稳定地对仅使用整数数据类型的非线性循环语言模型进行预训练；(2) 在推理任务中与GRPO竞争；(3) 在从头开始的强化学习设置中，尽管速度更快，但性能并不逊色于ES。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260217_0410.html">20260217_0410</a>
<a href="archive/20260216_0403.html">20260216_0403</a>
<a href="archive/20260215_0401.html">20260215_0401</a>
<a href="archive/20260213_0421.html">20260213_0421</a>
<a href="archive/20260212_0425.html">20260212_0425</a>
<a href="archive/20260210_0435.html">20260210_0435</a>
<a href="archive/20260209_0404.html">20260209_0404</a>
<a href="archive/20260208_0353.html">20260208_0353</a>
<a href="archive/20260207_0410.html">20260207_0410</a>
<a href="archive/20260206_0412.html">20260206_0412</a>
<a href="archive/20260205_0417.html">20260205_0417</a>
<a href="archive/20260204_0421.html">20260204_0421</a>
<a href="archive/20260202_0402.html">20260202_0402</a>
<a href="archive/20260201_0354.html">20260201_0354</a>
<a href="archive/20260131_0408.html">20260131_0408</a>
<a href="archive/20260130_0406.html">20260130_0406</a>
<a href="archive/20260129_0407.html">20260129_0407</a>
<a href="archive/20260128_0407.html">20260128_0407</a>
<a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
