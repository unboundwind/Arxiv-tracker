<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-24 03:55</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251224_0355</div>
    <div class="row"><div class="card">
<div class="title">The Prism Hypothesis: Harmonizing Semantic and Pixel Representations via Unified Autoencoding</div>
<div class="meta-line">Authors: Weichen Fan, Haiwen Diao, Quan Wang, Dahua Lin, Ziwei Liu</div>
<div class="meta-line">First: 2025-12-22T18:59:57+00:00 · Latest: 2025-12-22T18:59:57+00:00</div>
<div class="meta-line">Comments: Code link: https://github.com/WeichenFan/UAE</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19693v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19693v1">PDF</a> · <a href="https://github.com/WeichenFan/UAE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep representations across modalities are inherently intertwined. In this paper, we systematically analyze the spectral characteristics of various semantic and pixel encoders. Interestingly, our study uncovers a highly inspiring and rarely explored correspondence between an encoder&#x27;s feature spectrum and its functional role: semantic encoders primarily capture low-frequency components that encode abstract meaning, whereas pixel encoders additionally retain high-frequency information that conveys fine-grained detail. This heuristic finding offers a unifying perspective that ties encoder behavior to its underlying spectral structure. We define it as the Prism Hypothesis, where each data modality can be viewed as a projection of the natural world onto a shared feature spectrum, just like the prism. Building on this insight, we propose Unified Autoencoding (UAE), a model that harmonizes semantic structure and pixel details via an innovative frequency-band modulator, enabling their seamless coexistence. Extensive experiments on ImageNet and MS-COCO benchmarks validate that our UAE effectively unifies semantic abstraction and pixel-level fidelity into a single latent space with state-of-the-art performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>棱镜假说：通过统一自编码器协调语义与像素表示</div>
<div class="mono" style="margin-top:8px">不同模态的深度表示本质上是相互关联的。本文系统分析了各种语义和像素编码器的频谱特性。有趣的是，我们的研究揭示了编码器特征频谱与其功能角色之间高度启发且鲜有探索的对应关系：语义编码器主要捕捉编码抽象意义的低频成分，而像素编码器则额外保留传达细粒度细节的高频信息。这一启发性发现提供了一个统一的视角，将编码器的行为与其底层频谱结构联系起来。我们将这一观点定义为棱镜假说，即每种数据模态都可以视为自然世界投射到共享特征频谱上的图像，就像棱镜一样。基于这一见解，我们提出了统一自编码器（UAE），通过创新的频段调制器协调语义结构和像素细节，实现它们的无缝共存。在ImageNet和MS-COCO基准上的大量实验验证了我们的UAE能够将语义抽象和像素级保真度统一到单一的潜在空间中，并表现出最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Interact2Ar: Full-Body Human-Human Interaction Generation via Autoregressive Diffusion Models</div>
<div class="meta-line">Authors: Pablo Ruiz-Ponce, Sergio Escalera, José García-Rodríguez, Jiankang Deng, Rolandos Alexandros Potamias</div>
<div class="meta-line">First: 2025-12-22T18:59:50+00:00 · Latest: 2025-12-22T18:59:50+00:00</div>
<div class="meta-line">Comments: Project Page: https://pabloruizponce.com/papers/Interact2Ar</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19692v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19692v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://pabloruizponce.com/papers/Interact2Ar">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating realistic human-human interactions is a challenging task that requires not only high-quality individual body and hand motions, but also coherent coordination among all interactants. Due to limitations in available data and increased learning complexity, previous methods tend to ignore hand motions, limiting the realism and expressivity of the interactions. Additionally, current diffusion-based approaches generate entire motion sequences simultaneously, limiting their ability to capture the reactive and adaptive nature of human interactions. To address these limitations, we introduce Interact2Ar, the first end-to-end text-conditioned autoregressive diffusion model for generating full-body, human-human interactions. Interact2Ar incorporates detailed hand kinematics through dedicated parallel branches, enabling high-fidelity full-body generation. Furthermore, we introduce an autoregressive pipeline coupled with a novel memory technique that facilitates adaptation to the inherent variability of human interactions using efficient large context windows. The adaptability of our model enables a series of downstream applications, including temporal motion composition, real-time adaptation to disturbances, and extension beyond dyadic to multi-person scenarios. To validate the generated motions, we introduce a set of robust evaluators and extended metrics designed specifically for assessing full-body interactions. Through quantitative and qualitative experiments, we demonstrate the state-of-the-art performance of Interact2Ar.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Interact2Ar：通过自回归扩散模型生成全身人类互动</div>
<div class="mono" style="margin-top:8px">生成逼真的人类互动是一项具有挑战性的任务，不仅需要高质量的个体身体和手部运动，还需要所有互动者之间的协调一致。由于可用数据的限制和学习复杂性的增加，先前的方法往往忽略手部运动，从而限制了互动的逼真度和表现力。此外，当前基于扩散的方法同时生成整个运动序列，限制了其捕捉人类互动反应性和适应性的能力。为了解决这些限制，我们引入了Interact2Ar，这是首个用于生成全身人类互动的端到端文本条件自回归扩散模型。Interact2Ar通过专门的并行分支纳入详细的双手运动学，实现了高保真的全身生成。此外，我们引入了一种自回归流程，结合了一种新颖的记忆技术，以高效的大上下文窗口促进对人类互动固有变化性的适应。我们模型的适应性使得一系列下游应用成为可能，包括时间序列运动合成、实时扰动适应以及从双人场景扩展到多人场景。为了验证生成的运动，我们引入了一组鲁棒的评估器和专门设计的扩展指标，用于评估全身互动。通过定量和定性实验，我们展示了Interact2Ar的最先进性能。</div>
</details>
</div>
<div class="card">
<div class="title">Scalably Enhancing the Clinical Validity of a Task Benchmark with Physician Oversight</div>
<div class="meta-line">Authors: Junze Ye, Daniel Tawfik, Alex J. Goodell, Nikhil V. Kotha, Mark K. Buyyounouski, Mohsen Bayati</div>
<div class="meta-line">First: 2025-12-22T18:59:34+00:00 · Latest: 2025-12-22T18:59:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19691v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19691v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automating the calculation of clinical risk scores offers a significant opportunity to reduce physician administrative burden and enhance patient care. The current standard for evaluating this capability is MedCalc-Bench, a large-scale dataset constructed using LLM-based feature extraction and rule-based aggregation. However, treating such model-generated benchmarks as static oracles risks enshrining historical model errors as evaluation gold standards, a problem dangerously amplified when these datasets serve as reward signals for Reinforcement Learning (RL). In this work, we propose viewing benchmarks for complex tasks such as clinical score computation as &#x27;&#x27;in-progress living documents&#x27;&#x27; that should be periodically re-evaluated as the processes for creating them improve. We introduce a systematic, physician-in-the-loop pipeline that leverages advanced agentic verifiers to audit and relabel MedCalc-Bench, utilizing automated triage to reserve scarce clinician attention for the most contentious instances. Our audit reveals that a notable fraction of original labels diverge from medical ground truth due to extraction errors, calculator logic mismatches, and clinical ambiguity. To study whether this label noise meaningfully impacts downstream RL training, we fine-tune a Qwen3-8B model via Group Relative Policy Optimization (GRPO) and demonstrate that training on corrected labels yields an 8.7% absolute improvement in accuracy over the original baseline -- validating that label noise materially affects model evaluation. These findings underscore that in safety-critical domains, rigorous benchmark maintenance is a prerequisite for genuine model alignment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过医师监督可扩展地增强任务基准的临床有效性</div>
<div class="mono" style="margin-top:8px">自动化计算临床风险评分提供了显著的机会，以减轻医师的行政负担并提升患者护理。目前评估这一能力的标准是MedCalc-Bench，这是一个使用基于LLM的特征提取和基于规则的聚合构建的大规模数据集。然而，将此类模型生成的基准视为静态或acles可能会将历史模型错误固化为评估标准，而当这些数据集作为强化学习（RL）的奖励信号时，这一问题会变得尤为危险。在本工作中，我们提出将复杂任务（如临床评分计算）的基准视为&#x27;&#x27;进行中的活文档&#x27;&#x27;，应随着创建过程的改进定期重新评估。我们引入了一个系统性的、包含医师的流程管道，利用先进的代理验证器对MedCalc-Bench进行审计和重新标注，并通过自动化分诊将有限的临床医师注意力保留给最具争议的案例。我们的审计发现，原始标签中有一部分由于特征提取错误、计算逻辑不匹配和临床模糊性而偏离了医学真实值。为了研究这种标签噪声是否对下游的RL训练产生实质性影响，我们通过Group Relative Policy Optimization（GRPO）微调了一个Qwen3-8B模型，并证明在纠正后的标签上训练可使准确率比原始基线提升8.7%——验证了标签噪声对模型评估有实质性影响。这些发现强调，在安全关键领域，严格的基准维护是实现真正模型对齐的前提条件。</div>
</details>
</div>
<div class="card">
<div class="title">Pushing the Frontier of Audiovisual Perception with Large-Scale Multimodal Correspondence Learning</div>
<div class="meta-line">Authors: Apoorv Vyas, Heng-Jui Chang, Cheng-Fu Yang, Po-Yao Huang, Luya Gao, Julius Richter, Sanyuan Chen, Matt Le, Piotr Dollár, Christoph Feichtenhofer, Ann Lee, Wei-Ning Hsu</div>
<div class="meta-line">First: 2025-12-22T18:59:07+00:00 · Latest: 2025-12-22T18:59:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19687v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19687v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Perception Encoder Audiovisual, PE-AV, a new family of encoders for audio and video understanding trained with scaled contrastive learning. Built on PE, PE-AV makes several key contributions to extend representations to audio, and natively support joint embeddings across audio-video, audio-text, and video-text modalities. PE-AV&#x27;s unified cross-modal embeddings enable novel tasks such as speech retrieval, and set a new state of the art across standard audio and video benchmarks. We unlock this by building a strong audiovisual data engine that synthesizes high-quality captions for O(100M) audio-video pairs, enabling large-scale supervision consistent across modalities. Our audio data includes speech, music, and general sound effects-avoiding single-domain limitations common in prior work. We exploit ten pairwise contrastive objectives, showing that scaling cross-modality and caption-type pairs strengthens alignment and improves zero-shot performance. We further develop PE-A-Frame by fine-tuning PE-AV with frame-level contrastive objectives, enabling fine-grained audio-frame-to-text alignment for tasks such as sound event detection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用大规模多模态对应学习推动视听感知的边界</div>
<div class="mono" style="margin-top:8px">我们引入了Perception Encoder Audiovisual（PE-AV），这是一种用于音频和视频理解的新编码器家族，通过扩展对比学习进行训练。基于PE，PE-AV在扩展表示以支持音频、并原生支持跨音频-视频、音频-文本和视频-文本模态的联合嵌入方面做出了关键贡献。PE-AV的统一跨模态嵌入使得诸如语音检索等新任务成为可能，并在标准音频和视频基准测试中设立了新的性能标杆。我们通过构建一个强大的视听数据引擎实现这一点，该引擎为O(100M)音频-视频对合成高质量的字幕，从而实现跨模态的一致大规模监督。我们的音频数据包括语音、音乐和一般音效，避免了以往工作中常见的单一领域限制。我们利用了十个成对的对比学习目标，表明跨模态和字幕类型对的扩展能够增强对齐效果并提升零样本性能。我们进一步开发了PE-A-Frame，通过使用帧级对比学习目标对PE-AV进行微调，从而实现细粒度的音频帧到文本对齐，适用于诸如声音事件检测等任务。</div>
</details>
</div>
<div class="card">
<div class="title">Visual-Aware CoT: Achieving High-Fidelity Visual Consistency in Unified Models</div>
<div class="meta-line">Authors: Zixuan Ye, Quande Liu, Cong Wei, Yuanxing Zhang, Xintao Wang, Pengfei Wan, Kun Gai, Wenhan Luo</div>
<div class="meta-line">First: 2025-12-22T18:59:03+00:00 · Latest: 2025-12-22T18:59:03+00:00</div>
<div class="meta-line">Comments: Project Page: https://zixuan-ye.github.io/VACoT/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19686v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19686v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://zixuan-ye.github.io/VACoT/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, the introduction of Chain-of-Thought (CoT) has largely improved the generation ability of unified models. However, it is observed that the current thinking process during generation mainly focuses on the text consistency with the text prompt, ignoring the \textbf{visual context consistency} with the visual reference images during the multi-modal generation, e.g., multi-reference generation. The lack of such consistency results in the failure in maintaining key visual features (like human ID, object attribute, style). To this end, we integrate the visual context consistency into the reasoning of unified models, explicitly motivating the model to sustain such consistency by 1) Adaptive Visual Planning: generating structured visual check list to figure out the visual element of needed consistency keeping, and 2) Iterative Visual Correction: performing self-reflection with the guidance of check lists and refining the generated result in an iterative manner. To achieve this, we use supervised finetuning to teach the model how to plan the visual checking, conduct self-reflection and self-refinement, and use flow-GRPO to further enhance the visual consistency through a customized visual checking reward. The experiments show that our method outperforms both zero-shot unified models and those with text CoTs in multi-modal generation, demonstrating higher visual context consistency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉感知推理：在统一模型中实现高保真视觉一致性</div>
<div class="mono" style="margin-top:8px">最近，链式思维（Chain-of-Thought, CoT）的引入显著提升了统一模型的生成能力。然而，观察到当前生成过程中的思维主要关注文本与文本提示的一致性，忽略了多模态生成中与视觉参考图像的\textbf{视觉上下文一致性}，例如多参考图像生成。缺乏这种一致性导致关键视觉特征（如人物ID、物体属性、风格）无法被有效保持。为此，我们将视觉上下文一致性整合到统一模型的推理过程中，通过1）自适应视觉规划：生成结构化的视觉检查清单以确定需要保持一致的视觉元素，以及2）迭代视觉修正：在检查清单的指导下进行自我反思，并以迭代方式优化生成结果。为实现这一目标，我们使用监督微调来训练模型如何进行视觉检查规划、自我反思和自我优化，并通过定制化的视觉检查奖励机制进一步提升视觉一致性。实验表明，我们的方法在多模态生成任务中优于零样本统一模型和仅使用文本CoT的方法，展示了更高的视觉上下文一致性。</div>
</details>
</div>
<div class="card">
<div class="title">Zero-shot Reconstruction of In-Scene Object Manipulation from Video</div>
<div class="meta-line">Authors: Dixuan Lin, Tianyou Wang, Zhuoyang Pan, Yufu Wang, Lingjie Liu, Kostas Daniilidis</div>
<div class="meta-line">First: 2025-12-22T18:58:29+00:00 · Latest: 2025-12-22T18:58:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19684v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19684v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We build the first system to address the problem of reconstructing in-scene object manipulation from a monocular RGB video. It is challenging due to ill-posed scene reconstruction, ambiguous hand-object depth, and the need for physically plausible interactions. Existing methods operate in hand centric coordinates and ignore the scene, hindering metric accuracy and practical use. In our method, we first use data-driven foundation models to initialize the core components, including the object mesh and poses, the scene point cloud, and the hand poses. We then apply a two-stage optimization that recovers a complete hand-object motion from grasping to interaction, which remains consistent with the scene information observed in the input video.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从视频中零样本重建场景内物体操作</div>
<div class="mono" style="margin-top:8px">我们构建了首个系统，用于从单目RGB视频中重建场景内物体操作。该问题具有挑战性，原因包括场景重建的不适定性、手-物体深度的模糊性以及需要物理上合理的交互。现有方法通常基于手坐标系，忽略场景信息，从而限制了度量精度和实际应用。在我们的方法中，首先使用数据驱动的基础模型初始化核心组件，包括物体网格和姿态、场景点云以及手部姿态。然后我们应用一个两阶段优化过程，从抓取到交互恢复完整的手-物体运动，该运动与输入视频中观察到的场景信息保持一致。</div>
</details>
</div>
<div class="card">
<div class="title">From Indoor to Open World: Revealing the Spatial Reasoning Gap in MLLMs</div>
<div class="meta-line">Authors: Mingrui Wu, Zhaozhi Wang, Fangjinhua Wang, Jiaolong Yang, Marc Pollefeys, Tong Zhang</div>
<div class="meta-line">First: 2025-12-22T18:58:12+00:00 · Latest: 2025-12-22T18:58:12+00:00</div>
<div class="meta-line">Comments: Project page: https://harmlesssr.github.io/openbench/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19683v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19683v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://harmlesssr.github.io/openbench/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Multimodal Large Language Models (MLLMs) have achieved impressive performance on semantic tasks, their spatial intelligence--crucial for robust and grounded AI systems--remains underdeveloped. Existing benchmarks fall short of diagnosing this limitation: they either focus on overly simplified qualitative reasoning or rely on domain-specific indoor data, constrained by the lack of outdoor datasets with verifiable metric ground truth. To bridge this gap, we introduce a large-scale benchmark built from pedestrian-perspective videos captured with synchronized stereo cameras, LiDAR, and IMU/GPS sensors. This dataset provides metrically precise 3D information, enabling the automatic generation of spatial reasoning questions that span a hierarchical spectrum--from qualitative relational reasoning to quantitative metric and kinematic understanding. Evaluations reveal that the performance gains observed in structured indoor benchmarks vanish in open-world settings. Further analysis using synthetic abnormal scenes and blinding tests confirms that current MLLMs depend heavily on linguistic priors instead of grounded visual reasoning. Our benchmark thus provides a principled platform for diagnosing these limitations and advancing physically grounded spatial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从室内到开放世界：揭示多模态大语言模型中的空间推理差距</div>
<div class="mono" style="margin-top:8px">尽管多模态大语言模型（MLLMs）在语义任务上表现出色，但其空间智能——对于稳健且基于现实的AI系统至关重要——仍不成熟。现有基准测试未能有效诊断这一局限性：它们要么专注于过于简化的定性推理，要么依赖特定领域的室内数据，受限于缺乏具有可验证度量真实值的户外数据集。为弥合这一差距，我们引入了一个基于行人视角视频的大规模基准测试，视频由同步的立体相机、激光雷达和IMU/GPS传感器采集。该数据集提供了精确的三维度量信息，从而能够自动生成涵盖从定性关系推理到定量度量和运动学理解的多层次空间推理问题。评估结果表明，在结构化的室内基准测试中观察到的性能提升在开放世界环境中消失。进一步使用合成异常场景和盲测分析确认了当前MLLMs严重依赖语言先验，而非基于现实的视觉推理。因此，我们的基准测试为诊断这些局限性并推动基于物理的空间智能发展提供了一个原理性的平台。</div>
</details>
</div>
<div class="card">
<div class="title">Discrete Diffusion VLA: Bringing Discrete Diffusion to Action Decoding in Vision-Language-Action Policies</div>
<div class="meta-line">Authors: Zhixuan Liang, Yizhuo Li, Tianshuo Yang, Chengyue Wu, Sitong Mao, Tian Nian, Liuao Pei, Shunbo Zhou, Xiaokang Yang, Jiangmiao Pang, Yao Mu, Ping Luo</div>
<div class="meta-line">First: 2025-08-27T17:39:11+00:00 · Latest: 2025-12-22T18:57:39+00:00</div>
<div class="meta-line">Comments: New experiments on VL retention and new ablations. 18 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.20072v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.20072v3">PDF</a> · <a href="https://github.com/Liang-ZX/DiscreteDiffusionVLA/tree/libero">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions into robot actions. However, prevailing VLAs either generate actions auto-regressively in a fixed left-to-right order or attach separate MLP or diffusion heads outside the backbone, leading to fragmented information pathways and specialized training requirements that hinder a unified, scalable architecture. We present Discrete Diffusion VLA, a unified-transformer policy that models discretized action chunks with discrete diffusion. The design retains diffusion&#x27;s progressive refinement paradigm while remaining natively compatible with the discrete token interface of VLMs. Our method achieves an adaptive decoding order that resolves easy action elements before harder ones and uses secondary re-masking to revisit uncertain predictions across refinement rounds, which improves consistency and enables robust error correction. This unified decoder preserves pre-trained vision-language priors, supports parallel decoding, breaks the autoregressive bottleneck, and reduces the number of function evaluations. Discrete Diffusion VLA achieves 96.3% avg. success rates on LIBERO, 71.2% visual matching on SimplerEnv-Fractal and 54.2% overall on SimplerEnv-Bridge. We also provide ablation study on vision-language ability retention on LIBERO-OOD (Out-of-Distribution) benchmark, with our method improving over autoregressive, MLP decoder and continuous diffusion baselines. These findings indicate that discrete-diffusion VLA supports precise action modeling and consistent training, laying groundwork for scaling VLA to larger models and datasets. Our code is available at https://github.com/Liang-ZX/DiscreteDiffusionVLA/tree/libero.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>离散扩散VLA：将离散扩散引入视觉-语言-动作策略中的动作解码</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型通过调整大型视觉-语言主干网络，将图像和指令映射为机器人动作。然而，现有的VLA模型要么以固定左到右的顺序自回归地生成动作，要么在主干网络外部附加独立的MLP或扩散头，导致信息路径碎片化和专门化的训练需求，阻碍了统一且可扩展的架构。我们提出了离散扩散VLA，这是一种统一的Transformer策略，利用离散扩散模型对离散化动作块进行建模。该设计保留了扩散的渐进细化范式，同时原生兼容视觉语言模型（VLMs）的离散标记接口。我们的方法实现了自适应解码顺序，优先处理简单的动作元素，再处理复杂的部分，并通过二次重掩码机制在细化过程中重新审视不确定的预测，从而提升一致性并实现稳健的错误校正。这种统一的解码器保留了预训练的视觉-语言先验知识，支持并行解码，突破了自回归瓶颈，并减少了函数评估次数。离散扩散VLA在LIBERO上实现了96.3%的平均成功率，在SimplerEnv-Fractal上实现了71.2%的视觉匹配率，在SimplerEnv-Bridge上实现了54.2%的整体成功率。我们还在LIBERO-OOD（分布外）基准上进行了视觉-语言能力保留的消融实验，结果显示我们的方法优于自回归、MLP解码器和连续扩散基线。这些发现表明，离散扩散VLA支持精确的动作建模和一致的训练，为将VLA扩展到更大的模型和数据集奠定了基础。我们的代码可在https://github.com/Liang-ZX/DiscreteDiffusionVLA/tree/libero获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action (VLA) models adapt large vision-language backbones to map images and instructions into robot actions.</div>
</details>
</div>
<div class="card">
<div class="title">LiveOIBench: Can Large Language Models Outperform Human Contestants in Informatics Olympiads?</div>
<div class="meta-line">Authors: Kaijian Zou, Aaron Xiong, Yunxiang Zhang, Frederick Zhang, Yueqi Ren, Jirong Yang, Ayoung Lee, Shitanshu Bhushan, Lu Wang</div>
<div class="meta-line">First: 2025-10-10T17:54:24+00:00 · Latest: 2025-12-22T18:56:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.09595v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.09595v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Competitive programming problems increasingly serve as valuable benchmarks to evaluate the coding capabilities of large language models (LLMs) due to their complexity and ease of verification. Yet, current coding benchmarks face limitations such as lack of exceptionally challenging problems, insufficient test case coverage, reliance on online platform APIs that limit accessibility. To address these issues, we introduce LiveOIBench, a comprehensive benchmark featuring 403 expert-curated Olympiad-level competitive programming problems, each with an average of 60 expert-designed test cases. The problems are sourced directly from 72 official contests of 14 Informatics Olympiads in different regions conducted between 2023 and 2025. LiveOIBench distinguishes itself through four key features: (1) meticulously curated high-quality tasks with detailed subtask rubrics and extensive private test cases; (2) direct integration of elite contestant performance data to enable informative comparison against top-performing humans; (3) planned continuous, contamination-free updates from newly released Olympiad problems; and (4) a self-contained evaluation system facilitating offline and easy-to-reproduce assessments. Benchmarking 34 popular general-purpose and reasoning LLMs, we find that GPT-5 achieves a notable 81.76th percentile, a strong result that nonetheless falls short of top human contestants, who usually place above 90th. In contrast, among open-weight reasoning models, GPT-OSS-120B achieves only a 60th percentile, underscoring significant capability disparities from frontier closed models. Detailed analyses indicate that robust reasoning models prioritize precise problem analysis over excessive exploration, suggesting future models should emphasize structured analysis and minimize unnecessary exploration. All data, code, and leaderboard results are publicly available on our website.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LiveOIBench：大语言模型能否在信息学竞赛中超越人类选手？</div>
<div class="mono" style="margin-top:8px">由于其复杂性和易于验证性，竞赛编程问题正日益成为评估大语言模型（LLMs）编码能力的宝贵基准。然而，当前的编码基准存在一些局限，例如缺乏极具挑战性的问题、测试用例覆盖不足、依赖在线平台API从而限制了可访问性。为了解决这些问题，我们引入了LiveOIBench，这是一个包含403个专家整理的竞赛级编程问题的全面基准，每个问题平均有60个专家设计的测试用例。这些问题直接来源于2023年至2025年间在不同地区举办的14个信息学竞赛的72场官方比赛。LiveOIBench通过四个关键特点脱颖而出：(1) 详细子任务评分细则和大量私有测试用例的精心整理的高质量任务；(2) 直接整合精英选手表现数据，以便与顶尖人类选手进行有意义的比较；(3) 计划持续更新，确保无污染地引入新发布的竞赛问题；(4) 一个自包含的评估系统，便于离线和可重复的评估。我们对34个流行的通用型和推理型LLM进行了基准测试，发现GPT-5达到了显著的81.76百分位，这一成绩虽强，但仍未能达到通常超过90百分位的人类顶尖选手。相比之下，开放权重推理模型GPT-OSS-120B仅达到60百分位，突显了其与前沿封闭模型之间存在显著的能力差异。详细分析表明，强大的推理模型更注重精确的问题分析而非过度探索，这表明未来模型应强调结构化分析并减少不必要的探索。所有数据、代码和排行榜结果均可在我们的网站上公开获取。</div>
</details>
</div>
<div class="card">
<div class="title">VA-$π$: Variational Policy Alignment for Pixel-Aware Autoregressive Generation</div>
<div class="meta-line">Authors: Xinyao Liao, Qiyuan He, Kai Xu, Xiaoye Qu, Yicong Li, Wei Wei, Angela Yao</div>
<div class="meta-line">First: 2025-12-22T18:54:30+00:00 · Latest: 2025-12-22T18:54:30+00:00</div>
<div class="meta-line">Comments: 21 pages, 24 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19680v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19680v1">PDF</a> · <a href="https://github.com/Lil-Shake/VA-Pi">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences. However, tokenizers are trained to reconstruct clean images from ground-truth tokens, while AR generators are optimized only for token likelihood. This misalignment leads to generated token sequences that may decode into low-quality images, without direct supervision from the pixel space. We propose VA-$π$, a lightweight post-training framework that directly optimizes AR models with a principled pixel-space objective. VA-$π$ formulates the generator-tokenizer alignment as a variational optimization, deriving an evidence lower bound (ELBO) that unifies pixel reconstruction and autoregressive modeling. To optimize under the discrete token space, VA-$π$ introduces a reinforcement-based alignment strategy that treats the AR generator as a policy, uses pixel-space reconstruction quality as its intrinsic reward. The reward is measured by how well the predicted token sequences can reconstruct the original image under teacher forcing, giving the model direct pixel-level guidance without expensive free-running sampling. The regularization term of the ELBO serves as a natural regularizer, maintaining distributional consistency of tokens. VA-$π$ enables rapid adaptation of existing AR generators, without neither tokenizer retraining nor external reward models. With only 1% ImageNet-1K data and 25 minutes of tuning, it reduces FID from 14.36 to 7.65 and improves IS from 86.55 to 116.70 on LlamaGen-XXL, while also yielding notable gains in the text-to-image task on GenEval for both visual generation model (LlamaGen: from 0.306 to 0.339) and unified multi-modal model (Janus-Pro: from 0.725 to 0.744). Code is available at https://github.com/Lil-Shake/VA-Pi.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VA-π：面向像素感知的自回归生成的变分策略对齐</div>
<div class="mono" style="margin-top:8px">自回归（AR）视觉生成依赖于将图像映射到和从离散序列中进行转换的分词器。然而，分词器是训练以从真实分词中重建干净图像，而AR生成器仅优化分词的似然。这种不匹配导致生成的分词序列可能解码为低质量图像，而没有来自像素空间的直接监督。我们提出VA-π，一个轻量级的后训练框架，通过一个有原则的像素空间目标直接优化AR模型。VA-π将生成器-分词器对齐建模为一个变分优化问题，推导出一个证据下界（ELBO），统一了像素重建和自回归建模。为了在离散分词空间中进行优化，VA-π引入了一种基于强化学习的对齐策略，将AR生成器视为策略，使用像素空间的重建质量作为其内在奖励。该奖励通过在教师强制下预测的分词序列重建原始图像的质量来衡量，从而在不使用昂贵的自由运行采样的情况下，为模型提供直接的像素级指导。ELBO的正则化项作为自然的正则化器，保持分词的分布一致性。VA-π能够在不重新训练分词器或外部奖励模型的情况下，快速适应现有的AR生成器。仅使用1%的ImageNet-1K数据和25分钟的调参，它在LlamaGen-XXL上将FID从14.36降低到7.65，并将IS从86.55提升到116.70。同时，在GenEval的文本到图像任务中，它也显著提升了视觉生成模型（LlamaGen：从0.306提升到0.339）和统一多模态模型（Janus-Pro：从0.725提升到0.744）的性能。代码可在https://github.com/Lil-Shake/VA-Pi获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Autoregressive (AR) visual generation relies on tokenizers to map images to and from discrete sequences.</div>
</details>
</div>
<div class="card">
<div class="title">WorldWarp: Propagating 3D Geometry with Asynchronous Video Diffusion</div>
<div class="meta-line">Authors: Hanyang Kong, Xingyi Yang, Xiaoxu Zheng, Xinchao Wang</div>
<div class="meta-line">First: 2025-12-22T18:53:50+00:00 · Latest: 2025-12-22T18:53:50+00:00</div>
<div class="meta-line">Comments: Project page: https://hyokong.github.io/worldwarp-page/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19678v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19678v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hyokong.github.io/worldwarp-page/">Project1</a> · <a href="https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/">Project2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating long-range, geometrically consistent video presents a fundamental dilemma: while consistency demands strict adherence to 3D geometry in pixel space, state-of-the-art generative models operate most effectively in a camera-conditioned latent space. This disconnect causes current methods to struggle with occluded areas and complex camera trajectories. To bridge this gap, we propose WorldWarp, a framework that couples a 3D structural anchor with a 2D generative refiner. To establish geometric grounding, WorldWarp maintains an online 3D geometric cache built via Gaussian Splatting (3DGS). By explicitly warping historical content into novel views, this cache acts as a structural scaffold, ensuring each new frame respects prior geometry. However, static warping inevitably leaves holes and artifacts due to occlusions. We address this using a Spatio-Temporal Diffusion (ST-Diff) model designed for a &quot;fill-and-revise&quot; objective. Our key innovation is a spatio-temporal varying noise schedule: blank regions receive full noise to trigger generation, while warped regions receive partial noise to enable refinement. By dynamically updating the 3D cache at every step, WorldWarp maintains consistency across video chunks. Consequently, it achieves state-of-the-art fidelity by ensuring that 3D logic guides structure while diffusion logic perfects texture. Project page: \href{https://hyokong.github.io/worldwarp-page/}{https://hyokong.github.io/worldwarp-page/}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WorldWarp: 通过异步视频扩散传播3D几何</div>
<div class="mono" style="margin-top:8px">生成长距离、几何一致的视频面临一个根本性难题：虽然一致性要求在像素空间中严格遵循3D几何，但最先进的生成模型在相机条件下的潜在空间中效果最佳。这种脱节导致当前方法在遮挡区域和复杂相机轨迹上表现不佳。为了解决这一问题，我们提出了WorldWarp框架，该框架将3D结构锚点与2D生成细化器相结合。为了建立几何基础，WorldWarp通过高斯点云（3DGS）构建了一个在线的3D几何缓存。通过显式地将历史内容变形到新视角，该缓存充当结构支架，确保每个新帧都尊重先前的几何结构。然而，静态变形不可避免地会在遮挡区域留下空洞和伪影。我们采用了一种专门用于“填充和修正”目标的时空扩散（ST-Diff）模型来解决这个问题。我们的关键创新在于一个时空变化的噪声调度：空白区域接收完整的噪声以触发生成，而变形区域接收部分噪声以实现细化。通过在每一步动态更新3D缓存，WorldWarp能够在视频片段之间保持一致性。因此，它通过确保3D逻辑引导结构，而扩散逻辑完善纹理，实现了最先进的保真度。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Vision Mamba for MRI Super-Resolution via Hybrid Selective Scanning</div>
<div class="meta-line">Authors: Mojtaba Safari, Shansong Wang, Vanessa L Wildman, Mingzhe Hu, Zach Eidex, Chih-Wei Chang, Erik H Middlebrooks, Richard L. J Qiu, Pretesh Patel, Ashesh B. Jania, Hui Mao, Zhen Tian, Xiaofeng Yang</div>
<div class="meta-line">First: 2025-12-22T18:53:13+00:00 · Latest: 2025-12-22T18:53:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19676v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19676v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Background: High-resolution MRI is critical for diagnosis, but long acquisition times limit clinical use. Super-resolution (SR) can enhance resolution post-scan, yet existing deep learning methods face fidelity-efficiency trade-offs. Purpose: To develop a computationally efficient and accurate deep learning framework for MRI SR that preserves anatomical detail for clinical integration. Materials and Methods: We propose a novel SR framework combining multi-head selective state-space models (MHSSM) with a lightweight channel MLP. The model uses 2D patch extraction with hybrid scanning to capture long-range dependencies. Each MambaFormer block integrates MHSSM, depthwise convolutions, and gated channel mixing. Evaluation used 7T brain T1 MP2RAGE maps (n=142) and 1.5T prostate T2w MRI (n=334). Comparisons included Bicubic interpolation, GANs (CycleGAN, Pix2pix, SPSR), transformers (SwinIR), Mamba (MambaIR), and diffusion models (I2SB, Res-SRDiff). Results: Our model achieved superior performance with exceptional efficiency. For 7T brain data: SSIM=0.951+-0.021, PSNR=26.90+-1.41 dB, LPIPS=0.076+-0.022, GMSD=0.083+-0.017, significantly outperforming all baselines (p&lt;0.001). For prostate data: SSIM=0.770+-0.049, PSNR=27.15+-2.19 dB, LPIPS=0.190+-0.095, GMSD=0.087+-0.013. The framework used only 0.9M parameters and 57 GFLOPs, reducing parameters by 99.8% and computation by 97.5% versus Res-SRDiff, while outperforming SwinIR and MambaIR in accuracy and efficiency. Conclusion: The proposed framework provides an efficient, accurate MRI SR solution, delivering enhanced anatomical detail across datasets. Its low computational demand and state-of-the-art performance show strong potential for clinical translation.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Background: High-resolution MRI is critical for diagnosis, but long acquisition times limit clinical use.</div>
</details>
</div>
<div class="card">
<div class="title">Multimodal LLMs for Historical Dataset Construction from Archival Image Scans: German Patents (1877-1918)</div>
<div class="meta-line">Authors: Niclas Griesshaber, Jochen Streb</div>
<div class="meta-line">First: 2025-12-22T18:53:03+00:00 · Latest: 2025-12-22T18:53:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19675v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19675v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We leverage multimodal large language models (LLMs) to construct a dataset of 306,070 German patents (1877-1918) from 9,562 archival image scans using our LLM-based pipeline powered by Gemini-2.5-Pro and Gemini-2.5-Flash-Lite. Our benchmarking exercise provides tentative evidence that multimodal LLMs can create higher quality datasets than our research assistants, while also being more than 795 times faster and 205 times cheaper in constructing the patent dataset from our image corpus. About 20 to 50 patent entries are embedded on each page, arranged in a double-column format and printed in Gothic and Roman fonts. The font and layout complexity of our primary source material suggests to us that multimodal LLMs are a paradigm shift in how datasets are constructed in economic history. We open-source our benchmarking and patent datasets as well as our LLM-based data pipeline, which can be easily adapted to other image corpora using LLM-assisted coding tools, lowering the barriers for less technical researchers. Finally, we explain the economics of deploying LLMs for historical dataset construction and conclude by speculating on the potential implications for the field of economic history.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从档案图像扫描构建历史数据集的多模态大语言模型：德国专利（1877-1918）</div>
<div class="mono" style="margin-top:8px">我们利用多模态大语言模型（LLMs）通过基于Gemini-2.5-Pro和Gemini-2.5-Flash-Lite的LLM管道，从9,562份档案图像扫描中构建了一个包含306,070项德国专利（1877-1918）的数据集。我们的基准测试表明，多模态LLMs在构建专利数据集时，能够生成比我们的研究人员更高的质量数据，同时速度超过研究助理795倍，成本降低205倍。每页大约嵌入20到50项专利条目，采用双栏格式，并以哥特体和罗马体字体印刷。我们原始资料的字体和排版复杂性表明，多模态LLMs在经济史数据集构建中代表了一种范式转变。我们开源了基准测试和专利数据集，以及基于LLM的数据管道，该管道可以轻松适应其他图像语料库，通过LLM辅助编码工具进行调整，从而降低非技术研究人员的使用门槛。最后，我们解释了在历史数据集构建中部署LLMs的经济学，并推测其对经济史领域潜在的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We leverage multimodal large language models (LLMs) to construct a dataset of 306,070 German patents (1877-1918) from 9,562 archival image scans using our LLM-based pipeline powered by Gemini-2.5-Pro and Gemini-2.5-Flash-Lite.</div>
</details>
</div>
<div class="card">
<div class="title">Bottom-up Policy Optimization: Your Language Model Policy Secretly Contains Internal Policies</div>
<div class="meta-line">Authors: Yuqiao Tan, Minzheng Wang, Shizhu He, Huanxuan Liao, Chengfeng Zhao, Qiunan Lu, Tian Liang, Jun Zhao, Kang Liu</div>
<div class="meta-line">First: 2025-12-22T18:51:48+00:00 · Latest: 2025-12-22T18:51:48+00:00</div>
<div class="meta-line">Comments: Preprint. Our code is available at https://github.com/Trae1ounG/BuPO</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19673v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19673v1">PDF</a> · <a href="https://github.com/Trae1ounG/BuPO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing reinforcement learning (RL) approaches treat large language models (LLMs) as a single unified policy, overlooking their internal mechanisms. Understanding how policy evolves across layers and modules is therefore crucial for enabling more targeted optimization and raveling out complex reasoning mechanisms. In this paper, we decompose the language model policy by leveraging the intrinsic split of the Transformer residual stream and the equivalence between the composition of hidden states with the unembedding matrix and the resulting samplable policy. This decomposition reveals Internal Layer Policies, corresponding to contributions from individual layers, and Internal Modular Policies, which align with the self-attention and feed-forward network (FFN) components within each layer. By analyzing the entropy of internal policy, we find that: (a) Early layers keep high entropy for exploration, top layers converge to near-zero entropy for refinement, with convergence patterns varying across model series. (b) LLama&#x27;s prediction space rapidly converges in the final layer, whereas Qwen-series models, especially Qwen3, exhibit a more human-like, progressively structured reasoning pattern. Motivated by these findings, we propose Bottom-up Policy Optimization (BuPO), a novel RL paradigm that directly optimizes the internal layer policy during early training. By aligning training objective at lower layer, BuPO reconstructs foundational reasoning capabilities and achieves superior performance. Extensive experiments on complex reasoning benchmarks demonstrates the effectiveness of our method. Our code is available at https://github.com/Trae1ounG/BuPO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自底向上的策略优化：你的语言模型策略暗含内部策略</div>
<div class="mono" style="margin-top:8px">现有的强化学习（RL）方法将大型语言模型（LLMs）视为单一统一的策略，忽略了其内部机制。因此，理解策略如何在各层和模块中演变对于实现更精准的优化和揭示复杂的推理机制至关重要。在本文中，我们通过利用Transformer残差流的内在分割以及隐藏状态与解嵌入矩阵的组合等价性，对语言模型策略进行分解。这种分解揭示了内部层策略（对应于各层的贡献）和内部模块策略（对应于每层中的自注意力和前馈网络（FFN）组件）。通过分析内部策略的熵，我们发现：(a) 早期层保持高熵以进行探索，顶层收敛至接近零熵以进行精炼，且收敛模式在不同模型系列中有所差异；(b) LLaMA的预测空间在最后一层迅速收敛，而Qwen系列模型，尤其是Qwen3，表现出更接近人类的、逐步结构化的推理模式。基于这些发现，我们提出了自底向上的策略优化（BuPO），这是一种新颖的RL范式，可在早期训练阶段直接优化内部层策略。通过在较低层对齐训练目标，BuPO重建了基础推理能力并实现了更优的性能。我们在复杂推理基准上的大量实验验证了我们方法的有效性。我们的代码可在https://github.com/Trae1ounG/BuPO获取。</div>
</details>
</div>
<div class="card">
<div class="title">Critical percolation on the discrete torus in high dimensions</div>
<div class="meta-line">Authors: Arthur Blanc-Renaudie, Asaf Nachmias</div>
<div class="meta-line">First: 2025-12-22T18:51:41+00:00 · Latest: 2025-12-22T18:51:41+00:00</div>
<div class="meta-line">Comments: 79 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19672v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19672v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider percolation on the discrete torus $\mathbb{Z}_n^d$ at $p_c(\mathbb{Z}^d)$, the critical value for percolation on the corresponding infinite lattice $\mathbb{Z}^d$, and within the scaling window around it. We assume that $d$ is a large enough constant for the nearest neighbor model, or any fixed $d&gt;6$ for spread-out models. We prove that there exist constants $\mathbf{C},\mathbf{C}&#x27;$ depending only on the dimension and the spread-out parameter such that for any $λ\in \mathbb{R}$ if the edge probability is $p_c(\mathbb{Z}^d)+\mathbf{C} λn^{-d/3} + o(n^{-d/3})$, then the joint distribution of the largest clusters normalized by $\mathbf{C}&#x27; n^{-2d/3}$ converges as $n\to \infty$ to the ordered lengths of excursions above past minimum of an inhomogeneous Brownian motion started at $0$ with drift $λ-t$ at time $t\in[0,\infty)$. This canonical limit was identified by Aldous in the context of critical Erdős--Rényi graphs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高维离散环面上的临界渗流</div>
<div class="mono" style="margin-top:8px">我们考虑在离散环面 $\mathbb{Z}_n^d$ 上的渗流，其概率为 $p_c(\mathbb{Z}^d)$，即对应无限格子 $\mathbb{Z}^d$ 上的临界渗流值，并在该值附近的缩放窗口内进行研究。我们假设 $d$ 是足够大的常数，适用于最近邻模型，或任何固定的 $d&gt;6$，适用于扩展模型。我们证明存在仅依赖于维度和扩展参数的常数 $\mathbf{C},\mathbf{C}&#x27;$，使得对于任意 $λ\in \mathbb{R}$，如果边概率为 $p_c(\mathbb{Z}^d)+\mathbf{C} λn^{-d/3} + o(n^{-d/3})$，则最大团簇的联合分布除以 $\mathbf{C}&#x27; n^{-2d/3}$ 后，在 $n\to \infty$ 时收敛于从 0 开始、漂移为 $λ-t$ 的非齐次布朗运动在时间 $t\in[0,\infty)$ 上的过去极小值上方的游走长度的有序分布。这个典型的极限在 Aldous 关于临界 Erdős--Rényi 图的研究中被识别。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We consider percolation on the discrete torus $\mathbb{Z}_n^d$ at $p_c(\mathbb{Z}^d)$, the critical value for percolation on the corresponding infinite lattice $\mathbb{Z}^d$, and within the scaling window around it.</div>
</details>
</div>
<div class="card">
<div class="title">Probing forced responses and causality in data-driven climate emulators: conceptual limitations and the role of reduced-order models</div>
<div class="meta-line">Authors: Fabrizio Falasca</div>
<div class="meta-line">First: 2025-06-27T18:04:36+00:00 · Latest: 2025-12-22T18:48:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.22552v7">Abs</a> · <a href="https://arxiv.org/pdf/2506.22552v7">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A central challenge in climate science and applied mathematics is developing data-driven models of multiscale systems that capture both stationary statistics and responses to external perturbations. Current neural climate emulators aim to resolve the atmosphere-ocean system in all its complexity but often struggle to reproduce forced responses, limiting their use in causal studies such as Green&#x27;s function experiments. To explore the origin of these limitations, we first examine a simplified dynamical system that retains key features of climate variability. We interpret the results through linear response theory, providing a rigorous framework to evaluate neural models beyond stationary statistics and to probe causal mechanisms. We argue that the ability of emulators of multiscale systems to reproduce perturbed statistics depends critically on (i) the choice of an appropriate coarse-grained representation and (ii) careful parameterizations of unresolved processes. These insights highlight reduced-order models, tailored to specific goals, processes, and scales, as valuable alternatives to general-purpose emulators. We next consider a real-world application by developing a neural model to investigate the joint variability of the surface temperature field and radiative fluxes. The model infers a multiplicative noise process directly from data, largely reproduces the system&#x27;s probability distribution, and enables causal studies through forced responses. We discuss its limitations and outline directions for future work. Overall, these results expose key challenges in data-driven modeling of multiscale physical systems and underscore the value of coarse-grained, stochastic approaches, with response theory providing a principled framework to guide model design and enhance causal understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探究数据驱动气候模拟器中的强迫响应与因果性：概念局限性及降阶模型的作用</div>
<div class="mono" style="margin-top:8px">气候科学和应用数学中的一个核心挑战是开发能够捕捉稳定统计特征和对外部扰动响应的数据驱动模型，用于多尺度系统。当前的神经气候模拟器旨在展现大气-海洋系统的全部复杂性，但往往难以再现强迫响应，从而限制了其在因果研究（如格林函数实验）中的应用。为探讨这些局限性的来源，我们首先研究了一个简化动力系统，该系统保留了气候变率的关键特征。我们通过线性响应理论来解释结果，提供了一个严谨的框架，用于评估神经模型超越稳定统计特性，并探究因果机制。我们认为，多尺度系统模拟器再现扰动统计的能力，关键取决于（i）选择合适的粗粒化表示方法，以及（ii）对未解析过程的细致参数化。这些见解突显了针对特定目标、过程和尺度设计的降阶模型，作为通用模拟器的有价值替代方案。随后，我们考虑一个实际应用，通过构建神经模型来研究地表温度场与辐射通量的联合变率。该模型直接从数据中推断乘法噪声过程，基本再现了系统的概率分布，并通过强迫响应实现了因果研究。我们讨论了其局限性，并概述了未来工作的方向。总体而言，这些结果揭示了数据驱动多尺度物理系统建模中的关键挑战，并强调了粗粒化、随机方法的价值，响应理论为模型设计提供了原则性的框架，有助于增强因果理解。</div>
</details>
</div>
<div class="card">
<div class="title">DDAE++: Enhancing Diffusion Models Towards Unified Generative and Discriminative Learning</div>
<div class="meta-line">Authors: Weilai Xiang, Hongyu Yang, Di Huang, Yunhong Wang</div>
<div class="meta-line">First: 2025-05-16T08:47:16+00:00 · Latest: 2025-12-22T18:48:24+00:00</div>
<div class="meta-line">Comments: Updated version. Code available at https://github.com/FutureXiang/ddae_plus_plus</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.10999v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.10999v3">PDF</a> · <a href="https://github.com/FutureXiang/ddae_plus_plus">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While diffusion models excel at image synthesis, useful representations have been shown to emerge from generative pre-training, suggesting a path towards unified generative and discriminative learning. However, suboptimal semantic flow within current architectures can hinder this potential: features encoding the richest high-level semantics are underutilized and diluted when propagating through decoding layers, impeding the formation of an explicit semantic bottleneck layer. To address this, we introduce self-conditioning, a lightweight mechanism that reshapes the model&#x27;s layer-wise semantic hierarchy without external guidance. By aggregating and rerouting intermediate features to guide subsequent decoding layers, our method concentrates more high-level semantics, concurrently strengthening global generative guidance and forming more discriminative representations. This simple approach yields a dual-improvement trend across pixel-space UNet, UViT and latent-space DiT models with minimal overhead. Crucially, it creates an architectural semantic bridge that propagates discriminative improvements into generation and accommodates further techniques such as contrastive self-distillation. Experiments show that our enhanced models, especially self-conditioned DiT, are powerful dual learners that yield strong and transferable representations on image and dense classification tasks, surpassing various generative self-supervised models in linear probing while also improving or maintaining high generation quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DDAE++：提升扩散模型以实现生成与判别学习的统一</div>
<div class="mono" style="margin-top:8px">尽管扩散模型在图像生成方面表现出色，但生成式预训练已被证明能够产生有用的表示，这为实现生成与判别学习的统一提供了路径。然而，当前架构中的语义流动不理想可能会阻碍这一潜力：在解码层传播过程中，编码最丰富高层语义的特征被低估和稀释，阻碍了显式的语义瓶颈层的形成。为了解决这一问题，我们引入了自条件机制，这是一种轻量级方法，可以在不依赖外部指导的情况下重塑模型的逐层语义层次结构。通过聚合和重定向中间特征以指导后续的解码层，我们的方法更集中地保留高层语义，同时增强全局生成指导并形成更具判别性的表示。这种简单的方法在像素空间UNet、UViT和潜在空间DiT模型中实现了双重提升，且开销极小。关键的是，它创建了一个架构上的语义桥梁，将判别性改进传播到生成过程中，并能够容纳诸如对比自蒸馏等进一步技术。实验表明，我们的增强模型，尤其是自条件DiT，在图像和密集分类任务中能够产生强大且可迁移的表示，其在线性探针测试中超越了多种生成式自监督模型，同时在生成质量方面也有所提升或保持。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">While diffusion models excel at image synthesis, useful representations have been shown to emerge from generative pre-training, suggesting a path towards unified generative and discriminative learning.</div>
</details>
</div>
<div class="card">
<div class="title">FMCW Radar Principles and Human Activity Recognition Systems: Foundations, Techniques, and Applications</div>
<div class="meta-line">Authors: Ziqian Bi, Jiawei Xu, Xinyuan Song, Ming Liu</div>
<div class="meta-line">First: 2024-10-11T03:20:56+00:00 · Latest: 2025-12-22T18:46:33+00:00</div>
<div class="meta-line">Comments: 203pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.08483v2">Abs</a> · <a href="https://arxiv.org/pdf/2410.08483v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This book introduces the theoretical foundations of FMCW radar systems, including range and velocity estimation, signal processing techniques, and the generation of radar point clouds. A detailed discussion of Python and MATLAB as the primary programming tools for radar signal processing is provided, including the integration of libraries like NumPy, Matplotlib, and SciPy for data analysis and visualization. In addition, the book covers advanced techniques such as deep learning applications for radar signal processing, focusing on Convolutional Neural Networks (CNNs), Long Short-Term Memory (LSTM) networks, and Transformers for analyzing radar data. Furthermore, it highlights state-of-the-art methods for human activity recognition using radar, leveraging a combination of traditional signal processing techniques and machine learning models. The book is designed to cater to both beginners and experts in radar signal processing, offering practical examples, code implementations, and insights into the future of radar technology in various domains, including autonomous systems and security applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FMCW雷达原理与人体活动识别系统：基础、技术与应用</div>
<div class="mono" style="margin-top:8px">本书介绍了FMCW雷达系统的理论基础，包括距离和速度估计、信号处理技术以及雷达点云的生成。详细讨论了Python和MATLAB作为雷达信号处理的主要编程工具，包括集成NumPy、Matplotlib和SciPy等库进行数据分析和可视化。此外，本书还涵盖了深度学习在雷达信号处理中的应用，重点聚焦于卷积神经网络（CNNs）、长短期记忆网络（LSTM）和Transformer模型，用于分析雷达数据。同时，它还突出了利用雷达进行人体活动识别的最先进技术，结合传统信号处理技术和机器学习模型。本书旨在满足雷达信号处理领域初学者和专家的需求，提供实用示例、代码实现以及雷达技术在多个领域（包括自主系统和安全应用）未来发展方面的见解。</div>
</details>
</div>
<div class="card">
<div class="title">Bridging the Gap Between Scientific Laws Derived by AI Systems and Canonical Knowledge via Abductive Inference with AI-Noether</div>
<div class="meta-line">Authors: Karan Srivastava, Sanjeeb Dash, Ryan Cory-Wright, Barry Trager, Cristina Cornelio, Lior Horesh</div>
<div class="meta-line">First: 2025-09-26T23:50:25+00:00 · Latest: 2025-12-22T18:45:53+00:00</div>
<div class="meta-line">Comments: 47 Pages (20+appendix), 14 Figures, Preprint: Updated for recent submission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23004v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.23004v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Advances in AI have shown great potential in contributing to the acceleration of scientific discovery. Symbolic regression can fit interpretable models to data, but these models are not necessarily derivable from established theory. Recent systems (e.g., AI-Descartes, AI-Hilbert) enforce derivability from prior knowledge. However, when existing theories are incomplete or incorrect, these machine-generated hypotheses may fall outside the theoretical scope. Automatically finding corrections to axiom systems to close this gap remains a central challenge in scientific discovery. We propose a solution: an open-source algebraic geometry-based system that, given an incomplete axiom system expressible as polynomials and a hypothesis that the axioms cannot derive, generates a minimal set of candidate axioms that, when added to the theory, provably derive the (possibly noisy) hypothesis. We illustrate the efficacy of our approach by showing that it can reconstruct key axioms required to derive the carrier-resolved photo-Hall effect, Einstein&#x27;s relativistic laws, and several other laws.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过AI-Noether进行归纳推理弥合AI系统推导的科学定律与规范知识之间的差距</div>
<div class="mono" style="margin-top:8px">人工智能的进步在推动科学发现加速方面展现出巨大潜力。符号回归可以将可解释模型拟合到数据上，但这些模型不一定能从已有的理论中推导出来。最近的系统（如AI-Descartes、AI-Hilbert）强制要求模型从先验知识中推导。然而，当现有理论不完整或错误时，这些机器生成的假设可能超出理论范围。自动寻找公理系统的修正以弥合这一差距仍然是科学发现中的核心挑战。我们提出了解决方案：一个基于代数几何的开源系统，它可以根据一个不完整的公理系统（可表示为多项式）和一个该公理系统无法推导的假设，生成一组最小的候选公理，当这些公理被加入理论后，可以证明该假设（可能包含噪声）能够被推导出来。我们通过展示该方法可以重建推导载流子解析光电霍尔效应、爱因斯坦的相对论定律及其他若干定律所需的关键公理，来说明其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond CLIP: Knowledge-Enhanced Multimodal Transformers for Cross-Modal Alignment in Diabetic Retinopathy Diagnosis</div>
<div class="meta-line">Authors: Argha Kamal Samanta, Harshika Goyal, Vasudha Joshi, Tushar Mungle, Pabitra Mitra</div>
<div class="meta-line">First: 2025-12-22T18:41:45+00:00 · Latest: 2025-12-22T18:41:45+00:00</div>
<div class="meta-line">Comments: 14 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19663v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19663v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diabetic retinopathy (DR) is a leading cause of preventable blindness worldwide, demanding accurate automated diagnostic systems. While general-domain vision-language models like Contrastive Language-Image Pre-Training (CLIP) perform well on natural image tasks, they struggle in medical domain applications, particularly in cross-modal retrieval for ophthalmological images. We propose a novel knowledge-enhanced joint embedding framework that integrates retinal fundus images, clinical text, and structured patient data through a multimodal transformer architecture to address the critical gap in medical image-text alignment. Our approach employs separate encoders for each modality: a Vision Transformer (ViT-B/16) for retinal images, Bio-ClinicalBERT for clinical narratives, and a multilayer perceptron for structured demographic and clinical features. These modalities are fused through a joint transformer with modality-specific embeddings, trained using multiple objectives including contrastive losses between modality pairs, reconstruction losses for images and text, and classification losses for DR severity grading according to ICDR and SDRG schemes. Experimental results on the Brazilian Multilabel Ophthalmological Dataset (BRSET) demonstrate significant improvements over baseline models. Our framework achieves near-perfect text-to-image retrieval performance with Recall@1 of 99.94% compared to fine-tuned CLIP&#x27;s 1.29%, while maintaining state-of-the-art classification accuracy of 97.05% for SDRG and 97.97% for ICDR. Furthermore, zero-shot evaluation on the unseen DeepEyeNet dataset validates strong generalizability with 93.95% Recall@1 versus 0.22% for fine-tuned CLIP. These results demonstrate that our multimodal training approach effectively captures cross-modal relationships in the medical domain, establishing both superior retrieval capabilities and robust diagnostic performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越CLIP：用于糖尿病视网膜病变诊断的多模态知识增强型Transformer模型实现跨模态对齐</div>
<div class="mono" style="margin-top:8px">糖尿病视网膜病变（DR）是全球可预防失明的主要原因之一，需要精确的自动化诊断系统。尽管像对比语言-图像预训练（CLIP）这样的通用领域视觉-语言模型在自然图像任务中表现良好，但在医学领域应用，尤其是眼科图像的跨模态检索方面存在困难。我们提出了一种新颖的知识增强型联合嵌入框架，通过多模态Transformer架构整合视网膜底片图像、临床文本和结构化患者数据，以解决医学图像-文本对齐中的关键差距。我们的方法为每种模态使用单独的编码器：用于视网膜图像的Vision Transformer（ViT-B/16）、用于临床叙述的Bio-ClinicalBERT，以及用于结构化人口统计和临床特征的多层感知机。这些模态通过具有模态特定嵌入的联合Transformer进行融合，训练时采用包括模态对之间的对比损失、图像和文本的重建损失以及根据ICDR和SDRG方案对DR严重程度进行分类的损失等多重目标。在巴西多标签眼科数据集（BRSET）上的实验结果表明，我们的方法在基线模型上取得了显著提升。我们的框架在文本到图像检索任务中实现了接近完美的性能，Recall@1达到99.94%，而微调后的CLIP仅为1.29%。同时，我们的框架在SDRG和ICDR分类任务中分别保持了97.05%和97.97%的最先进分类准确率。此外，在未见过的DeepEyeNet数据集上的零样本评估验证了其强大的泛化能力，Recall@1达到93.95%，而微调后的CLIP仅为0.22%。这些结果表明，我们的多模态训练方法在医学领域有效捕捉了跨模态关系，建立了优越的检索能力和稳健的诊断性能。</div>
</details>
</div>
<div class="card">
<div class="title">A High-Resolution NUV Transmission Spectrum of KELT-9b: Mg II and Fe II Escaping from the Hottest Known Giant Planet</div>
<div class="meta-line">Authors: Austin Baldwin, Joshua D. Lothringer, Leonardo A. Dos Santos, David K. Sing, Zafar Rustamkulov, Nikolay K. Nikolov, Jeff Valenti, Hannah R. Wakeford</div>
<div class="meta-line">First: 2025-12-22T18:41:44+00:00 · Latest: 2025-12-22T18:41:44+00:00</div>
<div class="meta-line">Comments: 20 pages, 12 figures, 5 tables. Accepted in AJ</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19662v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19662v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present high-resolution NUV observations from Hubble Space Telescope&#x27;s (HST) Space Telescope Imaging Spectrograph (STIS) data for the hottest known gas planet, KELT-9b. Observations were collected with STIS/E230M (2300-3000 $Å$, R$\sim$ 30,000) and we de-correlate systematic effects from the telescope using jitter detrending. We show the clear presence of the Mg II doublet at 2800 $Å$ and Fe II at 2600 $Å$ in KELT-9b. The Mg II is measured above the planet&#x27;s Roche transit radius, indicating it is escaping. We fit 1D NLTE atmospheric escape models to these features, demonstrating a significant loss of mass in KELT-9b&#x27;s atmosphere ($\dot{M} \approx 10^{12} $ g/s); we also find a remarkably high line-broadening corresponding to a velocity of about $50-75$ km/s, and a net blueshift of the Mg II doublet greater than 30 km/s. Future 3D MHD modeling of the spectrum and gas kinematics is likely needed to explain these observations. We interpret these results in the context of the Mg II ``Cosmic Shoreline&quot; and show that the detection of escaping Mg II in KELT-9b and the non-detection in WASP-178b are consistent with the hypothesis that stars hotter than $T_{\mathrm{eff}} \sim$ 8250~K have relatively low levels of XUV radiation due to the lack of a chromosphere. Therefore planets around such early-type stars experience a different degree of atmospheric escape. This result highlights the importance of XUV irradiation in driving atmospheric escape inside and outside the Solar System.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KELT-9b的高分辨率NUV透射光谱：逃逸的Mg II和Fe II</div>
<div class="mono" style="margin-top:8px">我们利用哈勃空间望远镜（HST）的太空望远镜成像光谱仪（STIS）数据，对已知最热的气态巨行星KELT-9b进行了高分辨率NUV观测。观测使用STIS/E230M（波长范围2300-3000 Å，分辨率R≈30000）进行，并通过抖动趋势校正去除了望远镜的系统性效应。我们展示了KELT-9b中2800 Å处的Mg II双重线和2600 Å处的Fe II的明显存在。Mg II的测量结果位于行星的罗歇洛过境半径之上，表明其正在逃逸。我们对这些特征拟合了一维非局部热平衡（NLTE）大气逃逸模型，表明KELT-9b的大气质量损失显著（\dot{M} ≈ 10^{12} g/s）；我们还发现了一个显著的谱线展宽，对应约50-75 km/s的速度，以及Mg II双重线的净蓝移超过30 km/s。未来需要对光谱和气体动力学进行三维磁流体动力学（MHD）建模以解释这些观测结果。我们将这些结果置于Mg II的『宇宙海岸线』假说背景下，并表明在KELT-9b中检测到逃逸的Mg II而在WASP-178b中未检测到的结果，与恒星有效温度高于约8250 K时由于缺乏色球层而导致XUV辐射水平较低的假设是一致的。因此，围绕此类早期型恒星的行星经历的逃逸程度有所不同。这一结果突显了XUV辐射在驱动太阳系内外大气逃逸中的重要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present high-resolution NUV observations from Hubble Space Telescope&#x27;s (HST) Space Telescope Imaging Spectrograph (STIS) data for the hottest known gas planet, KELT-9b.</div>
</details>
</div>
<div class="card">
<div class="title">Over++: Generative Video Compositing for Layer Interaction Effects</div>
<div class="meta-line">Authors: Luchao Qi, Jiaye Wu, Jun Myeong Choi, Cary Phillips, Roni Sengupta, Dan B Goldman</div>
<div class="meta-line">First: 2025-12-22T18:39:58+00:00 · Latest: 2025-12-22T18:39:58+00:00</div>
<div class="meta-line">Comments: Project page: https://overplusplus.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19661v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19661v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://overplusplus.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In professional video compositing workflows, artists must manually create environmental interactions-such as shadows, reflections, dust, and splashes-between foreground subjects and background layers. Existing video generative models struggle to preserve the input video while adding such effects, and current video inpainting methods either require costly per-frame masks or yield implausible results. We introduce augmented compositing, a new task that synthesizes realistic, semi-transparent environmental effects conditioned on text prompts and input video layers, while preserving the original scene. To address this task, we present Over++, a video effect generation framework that makes no assumptions about camera pose, scene stationarity, or depth supervision. We construct a paired effect dataset tailored for this task and introduce an unpaired augmentation strategy that preserves text-driven editability. Our method also supports optional mask control and keyframe guidance without requiring dense annotations. Despite training on limited data, Over++ produces diverse and realistic environmental effects and outperforms existing baselines in both effect generation and scene preservation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Over++：用于分层交互效果的生成式视频合成</div>
<div class="mono" style="margin-top:8px">在专业视频合成工作流中，艺术家必须手动创建前景主体与背景图层之间的环境交互效果，如阴影、反射、灰尘和溅射。现有的视频生成模型在保留输入视频的同时添加此类效果方面存在困难，而当前的视频修复方法要么需要昂贵的逐帧掩码，要么产生不合理的修复结果。我们引入了增强合成任务，该任务基于文本提示和输入视频图层合成逼真且半透明的环境效果，同时保留原始场景。为解决这一任务，我们提出了Over++，一个视频效果生成框架，不依赖于相机姿态、场景静止性或深度监督的假设。我们构建了一个针对该任务的配对效果数据集，并引入了一种无需密集标注的非配对增强策略，以保持文本驱动的可编辑性。我们的方法还支持可选的掩码控制和关键帧引导。</div>
</details>
</div>
<div class="card">
<div class="title">Clustering with Label Consistency</div>
<div class="meta-line">Authors: Diptarka Chakraborty, Hendrik Fichtenberger, Bernhard Haeupler, Silvio Lattanzi, Ashkan Norouzi-Fard, Ola Svensson</div>
<div class="meta-line">First: 2025-12-22T18:32:23+00:00 · Latest: 2025-12-22T18:32:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19654v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19654v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Designing efficient, effective, and consistent metric clustering algorithms is a significant challenge attracting growing attention. Traditional approaches focus on the stability of cluster centers; unfortunately, this neglects the real-world need for stable point labels, i.e., stable assignments of points to named sets (clusters). In this paper, we address this gap by initiating the study of label-consistent metric clustering. We first introduce a new notion of consistency, measuring the label distance between two consecutive solutions. Then, armed with this new definition, we design new consistent approximation algorithms for the classical $k$-center and $k$-median problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>带标签一致性的聚类</div>
<div class="mono" style="margin-top:8px">设计高效、有效且一致的度量聚类算法是一个具有挑战性的问题，正吸引越来越多的关注。传统方法关注聚类中心的稳定性；不幸的是，这忽略了现实世界中对点标签稳定性的需求，即点到命名集合（聚类）的稳定分配。本文通过启动对标签一致度量聚类的研究来解决这一问题。我们首先引入一个新的一致性概念，用于衡量两个连续解之间的标签距离。然后，借助这一新定义，我们为经典的 $k$-中心和 $k$-中位问题设计了新的一致近似算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Designing efficient, effective, and consistent metric clustering algorithms is a significant challenge attracting growing attention.</div>
</details>
</div>
<div class="card">
<div class="title">CodeTF: One-stop Transformer Library for State-of-the-art Code LLMs</div>
<div class="meta-line">Authors: Nghi D. Q. Bui, Hung Le, Yue Wang, Junnan Li, Akhilesh Deepak Gotmare, Steven C. H. Hoi</div>
<div class="meta-line">First: 2023-05-31T05:24:48+00:00 · Latest: 2025-12-22T18:29:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2306.00029v2">Abs</a> · <a href="https://arxiv.org/pdf/2306.00029v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Code intelligence plays a key role in transforming modern software engineering. Recently, deep learning-based models, especially Transformer-based large language models (LLMs), have demonstrated remarkable potential in tackling these tasks by leveraging massive open-source code data and programming language features. However, the development and deployment of such models often require expertise in both machine learning and software engineering, creating a barrier for the model adoption. In this paper, we present CodeTF, an open-source Transformer-based library for state-of-the-art Code LLMs and code intelligence. Following the principles of modular design and extensible framework, we design CodeTF with a unified interface to enable rapid access and development across different types of models, datasets and tasks. Our library supports a collection of pretrained Code LLM models and popular code benchmarks, including a standardized interface to train and serve code LLMs efficiently, and data features such as language-specific parsers and utility functions for extracting code attributes. In this paper, we describe the design principles, the architecture, key modules and components, and compare with other related library tools. Finally, we hope CodeTF is able to bridge the gap between machine learning/generative AI and software engineering, providing a comprehensive open-source solution for developers, researchers, and practitioners.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CodeTF：面向前沿代码大语言模型的统一Transformer库</div>
<div class="mono" style="margin-top:8px">代码智能在现代软件工程中起着关键作用。最近，基于深度学习的模型，尤其是基于Transformer的大语言模型（LLMs），在利用大量开源代码数据和编程语言特性解决这些任务方面展现出巨大潜力。然而，这类模型的开发和部署通常需要同时具备机器学习和软件工程方面的专业知识，这成为模型应用的障碍。本文中，我们提出了CodeTF，这是一个面向前沿代码大语言模型和代码智能的开源Transformer库。遵循模块化设计和可扩展框架的原则，我们为CodeTF设计了统一的接口，以实现不同类型模型、数据集和任务的快速访问与开发。我们的库支持一系列预训练的代码大语言模型和流行的代码基准测试，包括标准化的训练和部署接口，以及语言特定的解析器和用于提取代码属性的实用函数。本文中，我们描述了CodeTF的设计原则、架构、关键模块和组件，并与其他相关库工具进行了比较。最后，我们希望CodeTF能够弥合机器学习/生成式AI与软件工程之间的差距，为开发者、研究人员和实践者提供一个全面的开源解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Differentiable Nonlinear Model Predictive Control</div>
<div class="meta-line">Authors: Jonathan Frey, Katrin Baumgärtner, Gianluca Frison, Dirk Reinhardt, Jasper Hoffmann, Leonard Fichtner, Sebastien Gros, Moritz Diehl</div>
<div class="meta-line">First: 2025-05-02T15:43:37+00:00 · Latest: 2025-12-22T18:27:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.01353v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.01353v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The efficient computation of parametric solution sensitivities is a key challenge in the integration of learning-enhanced methods with nonlinear model predictive control (MPC), as their availability is crucial for many learning algorithms. This paper discusses the computation of solution sensitivities of general nonlinear programs (NLPs) using the implicit function theorem (IFT) and smoothed optimality conditions treated in interior-point methods (IPM). We detail sensitivity computation within a sequential quadratic programming (SQP) method which employs an IPM for the quadratic subproblems. Previous works presented in the machine learning community are limited to convex or unconstrained formulations, or lack an implementation for efficient sensitivity evaluation. The publication is accompanied by an efficient open-source implementation within the acados framework, providing both forward and adjoint sensitivities for general optimal control problems, achieving speedups exceeding 3x over the state-of-the-art solvers mpc.pytorch and cvxpygen.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可微非线性模型预测控制</div>
<div class="mono" style="margin-top:8px">参数解灵敏度的高效计算是将增强学习方法与非线性模型预测控制（MPC）集成的关键挑战，因为它们对许多学习算法至关重要。本文讨论了使用隐函数定理（IFT）和内点法（IPM）中处理的平滑最优性条件来计算一般非线性规划（NLP）的解灵敏度。我们详细介绍了在顺序二次规划（SQP）方法中使用内点法求解二次子问题时的灵敏度计算。机器学习领域先前的工作大多局限于凸或无约束的公式，或缺乏用于高效灵敏度评估的实现。本文提供了基于acados框架的高效开源实现，支持一般最优控制问题的前向和伴随灵敏度计算，其速度提升超过现有最先进的求解器mpc.pytorch和cvxpygen的3倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The efficient computation of parametric solution sensitivities is a key challenge in the integration of learning-enhanced methods with nonlinear model predictive control (MPC), as their availability is crucial for many learning algorithms.</div>
</details>
</div>
<div class="card">
<div class="title">AsyMoE: Leveraging Modal Asymmetry for Enhanced Expert Specialization in Large Vision-Language Models</div>
<div class="meta-line">Authors: Heng Zhang, Haichuan Hu, Yaomin Shen, Weihao Yu, Yilei Yuan, Haochen You, Guo Cheng, Zijian Zhang, Lubin Gan, Huihui Wei, Hao Zhang, Jin Huang</div>
<div class="meta-line">First: 2025-09-16T06:16:05+00:00 · Latest: 2025-12-22T18:22:20+00:00</div>
<div class="meta-line">Comments: This submission has been withdrawn by the authors due to a fundamental error in the methodology that affects the validity of the main results</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.12715v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.12715v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (LVLMs) have demonstrated impressive performance on multimodal tasks through scaled architectures and extensive training. However, existing Mixture of Experts (MoE) approaches face challenges due to the asymmetry between visual and linguistic processing. Visual information is spatially complete, while language requires maintaining sequential context. As a result, MoE models struggle to balance modality-specific features and cross-modal interactions. Through systematic analysis, we observe that language experts in deeper layers progressively lose contextual grounding and rely more on parametric knowledge rather than utilizing the provided visual and linguistic information. To address this, we propose AsyMoE, a novel architecture that models this asymmetry using three specialized expert groups. We design intra-modality experts for modality-specific processing, hyperbolic inter-modality experts for hierarchical cross-modal interactions, and evidence-priority language experts to suppress parametric biases and maintain contextual grounding. Extensive experiments demonstrate that AsyMoE achieves 26.58% and 15.45% accuracy improvements over vanilla MoE and modality-specific MoE respectively, with 25.45% fewer activated parameters than dense models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AsyMoE：利用模态不对称性提升大视觉-语言模型中专家专业化</div>
<div class="mono" style="margin-top:8px">大视觉-语言模型（LVLMs）通过扩展架构和大量训练在多模态任务中表现出色。然而，现有的专家混合（MoE）方法由于视觉和语言处理之间的不对称性面临挑战。视觉信息是空间完整的，而语言需要维护序列上下文。因此，MoE模型难以平衡模态特异性特征和跨模态交互。通过系统分析，我们发现深层语言专家逐渐失去上下文基础，更多依赖参数化知识而非利用提供的视觉和语言信息。为了解决这一问题，我们提出了AsyMoE，一种新颖的架构，通过三个专门的专家组建模这种不对称性。我们设计了用于模态特异性处理的同模态专家，用于分层跨模态交互的双曲跨模态专家，以及用于抑制参数偏差并保持上下文基础的证据优先语言专家。大量实验表明，AsyMoE在基础MoE和模态特异性MoE上分别实现了26.58%和15.45%的准确率提升，同时激活参数比密集模型减少了25.45%。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Legendre Transform</div>
<div class="meta-line">Authors: Aleksey Minabutdinov, Patrick Cheridito</div>
<div class="meta-line">Venue: NeurIPS 2025 poster</div>
<div class="meta-line">First: 2025-12-22T18:22:11+00:00 · Latest: 2025-12-22T18:22:11+00:00</div>
<div class="meta-line">Comments: Accepted at NeurIPS 2025 (poster). NeurIPS page: https://neurips.cc/virtual/2025/loc/san-diego/poster/120307</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19649v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19649v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce a novel deep learning algorithm for computing convex conjugates of differentiable convex functions, a fundamental operation in convex analysis with various applications in different fields such as optimization, control theory, physics and economics. While traditional numerical methods suffer from the curse of dimensionality and become computationally intractable in high dimensions, more recent neural network-based approaches scale better, but have mostly been studied with the aim of solving optimal transport problems and require the solution of complicated optimization or max-min problems. Using an implicit Fenchel formulation of convex conjugation, our approach facilitates an efficient gradient-based framework for the minimization of approximation errors and, as a byproduct, also provides a posteriori error estimates for the approximation quality. Numerical experiments demonstrate our method&#x27;s ability to deliver accurate results across different high-dimensional examples. Moreover, by employing symbolic regression with Kolmogorov--Arnold networks, it is able to obtain the exact convex conjugates of specific convex functions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度Legendre变换</div>
<div class="mono" style="margin-top:8px">我们提出了一种新颖的深度学习算法，用于计算可微凸函数的凸共轭，这是凸分析中的基本操作，在优化、控制理论、物理和经济学等多个领域有广泛应用。虽然传统数值方法受维度诅咒的影响，在高维情况下计算变得不可行，但近期基于神经网络的方法在扩展性上表现更好，但主要研究目的是解决最优传输问题，且需要求解复杂的优化或极大极小问题。通过使用凸共轭的隐式Fenchel形式，我们的方法提供了一个高效的基于梯度的框架来最小化近似误差，并作为副产品，也提供了后验误差估计以评估近似质量。数值实验表明，我们的方法能够在不同的高维示例中提供准确的结果。此外，通过使用Kolmogorov--Arnold网络进行符号回归，该方法能够获得特定凸函数的精确凸共轭。</div>
</details>
</div>
<div class="card">
<div class="title">GraphGeo: Multi-Agent Debate Framework for Visual Geo-localization with Heterogeneous Graph Neural Networks</div>
<div class="meta-line">Authors: Heng Zheng, Yuling Shi, Xiaodong Gu, Haochen You, Zijian Zhang, Lubin Gan, Hao Zhang, Wenjun Huang, Jin Huang</div>
<div class="meta-line">First: 2025-11-02T11:58:55+00:00 · Latest: 2025-12-22T18:21:18+00:00</div>
<div class="meta-line">Comments: This submission has been withdrawn by the authors due to a fundamental error in the methodology that affects the validity of the main results</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00908v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.00908v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual geo-localization requires extensive geographic knowledge and sophisticated reasoning to determine image locations without GPS metadata. Traditional retrieval methods are constrained by database coverage and quality. Recent Large Vision-Language Models (LVLMs) enable direct location reasoning from image content, yet individual models struggle with diverse geographic regions and complex scenes. Existing multi-agent systems improve performance through model collaboration but treat all agent interactions uniformly. They lack mechanisms to handle conflicting predictions effectively. We propose \textbf{GraphGeo}, a multi-agent debate framework using heterogeneous graph neural networks for visual geo-localization. Our approach models diverse debate relationships through typed edges, distinguishing supportive collaboration, competitive argumentation, and knowledge transfer. We introduce a dual-level debate mechanism combining node-level refinement and edge-level argumentation modeling. A cross-level topology refinement strategy enables co-evolution between graph structure and agent representations. Experiments on multiple benchmarks demonstrate GraphGeo significantly outperforms state-of-the-art methods. Our framework transforms cognitive conflicts between agents into enhanced geo-localization accuracy through structured debate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GraphGeo：基于异构图神经网络的多智能体辩论框架用于无GPS元数据的视觉地理定位</div>
<div class="mono" style="margin-top:8px">视觉地理定位需要大量的地理知识和复杂的推理来确定图像的位置。传统检索方法受限于数据库的覆盖范围和质量。近期的大型视觉-语言模型（LVLMs）能够直接从图像内容进行位置推理，但单个模型在处理多样化的地理区域和复杂场景时表现不佳。现有的多智能体系统通过模型协作提升性能，但将所有智能体交互视为统一处理，缺乏有效处理冲突预测的机制。我们提出\textbf{GraphGeo}，一种基于异构图神经网络的多智能体辩论框架，用于视觉地理定位。我们的方法通过类型边建模多样化的辩论关系，区分支持性协作、竞争性辩论和知识传递。我们引入一种双层级辩论机制，结合节点级细化和边级辩论建模。一种跨层级拓扑结构优化策略使得图结构与智能体表示能够协同进化。在多个基准数据集上的实验表明，GraphGeo显著优于现有最先进的方法。我们的框架通过结构化的辩论将智能体之间的认知冲突转化为更高的地理定位准确性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Visual geo-localization requires extensive geographic knowledge and sophisticated reasoning to determine image locations without GPS metadata.</div>
</details>
</div>
<div class="card">
<div class="title">D3MAS: Decompose, Deduce, and Distribute for Enhanced Knowledge Sharing in Multi-Agent Systems</div>
<div class="meta-line">Authors: Heng Zhang, Yuling Shi, Xiaodong Gu, Haochen You, Zijian Zhang, Lubin Gan, Yilei Yuan, Jin Huang</div>
<div class="meta-line">First: 2025-10-12T13:01:41+00:00 · Latest: 2025-12-22T18:20:32+00:00</div>
<div class="meta-line">Comments: This submission has been withdrawn by the authors due to a fundamental error in the methodology that affects the validity of the main results</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.10585v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.10585v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent systems powered by large language models exhibit strong capabilities in collaborative problem-solving. However, these systems suffer from substantial knowledge redundancy. Agents duplicate efforts in retrieval and reasoning processes. This inefficiency stems from a deeper issue: current architectures lack mechanisms to ensure agents share minimal sufficient information at each operational stage. Empirical analysis reveals an average knowledge duplication rate of 47.3\% across agent communications. We propose D3MAS (Decompose, Deduce, and Distribute), a hierarchical coordination framework addressing redundancy through structural design rather than explicit optimization. The framework organizes collaboration across three coordinated layers. Task decomposition filters irrelevant sub-problems early. Collaborative reasoning captures complementary inference paths across agents. Distributed memory provides access to non-redundant knowledge. These layers coordinate through structured message passing in a unified heterogeneous graph. This cross-layer alignment ensures information remains aligned with actual task needs. Experiments on four challenging datasets show that D3MAS consistently improves reasoning accuracy by 8.7\% to 15.6\% and reduces knowledge redundancy by 46\% on average.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>D3MAS：通过分解、推导和分布提升多智能体系统中的知识共享</div>
<div class="mono" style="margin-top:8px">基于大语言模型的多智能体系统在协作问题解决方面表现出强大的能力。然而，这些系统存在显著的知识冗余问题。智能体在检索和推理过程中重复劳动。这种低效源于更深层次的问题：当前架构缺乏确保智能体在每个操作阶段共享最少必要信息的机制。实证分析显示，智能体间通信的平均知识重复率高达47.3%。我们提出D3MAS（分解、推导和分布），这是一种分层协调框架，通过结构设计而非显式优化来解决冗余问题。该框架在三个协调层上组织协作。任务分解在早期过滤无关子问题。协作推理捕捉智能体间的互补推理路径。分布式记忆提供非冗余知识的访问。这些层通过统一的异构图中的结构化消息传递进行协调。这种跨层对齐确保信息与实际任务需求保持一致。在四个具有挑战性的数据集上的实验表明，D3MAS在推理准确性上平均提升了8.7%至15.6%，并减少了46%的知识冗余。</div>
</details>
</div>
<div class="card">
<div class="title">4D Gaussian Splatting as a Learned Dynamical System</div>
<div class="meta-line">Authors: Arnold Caleb Asiimwe, Carl Vondrick</div>
<div class="meta-line">First: 2025-12-22T18:20:29+00:00 · Latest: 2025-12-22T18:20:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19648v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.19648v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We reinterpret 4D Gaussian Splatting as a continuous-time dynamical system, where scene motion arises from integrating a learned neural dynamical field rather than applying per-frame deformations. This formulation, which we call EvoGS, treats the Gaussian representation as an evolving physical system whose state evolves continuously under a learned motion law. This unlocks capabilities absent in deformation-based approaches:(1) sample-efficient learning from sparse temporal supervision by modeling the underlying motion law; (2) temporal extrapolation enabling forward and backward prediction beyond observed time ranges; and (3) compositional dynamics that allow localized dynamics injection for controllable scene synthesis. Experiments on dynamic scene benchmarks show that EvoGS achieves better motion coherence and temporal consistency compared to deformation-field baselines while maintaining real-time rendering</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将4D高斯泼溅视为学习的动态系统</div>
<div class="mono" style="margin-top:8px">我们将4D高斯泼溅重新诠释为一个连续时间动态系统，其中场景运动来源于对学习到的神经动态场的积分，而非逐帧应用形变。我们称之为EvoGS的这种表述方式，将高斯表示视为一个演化的物理系统，其状态在学习到的运动规律下连续演化。这使得基于变形的方法所不具备的能力得以实现：(1) 通过建模底层运动规律，从稀疏的时间监督中实现样本高效的训练；(2) 时间外推，实现超出观测时间范围的前后预测；(3) 组合动态，允许局部动态注入以实现可控的场景合成。在动态场景基准数据集上的实验表明，EvoGS在保持实时渲染的同时，比基于形变场的方法在运动连贯性和时间一致性方面表现更优。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
