<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-26 04:18</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260226_0418</div>
    <div class="row"><div class="card">
<div class="title">Test-Time Training with KV Binding Is Secretly Linear Attention</div>
<div class="meta-line">Authors: Junchen Liu, Sven Elflein, Or Litany, Zan Gojcic, Ruilong Li</div>
<div class="meta-line">First: 2026-02-24T18:59:30+00:00 · Latest: 2026-02-24T18:59:30+00:00</div>
<div class="meta-line">Comments: Webpage: https://research.nvidia.com/labs/sil/projects/tttla/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21204v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21204v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://research.nvidia.com/labs/sil/projects/tttla/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test-time training (TTT) with KV binding as sequence modeling layer is commonly interpreted as a form of online meta-learning that memorizes a key-value mapping at test time. However, our analysis reveals multiple phenomena that contradict this memorization-based interpretation. Motivated by these findings, we revisit the formulation of TTT and show that a broad class of TTT architectures can be expressed as a form of learned linear attention operator. Beyond explaining previously puzzling model behaviors, this perspective yields multiple practical benefits: it enables principled architectural simplifications, admits fully parallel formulations that preserve performance while improving efficiency, and provides a systematic reduction of diverse TTT variants to a standard linear attention form. Overall, our results reframe TTT not as test-time memorization, but as learned linear attention with enhanced representational capacity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用KV绑定进行测试时训练实际上是秘密的线性注意力</div>
<div class="mono" style="margin-top:8px">将KV绑定作为序列建模层的测试时训练（TTT）通常被解释为一种在线元学习形式，它在测试时记忆键值映射。然而，我们的分析揭示了多个与这种记忆解释相矛盾的现象。基于这些发现，我们重新审视TTT的公式，并展示了一类广泛的TTT架构可以表示为一种学习得到的线性注意力操作。除了解释之前令人困惑的模型行为外，这种视角还带来了多个实际优势：它使架构简化具有理论依据，允许完全并行的公式化形式，在保持性能的同时提高效率，并为各种TTT变体提供系统性的归约方法到标准的线性注意力形式。总体而言，我们的结果将TTT重新定义为一种增强表示能力的学习线性注意力，而非测试时的记忆。</div>
</details>
</div>
<div class="card">
<div class="title">Recursive Self-Aggregation Unlocks Deep Thinking in Large Language Models</div>
<div class="meta-line">Authors: Siddarth Venkatraman, Vineet Jain, Sarthak Mittal, Vedant Shah, Johan Obando-Ceron, Yoshua Bengio, Brian R. Bartoldson, Bhavya Kailkhura, Guillaume Lajoie, Glen Berseth, Nikolay Malkin, Moksh Jain</div>
<div class="meta-line">First: 2025-09-30T17:58:03+00:00 · Latest: 2026-02-24T18:58:30+00:00</div>
<div class="meta-line">Comments: 23 pages, 10 figures. Project page: https://rsa-llm.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.26626v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.26626v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://rsa-llm.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test-time scaling methods improve the capabilities of large language models (LLMs) by increasing the amount of compute used during inference to make a prediction. Inference-time compute can be scaled in parallel by choosing among multiple independent solutions or sequentially through self-refinement. We propose Recursive Self-Aggregation (RSA), a test-time scaling method inspired by evolutionary methods that combines the benefits of both parallel and sequential scaling. Each step of RSA refines a population of candidate reasoning chains through aggregation of subsets to yield a population of improved solutions, which are then used as the candidate pool for the next iteration. Empirically, RSA delivers substantial performance gains with increasing compute budgets across diverse tasks, model families and sizes. Notably, RSA with Gemini 3 Flash attains performance near the top of the ARC-AGI-2 public leaderboard. RSA also enables Qwen3-4B-Instruct-2507 to achieve competitive performance with larger reasoning models, including DeepSeek-R1 and o3-mini (high), outperforming purely parallel and sequential scaling strategies across AIME-25, HMMT-25, Reasoning Gym, LiveCodeBench-v6, and SuperGPQA. We further propose a novel aggregation-aware reinforcement learning approach that yields significant performance gains by training the model to combine solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>递归自我聚合在大语言模型中释放深度思考</div>
<div class="mono" style="margin-top:8px">测试时扩展方法通过在推理过程中增加计算量来提升大语言模型（LLMs）的能力以进行预测。推理时的计算量可以通过并行选择多个独立解或通过自我优化进行序列扩展。我们提出了一种受进化方法启发的测试时扩展方法——递归自我聚合（RSA），它结合了并行和序列扩展的优势。RSA的每一步通过子集聚合来优化候选推理链的种群，从而得到改进的解集，这些解集随后作为下一轮迭代的候选池。实验证明，随着计算预算的增加，RSA在各种任务、模型家族和规模上都带来了显著的性能提升。值得注意的是，使用Gemini 3 Flash的RSA在ARC-AGI-2公共排行榜上接近榜首。此外，RSA还使Qwen3-4B-Instruct-2507能够与更大的推理模型（如DeepSeek-R1和o3-mini（高））竞争，其性能在AIME-25、HMMT-25、Reasoning Gym、LiveCodeBench-v6和SuperGPQA等数据集上均优于单纯的并行和序列扩展策略。我们进一步提出了一种新颖的聚合感知强化学习方法，通过训练模型结合解来实现显著的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Squint: Fast Visual Reinforcement Learning for Sim-to-Real Robotics</div>
<div class="meta-line">Authors: Abdulaziz Almuzairee, Henrik I. Christensen</div>
<div class="meta-line">First: 2026-02-24T18:58:11+00:00 · Latest: 2026-02-24T18:58:11+00:00</div>
<div class="meta-line">Comments: For website and code, see https://aalmuzairee.github.io/squint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21203v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21203v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://aalmuzairee.github.io/squint">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual reinforcement learning is appealing for robotics but expensive -- off-policy methods are sample-efficient yet slow; on-policy methods parallelize well but waste samples. Recent work has shown that off-policy methods can train faster than on-policy methods in wall-clock time for state-based control. Extending this to vision remains challenging, where high-dimensional input images complicate training dynamics and introduce substantial storage and encoding overhead. To address these challenges, we introduce Squint, a visual Soft Actor Critic method that achieves faster wall-clock training than prior visual off-policy and on-policy methods. Squint achieves this via parallel simulation, a distributional critic, resolution squinting, layer normalization, a tuned update-to-data ratio, and an optimized implementation. We evaluate on the SO-101 Task Set, a new suite of eight manipulation tasks in ManiSkill3 with heavy domain randomization, and demonstrate sim-to-real transfer to a real SO-101 robot. We train policies for 15 minutes on a single RTX 3090 GPU, with most tasks converging in under 6 minutes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Squint：用于模拟到现实机器人快速视觉强化学习</div>
<div class="mono" style="margin-top:8px">视觉强化学习在机器人领域具有吸引力，但成本高昂——离策略方法虽然样本效率高但速度慢；而在线策略方法虽然可以并行化，但浪费样本。最近的研究表明，在基于状态的控制中，离策略方法在实际时间上可以比在线策略方法训练得更快。然而，将其扩展到视觉任务仍具有挑战性，因为高维输入图像会复杂化训练动态，并引入显著的存储和编码开销。为了解决这些挑战，我们引入了Squint，这是一种视觉Soft Actor-Critic方法，能够在实际时间上比以往的视觉离策略和在线策略方法更快地进行训练。Squint通过并行模拟、分布式批评者、分辨率缩小、层归一化、调整后的更新与数据比例以及优化的实现方式实现了这一目标。我们在ManiSkill3中新的SO-101任务集上进行了评估，该任务集包含八个操作任务，并采用大量领域随机化，同时展示了模拟到现实的迁移效果。我们在单个RTX 3090 GPU上训练策略15分钟，大多数任务在6分钟内即可收敛。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Vector Index Compression in Any Modality</div>
<div class="meta-line">Authors: Hanxiang Qin, Alexander Martin, Rohan Jha, Chunsheng Zuo, Reno Kriz, Benjamin Van Durme</div>
<div class="meta-line">First: 2026-02-24T18:57:33+00:00 · Latest: 2026-02-24T18:57:33+00:00</div>
<div class="meta-line">Comments: 12 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21202v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21202v1">PDF</a> · <a href="http://github.com/hanxiangqin/omni-col-press">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study efficient multi-vector retrieval for late interaction in any modality. Late interaction has emerged as a dominant paradigm for information retrieval in text, images, visual documents, and videos, but its computation and storage costs grow linearly with document length, making it costly for image-, video-, and audio-rich corpora. To address this limitation, we explore query-agnostic methods for compressing multi-vector document representations under a constant vector budget. We introduce four approaches for index compression: sequence resizing, memory tokens, hierarchical pooling, and a novel attention-guided clustering (AGC). AGC uses an attention-guided mechanism to identify the most semantically salient regions of a document as cluster centroids and to weight token aggregation. Evaluating these methods on retrieval tasks spanning text (BEIR), visual-document (ViDoRe), and video (MSR-VTT, MultiVENT 2.0), we show that attention-guided clustering consistently outperforms other parameterized compression methods (sequence resizing and memory tokens), provides greater flexibility in index size than non-parametric hierarchical clustering, and achieves competitive or improved performance compared to a full, uncompressed index. The source code is available at: github.com/hanxiangqin/omni-col-press.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>任意模态下的多向量索引压缩</div>
<div class="mono" style="margin-top:8px">我们研究了任意模态中后期交互的高效多向量检索。后期交互已成为文本、图像、视觉文档和视频信息检索的主流范式，但其计算和存储成本随着文档长度线性增长，对于图像、视频和音频丰富的语料库而言成本较高。为了解决这一限制，我们探索了在固定向量预算下，不依赖查询的多向量文档表示压缩方法。我们提出了四种索引压缩方法：序列缩放、记忆标记、分层池化以及一种新颖的注意力引导聚类（AGC）。AGC利用注意力引导机制识别文档中最语义显著的区域作为聚类中心，并对标记聚合进行加权。我们在涵盖文本（BEIR）、视觉文档（ViDoRe）和视频（MSR-VTT、MultiVENT 2.0）的检索任务上评估了这些方法，结果表明注意力引导聚类在性能上优于其他参数化压缩方法（序列缩放和记忆标记），在索引大小的灵活性上优于非参数化分层聚类，并且在性能上与完整未压缩索引相当甚至更优。源代码可在：github.com/hanxiangqin/omni-col-press 获取。</div>
</details>
</div>
<div class="card">
<div class="title">Aletheia tackles FirstProof autonomously</div>
<div class="meta-line">Authors: Tony Feng, Junehyuk Jung, Sang-hyun Kim, Carlo Pagano, Sergei Gukov, Chiang-Chiang Tsai, David Woodruff, Adel Javanmard, Aryan Mokhtari, Dawsen Hwang, Yuri Chervonyi, Jonathan N. Lee, Garrett Bingham, Trieu H. Trinh, Vahab Mirrokni, Quoc V. Le, Thang Luong</div>
<div class="meta-line">First: 2026-02-24T18:56:10+00:00 · Latest: 2026-02-24T18:56:10+00:00</div>
<div class="meta-line">Comments: 34 pages. Project page: https://github.com/google-deepmind/superhuman/tree/main/aletheia</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21201v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21201v1">PDF</a> · <a href="https://github.com/google-deepmind/superhuman/tree/main/aletheia">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We report the performance of Aletheia (Feng et al., 2026b), a mathematics research agent powered by Gemini 3 Deep Think, on the inaugural FirstProof challenge. Within the allowed timeframe of the challenge, Aletheia autonomously solved 6 problems (2, 5, 7, 8, 9, 10) out of 10 according to majority expert assessments; we note that experts were not unanimous on Problem 8 (only). For full transparency, we explain our interpretation of FirstProof and disclose details about our experiments as well as our evaluation. Raw prompts and outputs are available at https://github.com/google-deepmind/superhuman/tree/main/aletheia.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Aletheia 独立应对首个 FirstProof 挑战</div>
<div class="mono" style="margin-top:8px">我们报告了 Aletheia（Feng 等，2026b）在首个 FirstProof 挑战中的表现，Aletheia 是由 Gemini 3 Deep Think 驱动的数学研究代理。根据多数专家评估，在允许的时间范围内，Aletheia 独立解决了 10 道题目中的 6 道（题目 2、5、7、8、9、10）；我们注意到专家对题目 8 的评估并不一致。为确保完全透明，我们解释了我们对 FirstProof 的理解，并披露了实验和评估的详细信息。原始提示和输出可在 https://github.com/google-deepmind/superhuman/tree/main/aletheia 获取。</div>
</details>
</div>
<div class="card">
<div class="title">Learning from Trials and Errors: Reflective Test-Time Planning for Embodied LLMs</div>
<div class="meta-line">Authors: Yining Hong, Huang Huang, Manling Li, Li Fei-Fei, Jiajun Wu, Yejin Choi</div>
<div class="meta-line">First: 2026-02-24T18:55:18+00:00 · Latest: 2026-02-24T18:55:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21198v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21198v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Embodied LLMs endow robots with high-level task reasoning, but they cannot reflect on what went wrong or why, turning deployment into a sequence of independent trials where mistakes repeat rather than accumulate into experience. Drawing upon human reflective practitioners, we introduce Reflective Test-Time Planning, which integrates two modes of reflection: \textit{reflection-in-action}, where the agent uses test-time scaling to generate and score multiple candidate actions using internal reflections before execution; and \textit{reflection-on-action}, which uses test-time training to update both its internal reflection model and its action policy based on external reflections after execution. We also include retrospective reflection, allowing the agent to re-evaluate earlier decisions and perform model updates with hindsight for proper long-horizon credit assignment. Experiments on our newly-designed Long-Horizon Household benchmark and MuJoCo Cupboard Fitting benchmark show significant gains over baseline models, with ablative studies validating the complementary roles of reflection-in-action and reflection-on-action. Qualitative analyses, including real-robot trials, highlight behavioral correction through reflection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从试错中学习：具身LLM的反思性测试时规划</div>
<div class="mono" style="margin-top:8px">具身LLM赋予机器人高级任务推理能力，但它们无法反思哪里出了问题或为何出错，导致部署变成一系列独立的试验，错误重复而非积累为经验。借鉴人类反思性实践者，我们引入了反思性测试时规划，该方法整合了两种反思模式：\textit{行动中的反思}，即代理在执行前利用测试时缩放生成并评分多个候选动作；以及\textit{行动后的反思}，即利用测试时训练，在执行后根据外部反思更新其内部反思模型和动作策略。我们还引入了回顾性反思，使代理能够重新评估之前的决策，并利用后见之明进行模型更新，以实现正确的长时序信用分配。我们在新设计的长时序家庭任务基准和MuJoCo橱柜装配基准上的实验表明，与基线模型相比取得了显著提升，消融研究验证了行动中反思和行动后反思的互补作用。定性分析，包括真实机器人试验，突出了通过反思实现行为修正的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Untied Ulysses: Memory-Efficient Context Parallelism via Headwise Chunking</div>
<div class="meta-line">Authors: Ravi Ghadia, Maksim Abraham, Sergei Vorobyov, Max Ryabinin</div>
<div class="meta-line">First: 2026-02-24T18:54:39+00:00 · Latest: 2026-02-24T18:54:39+00:00</div>
<div class="meta-line">Comments: 14 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21196v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21196v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Efficiently processing long sequences with Transformer models usually requires splitting the computations across accelerators via context parallelism. The dominant approaches in this family of methods, such as Ring Attention or DeepSpeed Ulysses, enable scaling over the context dimension but do not focus on memory efficiency, which limits the sequence lengths they can support. More advanced techniques, such as Fully Pipelined Distributed Transformer or activation offloading, can further extend the possible context length at the cost of training throughput. In this paper, we present UPipe, a simple yet effective context parallelism technique that performs fine-grained chunking at the attention head level. This technique significantly reduces the activation memory usage of self-attention, breaking the activation memory barrier and unlocking much longer context lengths. Our approach reduces intermediate tensor memory usage in the attention layer by as much as 87.5$\%$ for 32B Transformers, while matching previous context parallelism techniques in terms of training speed. UPipe can support the context length of 5M tokens when training Llama3-8B on a single 8$\times$H100 node, improving upon prior methods by over 25$\%$.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Untied Ulysses: 通过头级分块实现的内存高效的上下文并行</div>
<div class="mono" style="margin-top:8px">使用Transformer模型高效处理长序列通常需要通过上下文并行将计算分布在加速器上。该类方法中的主流方案，如Ring Attention或DeepSpeed Ulysses，虽然能够扩展上下文维度的规模，但并未专注于内存效率，从而限制了其支持的序列长度。更先进的技术，如Fully Pipelined Distributed Transformer或激活卸载，虽然可以进一步扩展可能的上下文长度，但会以训练吞吐量为代价。在本文中，我们提出了UPipe，这是一种简单而有效的上下文并行技术，它在注意力头级别进行细粒度分块。该技术显著降低了自注意力的激活内存使用，突破了激活内存的瓶颈，从而解锁了更长的上下文长度。我们的方法在320亿参数的Transformer模型中，将注意力层中的中间张量内存使用减少了高达87.5$\%$，同时在训练速度上与之前的方法相当。UPipe在单个8$\times$H100节点上训练Llama3-8B时，可支持500万token的上下文长度，比之前的方法提升了超过25$\%$。</div>
</details>
</div>
<div class="card">
<div class="title">Region of Interest Segmentation and Morphological Analysis for Membranes in Cryo-Electron Tomography</div>
<div class="meta-line">Authors: Xingyi Cheng, Julien Maufront, Aurélie Di Cicco, Daniël M. Pelt, Manuela Dezi, Daniel Lévy</div>
<div class="meta-line">First: 2026-02-24T18:53:33+00:00 · Latest: 2026-02-24T18:53:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21195v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21195v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cryo-electron tomography (cryo-ET) enables high resolution, three-dimensional reconstruction of biological structures, including membranes and membrane proteins. Identification of regions of interest (ROIs) is central to scientific imaging, as it enables isolation and quantitative analysis of specific structural features within complex datasets. In practice, however, ROIs are typically derived indirectly through full structure segmentation followed by post hoc analysis. This limitation is especially apparent for continuous and geometrically complex structures such as membranes, which are segmented as single entities. Here, we developed TomoROIS-SurfORA, a two step framework for direct, shape-agnostic ROI segmentation and morphological surface analysis. TomoROIS performs deep learning-based ROI segmentation and can be trained from scratch using small annotated datasets, enabling practical application across diverse imaging data. SurfORA processes segmented structures as point clouds and surface meshes to extract quantitative morphological features, including inter-membrane distances, curvature, and surface roughness. It supports both closed and open surfaces, with specific considerations for open surfaces, which are common in cryo-ET due to the missing wedge effect. We demonstrate both tools using in vitro reconstituted membrane systems containing deformable vesicles with complex geometries, enabling automatic quantitative analysis of membrane contact sites and remodeling events such as invagination. While demonstrated here on cryo-ET membrane data, the combined approach is applicable to ROI detection and surface analysis in broader scientific imaging contexts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>冷冻电子断层扫描中膜结构感兴趣区域分割与形态学分析</div>
<div class="mono" style="margin-top:8px">冷冻电子断层扫描（cryo-ET）能够实现生物结构，包括膜和膜蛋白的高分辨率三维重建。感兴趣区域（ROIs）的识别在科学成像中至关重要，因为它可以实现对复杂数据集中特定结构特征的隔离和定量分析。然而，在实际应用中，ROIs通常是通过完整的结构分割后进行后处理分析间接获得的。这种局限性在连续且几何结构复杂的结构如膜中尤为明显，因为这些结构通常被分割为单一实体。为此，我们开发了TomoROIS-SurfORA，一个两步框架，用于直接、形状无关的ROI分割和形态学表面分析。TomoROIS基于深度学习进行ROI分割，并且可以使用小规模标注数据集从头开始训练，从而使其在多样化的成像数据中具有实际应用价值。SurfORA将分割后的结构处理为点云和表面网格，以提取定量形态学特征，包括膜间距离、曲率和表面粗糙度。它支持封闭和开放表面，特别针对由于缺失楔效应在冷冻电子断层扫描中常见的开放表面进行了具体考虑。我们使用体外重构的膜系统（包含具有复杂几何形状的可变形囊泡）来演示这两种工具，从而实现了膜接触位点和如内陷等重塑事件的自动定量分析。虽然此处展示的是针对冷冻电子断层扫描膜数据的工具，但该综合方法也可应用于更广泛的科学成像场景中的ROI检测和表面分析。</div>
</details>
</div>
<div class="card">
<div class="title">On Data Engineering for Scaling LLM Terminal Capabilities</div>
<div class="meta-line">Authors: Renjie Pi, Grace Lam, Mohammad Shoeybi, Pooya Jannaty, Bryan Catanzaro, Wei Ping</div>
<div class="meta-line">First: 2026-02-24T18:51:04+00:00 · Latest: 2026-02-24T18:51:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21193v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21193v1">PDF</a> · <a href="https://huggingface.co/collections/nvidia/nemotron-terminal">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite rapid recent progress in the terminal capabilities of large language models, the training data strategies behind state-of-the-art terminal agents remain largely undisclosed. We address this gap through a systematic study of data engineering practices for terminal agents, making two key contributions: (1) Terminal-Task-Gen, a lightweight synthetic task generation pipeline that supports seed-based and skill-based task construction, and (2) a comprehensive analysis of data and training strategies, including filtering, curriculum learning, long context training, and scaling behavior. Our pipeline yields Terminal-Corpus, a large-scale open-source dataset for terminal tasks. Using this dataset, we train Nemotron-Terminal, a family of models initialized from Qwen3(8B, 14B, 32B) that achieve substantial gains on Terminal-Bench 2.0: Nemotron-Terminal-8B improves from 2.5% to 13.0% Nemotron-Terminal-14B improves from 4.0% to 20.2%, and Nemotron-Terminal-32B improves from 3.4% to 27.4%, matching the performance of significantly larger models. To accelerate research in this domain, we open-source our model checkpoints and most of our synthetic datasets at https://huggingface.co/collections/nvidia/nemotron-terminal.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于扩展大语言模型终端能力的数据工程研究</div>
<div class="mono" style="margin-top:8px">尽管大型语言模型的终端能力在最近取得了快速进展，但最先进的终端代理背后的数据训练策略仍大多未公开。我们通过系统研究终端代理的数据工程实践，填补了这一空白，做出了两项关键贡献：(1) Terminal-Task-Gen，一个轻量级的合成任务生成流水线，支持基于种子和基于技能的任务构建；(2) 对数据和训练策略的全面分析，包括过滤、课程学习、长上下文训练和扩展行为。我们的流水线生成了Terminal-Corpus，一个大规模的开源终端任务数据集。使用该数据集，我们训练了Nemotron-Terminal模型系列，这些模型基于Qwen3（8B、14B、32B）进行初始化，并在Terminal-Bench 2.0上取得了显著提升：Nemotron-Terminal-8B从2.5%提升至13.0%，Nemotron-Terminal-14B从4.0%提升至20.2%，Nemotron-Terminal-32B从3.4%提升至27.4%，与显著更大的模型表现相当。为加速该领域的研究，我们开源了模型检查点和大部分合成数据集，网址为https://huggingface.co/collections/nvidia/nemotron-terminal。</div>
</details>
</div>
<div class="card">
<div class="title">Transfer Learning in Infinite Width Feature Learning Networks</div>
<div class="meta-line">Authors: Clarissa Lauditi, Blake Bordelon, Cengiz Pehlevan</div>
<div class="meta-line">First: 2025-07-06T16:14:43+00:00 · Latest: 2026-02-24T18:49:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.04448v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.04448v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We develop a theory of transfer learning in infinitely wide neural networks under gradient flow that quantifies when pretraining on a source task improves generalization on a target task. We analyze both (i) fine-tuning, when the downstream predictor is trained on top of source-induced features and (ii) a jointly rich setting, where both pretraining and downstream tasks can operate in a feature learning regime, but the downstream model is initialized with the features obtained after pre-training. In this setup, the summary statistics of randomly initialized networks after a rich pre-training are adaptive kernels which depend on both source data and labels. For (i), we analyze the performance of a readout for different pretraining data regimes. For (ii), the summary statistics after learning the target task are still adaptive kernels with features from both source and target tasks. We test our theory on linear and polynomial regression tasks as well as real datasets. Our theory allows interpretable conclusions on performance, which depend on the amount of data on both tasks, the alignment between tasks, and the feature learning strength.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无限宽度特征学习网络中的迁移学习</div>
<div class="mono" style="margin-top:8px">我们发展了一种迁移学习的理论，在梯度流下的无限宽神经网络中量化了在源任务上预训练如何提升目标任务的泛化能力。我们分析了两种情况：(i) 细调，即下游预测器是在源任务诱导的特征上进行训练；(ii) 联合丰富设置，其中预训练和下游任务都可以在特征学习模式下运行，但下游模型使用预训练后获得的特征进行初始化。在这种设置下，随机初始化网络在丰富预训练后的总结统计量是自适应核，依赖于源数据和标签。对于(i)，我们分析了不同预训练数据模式下的读出器性能。对于(ii)，在学习目标任务后的总结统计量仍然是自适应核，包含来自源任务和目标任务的特征。我们在线性回归、多项式回归以及真实数据集上测试了我们的理论。我们的理论允许对性能进行可解释的结论，这些结论依赖于两个任务的数据量、任务之间的对齐程度以及特征学习的强度。</div>
</details>
</div>
<div class="card">
<div class="title">Games That Teach, Chats That Convince: Comparing Interactive and Static Formats for Persuasive Learning</div>
<div class="meta-line">Authors: Seyed Hossein Alavi, Zining Wang, Shruthi Chockkalingam, Raymond T. Ng, Vered Shwartz</div>
<div class="meta-line">First: 2026-02-20T00:07:18+00:00 · Latest: 2026-02-24T18:49:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17905v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.17905v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Interactive systems such as chatbots and games are increasingly used to persuade and educate on sustainability-related topics, yet it remains unclear how different delivery formats shape learning and persuasive outcomes when content is held constant. Grounding on identical arguments and factual content across conditions, we present a controlled user study comparing three modes of information delivery: static essays, conversational chatbots, and narrative text-based games. Across subjective measures, the chatbot condition consistently outperformed the other modes and increased perceived importance of the topic. However, perceived learning did not reliably align with objective outcomes: participants in the text-based game condition reported learning less than those reading essays, yet achieved higher scores on a delayed (24-hour) knowledge quiz. Additional exploratory analyses further suggest that common engagement proxies, such as verbosity and interaction length, are more closely related to subjective experience than to actual learning. These findings highlight a dissociation between how persuasive experiences feel and what participants retain, and point to important design trade-offs between interactivity, realism, and learning in persuasive systems and serious games.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>以游戏教学，以对话说服：比较互动与静态格式在说服性学习中的效果</div>
<div class="mono" style="margin-top:8px">聊天机器人和游戏等互动系统正越来越多地用于在可持续性相关主题上进行说服和教育，但当内容保持不变时，不同交付格式如何影响学习和说服效果仍不清楚。基于相同论点和事实内容，我们呈现了一项受控用户研究，比较了三种信息传递模式：静态文章、对话式聊天机器人和叙事文本游戏。在主观测量方面，聊天机器人条件始终优于其他模式，并提高了参与者对主题重要性的感知。然而，主观学习感知并未与客观结果可靠一致：文本游戏条件的参与者报告称学习较少，但他们在延迟（24小时）知识测验中得分更高。进一步的探索性分析还表明，常见的参与度代理指标，如冗长程度和互动时长，更与主观体验相关，而非实际学习效果。这些发现突显了说服体验感受与参与者实际记忆之间的分离，并指出了说服系统和严肃游戏中互动性、真实性和学习效果之间的重要设计权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Statistical Query Lower Bounds for Smoothed Agnostic Learning</div>
<div class="meta-line">Authors: Ilias Diakonikolas, Daniel M. Kane</div>
<div class="meta-line">First: 2026-02-24T18:46:46+00:00 · Latest: 2026-02-24T18:46:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21191v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21191v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the complexity of smoothed agnostic learning, recently introduced by~\cite{CKKMS24}, in which the learner competes with the best classifier in a target class under slight Gaussian perturbations of the inputs. Specifically, we focus on the prototypical task of agnostically learning halfspaces under subgaussian distributions in the smoothed model. The best known upper bound for this problem relies on $L_1$-polynomial regression and has complexity $d^{\tilde{O}(1/σ^2) \log(1/ε)}$, where $σ$ is the smoothing parameter and $ε$ is the excess error. Our main result is a Statistical Query (SQ) lower bound providing formal evidence that this upper bound is close to best possible. In more detail, we show that (even for Gaussian marginals) any SQ algorithm for smoothed agnostic learning of halfspaces requires complexity $d^{Ω(1/σ^{2}+\log(1/ε))}$. This is the first non-trivial lower bound on the complexity of this task and nearly matches the known upper bound. Roughly speaking, we show that applying $L_1$-polynomial regression to a smoothed version of the function is essentially best possible. Our techniques involve finding a moment-matching hard distribution by way of linear programming duality. This dual program corresponds exactly to finding a low-degree approximating polynomial to the smoothed version of the target function (which turns out to be the same condition required for the $L_1$-polynomial regression to work). Our explicit SQ lower bound then comes from proving lower bounds on this approximation degree for the class of halfspaces.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>平滑敌对学习中统计查询的下界</div>
<div class="mono" style="margin-top:8px">我们研究了最近由\cite{CKKMS24}提出的平滑敌对学习的复杂性，其中学习者在输入略有高斯扰动的情况下，与目标类中的最佳分类器竞争。具体而言，我们关注在平滑模型下，子高斯分布下敌对学习半空间的典型任务。目前已知的上界依赖于$L_1$-多项式回归，其复杂度为$d^{\tilde{O}(1/σ^2) \log(1/ε)}$，其中$σ$为平滑参数，$ε$为超额误差。我们的主要结果是一个统计查询（SQ）下界，提供了这一上界接近最优的正式证据。更具体地说，我们证明了（即使对于高斯边缘分布）任何用于平滑敌对学习半空间的SQ算法都需要复杂度$d^{Ω(1/σ^{2}+\log(1/ε))}$。这是该任务复杂性上的第一个非平凡下界，并且几乎与已知的上界相匹配。粗略地说，我们证明了将$L_1$-多项式回归应用于函数的平滑版本本质上是最佳的。我们的技术涉及通过线性规划对偶性找到一个匹配矩的困难分布。该对偶程序正好对应于寻找目标函数平滑版本的低次近似多项式（这实际上与$L_1$-多项式回归有效所需的条件相同）。我们显式的SQ下界则来自于对半空间类的近似次数证明其下界。</div>
</details>
</div>
<div class="card">
<div class="title">Why Pass@k Optimization Can Degrade Pass@1: Prompt Interference in LLM Post-training</div>
<div class="meta-line">Authors: Anas Barakat, Souradip Chakraborty, Khushbu Pahwa, Amrit Singh Bedi</div>
<div class="meta-line">First: 2026-02-24T18:43:08+00:00 · Latest: 2026-02-24T18:43:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21189v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21189v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pass@k is a widely used performance metric for verifiable large language model tasks, including mathematical reasoning, code generation, and short-answer reasoning. It defines success if any of $k$ independently sampled solutions passes a verifier. This multi-sample inference metric has motivated inference-aware fine-tuning methods that directly optimize pass@$k$. However, prior work reports a recurring trade-off: pass@k improves while pass@1 degrades under such methods. This trade-off is practically important because pass@1 often remains a hard operational constraint due to latency and cost budgets, imperfect verifier coverage, and the need for a reliable single-shot fallback. We study the origin of this trade-off and provide a theoretical characterization of when pass@k policy optimization can reduce pass@1 through gradient conflict induced by prompt interference. We show that pass@$k$ policy gradients can conflict with pass@1 gradients because pass@$k$ optimization implicitly reweights prompts toward low-success prompts; when these prompts are what we term negatively interfering, their upweighting can rotate the pass@k update direction away from the pass@1 direction. We illustrate our theoretical findings with large language model experiments on verifiable mathematical reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>为什么Pass@k优化会降低Pass@1：LLM后训练中的提示干扰</div>
<div class="mono" style="margin-top:8px">Pass@k是用于可验证大语言模型任务（如数学推理、代码生成和短回答推理）的广泛使用的性能指标。它定义如果$k$个独立采样的解决方案中任意一个通过验证器则视为成功。这种多样本推理指标促使了面向推理的微调方法，这些方法直接优化Pass@k。然而，先前的研究报告了一个反复出现的权衡：在这些方法下，Pass@k提升而Pass@1下降。这种权衡在实践中非常重要，因为Pass@1通常仍是一个严格的运营约束，由于延迟和成本预算、验证器覆盖不完全以及需要可靠的单次回退机制。我们研究了这种权衡的来源，并提供了理论上的分析，说明在何种情况下Pass@k策略优化会因提示干扰引起的梯度冲突而降低Pass@1。我们展示了Pass@k策略梯度可能与Pass@1梯度发生冲突，因为Pass@k优化会隐式地将提示权重向低成功率的提示倾斜；当这些提示是我们所称的负干扰提示时，其权重增加可能会使Pass@k的更新方向偏离Pass@1的方向。我们通过在可验证数学推理任务上的大语言模型实验来验证我们的理论发现。</div>
</details>
</div>
<div class="card">
<div class="title">Human Video Generation from a Single Image with 3D Pose and View Control</div>
<div class="meta-line">Authors: Tiantian Wang, Chun-Han Yao, Tao Hu, Mallikarjun Byrasandra Ramalinga Reddy, Ming-Hsuan Yang, Varun Jampani</div>
<div class="meta-line">First: 2026-02-24T18:42:20+00:00 · Latest: 2026-02-24T18:42:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21188v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21188v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent diffusion methods have made significant progress in generating videos from single images due to their powerful visual generation capabilities. However, challenges persist in image-to-video synthesis, particularly in human video generation, where inferring view-consistent, motion-dependent clothing wrinkles from a single image remains a formidable problem. In this paper, we present Human Video Generation in 4D (HVG), a latent video diffusion model capable of generating high-quality, multi-view, spatiotemporally coherent human videos from a single image with 3D pose and view control. HVG achieves this through three key designs: (i) Articulated Pose Modulation, which captures the anatomical relationships of 3D joints via a novel dual-dimensional bone map and resolves self-occlusions across views by introducing 3D information; (ii) View and Temporal Alignment, which ensures multi-view consistency and alignment between a reference image and pose sequences for frame-to-frame stability; and (iii) Progressive Spatio-Temporal Sampling with temporal alignment to maintain smooth transitions in long multi-view animations. Extensive experiments on image-to-video tasks demonstrate that HVG outperforms existing methods in generating high-quality 4D human videos from diverse human images and pose inputs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于单张图像与3D姿态及视角控制的人类视频生成</div>
<div class="mono" style="margin-top:8px">由于扩散方法在视觉生成方面具有强大的能力，近期在从单张图像生成视频方面取得了显著进展。然而，图像到视频的合成仍面临挑战，尤其是在人类视频生成中，从单张图像推断出视角一致、运动相关的衣物褶皱仍然是一个严峻的问题。本文提出了一种名为HVG的4D人类视频生成模型，该模型能够从带有3D姿态和视角控制的单张图像中生成高质量、多视角且时空一致的人类视频。HVG通过三个关键设计实现这一目标：(i) 关节姿态调制，通过新颖的双维度骨骼图捕捉3D关节的解剖关系，并引入3D信息以解决不同视角下的自遮挡问题；(ii) 视角与时间对齐，确保参考图像与姿态序列之间的多视角一致性及帧间稳定性；(iii) 带有时序对齐的渐进式时空采样，以维持长时多视角动画的平滑过渡。在图像到视频任务上的大量实验表明，HVG在从多样化的图像和姿态输入生成高质量4D人类视频方面优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Spa3R: Predictive Spatial Field Modeling for 3D Visual Reasoning</div>
<div class="meta-line">Authors: Haoyi Jiang, Liu Liu, Xinjie Wang, Yonghao He, Wei Sui, Zhizhong Su, Wenyu Liu, Xinggang Wang</div>
<div class="meta-line">First: 2026-02-24T18:37:34+00:00 · Latest: 2026-02-24T18:37:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21186v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21186v1">PDF</a> · <a href="https://github.com/hustvl/Spa3R">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Vision-Language Models (VLMs) exhibit exceptional 2D visual understanding, their ability to comprehend and reason about 3D space--a cornerstone of spatial intelligence--remains superficial. Current methodologies attempt to bridge this domain gap either by relying on explicit 3D modalities or by augmenting VLMs with partial, view-conditioned geometric priors. However, such approaches hinder scalability and ultimately burden the language model with the ill-posed task of implicitly reconstructing holistic 3D geometry from sparse cues. In this paper, we argue that spatial intelligence can emerge inherently from 2D vision alone, rather than being imposed via explicit spatial instruction tuning. To this end, we introduce Spa3R, a self-supervised framework that learns a unified, view-invariant spatial representation directly from unposed multi-view images. Spa3R is built upon the proposed Predictive Spatial Field Modeling (PSFM) paradigm, where Spa3R learns to synthesize feature fields for arbitrary unseen views conditioned on a compact latent representation, thereby internalizing a holistic and coherent understanding of the underlying 3D scene. We further integrate the pre-trained Spa3R Encoder into existing VLMs via a lightweight adapter to form Spa3-VLM, effectively grounding language reasoning in a global spatial context. Experiments on the challenging VSI-Bench demonstrate that Spa3-VLM achieves state-of-the-art accuracy of 58.6% on 3D VQA, significantly outperforming prior methods. These results highlight PSFM as a scalable path toward advancing spatial intelligence. Code is available at https://github.com/hustvl/Spa3R.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Spa3R：用于3D视觉推理的预测空间场建模</div>
<div class="mono" style="margin-top:8px">尽管视觉-语言模型（VLMs）在2D视觉理解方面表现出色，但它们对3D空间的理解和推理能力仍然较为浅显，这限制了空间智能的发展。当前方法通过依赖显式的3D模态或在VLMs中引入部分、视图条件的几何先验来弥合这一领域差距。然而，这些方法限制了可扩展性，并最终使语言模型承担了隐式重建整体3D几何的不明确任务。本文我们认为，空间智能可以仅通过2D视觉自然地产生，而非通过显式的空间指令调优来强制实现。为此，我们提出了Spa3R，一个自监督框架，它直接从无标注的多视角图像中学习统一的、视图不变的空间表示。Spa3R基于我们提出的预测空间场建模（PSFM）范式，通过一个紧凑的潜在表示来合成任意未见过视角的特征场，从而内部化对底层3D场景的整体且一致的理解。我们进一步通过一个轻量级适配器将预训练的Spa3R编码器集成到现有VLMs中，形成Spa3-VLM，从而将语言推理有效地锚定在全局空间上下文中。在具有挑战性的VSI-Bench数据集上的实验表明，Spa3-VLM在3D视觉问答任务中达到了58.6%的最先进准确率，显著优于之前的方法。这些结果突显了PSFM作为提升空间智能的可扩展路径。代码可在https://github.com/hustvl/Spa3R获取。</div>
</details>
</div>
<div class="card">
<div class="title">The Diffusion Duality, Chapter II: $Ψ$-Samplers and Efficient Curriculum</div>
<div class="meta-line">Authors: Justin Deschenaux, Caglar Gulcehre, Subham Sekhar Sahoo</div>
<div class="meta-line">First: 2026-02-24T18:35:22+00:00 · Latest: 2026-02-24T18:35:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21185v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21185v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Uniform-state discrete diffusion models excel at few-step generation and guidance due to their ability to self-correct, making them preferred over autoregressive or Masked diffusion models in these settings. However, their sampling quality plateaus with ancestral samplers as the number of steps increases. We introduce a family of Predictor-Corrector (PC) samplers for discrete diffusion that generalize prior methods and apply to arbitrary noise processes. When paired with uniform-state diffusion, our samplers outperform ancestral sampling on both language and image modeling, achieving lower generative perplexity at matched unigram entropy on OpenWebText and better FID/IS scores on CIFAR10. Crucially, unlike conventional samplers, our PC methods continue to improve with more sampling steps. Taken together, these findings call into question the assumption that Masked diffusion is the inevitable future of diffusion-based language modeling. Beyond sampling, we develop a memory-efficient curriculum for the Gaussian relaxation training phase, reducing training time by 25% and memory by 33% compared to Duo while maintaining comparable perplexity on OpenWebText and LM1B and strong downstream performance. We release code, checkpoints, and a video-tutorial on: https://s-sahoo.com/duo-ch2</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散二元性，第二章：$Ψ$-采样器与高效课程</div>
<div class="mono" style="margin-top:8px">均匀状态离散扩散模型因其具备自我校正能力，在少步生成和引导方面表现出色，因此在这些场景下优于自回归或掩码扩散模型。然而，随着采样步数的增加，其采样质量在祖先采样器中趋于平稳。我们引入了一类适用于离散扩散的预测-校正（PC）采样器，该方法推广了先前的方法，并适用于任意噪声过程。当与均匀状态扩散模型结合时，我们的采样器在语言和图像建模任务中均优于祖先采样，实现了与OpenWebText上单字熵相匹配的更低生成困惑度，并在CIFAR10上取得了更好的FID/IS分数。关键的是，与传统采样器不同，我们的PC方法在采样步数增加时仍能持续改进。综上所述，这些发现对假设掩码扩散是基于扩散的语言建模必然未来提出了质疑。此外，我们还开发了一种内存高效的课程，用于高斯松弛训练阶段，与Duo相比，训练时间减少了25%，内存减少了33%，同时在OpenWebText和LM1B上保持了相当的困惑度，并在下游任务中表现出色。我们发布了代码、检查点和视频教程，网址为：https://s-sahoo.com/duo-ch2</div>
</details>
</div>
<div class="card">
<div class="title">823-OLT @ BUET DL Sprint 4.0: Context-Aware Windowing for ASR and Fine-Tuned Speaker Diarization in Bengali Long Form Audio</div>
<div class="meta-line">Authors: Ratnajit Dhar, Arpita Mallik</div>
<div class="meta-line">First: 2026-02-24T18:34:14+00:00 · Latest: 2026-02-24T18:34:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21183v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21183v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bengali, despite being one of the most widely spoken languages globally, remains underrepresented in long form speech technology, particularly in systems addressing transcription and speaker attribution. We present frameworks for long form Bengali speech intelligence that address automatic speech recognition using a Whisper Medium based model and speaker diarization using a finetuned segmentation model. The ASR pipeline incorporates vocal separation, voice activity detection, and a gap aware windowing strategy to construct context preserving segments for stable decoding. For diarization, a pretrained speaker segmentation model is finetuned on the official competition dataset (provided as part of the DL Sprint 4.0 competition organized under BUET CSE Fest), to better capture Bengali conversational patterns. The resulting systems deliver both efficient transcription of long form audio and speaker aware transcription to provide scalable speech technology solutions for low resource languages.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>823-OLT @ BUET DL Sprint 4.0：面向孟加拉语长音频的上下文感知窗口化与精细调整的说话人分割</div>
<div class="mono" style="margin-top:8px">尽管孟加拉语是全球使用最广泛的语言之一，但在长音频语音技术中仍处于代表性不足的状态，尤其是在处理转录和说话人归属的系统中。我们提出了针对长音频孟加拉语语音智能的框架，使用基于Whisper Medium的模型进行自动语音识别，并使用经过精细调整的分割模型进行说话人分割。ASR流程集成了语音分离、语音活动检测以及一种感知间隙的窗口化策略，以构建保持上下文的片段，实现稳定的解码。对于说话人分割，我们使用了在BUET CSE Fest组织的DL Sprint 4.0竞赛官方数据集上进行精细调整的预训练说话人分割模型，以更好地捕捉孟加拉语的会话模式。最终系统能够高效地转录长音频，并提供说话人感知的转录，为低资源语言提供可扩展的语音技术解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Memory Undone: Between Knowing and Not Knowing in Data Systems</div>
<div class="meta-line">Authors: Viktoriia Makovska, George Fletcher, Julia Stoyanovich, Tetiana Zakharchenko</div>
<div class="meta-line">First: 2026-02-24T18:29:17+00:00 · Latest: 2026-02-24T18:29:17+00:00</div>
<div class="meta-line">Comments: Undone Computer Science 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21180v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21180v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning and data systems increasingly function as infrastructures of memory: they ingest, store, and operationalize traces of personal, political, and cultural life. Yet contemporary governance demands credible forms of forgetting, from GDPR-backed deletion to harm-mitigation and the removal of manipulative content, while technical infrastructures are optimized to retain, replicate, and reuse. This work argues that &quot;forgetting&quot; in computational systems cannot be reduced to a single operation (e.g., record deletion) and should instead be treated as a sociotechnical practice with distinct mechanisms and consequences. We clarify a vocabulary that separates erasure (removing or disabling access to data artifacts), unlearning (interventions that bound or remove a data point influence on learned parameters and outputs), exclusion (upstream non-collection and omission), and forgetting as an umbrella term spanning agency, temporality, reversibility, and scale. Building on examples from machine unlearning, semantic dependencies in data management, participatory data modeling, and manipulation at scale, we show how forgetting can simultaneously protect rights and enable silencing. We propose reframing unlearning as a first-class capability in knowledge infrastructures, evaluated not only by compliance or utility retention, but by its governance properties: transparency, accountability, and epistemic justice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>记忆被解构：数据系统中认知与无知之间的张力</div>
<div class="mono" style="margin-top:8px">机器学习和数据系统日益成为记忆的基础设施：它们摄入、存储并操作个人、政治和文化生活的痕迹。然而，当代治理要求可信的遗忘形式，如GDPR支持的数据删除、危害缓解以及操纵性内容的移除，而技术基础设施则被优化以保留、复制和再利用数据。本研究认为，计算系统中的“遗忘”不能简化为单一操作（例如记录删除），而应被视为一种社会技术实践，具有独特的机制和后果。我们厘清了一种词汇体系，将删除（移除或禁用对数据制品的访问）、去学习（干预以限制或消除数据点对学习参数和输出的影响）、排除（上游不收集和遗漏）与遗忘作为涵盖代理、时间性、可逆性与规模的总称。基于机器去学习、数据管理中的语义依赖、参与式数据建模以及大规模操控的例子，我们展示了遗忘如何同时保护权利并实现沉默。我们建议将去学习重新定义为知识基础设施中的首要能力，并不仅依据合规性或效用保留来评估，还应依据其治理属性：透明度、问责制和知识正义。</div>
</details>
</div>
<div class="card">
<div class="title">Mask-HybridGNet: Graph-based segmentation with emergent anatomical correspondence from pixel-level supervision</div>
<div class="meta-line">Authors: Nicolás Gaggion, Maria J. Ledesma-Carbayo, Stergios Christodoulidis, Maria Vakalopoulou, Enzo Ferrante</div>
<div class="meta-line">First: 2026-02-24T18:29:13+00:00 · Latest: 2026-02-24T18:29:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21179v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21179v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph-based medical image segmentation represents anatomical structures using boundary graphs, providing fixed-topology landmarks and inherent population-level correspondences. However, their clinical adoption has been hindered by a major requirement: training datasets with manually annotated landmarks that maintain point-to-point correspondences across patients rarely exist in practice. We introduce Mask-HybridGNet, a framework that trains graph-based models directly using standard pixel-wise masks, eliminating the need for manual landmark annotations. Our approach aligns variable-length ground truth boundaries with fixed-length landmark predictions by combining Chamfer distance supervision and edge-based regularization to ensure local smoothness and regular landmark distribution, further refined via differentiable rasterization. A significant emergent property of this framework is that predicted landmark positions become consistently associated with specific anatomical locations across patients without explicit correspondence supervision. This implicit atlas learning enables temporal tracking, cross-slice reconstruction, and morphological population analyses. Beyond direct segmentation, Mask-HybridGNet can extract correspondences from existing segmentation masks, allowing it to generate stable anatomical atlases from any high-quality pixel-based model. Experiments across chest radiography, cardiac ultrasound, cardiac MRI, and fetal imaging demonstrate that our model achieves competitive results against state-of-the-art pixel-based methods, while ensuring anatomical plausibility by enforcing boundary connectivity through a fixed graph adjacency matrix. This framework leverages the vast availability of standard segmentation masks to build structured models that maintain topological integrity and provide implicit correspondences.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Mask-HybridGNet：基于像素级监督的图结构分割与新兴解剖对应关系</div>
<div class="mono" style="margin-top:8px">基于图的医学图像分割方法通过边界图表示解剖结构，提供固定拓扑的地标和固有的群体级对应关系。然而，其临床应用受到一个主要限制：在实际中，很少有训练数据集包含手动标注的地标，并且这些地标在患者之间保持点对点对应关系。我们引入了Mask-HybridGNet框架，该框架可以直接使用标准像素级掩码训练图结构模型，从而无需手动地标标注。我们的方法通过结合Chamfer距离监督和基于边的正则化，将可变长度的真实边界与固定长度的地标预测对齐，以确保局部平滑性和地标分布的规律性，并通过可微分光栅化进一步优化。该框架的一个显著新兴特性是，预测的地标位置在不同患者之间能够一致地关联到特定的解剖位置，而无需显式的对应关系监督。这种隐式的图谱学习使得时间追踪、跨切片重建和形态学群体分析成为可能。除了直接分割外，Mask-HybridGNet还能从现有的分割掩码中提取对应关系，从而能够从任何高质量的像素级模型中生成稳定的解剖图谱。在胸部X光、心脏超声、心脏MRI和胎儿成像的实验中，我们的模型在与最先进的像素级方法竞争时表现出色，同时通过固定图邻接矩阵强制边界连通性，确保了解剖合理性。该框架利用标准分割掩码的广泛可用性，构建了保持拓扑完整性和提供隐式对应关系的结构化模型。</div>
</details>
</div>
<div class="card">
<div class="title">XMorph: Explainable Brain Tumor Analysis Via LLM-Assisted Hybrid Deep Intelligence</div>
<div class="meta-line">Authors: Sepehr Salem Ghahfarokhi, M. Moein Esfahani, Raj Sunderraman, Vince Calhoun, Mohammed Alser</div>
<div class="meta-line">First: 2026-02-24T18:28:08+00:00 · Latest: 2026-02-24T18:28:08+00:00</div>
<div class="meta-line">Comments: Accepted in ICCABS 2026: The 14th International Conference on Computational Advances in Bio and Medical Sciences</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21178v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21178v1">PDF</a> · <a href="https://github.com/ALSER-Lab/XMorph">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep learning has significantly advanced automated brain tumor diagnosis, yet clinical adoption remains limited by interpretability and computational constraints. Conventional models often act as opaque &#x27;&#x27;black boxes&#x27;&#x27; and fail to quantify the complex, irregular tumor boundaries that characterize malignant growth. To address these challenges, we present XMorph, an explainable and computationally efficient framework for fine-grained classification of three prominent brain tumor types: glioma, meningioma, and pituitary tumors. We propose an Information-Weighted Boundary Normalization (IWBN) mechanism that emphasizes diagnostically relevant boundary regions alongside nonlinear chaotic and clinically validated features, enabling a richer morphological representation of tumor growth. A dual-channel explainable AI module combines GradCAM++ visual cues with LLM-generated textual rationales, translating model reasoning into clinically interpretable insights. The proposed framework achieves a classification accuracy of 96.0%, demonstrating that explainability and high performance can co-exist in AI-based medical imaging systems. The source code and materials for XMorph are all publicly available at: https://github.com/ALSER-Lab/XMorph.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>XMorph：通过LLM辅助混合深度智能实现可解释的脑肿瘤分析</div>
<div class="mono" style="margin-top:8px">深度学习显著推动了脑肿瘤自动诊断的发展，但其在临床应用中仍受限于可解释性和计算约束。传统模型常被视为不透明的&#x27;&#x27;黑箱&#x27;&#x27;，且难以量化恶性肿瘤中复杂的不规则边界。为解决这些问题，我们提出了XMorph，一个可解释且计算高效的框架，用于对三种主要脑肿瘤类型（胶质瘤、脑膜瘤和垂体瘤）进行细粒度分类。我们提出了一种信息加权边界归一化（IWBN）机制，强调诊断相关的边界区域以及非线性混沌和临床验证的特征，从而实现更丰富的肿瘤生长形态表示。一个双通道的可解释AI模块结合了GradCAM++的视觉提示与LLM生成的文本推理，将模型的推理过程转化为临床可解释的见解。所提出的框架在分类准确率上达到了96.0%，证明了在基于AI的医学影像系统中，可解释性与高性能可以共存。XMorph的源代码和相关材料均可在https://github.com/ALSER-Lab/XMorph上公开获取。</div>
</details>
</div>
<div class="card">
<div class="title">How much does context affect the accuracy of AI health advice?</div>
<div class="meta-line">Authors: Prashant Garg, Thiemo Fetzer</div>
<div class="meta-line">First: 2025-04-25T12:37:15+00:00 · Latest: 2026-02-24T18:23:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.18310v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.18310v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly used to provide health advice, yet evidence on how their accuracy varies across languages, topics and information sources remains limited. We assess how linguistic and contextual factors affect the accuracy of AI-based health-claim verification. We evaluated seven widely used LLMs on two datasets: (i) 1,975 legally authorised nutrition and health claims from UK and EU regulatory registers translated into 21 languages; and (ii) 9,088 journalist-vetted public-health claims from the PUBHEALTH corpus spanning COVID-19, abortion, politics and general health, drawn from government advisories, scientific abstracts and media sources. Models classified each claim as supported or unsupported using majority voting across repeated runs. Accuracy was analysed by language, topic, source and model. Accuracy on authorised claims was highest in English and closely related European languages and declined in several widely spoken non-European languages, decreasing with syntactic distance from English. On real-world public-health claims, accuracy was substantially lower and varied systematically by topic and source. Models performed best on COVID-19 and government-attributed claims and worst on general health and scientific abstracts. High performance on English, canonical health claims masks substantial context-dependent gaps. Differences in training data exposure, editorial framing and topic-specific tuning likely contribute to these disparities, which are comparable in magnitude to cross-language differences. LLM accuracy in health-claim verification depends strongly on language, topic and information source. English-language performance does not reliably generalise across contexts, underscoring the need for multilingual, domain-specific evaluation before deployment in public-health communication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>上下文对AI健康建议准确性有多大影响？</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越多地被用于提供健康建议，但关于其准确性在语言、主题和信息来源上的差异仍缺乏证据。我们评估了语言和上下文因素对基于AI的健康声明验证准确性的影响。我们对两个数据集评估了七种广泛使用的LLMs：(i) 1,975条经法律授权的营养和健康声明，来自英国和欧盟监管登记，翻译成21种语言；(ii) 9,088条由记者审核的公共卫生声明，来自PUBHEALTH语料库，涵盖新冠、堕胎、政治和一般健康等领域，信息来源包括政府指导、科学摘要和媒体。模型通过多次运行的多数投票对每条声明进行分类，判断其是否被支持。准确性按语言、主题、来源和模型进行分析。在授权声明中，英语和相关欧洲语言的准确性最高，而在几种广泛使用的非欧洲语言中则下降，且与英语的句法距离越远，准确性越低。在现实世界中的公共卫生声明中，准确性显著降低，并且在主题和来源上系统性地变化。模型在新冠和政府相关声明上的表现最佳，在一般健康和科学摘要上的表现最差。在英语和标准健康声明上的高性能掩盖了大量依赖上下文的差距。训练数据的暴露差异、编辑框架和主题特定的调优可能解释了这些差异，其幅度与跨语言差异相当。健康声明验证中的LLMs准确性强烈依赖于语言、主题和信息来源。英语的表现不能可靠地推广到其他上下文中，这突显了在部署到公共卫生传播之前进行多语言、领域特定评估的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Seeing Through Words: Controlling Visual Retrieval Quality with Language Models</div>
<div class="meta-line">Authors: Jianglin Lu, Simon Jenni, Kushal Kafle, Jing Shi, Handong Zhao, Yun Fu</div>
<div class="meta-line">First: 2026-02-24T18:20:57+00:00 · Latest: 2026-02-24T18:20:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21175v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21175v1">PDF</a> · <a href="https://github.com/Jianglin954/QCQC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image retrieval is a fundamental task in vision-language learning, yet in real-world scenarios it is often challenged by short and underspecified user queries. Such queries are typically only one or two words long, rendering them semantically ambiguous, prone to collisions across diverse visual interpretations, and lacking explicit control over the quality of retrieved images. To address these issues, we propose a new paradigm of quality-controllable retrieval, which enriches short queries with contextual details while incorporating explicit notions of image quality. Our key idea is to leverage a generative language model as a query completion function, extending underspecified queries into descriptive forms that capture fine-grained visual attributes such as pose, scene, and aesthetics. We introduce a general framework that conditions query completion on discretized quality levels, derived from relevance and aesthetic scoring models, so that query enrichment is not only semantically meaningful but also quality-aware. The resulting system provides three key advantages: 1) flexibility, it is compatible with any pretrained vision-language model (VLMs) without modification; 2) transparency, enriched queries are explicitly interpretable by users; and 3) controllability, enabling retrieval results to be steered toward user-preferred quality levels. Extensive experiments demonstrate that our proposed approach significantly improves retrieval results and provides effective quality control, bridging the gap between the expressive capacity of modern VLMs and the underspecified nature of short user queries. Our code is available at https://github.com/Jianglin954/QCQC.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>透过文字看见：利用语言模型控制视觉检索质量</div>
<div class="mono" style="margin-top:8px">文本到图像检索是视觉-语言学习中的基础任务，但在现实场景中，它常受到用户短且不明确的查询的挑战。这类查询通常只有1到2个词，导致语义模糊，容易在多样化的视觉解释中产生歧义，并且缺乏对检索图像质量的显式控制。为了解决这些问题，我们提出了一种新的可控质量检索范式，通过在查询中添加上下文细节并引入图像质量的显式概念来增强短查询。我们的核心思想是利用生成式语言模型作为查询补全函数，将不明确的查询扩展为描述性形式，以捕捉姿态、场景和美学等细粒度的视觉属性。我们引入了一个通用框架，该框架基于从相关性和美学评分模型中得出的离散质量等级来对查询补全进行条件控制，使得查询增强不仅在语义上具有意义，而且具备质量感知能力。所得到的系统具有三个关键优势：1）灵活性，无需修改即可兼容任何预训练的视觉-语言模型（VLMs）；2）透明性，增强后的查询对用户是显式可解释的；3）可控性，使检索结果能够引导至用户偏好的质量等级。大量实验表明，我们的方法显著提升了检索结果，并提供了有效的质量控制，弥合了现代VLMs的表达能力与短用户查询的不明确性之间的差距。我们的代码可在https://github.com/Jianglin954/QCQC获取。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Hierarchical Any-Angle Path Planning on Multi-Resolution 3D Grids</div>
<div class="meta-line">Authors: Victor Reijgwart, Cesar Cadena, Roland Siegwart, Lionel Ott</div>
<div class="meta-line">Venue: RSS 2025</div>
<div class="meta-line">First: 2026-02-24T18:18:36+00:00 · Latest: 2026-02-24T18:18:36+00:00</div>
<div class="meta-line">Comments: 12 pages, 9 figures, 4 tables, accepted to RSS 2025, code is open-source: https://github.com/ethz-asl/wavestar</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21174v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21174v1">PDF</a> · <a href="https://github.com/ethz-asl/wavestar">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hierarchical, multi-resolution volumetric mapping approaches are widely used to represent large and complex environments as they can efficiently capture their occupancy and connectivity information. Yet widely used path planning methods such as sampling and trajectory optimization do not exploit this explicit connectivity information, and search-based methods such as A* suffer from scalability issues in large-scale high-resolution maps. In many applications, Euclidean shortest paths form the underpinning of the navigation system. For such applications, any-angle planning methods, which find optimal paths by connecting corners of obstacles with straight-line segments, provide a simple and efficient solution. In this paper, we present a method that has the optimality and completeness properties of any-angle planners while overcoming computational tractability issues common to search-based methods by exploiting multi-resolution representations. Extensive experiments on real and synthetic environments demonstrate the proposed approach&#x27;s solution quality and speed, outperforming even sampling-based methods. The framework is open-sourced to allow the robotics and planning community to build on our research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高效多分辨率三维网格上的分层任意角度路径规划</div>
<div class="mono" style="margin-top:8px">分层、多分辨率的体素映射方法被广泛用于表示大型和复杂环境，因为它们可以高效地捕捉环境的占用和连通性信息。然而，常用的路径规划方法，如采样和轨迹优化，并未利用这种显式的连通性信息，而基于搜索的方法如A*在大规模高分辨率地图中存在可扩展性问题。在许多应用中，欧几里得最短路径构成了导航系统的基础。对于这些应用，任意角度规划方法通过用直线段连接障碍物的角落来寻找最优路径，提供了一种简单且高效的解决方案。本文提出了一种方法，该方法具有任意角度规划器的最优性和完备性特性，同时通过利用多分辨率表示克服了基于搜索方法常见的计算可行性问题。在真实和合成环境上的大量实验验证了所提出方法的求解质量和速度，其表现甚至优于基于采样的方法。该框架已开源，以便机器人学和规划研究社区在此基础上进行进一步研究。</div>
</details>
</div>
<div class="card">
<div class="title">NoRD: A Data-Efficient Vision-Language-Action Model that Drives without Reasoning</div>
<div class="meta-line">Authors: Ishaan Rawal, Shubh Gupta, Yihan Hu, Wei Zhan</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-02-24T18:17:21+00:00 · Latest: 2026-02-24T18:17:21+00:00</div>
<div class="meta-line">Comments: Accepted to CVPR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21172v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21172v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models are advancing autonomous driving by replacing modular pipelines with unified end-to-end architectures. However, current VLAs face two expensive requirements: (1) massive dataset collection, and (2) dense reasoning annotations. In this work, we address both challenges with \modelname (\textbf{No} \textbf{R}easoning for \textbf{D}riving). Compared to existing VLAs, \modelname achieves competitive performance while being fine-tuned on $&lt;$60\% of the data and no reasoning annotations, resulting in 3$\times$ fewer tokens. We identify that standard Group Relative Policy Optimization (GRPO) fails to yield significant improvements when applied to policies trained on such small, reasoning-free datasets. We show that this limitation stems from difficulty bias, which disproportionately penalizes reward signals from scenarios that produce high-variance rollouts within GRPO. \modelname overcomes this by incorporating Dr.~GRPO, a recent algorithm designed to mitigate difficulty bias in LLMs. As a result, \modelname achieves competitive performance on Waymo and NAVSIM with a fraction of the training data and no reasoning overhead, enabling more efficient autonomous systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NoRD：一种无需推理的数据高效视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型通过用统一的端到端架构替代模块化流水线，正在推动自动驾驶的发展。然而，当前的VLA模型面临两个昂贵的要求：（1）大规模数据集收集，（2）密集的推理注释。在本工作中，我们通过\modelname（\textbf{No} \textbf{R}easoning for \textbf{D}riving）解决了这两个挑战。与现有VLA模型相比，\modelname在仅使用不到60\%的数据和无推理注释的情况下实现了具有竞争力的性能，从而减少了3倍的token数量。我们发现，当将标准的Group Relative Policy Optimization（GRPO）应用于在如此小且无推理数据集上训练的策略时，无法获得显著的性能提升。我们表明，这一限制源于难度偏差，它在GRPO中不成比例地惩罚了高方差 rollout 的场景的奖励信号。\modelname通过引入Dr.~GRPO，一种旨在缓解大型语言模型（LLMs）中难度偏差的最新算法，克服了这一问题。因此，\modelname在Waymo和NAVSIM上使用少量训练数据和无推理开销即可实现具有竞争力的性能，从而实现更高效的自动驾驶系统。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Round Human-AI Collaboration with User-Specified Requirements</div>
<div class="meta-line">Authors: Sima Noorani, Shayan Kiyani, Hamed Hassani, George Pappas</div>
<div class="meta-line">First: 2026-02-19T18:54:34+00:00 · Latest: 2026-02-24T18:15:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17646v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.17646v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As humans increasingly rely on multiround conversational AI for high stakes decisions, principled frameworks are needed to ensure such interactions reliably improve decision quality. We adopt a human centric view governed by two principles: counterfactual harm, ensuring the AI does not undermine human strengths, and complementarity, ensuring it adds value where the human is prone to err. We formalize these concepts via user defined rules, allowing users to specify exactly what harm and complementarity mean for their specific task. We then introduce an online, distribution free algorithm with finite sample guarantees that enforces the user-specified constraints over the collaboration dynamics. We evaluate our framework across two interactive settings: LLM simulated collaboration on a medical diagnostic task and a human crowdsourcing study on a pictorial reasoning task. We show that our online procedure maintains prescribed counterfactual harm and complementarity violation rates even under nonstationary interaction dynamics. Moreover, tightening or loosening these constraints produces predictable shifts in downstream human accuracy, confirming that the two principles serve as practical levers for steering multi-round collaboration toward better decision quality without the need to model or constrain human behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用户指定需求下的多轮人机协作</div>
<div class="mono" style="margin-top:8px">随着人类越来越多地依赖多轮对话AI进行高风险决策，需要有原则性的框架来确保这些互动能可靠地提升决策质量。我们采用以人类为中心的观点，遵循两个原则：反事实伤害，确保AI不会削弱人类的优势；互补性，确保AI在人类容易出错的领域提供价值。我们通过用户定义的规则形式化了这些概念，使用户能够精确指定其特定任务中伤害和互补性的含义。随后，我们引入了一种在线、无分布假设的算法，该算法在有限样本下具有保证，能够强制执行用户指定的约束条件，以规范协作动态。我们在两个交互场景中评估了我们的框架：在医疗诊断任务中使用大型语言模型模拟协作，以及在图像推理任务中进行的人类众包研究。我们展示了我们的在线过程即使在非平稳的交互动态下，也能维持指定的反事实伤害和互补性违规率。此外，收紧或放宽这些约束会导致下游人类准确率的可预测变化，证实了这两个原则可以作为实际杠杆，引导多轮协作朝着更好的决策质量发展，而无需建模或限制人类行为。</div>
</details>
</div>
<div class="card">
<div class="title">Sequential Counterfactual Inference for Temporal Clinical Data: Addressing the Time Traveler Dilemma</div>
<div class="meta-line">Authors: Jingya Cheng, Alaleh Azhir, Jiazi Tian, Hossein Estiri</div>
<div class="meta-line">First: 2026-02-24T18:11:23+00:00 · Latest: 2026-02-24T18:11:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21168v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21168v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Counterfactual inference enables clinicians to ask &quot;what if&quot; questions about patient outcomes, but standard methods assume feature independence and simultaneous modifiability -- assumptions violated by longitudinal clinical data. We introduce the Sequential Counterfactual Framework, which respects temporal dependencies in electronic health records by distinguishing immutable features (chronic diagnoses) from controllable features (lab values) and modeling how interventions propagate through time. Applied to 2,723 COVID-19 patients (383 Long COVID heart failure cases, 2,340 matched controls), we demonstrate that 38-67% of patients with chronic conditions would require biologically impossible counterfactuals under naive methods. We identify a cardiorenal cascade (CKD -&gt; AKI -&gt; HF) with relative risks of 2.27 and 1.19 at each step, illustrating temporal propagation that sequential -- but not naive -- counterfactuals can capture. Our framework transforms counterfactual explanation from &quot;what if this feature were different?&quot; to &quot;what if we had intervened earlier, and how would that propagate forward?&quot; --  yielding clinically actionable insights grounded in biological plausibility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于时间序列临床数据的顺序反事实推理：解决时间旅行者困境</div>
<div class="mono" style="margin-top:8px">反事实推理使临床医生能够对患者结果提出&quot;如果...会怎样&quot;的问题，但标准方法假设特征独立性和同时可修改性，这与纵向临床数据的假设相违背。我们引入了顺序反事实框架，通过区分不可变特征（慢性诊断）和可控特征（实验室值），并在电子健康记录中尊重时间依赖性，建模干预如何随时间传播。我们将该框架应用于2,723名新冠患者（其中383例为长期新冠心力衰竭病例，2,340例为匹配对照组），证明在简单方法下，38-67%的慢性病患者需要生物上不可能的反事实。我们识别出一个心肾级联（CKD -&gt; AKI -&gt; HF），其每一步的相对风险分别为2.27和1.19，展示了时间传播现象，这种现象只有顺序反事实方法才能捕捉。我们的框架将反事实解释从&quot;如果这个特征不同会怎样？&quot;转变为&quot;如果我们更早干预，其影响会如何传播？&quot;，从而产生基于生物合理性的临床可操作见解。</div>
</details>
</div>
<div class="card">
<div class="title">Characterizing Dark Bosons at Chiral Belle</div>
<div class="meta-line">Authors: Carlos Henrique de Lima, David McKeen, Afif Omar, Douglas Tuckler</div>
<div class="meta-line">First: 2025-07-21T18:00:02+00:00 · Latest: 2026-02-24T18:11:20+00:00</div>
<div class="meta-line">Comments: Version published at PRD, 11 pages, 3 figures, 1 appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.15931v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.15931v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We explore the advantages of a polarized electron beam at Belle II, as proposed for ``Chiral Belle&#x27;&#x27;, in the search for invisibly decaying (dark) bosons that weakly couple to the Standard Model. By measuring the polarization dependence of the production cross section of dark bosons in association with a photon, the dark boson&#x27;s spin and Lorentz structure of its couplings can potentially be determined. We analyze the mono-photon channel, $e^+ e^- \rightarrow γ+ \text{invisible}$, in detail, focusing on the production of an on-shell spin-1 boson. We explore this in the context of three separate scenarios for a new dark vector: a dark photon, a mass-mixed ``dark $Z$&#x27;&#x27;, and a vector that couples to right-handed electrons, and estimate how well the couplings of such bosons to electrons can be constrained in the event of a positive signal.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在Chiral Belle上表征暗玻色子</div>
<div class="mono" style="margin-top:8px">我们探讨了在Belle II中采用极化电子束（如Chiral Belle所提议）在寻找与标准模型弱耦合的不可见（暗）玻色子方面的优势。通过测量暗玻色子与光子关联产生的截面随极化的变化，可以潜在地确定暗玻色子的自旋及其耦合的洛伦兹结构。我们详细分析了单光子通道 $e^+ e^- \rightarrow γ+ \text{invisible}$，重点关注了在壳的自旋-1玻色子的产生。我们在此背景下探讨了三种新的暗矢量情形：暗光子、质量混合的 ``暗 $Z$&#x27;&#x27; 以及仅与右旋电子耦合的矢量，并估计在出现正信号的情况下，这些玻色子与电子的耦合能被约束到何种程度。</div>
</details>
</div>
<div class="card">
<div class="title">PVminer: A Domain-Specific Tool to Detect the Patient Voice in Patient Generated Data</div>
<div class="meta-line">Authors: Samah Fodeh, Linhai Ma, Yan Wang, Srivani Talakokkul, Ganesh Puthiaraju, Afshan Khan, Ashley Hagaman, Sarah Lowe, Aimee Roundtree</div>
<div class="meta-line">First: 2026-02-24T18:10:00+00:00 · Latest: 2026-02-24T18:10:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21165v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21165v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Patient-generated text such as secure messages, surveys, and interviews contains rich expressions of the patient voice (PV), reflecting communicative behaviors and social determinants of health (SDoH). Traditional qualitative coding frameworks are labor intensive and do not scale to large volumes of patient-authored messages across health systems. Existing machine learning (ML) and natural language processing (NLP) approaches provide partial solutions but often treat patient-centered communication (PCC) and SDoH as separate tasks or rely on models not well suited to patient-facing language. We introduce PVminer, a domain-adapted NLP framework for structuring patient voice in secure patient-provider communication. PVminer formulates PV detection as a multi-label, multi-class prediction task integrating patient-specific BERT encoders (PV-BERT-base and PV-BERT-large), unsupervised topic modeling for thematic augmentation (PV-Topic-BERT), and fine-tuned classifiers for Code, Subcode, and Combo-level labels. Topic representations are incorporated during fine-tuning and inference to enrich semantic inputs. PVminer achieves strong performance across hierarchical tasks and outperforms biomedical and clinical pre-trained baselines, achieving F1 scores of 82.25% (Code), 80.14% (Subcode), and up to 77.87% (Combo). An ablation study further shows that author identity and topic-based augmentation each contribute meaningful gains. Pre-trained models, source code, and documentation will be publicly released, with annotated datasets available upon request for research use.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PVminer：一种用于检测患者声音的领域专用工具</div>
<div class="mono" style="margin-top:8px">患者生成的数据，如安全消息、调查和访谈，包含丰富的患者声音（PV）表达，反映了沟通行为和社会决定因素（SDoH）。传统的定性编码框架需要大量人力，难以扩展到跨健康系统的大规模患者自述信息。现有的机器学习（ML）和自然语言处理（NLP）方法提供了部分解决方案，但通常将患者中心沟通（PCC）和社会决定因素（SDoH）视为独立任务，或依赖不适合患者面向语言的模型。我们引入了PVminer，这是一个针对安全患者-提供者沟通的领域适应NLP框架，用于结构化患者声音。PVminer将PV检测建模为一个结合患者特定BERT编码器（PV-BERT-base和PV-BERT-large）、无监督主题建模用于主题增强（PV-Topic-BERT）以及微调分类器用于代码、子代码和组合级别的标签的多标签、多类预测任务。主题表示在微调和推理过程中被整合，以增强语义输入。PVminer在分层任务中表现出色，优于生物医学和临床预训练基线模型，分别达到82.25%（代码）、80.14%（子代码）和最高77.87%（组合）的F1分数。进一步的消融研究显示，作者身份和基于主题的增强各自带来了有意义的性能提升。预训练模型、源代码和文档将公开发布，研究者可申请获取带注释的数据集。</div>
</details>
</div>
<div class="card">
<div class="title">A Light Fixture Color Temperature and Color Rendering Index Measuring Device</div>
<div class="meta-line">Authors: Gianluca Hiss Garbim, Luis Carlos Mathias, André Massami Assakawa, Taufik Abrão</div>
<div class="meta-line">First: 2026-02-24T18:08:53+00:00 · Latest: 2026-02-24T18:08:53+00:00</div>
<div class="meta-line">Comments: 11 pages, 12 figures, full paper</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.21163v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.21163v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The correlated color temperature (CCT) and color rendering index (CRI) of artificial light sources are important because they have implications for human biology and professional applications. Although CCT information is generally available for commercial lamps, CRI is commonly not reported. In addition, devices measuring these parameters are difficult to access as they require a spectrophotometer, a commonly expensive device. In this context, the present work designs and builds a meter in detail, from the structural part of the equipment, interface with sensors, and the calculation to the compensation algorithms implementation, aiming to build the dedicated functionalities of a spectrophotometer, which is designed without the use of optical lenses. In addition to simplifying the device, this approach allows measurements free from dispersions caused by chromatic aberrations typical of optical lenses. The prototype obtained proved to be effective, capturing the spectral power distributions of various light sources and calculating their CCT and CRI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种光源色温与显色指数测量装置</div>
<div class="mono" style="margin-top:8px">人工光源的相关色温（CCT）和显色指数（CRI）对于人类生物学和专业应用具有重要意义。尽管商业灯泡通常会提供CCT信息，但CRI信息却很少被报告。此外，测量这些参数的设备难以获得，因为它们通常需要一台光谱仪，而光谱仪是一种常见的昂贵设备。在此背景下，本工作详细设计并构建了一种测量装置，从设备结构、传感器接口、计算到补偿算法的实现，旨在构建一台专用的光谱仪功能设备，且该设备不使用光学透镜。除了简化设备结构外，这种方法还避免了光学透镜常见的色差引起的测量误差。所获得的原型设备已被证明是有效的，能够捕捉各种光源的光谱功率分布，并计算其CCT和CRI。</div>
</details>
</div>
<div class="card">
<div class="title">MoEMba: A Mamba-based Mixture of Experts for High-Density EMG-based Hand Gesture Recognition</div>
<div class="meta-line">Authors: Mehran Shabanpour, Kasra Rad, Sadaf Khademi, Arash Mohammadi</div>
<div class="meta-line">First: 2025-02-09T17:07:46+00:00 · Latest: 2026-02-24T18:08:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.17457v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.17457v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-Density surface Electromyography (HDsEMG) has emerged as a pivotal resource for Human-Computer Interaction (HCI), offering direct insights into muscle activities and motion intentions. However, a significant challenge in practical implementations of HD-sEMG-based models is the low accuracy of inter-session and inter-subject classification. Variability between sessions can reach up to 40% due to the inherent temporal variability of HD-sEMG signals. Targeting this challenge, the paper introduces the MoEMba framework, a novel approach leveraging Selective StateSpace Models (SSMs) to enhance HD-sEMG-based gesture recognition. The MoEMba framework captures temporal dependencies and cross-channel interactions through channel attention techniques. Furthermore, wavelet feature modulation is integrated to capture multi-scale temporal and spatial relations, improving signal representation. Experimental results on the CapgMyo HD-sEMG dataset demonstrate that MoEMba achieves a balanced accuracy of 56.9%, outperforming its state-of-the-art counterparts. The proposed framework&#x27;s robustness to session-to-session variability and its efficient handling of high-dimensional multivariate time series data highlight its potential for advancing HD-sEMG-powered HCI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MoEMba：基于Mamba的专家混合模型用于高密度EMG手部动作识别</div>
<div class="mono" style="margin-top:8px">高密度表面肌电图（HDsEMG）已成为人机交互（HCI）的关键资源，能够直接提供肌肉活动和运动意图的信息。然而，基于HD-sEMG的模型在实际应用中面临的主要挑战是会话间和个体间的分类准确率较低。由于HD-sEMG信号本身具有时间可变性，不同会话之间的差异可达40%。为解决这一问题，本文提出了MoEMba框架，这是一种利用选择性状态空间模型（SSMs）的新方法，以提升基于HD-sEMG的手部动作识别性能。MoEMba框架通过通道注意力技术捕捉时间依赖性和跨通道交互。此外，还集成了小波特征调制，以捕捉多尺度的时间和空间关系，从而改善信号表示。在CapgMyo HD-sEMG数据集上的实验结果表明，MoEMba实现了56.9%的平衡准确率，优于现有的最先进方法。所提出的框架对会话间差异具有鲁棒性，并能高效处理高维多变量时间序列数据，凸显了其在推动HD-sEMG驱动的人机交互系统发展方面的潜力。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260225_0421.html">20260225_0421</a>
<a href="archive/20260224_0435.html">20260224_0435</a>
<a href="archive/20260223_0401.html">20260223_0401</a>
<a href="archive/20260222_0402.html">20260222_0402</a>
<a href="archive/20260221_0415.html">20260221_0415</a>
<a href="archive/20260220_0410.html">20260220_0410</a>
<a href="archive/20260219_0419.html">20260219_0419</a>
<a href="archive/20260218_0416.html">20260218_0416</a>
<a href="archive/20260217_0410.html">20260217_0410</a>
<a href="archive/20260216_0403.html">20260216_0403</a>
<a href="archive/20260215_0401.html">20260215_0401</a>
<a href="archive/20260213_0421.html">20260213_0421</a>
<a href="archive/20260212_0425.html">20260212_0425</a>
<a href="archive/20260210_0435.html">20260210_0435</a>
<a href="archive/20260209_0404.html">20260209_0404</a>
<a href="archive/20260208_0353.html">20260208_0353</a>
<a href="archive/20260207_0410.html">20260207_0410</a>
<a href="archive/20260206_0412.html">20260206_0412</a>
<a href="archive/20260205_0417.html">20260205_0417</a>
<a href="archive/20260204_0421.html">20260204_0421</a>
<a href="archive/20260202_0402.html">20260202_0402</a>
<a href="archive/20260201_0354.html">20260201_0354</a>
<a href="archive/20260131_0408.html">20260131_0408</a>
<a href="archive/20260130_0406.html">20260130_0406</a>
<a href="archive/20260129_0407.html">20260129_0407</a>
<a href="archive/20260128_0407.html">20260128_0407</a>
<a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
