<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-30 03:56</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251230_0356</div>
    <div class="row"><div class="card">
<div class="title">Information Critical Phases under Decoherence</div>
<div class="meta-line">Authors: Akash Vijay, Jong Yeon Lee</div>
<div class="meta-line">First: 2025-12-26T18:59:49+00:00 · Latest: 2025-12-26T18:59:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22121v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22121v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quantum critical phases are extended regions of phase space characterized by a diverging correlation length. By analogy, we define an information critical phase as an extended region of a mixed state phase diagram where the Markov length, the characteristic length scale governing the decay of the conditional mutual information (CMI), diverges.
  We demonstrate that such a phase arises in decohered $\mathbb{Z}_{N}$ Toric codes by assessing both the CMI and the coherent information, the latter quantifying the robustness of the encoded logical qudits. For $N&gt;4$, we find that the system hosts an information critical phase intervening between the decodable and non-decodable phases where the coherent information saturates to a fractional value in the thermodynamic limit, indicating that a finite fraction of logical information is still preserved. We show that the density matrix in this phase can be decomposed into a convex sum of Coulombic pure states, where gapped anyons reorganize into gapless photons. We further consider the ungauged $\mathbb{Z}_{N}$ Toric code and interpret its mixed state phase diagram in the language of strong-to-weak spontaneous symmetry breaking. We argue that in the dual model, the information critical phase arises because the spontaneously broken off-diagonal $\mathbb{Z}_{N}$ symmetry gets enhanced to a U(1) symmetry, resulting in a novel superfluid phase whose gapless modes involve coherent excitations of both the system and the environment. Finally, we propose an optimal decoding protocol for the corrupted $\mathbb{Z}_{N}$ Toric code and evaluate its effectiveness in recovering the fractional logical information preserved in the information critical phase. Our findings identify a gapless analog for mixed-state phases that still acts as a fractional topological quantum memory, thereby extending the conventional paradigm of quantum memory phases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>退相干下的信息临界相</div>
<div class="mono" style="margin-top:8px">量子临界相是相空间中具有发散相关长度的扩展区域。类比地，我们将信息临界相定义为混合态相图中的一个扩展区域，其中马尔可夫长度，即控制条件互信息（CMI）衰减的特征长度尺度，发散。我们证明，在退相干的 $\mathbb{Z}_{N}$ 二维拓扑码中会出现这样的相，通过评估条件互信息和相干信息，后者衡量编码逻辑量子比特的鲁棒性。对于 $N&gt;4$，我们发现系统在可解码相和不可解码相之间存在一个信息临界相，其中相干信息在热力学极限下饱和为分数值，表明仍有一部分逻辑信息被保留。我们展示，在该相中密度矩阵可以分解为库仑纯态的凸和，其中存在能隙的任何子粒子重新组织为无能隙的光子。我们进一步考虑未规范化的 $\mathbb{Z}_{N}$ 二维拓扑码，并用强到弱自发对称破缺的语言来解释其混合态相图。我们论证，在对偶模型中，信息临界相的出现是因为自发破缺的非对角 $\mathbb{Z}_{N}$ 对称性被增强为 U(1) 对称性，从而产生一种新型超流相，其无能隙模式涉及系统和环境的相干激发。最后，我们提出了一种用于受损 $\mathbb{Z}_{N}$ 二维拓扑码的最优解码协议，并评估其在恢复信息临界相中保留的分数逻辑信息方面的有效性。我们的研究结果识别出一种无能隙的混合态相的类比，该相仍作为分数拓扑量子存储器起作用，从而扩展了传统量子存储器相的范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Quantum critical phases are extended regions of phase space characterized by a diverging correlation length.</div>
</details>
</div>
<div class="card">
<div class="title">See Less, See Right: Bi-directional Perceptual Shaping For Multimodal Reasoning</div>
<div class="meta-line">Authors: Shuoshuo Zhang, Yizhen Zhang, Jingjing Fu, Lei Song, Jiang Bian, Yujiu Yang, Rui Wang</div>
<div class="meta-line">First: 2025-12-26T18:59:47+00:00 · Latest: 2025-12-26T18:59:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22120v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22120v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (VLMs) often benefit from intermediate visual cues, either injected via external tools or generated as latent visual tokens during reasoning, but these mechanisms still overlook fine-grained visual evidence (e.g., polylines in charts), generalize poorly across domains, and incur high inference-time cost. In this paper, we propose Bi-directional Perceptual Shaping (BiPS), which transforms question-conditioned masked views into bidirectional where-to-look signals that shape perception during training. BiPS first applies a KL-consistency constraint between the original image and an evidence-preserving view that keeps only question-relevant regions, encouraging coarse but complete coverage of supporting pixels. It then applies a KL-separation constraint between the original and an evidence-ablated view where critical pixels are masked so the image no longer supports the original answer, discouraging text-only shortcuts (i.e., answering from text alone) and enforcing fine-grained visual reliance. Across eight benchmarks, BiPS boosts Qwen2.5-VL-7B by 8.2% on average and shows strong out-of-domain generalization to unseen datasets and image types.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>少看多看：用于多模态推理的双向感知塑造</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（VLMs）通常受益于中间视觉线索，这些线索要么通过外部工具注入，要么在推理过程中生成为潜在的视觉标记，但这些机制仍然忽略了细粒度的视觉证据（例如图表中的折线），跨领域泛化能力较差，并且在推理时间成本较高。在本文中，我们提出了双向感知塑造（BiPS），该方法将问题条件下的掩码视图转换为双向的注意力信号，从而在训练过程中塑造感知。BiPS首先在原始图像和一个保留证据的视图之间应用KL一致性约束，该视图仅保留与问题相关区域，鼓励对支持像素进行粗粒度但完整的覆盖。然后，它在原始图像和一个去除关键证据的视图之间应用KL分离约束，该视图通过掩码关键像素使得图像不再支持原始答案，从而抑制仅依赖文本的捷径，并强制依赖细粒度的视觉信息。在八个基准测试中，BiPS平均将Qwen2.5-VL-7B的性能提升了8.2%，并在未见过的数据集和图像类型上展示了强大的跨领域泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Charge-Informed Quantum Error Correction</div>
<div class="meta-line">Authors: Vlad Temkin, Zack Weinstein, Ruihua Fan, Daniel Podolsky, Ehud Altman</div>
<div class="meta-line">First: 2025-12-26T18:59:21+00:00 · Latest: 2025-12-26T18:59:21+00:00</div>
<div class="meta-line">Comments: 7+22 pages, 2+10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22119v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22119v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate the statistical physics of quantum error correction in ${\rm U}(1)$ symmetry-enriched topological quantum memories. Starting from a phenomenological error model of charge-conserving noise, we study the optimal decoder assuming the local charges of each anyon can be measured. The error threshold of the optimal decoder corresponds to a continuous phase transition in a disordered two-dimensional integer loop model on the Nishimori line. Using an effective replica field theory analysis and Monte Carlo numerics, we show that the optimal decoding transition exhibits Berezinskii-Kosterlitz-Thouless universality with a modified universal jump in winding number variance. We further generalize the model beyond the Nishimori line, which defines a large class of suboptimal decoders. At low nonzero temperatures and strong disorder, we find numerical evidence of a disorder-dominated loop-glass phase which corresponds to a &quot;confidently incorrect&quot; decoder. The zero-temperature limit defines the minimum-cost flow decoder, which serves as the ${\rm U}(1)$ analog of minimum-weight perfect matching in $\mathbb{Z}_2$ topological codes. Both the optimal and minimum-cost flow decoders are shown to dramatically outperform the charge-agnostic optimal decoder in symmetry-enriched topological codes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于电荷信息的量子错误校正</div>
<div class="mono" style="margin-top:8px">我们研究了在${\rm U}(1)$对称性增强的拓扑量子存储器中量子错误校正的统计物理特性。从一个现象学的电荷守恒噪声模型出发，我们研究了在能够测量每个任何子的局部电荷的情况下最优解码器的特性。最优解码器的错误阈值对应于无序二维整数环模型在Nishimori线上的连续相变。通过有效复制场论分析和蒙特卡洛数值计算，我们展示了最优解码相变表现出Berezinskii-Kosterlitz-Thouless普适性，并具有修改后的缠绕数方差的普适跳跃。我们进一步将模型推广到Nishimori线之外，这定义了一大类次优解码器。在非零低温和强无序条件下，我们发现了无序主导的环玻璃相，这对应于一个&quot;自信错误&quot;的解码器。零温极限定义了最小成本流解码器，它作为${\rm U}(1)$对称性增强拓扑码中最小权重完美匹配的类比。我们证明了最优解码器和最小成本流解码器在对称性增强拓扑码中显著优于电荷无关的最优解码器。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We investigate the statistical physics of quantum error correction in ${\rm U}(1)$ symmetry-enriched topological quantum memories.</div>
</details>
</div>
<div class="card">
<div class="title">ProEdit: Inversion-based Editing From Prompts Done Right</div>
<div class="meta-line">Authors: Zhi Ouyang, Dian Zheng, Xiao-Ming Wu, Jian-Jian Jiang, Kun-Yu Lin, Jingke Meng, Wei-Shi Zheng</div>
<div class="meta-line">First: 2025-12-26T18:59:14+00:00 · Latest: 2025-12-26T18:59:14+00:00</div>
<div class="meta-line">Comments: Equal contributions from first two authors. Project page: https://isee-laboratory.github.io/ProEdit/ Code: https://github.com/iSEE-Laboratory/ProEdit</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22118v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22118v1">PDF</a> · <a href="https://github.com/iSEE-Laboratory/ProEdit">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://isee-laboratory.github.io/ProEdit/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions. Existing methods typically inject source image information during the sampling process to maintain editing consistency. However, this sampling strategy overly relies on source information, which negatively affects the edits in the target image (e.g., failing to change the subject&#x27;s atributes like pose, number, or color as instructed). In this work, we propose ProEdit to address this issue both in the attention and the latent aspects. In the attention aspect, we introduce KV-mix, which mixes KV features of the source and the target in the edited region, mitigating the influence of the source image on the editing region while maintaining background consistency. In the latent aspect, we propose Latents-Shift, which perturbs the edited region of the source latent, eliminating the influence of the inverted latent on the sampling. Extensive experiments on several image and video editing benchmarks demonstrate that our method achieves SOTA performance. In addition, our design is plug-and-play, which can be seamlessly integrated into existing inversion and editing methods, such as RF-Solver, FireFlow and UniEdit.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProEdit: 正确实现的基于反向的编辑</div>
<div class="mono" style="margin-top:8px">基于反向的视觉编辑提供了一种有效且无需训练的方法，根据用户指令对图像或视频进行编辑。现有方法通常在采样过程中注入源图像信息以保持编辑的一致性。然而，这种采样策略过度依赖源信息，会对目标图像的编辑产生负面影响（例如，无法按照指令更改主体的属性，如姿态、数量或颜色）。在本工作中，我们提出ProEdit来解决这两个方面的问题：在注意力方面，我们引入KV-mix，将源图像和目标图像在编辑区域的KV特征进行混合，从而减少源图像对编辑区域的影响，同时保持背景一致性；在潜在空间方面，我们提出Latents-Shift，对源潜在特征的编辑区域进行扰动，消除反向潜在特征对采样的影响。我们在多个图像和视频编辑基准数据集上的大量实验表明，我们的方法达到了SOTA性能。此外，我们的设计是即插即用型，可以无缝集成到现有的反向和编辑方法中，例如RF-Solver、FireFlow和UniEdit。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Inversion-based visual editing provides an effective and training-free way to edit an image or a video based on user instructions.</div>
</details>
</div>
<div class="card">
<div class="title">Agentic Structured Graph Traversal for Root Cause Analysis of Code-related Incidents in Cloud Applications</div>
<div class="meta-line">Authors: Shengkun Cui, Rahul Krishna, Saurabh Jha, Ravishankar K. Iyer</div>
<div class="meta-line">First: 2025-12-26T18:56:18+00:00 · Latest: 2025-12-26T18:56:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22113v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22113v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cloud incidents pose major operational challenges in production, with unresolved production cloud incidents cost on average over $2M per hour. Prior research identifies code- and configuration-related issues as the predominant category of root causes in cloud incidents. This paper introduces PRAXIS, an orchestrator that manages and deploys an agentic workflow for diagnosing code- and configuration-caused cloud incidents. PRAXIS employs an LLM-driven structured traversal over two types of graph: (1) a service dependency graph (SDG) that captures microservice-level dependencies; and (2) a hammock-block program dependence graph (PDG) that captures code-level dependencies for each microservice. Together, these graphs encode microservice- and code-level dependencies and the LLM acts as a traversal policy over these graphs, moving between services and code dependencies to localize and explain failures. Compared to state-of-the-art ReAct baselines, PRAXIS improves RCA accuracy by up to 3.1x while reducing token consumption by 3.8x. PRAXIS is demonstrated on a set of 30 comprehensive real-world incidents that is being compiled into an RCA benchmark.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于代理的结构化图遍历在云应用代码相关事件根本原因分析中的应用</div>
<div class="mono" style="margin-top:8px">云事件在生产环境中带来重大运营挑战，未解决的生产云事件平均每小时造成超过200万美元的损失。先前的研究指出，代码和配置相关问题是最常见的云事件根本原因类别。本文介绍了PRAXIS，一个用于诊断由代码和配置引起的云事件的代理工作流编排器。PRAXIS利用大型语言模型（LLM）驱动的结构化遍历方法，对两种类型的图进行操作：(1) 服务依赖图（SDG），用于捕捉微服务级别的依赖关系；(2) hammock-block程序依赖图（PDG），用于捕捉每个微服务的代码级别依赖关系。这些图共同编码了微服务和代码级别的依赖关系，而LLM则作为遍历策略，在这些图上进行服务和代码依赖关系的移动，以定位和解释故障。与最先进的ReAct基线相比，PRAXIS将根本原因分析（RCA）的准确性提高了最高3.1倍，同时减少了3.8倍的token消耗。PRAXIS已在一组30个全面的现实世界事件上进行了演示，这些事件正在被编入一个RCA基准测试中。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Cloud incidents pose major operational challenges in production, with unresolved production cloud incidents cost on average over $2M per hour.</div>
</details>
</div>
<div class="card">
<div class="title">Experimental End-to-End Optimization of Directly Modulated Laser-based IM/DD Transmission</div>
<div class="meta-line">Authors: Sergio Hernandez, Christophe Peucheret, Francesco Da Ros, Darko Zibar</div>
<div class="meta-line">First: 2025-08-27T14:13:59+00:00 · Latest: 2025-12-26T18:55:41+00:00</div>
<div class="meta-line">Comments: 10 pages, 10 figures, published in journal of lightwave technology</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.19910v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.19910v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Directly modulated lasers (DMLs) are an attractive technology for short-reach intensity modulation and direct detection communication systems. However, their complex nonlinear dynamics make the modeling and optimization of DML-based systems challenging. In this paper, we study the end-to-end optimization of DML-based systems based on a data-driven surrogate model trained on experimental data. The end-to-end optimization includes the pulse shaping and equalizer filters, the bias current and the modulation radio-frequency (RF) power applied to the laser. The performance of the end-to-end optimization scheme is tested on the experimental setup and compared to 4 different benchmark schemes based on linear and nonlinear receiver-side equalization. The results show that the proposed end-to-end scheme is able to deliver better performance throughout the studied symbol rates and transmission distances while employing lower modulation RF power, fewer filter taps and utilizing a smaller signal bandwidth.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于直接调制激光的IM/DD传输系统端到端优化实验研究</div>
<div class="mono" style="margin-top:8px">直接调制激光器（DMLs）是短距离强度调制和直接检测通信系统中具有吸引力的技术。然而，其复杂的非线性动态特性使得基于DML的系统建模和优化变得具有挑战性。本文基于实验数据训练的数据驱动代理模型，研究了基于DML的系统的端到端优化。端到端优化包括脉冲成形和均衡滤波器、施加于激光器的偏置电流和调制射频（RF）功率。该端到端优化方案在实验平台上进行了测试，并与基于线性和非线性接收端均衡的四种基准方案进行了比较。结果表明，所提出的端到端方案在研究的符号率和传输距离范围内均能提供更优的性能，同时采用更低的调制RF功率、更少的滤波器抽头数，并利用更小的信号带宽。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Directly modulated lasers (DMLs) are an attractive technology for short-reach intensity modulation and direct detection communication systems.</div>
</details>
</div>
<div class="card">
<div class="title">Cost-aware Stopping for Bayesian Optimization</div>
<div class="meta-line">Authors: Qian Xie, Linda Cai, Alexander Terenin, Peter I. Frazier, Ziv Scully</div>
<div class="meta-line">First: 2025-07-16T17:54:14+00:00 · Latest: 2025-12-26T18:48:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.12453v4">Abs</a> · <a href="https://arxiv.org/pdf/2507.12453v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In automated machine learning, scientific discovery, and other applications of Bayesian optimization, deciding when to stop evaluating expensive black-box functions in a cost-aware manner is an important but underexplored practical consideration. A natural performance metric for this purpose is the cost-adjusted simple regret, which captures the trade-off between solution quality and cumulative evaluation cost. While several heuristic or adaptive stopping rules have been proposed, they lack guarantees ensuring stopping before incurring excessive function evaluation costs. We propose a principled cost-aware stopping rule for Bayesian optimization that adapts to varying evaluation costs without heuristic tuning. Our rule is grounded in a theoretical connection to state-of-the-art cost-aware acquisition functions, namely the Pandora&#x27;s Box Gittins Index (PBGI) and log expected improvement per cost (LogEIPC). We prove a theoretical guarantee bounding the expected cost-adjusted simple regret incurred by our stopping rule when paired with either acquisition function. Across synthetic and empirical tasks, including hyperparameter optimization and neural architecture size search, pairing our stopping rule with PBGI or LogEIPC usually matches or outperforms other acquisition-function--stopping-rule pairs in terms of cost-adjusted simple regret.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向成本的贝叶斯优化停止策略</div>
<div class="mono" style="margin-top:8px">在自动机器学习、科学发现等贝叶斯优化的应用中，以成本感知的方式决定何时停止评估昂贵的黑盒函数是一个重要但尚未充分研究的实践问题。一个自然的性能度量是成本调整后的简单遗憾，它捕捉了解的质量与累积评估成本之间的权衡。尽管已有多种启发式或自适应停止规则被提出，但它们缺乏保证在发生过高函数评估成本之前停止的理论保障。我们提出了一种基于理论的、面向成本的贝叶斯优化停止规则，无需启发式调参即可适应不同的评估成本。我们的规则建立在与最先进的成本感知获取函数（如 Pandora&#x27;s Box Gittins Index (PBGI) 和 log expected improvement per cost (LogEIPC)）的理论联系之上。我们证明了当与这些获取函数配对使用时，该停止规则所导致的成本调整后的简单遗憾的期望值具有理论保证。在合成任务和实际任务（包括超参数优化和神经网络架构规模搜索）中，我们的停止规则与 PBGI 或 LogEIPC 配对使用时，通常能与其它获取函数-停止规则组合相媲美甚至优于它们，在成本调整后的简单遗憾方面表现更优。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In automated machine learning, scientific discovery, and other applications of Bayesian optimization, deciding when to stop evaluating expensive black-box functions in a cost-aware manner is an important but underexplored practical consideration.</div>
</details>
</div>
<div class="card">
<div class="title">Pruning as a Game: Equilibrium-Driven Sparsification of Neural Networks</div>
<div class="meta-line">Authors: Zubair Shah, Noaman Khan</div>
<div class="meta-line">First: 2025-12-26T18:25:38+00:00 · Latest: 2025-12-26T18:25:38+00:00</div>
<div class="meta-line">Comments: Preprint. Under review / to be submitted to a conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22106v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22106v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural network pruning is widely used to reduce model size and computational cost. Yet, most existing methods treat sparsity as an externally imposed constraint, enforced through heuristic importance scores or training-time regularization. In this work, we propose a fundamentally different perspective: pruning as an equilibrium outcome of strategic interaction among model components. We model parameter groups such as weights, neurons, or filters as players in a continuous non-cooperative game, where each player selects its level of participation in the network to balance contribution against redundancy and competition. Within this formulation, sparsity emerges naturally when continued participation becomes a dominated strategy at equilibrium. We analyze the resulting game and show that dominated players collapse to zero participation under mild conditions, providing a principled explanation for pruning behavior. Building on this insight, we derive a simple equilibrium-driven pruning algorithm that jointly updates network parameters and participation variables without relying on explicit importance scores. This work focuses on establishing a principled formulation and empirical validation of pruning as an equilibrium phenomenon, rather than exhaustive architectural or large-scale benchmarking. Experiments on standard benchmarks demonstrate that the proposed approach achieves competitive sparsity-accuracy trade-offs while offering an interpretable, theory-grounded alternative to existing pruning methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>剪枝作为一种博弈：基于均衡驱动的神经网络稀疏化</div>
<div class="mono" style="margin-top:8px">神经网络剪枝被广泛用于减少模型规模和计算成本。然而，大多数现有方法将稀疏性视为外部施加的约束，通过启发式重要性评分或训练时的正则化来强制执行。在本文中，我们提出了一种根本不同的视角：将剪枝视为模型组件之间策略性互动的均衡结果。我们将参数组（如权重、神经元或滤波器）建模为连续非合作博弈中的玩家，每个玩家选择其在网络中的参与程度，以在贡献、冗余和竞争之间取得平衡。在此框架下，当持续参与成为均衡下的劣势策略时，稀疏性自然出现。我们分析了由此产生的博弈，并表明在温和条件下，劣势玩家会收敛到零参与，从而为剪枝行为提供了原理性的解释。基于这一见解，我们推导出一种简单的基于均衡驱动的剪枝算法，该算法在不依赖显式重要性评分的情况下，联合更新网络参数和参与变量。本文的重点是建立剪枝作为一种均衡现象的原理性框架，并进行实证验证，而不是详尽的架构分析或大规模基准测试。在标准基准上的实验表明，所提出的方法在稀疏性与准确性的权衡上具有竞争力，同时为现有的剪枝方法提供了一种可解释且基于理论的替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Neural network pruning is widely used to reduce model size and computational cost.</div>
</details>
</div>
<div class="card">
<div class="title">Learning Association via Track-Detection Matching for Multi-Object Tracking</div>
<div class="meta-line">Authors: Momir Adžemović</div>
<div class="meta-line">First: 2025-12-26T18:19:39+00:00 · Latest: 2025-12-26T18:19:39+00:00</div>
<div class="meta-line">Comments: 14 pages (+4 for references), 8 tables, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22105v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22105v1">PDF</a> · <a href="https://github.com/Robotmurlock/TDLP}{https://github.com/Robotmurlock/TDLP">Code1</a> · <a href="https://github.com/Robotmurlock/TDLP">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-object tracking aims to maintain object identities over time by associating detections across video frames. Two dominant paradigms exist in literature: tracking-by-detection methods, which are computationally efficient but rely on handcrafted association heuristics, and end-to-end approaches, which learn association from data at the cost of higher computational complexity. We propose Track-Detection Link Prediction (TDLP), a tracking-by-detection method that performs per-frame association via link prediction between tracks and detections, i.e., by predicting the correct continuation of each track at every frame. TDLP is architecturally designed primarily for geometric features such as bounding boxes, while optionally incorporating additional cues, including pose and appearance. Unlike heuristic-based methods, TDLP learns association directly from data without handcrafted rules, while remaining modular and computationally efficient compared to end-to-end trackers. Extensive experiments on multiple benchmarks demonstrate that TDLP consistently surpasses state-of-the-art performance across both tracking-by-detection and end-to-end methods. Finally, we provide a detailed analysis comparing link prediction with metric learning-based association and show that link prediction is more effective, particularly when handling heterogeneous features such as detection bounding boxes. Our code is available at \href{https://github.com/Robotmurlock/TDLP}{https://github.com/Robotmurlock/TDLP}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过轨迹-检测匹配进行学习关联的多目标跟踪</div>
<div class="mono" style="margin-top:8px">多目标跟踪旨在通过在视频帧之间关联检测结果来维持目标的身份。文献中存在两种主导范式：基于检测的跟踪方法，其计算效率高但依赖手工设计的关联启发式规则；以及端到端方法，其通过数据学习关联关系，但计算复杂度更高。我们提出了一种基于检测的跟踪方法Track-Detection Link Prediction (TDLP)，它通过预测轨迹与检测之间的正确关联来实现每帧的关联。TDLP主要针对几何特征（如边界框）进行架构设计，同时可选地结合其他特征，如姿态和外观。与基于启发式的方法不同，TDLP直接从数据中学习关联关系，无需手工规则，同时相比端到端跟踪器保持模块化和计算效率。我们在多个基准数据集上的大量实验表明，TDLP在基于检测和端到端方法中均优于当前最先进的方法。最后，我们提供了对基于链接预测与基于度量学习的关联方法的详细比较分析，并表明链接预测在处理异构特征（如检测边界框）时更为有效。我们的代码可在\href{https://github.com/Robotmurlock/TDLP}{https://github.com/Robotmurlock/TDLP}获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-object tracking aims to maintain object identities over time by associating detections across video frames.</div>
</details>
</div>
<div class="card">
<div class="title">Explainable Multimodal Regression via Information Decomposition</div>
<div class="meta-line">Authors: Zhaozhao Ma, Shujian Yu</div>
<div class="meta-line">First: 2025-12-26T18:07:18+00:00 · Latest: 2025-12-26T18:07:18+00:00</div>
<div class="meta-line">Comments: Project Page: https://github.com/zhaozhaoma/PIDReg</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22102v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22102v1">PDF</a> · <a href="https://github.com/zhaozhaoma/PIDReg">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal regression aims to predict a continuous target from heterogeneous input sources and typically relies on fusion strategies such as early or late fusion. However, existing methods lack principled tools to disentangle and quantify the individual contributions of each modality and their interactions, limiting the interpretability of multimodal fusion. We propose a novel multimodal regression framework grounded in Partial Information Decomposition (PID), which decomposes modality-specific representations into unique, redundant, and synergistic components. The basic PID framework is inherently underdetermined. To resolve this, we introduce inductive bias by enforcing Gaussianity in the joint distribution of latent representations and the transformed response variable (after inverse normal transformation), thereby enabling analytical computation of the PID terms. Additionally, we derive a closed-form conditional independence regularizer to promote the isolation of unique information within each modality. Experiments on six real-world datasets, including a case study on large-scale brain age prediction from multimodal neuroimaging data, demonstrate that our framework outperforms state-of-the-art methods in both predictive accuracy and interpretability, while also enabling informed modality selection for efficient inference. Implementation is available at https://github.com/zhaozhaoma/PIDReg.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过信息分解实现可解释的多模态回归</div>
<div class="mono" style="margin-top:8px">多模态回归旨在从异构输入源预测连续目标，通常依赖于早期或晚期融合策略。然而，现有方法缺乏对各模态及其交互作用的个体贡献进行分离和量化的原则性工具，限制了多模态融合的可解释性。我们提出了一种基于部分信息分解（PID）的新型多模态回归框架，该框架将模态特定的表示分解为独特、冗余和协同成分。基本的PID框架本质上是欠确定的。为了解决这一问题，我们通过在潜在表示和变换响应变量（经过反向正态变换后）的联合分布中强制引入高斯性，从而实现PID项的解析计算。此外，我们推导出一个闭合形式的条件独立性正则化项，以促进每个模态中独特信息的隔离。在六个真实世界数据集上的实验，包括一个基于多模态神经影像数据的大规模脑龄预测案例研究，表明我们的框架在预测准确性和可解释性方面均优于现有最先进的方法，同时还能支持高效推理的模态选择。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multimodal regression aims to predict a continuous target from heterogeneous input sources and typically relies on fusion strategies such as early or late fusion.</div>
</details>
</div>
<div class="card">
<div class="title">Rewards-based image analysis in microscopy</div>
<div class="meta-line">Authors: Kamyar Barakati, Yu Liu, Utkarsh Pratiush, Boris N. Slautin, Sergei V. Kalinin</div>
<div class="meta-line">First: 2025-02-23T19:19:38+00:00 · Latest: 2025-12-26T18:04:07+00:00</div>
<div class="meta-line">Comments: 41 pages, 11 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.18522v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.18522v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Imaging and hyperspectral data analysis is central to progress across biology, medicine, chemistry, and physics. The core challenge lies in converting high-resolution or high-dimensional datasets into interpretable representations that enable insight into the underlying physical or chemical properties of a system. Traditional analysis relies on expert-designed, multistep workflows, such as denoising, feature extraction, clustering, dimensionality reduction, and physics-based deconvolution, or on machine learning (ML) methods that accelerate individual steps. Both approaches, however, typically demand significant human intervention, including hyperparameter tuning and data labeling. Achieving the next level of autonomy in scientific imaging requires designing effective reward-based workflows that guide algorithms toward best data representation for human or automated decision-making. Here, we discuss recent advances in reward-based workflows for image analysis, which capture key elements of human reasoning and exhibit strong transferability across various tasks. We highlight how reward-driven approaches enable a shift from supervised black-box models toward explainable, unsupervised optimization on the examples of Scanning Probe and Electron Microscopies. Such reward-based frameworks are promising for a broad range of applications, including classification, regression, structure-property mapping, and general hyperspectral data processing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于奖励的显微图像分析</div>
<div class="mono" style="margin-top:8px">成像和高光谱数据分析在生物学、医学、化学和物理学的进步中起着核心作用。主要挑战在于将高分辨率或高维数据集转化为可解释的表示形式，以揭示系统底层的物理或化学特性。传统分析依赖于专家设计的多步骤工作流程，如降噪、特征提取、聚类、降维和基于物理的反卷积，或者依赖于机器学习（ML）方法来加速单个步骤。然而，这两种方法通常都需要大量的人工干预，包括超参数调优和数据标注。实现科学成像的更高自主性需要设计有效的基于奖励的工作流程，以引导算法生成适合人类或自动化决策的最佳数据表示。本文讨论了近期在图像分析中基于奖励的工作流程的进展，这些方法捕捉了人类推理的关键要素，并在各种任务中表现出良好的迁移能力。我们还强调了奖励驱动方法如何推动从监督式黑箱模型向可解释的无监督优化转变，以扫描探针显微镜和电子显微镜为例。此类基于奖励的框架在广泛的应用中具有前景，包括分类、回归、结构-性质映射和一般高光谱数据处理。</div>
</details>
</div>
<div class="card">
<div class="title">A2P-Vis: an Analyzer-to-Presenter Agentic Pipeline for Visual Insights Generation and Reporting</div>
<div class="meta-line">Authors: Shuyu Gan, Renxiang Wang, James Mooney, Dongyeop Kang</div>
<div class="meta-line">Venue: 1st Workshop on GenAI, Agents, and the Future of VIS (VIS x GenAI), November 2025, Vienna, Austria</div>
<div class="meta-line">First: 2025-12-26T18:02:12+00:00 · Latest: 2025-12-26T18:02:12+00:00</div>
<div class="meta-line">Comments: 3 pages, 3 figures; Accepted by 1st Workshop on GenAI, Agents and the Future of VIS as Mini-challenge paper and win the Honorable Mention award. Submit number is 7597 and the paper is archived on the workshop website: https://visxgenai.github.io/subs-2025/7597/7597-doc.pdf</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22101v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22101v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://visxgenai.github.io/subs-2025/7597/7597-doc.pdf">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automating end-to-end data science pipeline with AI agents still stalls on two gaps: generating insightful, diverse visual evidence and assembling it into a coherent, professional report. We present A2P-Vis, a two-part, multi-agent pipeline that turns raw datasets into a high-quality data-visualization report. The Data Analyzer orchestrates profiling, proposes diverse visualization directions, generates and executes plotting code, filters low-quality figures with a legibility checker, and elicits candidate insights that are automatically scored for depth, correctness, specificity, depth and actionability. The Presenter then orders topics, composes chart-grounded narratives from the top-ranked insights, writes justified transitions, and revises the document for clarity and consistency, yielding a coherent, publication-ready report. Together, these agents convert raw data into curated materials (charts + vetted insights) and into a readable narrative without manual glue work. We claim that by coupling a quality-assured Analyzer with a narrative Presenter, A2P-Vis operationalizes co-analysis end-to-end, improving the real-world usefulness of automated data analysis for practitioners. For the complete dataset report, please see: https://www.visagent.org/api/output/f2a3486d-2c3b-4825-98d4-5af25a819f56.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>A2P-Vis：一种分析到展示的代理流程，用于生成和报告视觉洞察</div>
<div class="mono" style="margin-top:8px">利用AI代理自动化端到端数据科学流程仍存在两个瓶颈：生成有洞察力且多样的视觉证据，以及将其整合为连贯且专业的报告。我们提出了A2P-Vis，一种由两个部分组成的多代理流程，可将原始数据集转换为高质量的数据可视化报告。数据分析师负责数据概览、提出多样化的可视化方向、生成并执行绘图代码、使用可读性检查器过滤低质量图表，并提取候选洞察，这些洞察会根据深度、正确性、具体性、深度和可操作性进行自动评分。展示者则对主题进行排序，从排名最高的洞察中构建图表支撑的叙述，撰写合理的过渡段落，并修订文档以提高清晰度和一致性，最终生成一个连贯且适合发表的报告。通过这两个代理，A2P-Vis将原始数据转化为经过筛选的材料（图表 + 已验证的洞察），并生成可读的叙述，无需人工粘合。我们主张，通过将一个经过质量保证的分析代理与一个叙述展示代理结合，A2P-Vis实现了端到端的协同分析，提高了自动化数据分析在实践中的实用性。完整数据集报告请参见：https://www.visagent.org/api/output/f2a3486d-2c3b-4825-98d4-5af25a819f56。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Automating end-to-end data science pipeline with AI agents still stalls on two gaps: generating insightful, diverse visual evidence and assembling it into a coherent, professional report.</div>
</details>
</div>
<div class="card">
<div class="title">Introducing TrGLUE and SentiTurca: A Comprehensive Benchmark for Turkish General Language Understanding and Sentiment Analysis</div>
<div class="meta-line">Authors: Duygu Altinok</div>
<div class="meta-line">First: 2025-12-26T18:02:09+00:00 · Latest: 2025-12-26T18:02:09+00:00</div>
<div class="meta-line">Comments: under review by Springer</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22100v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22100v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating the performance of various model architectures, such as transformers, large language models (LLMs), and other NLP systems, requires comprehensive benchmarks that measure performance across multiple dimensions. Among these, the evaluation of natural language understanding (NLU) is particularly critical as it serves as a fundamental criterion for assessing model capabilities. Thus, it is essential to establish benchmarks that enable thorough evaluation and analysis of NLU abilities from diverse perspectives. While the GLUE benchmark has set a standard for evaluating English NLU, similar benchmarks have been developed for other languages, such as CLUE for Chinese, FLUE for French, and JGLUE for Japanese. However, no comparable benchmark currently exists for the Turkish language. To address this gap, we introduce TrGLUE, a comprehensive benchmark encompassing a variety of NLU tasks for Turkish. In addition, we present SentiTurca, a specialized benchmark for sentiment analysis. To support researchers, we also provide fine-tuning and evaluation code for transformer-based models, facilitating the effective use of these benchmarks. TrGLUE comprises Turkish-native corpora curated to mirror the domains and task formulations of GLUE-style evaluations, with labels obtained through a semi-automated pipeline that combines strong LLM-based annotation, cross-model agreement checks, and subsequent human validation. This design prioritizes linguistic naturalness, minimizes direct translation artifacts, and yields a scalable, reproducible workflow. With TrGLUE, our goal is to establish a robust evaluation framework for Turkish NLU, empower researchers with valuable resources, and provide insights into generating high-quality semi-automated datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>介绍TrGLUE和SentiTurca：土耳其通用语言理解与情感分析的全面基准</div>
<div class="mono" style="margin-top:8px">评估各种模型架构（如Transformer、大型语言模型（LLMs）和其他NLP系统）的性能需要全面的基准，以衡量其在多个维度上的表现。其中，自然语言理解（NLU）的评估尤为重要，因为它是评估模型能力的基本标准。因此，建立能够从多角度全面评估和分析NLU能力的基准至关重要。虽然GLUE基准为评估英语NLU设定了标准，但其他语言也开发了类似的基准，如CLUE用于中文，FLUE用于法语，JGLUE用于日语。然而，目前尚无类似的基准用于土耳其语。为解决这一问题，我们引入了TrGLUE，这是一个涵盖多种土耳其NLU任务的全面基准。此外，我们还提出了SentiTurca，一个专门用于情感分析的基准。为了支持研究人员，我们还提供了基于Transformer模型的微调和评估代码，便于有效使用这些基准。TrGLUE包含由土耳其本地语料库构成的数据集，这些数据集经过精心整理，以反映GLUE风格评估的领域和任务形式。标签是通过一个半自动流程获得的，该流程结合了强大的LLM注释、跨模型一致性检查以及后续的人工验证。这种设计优先考虑语言的自然性，减少直接翻译的痕迹，并产生可扩展且可复现的工作流程。通过TrGLUE，我们的目标是建立一个稳健的土耳其NLU评估框架，为研究人员提供有价值的资源，并提供生成高质量半自动数据集的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Evaluating the performance of various model architectures, such as transformers, large language models (LLMs), and other NLP systems, requires comprehensive benchmarks that measure performance across multiple dimensions.</div>
</details>
</div>
<div class="card">
<div class="title">Yume-1.5: A Text-Controlled Interactive World Generation Model</div>
<div class="meta-line">Authors: Xiaofeng Mao, Zhen Li, Chuanhao Li, Xiaojie Xu, Kaining Ying, Tong He, Jiangmiao Pang, Yu Qiao, Kaipeng Zhang</div>
<div class="meta-line">First: 2025-12-26T17:52:49+00:00 · Latest: 2025-12-26T17:52:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22096v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22096v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds. However, most of these methods face critical challenges such as excessively large parameter sizes, reliance on lengthy inference steps, and rapidly growing historical context, which severely limit real-time performance and lack text-controlled generation capabilities. To address these challenges, we propose \method, a novel framework designed to generate realistic, interactive, and continuous worlds from a single image or text prompt. \method achieves this through a carefully designed framework that supports keyboard-based exploration of the generated worlds. The framework comprises three core components: (1) a long-video generation framework integrating unified context compression with linear attention; (2) a real-time streaming acceleration strategy powered by bidirectional attention distillation and an enhanced text embedding scheme; (3) a text-controlled method for generating world events. We have provided the codebase in the supplementary material.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Yume-1.5：一个文本控制的交互式世界生成模型</div>
<div class="mono" style="margin-top:8px">近期的方法展示了使用扩散模型生成交互式和可探索世界的可能性。然而，大多数方法面临诸如参数规模过大、依赖长时间推理步骤以及历史上下文迅速增长等关键挑战，这严重限制了实时性能，并缺乏文本控制的生成能力。为了解决这些问题，我们提出了\method，一个新颖的框架，旨在从单张图像或文本提示生成逼真、交互式和连续的世界。\method 通过精心设计的框架实现，该框架支持基于键盘的生成世界探索。该框架包含三个核心组件：(1) 一个结合统一上下文压缩与线性注意力的长视频生成框架；(2) 一种由双向注意力蒸馏和增强的文本嵌入方案驱动的实时流加速策略；(3) 一种用于生成世界事件的文本控制方法。我们在补充材料中提供了代码库。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent approaches have demonstrated the promise of using diffusion models to generate interactive and explorable worlds.</div>
</details>
</div>
<div class="card">
<div class="title">Accelerating Diffusion Planners in Offline RL via Reward-Aware Consistency Trajectory Distillation</div>
<div class="meta-line">Authors: Xintong Duan, Yutong He, Fahim Tajwar, Ruslan Salakhutdinov, J. Zico Kolter, Jeff Schneider</div>
<div class="meta-line">First: 2025-06-09T14:48:19+00:00 · Latest: 2025-12-26T17:50:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.07822v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.07822v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although diffusion models have achieved strong results in decision-making tasks, their slow inference speed remains a key limitation. While consistency models offer a potential solution, existing applications to decision-making either struggle with suboptimal demonstrations under behavior cloning or rely on complex concurrent training of multiple networks under the actor-critic framework. In this work, we propose a novel approach to consistency distillation for offline reinforcement learning that directly incorporates reward optimization into the distillation process. Our method achieves single-step sampling while generating higher-reward action trajectories through decoupled training and noise-free reward signals. Empirical evaluations on the Gym MuJoCo, FrankaKitchen, and long horizon planning benchmarks demonstrate that our approach can achieve a 9.7% improvement over previous state-of-the-art while offering up to 142x speedup over diffusion counterparts in inference time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过奖励感知的一致性轨迹蒸馏加速离线强化学习中的扩散规划器</div>
<div class="mono" style="margin-top:8px">尽管扩散模型在决策任务中取得了显著成果，但其推理速度缓慢仍然是一个关键限制。虽然一致性模型提供了一种潜在的解决方案，但现有的应用在行为克隆下要么难以获得次优的示范轨迹，要么依赖于在actor-critic框架下对多个网络进行复杂的并行训练。在本工作中，我们提出了一种新的用于离线强化学习的一致性蒸馏方法，该方法直接将奖励优化纳入蒸馏过程。我们的方法通过解耦训练和无噪声奖励信号，在实现单步采样的同时生成高奖励的动作轨迹。在Gym MuJoCo、FrankaKitchen和长时间规划基准上的实证评估表明，我们的方法在奖励表现上比之前最先进的方法提高了9.7%，并且在推理时间上比扩散模型快了高达142倍。</div>
</details>
</div>
<div class="card">
<div class="title">Towards a Functionally Complete and Parameterizable TFHE Processor</div>
<div class="meta-line">Authors: Valentin Reyes Häusler, Gabriel Ott, Aruna Jayasena, Andreas Peter</div>
<div class="meta-line">First: 2025-10-27T16:16:40+00:00 · Latest: 2025-12-26T17:47:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23483v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.23483v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fully homomorphic encryption allows the evaluation of arbitrary functions on encrypted data. It can be leveraged to secure outsourced and multiparty computation. TFHE is a fast torus-based fully homomorphic encryption scheme that allows both linear operations, as well as the evaluation of arbitrary non-linear functions. It currently provides the fastest bootstrapping operation performance of any other FHE scheme. Despite its fast performance, TFHE suffers from a considerably higher computational overhead for the evaluation of homomorphic circuits. Computations in the encrypted domain are orders of magnitude slower than their unencrypted equivalents. This bottleneck hinders the widespread adoption of (T)FHE for the protection of sensitive data. While state-of-the-art implementations focused on accelerating and outsourcing single operations, their scalability and practicality are constrained by high memory bandwidth costs. In order to overcome this, we propose an FPGA-based hardware accelerator for the evaluation of homomorphic circuits. Specifically, we design a functionally complete TFHE processor for FPGA hardware capable of processing instructions on the data completely on the FPGA. In order to achieve a higher throughput from our TFHE processor, we implement an improved programmable bootstrapping module, which outperforms the current state-of-the-art by 240% to 480% more bootstrappings per second. Our efficient, compact, and scalable design lays the foundation for implementing complete FPGA-based TFHE processor architectures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向功能完备且可参数化的TFHE处理器</div>
<div class="mono" style="margin-top:8px">全同态加密允许在加密数据上评估任意函数。它可以用于保护外包计算和多方计算。TFHE是一种基于环的快速全同态加密方案，支持线性操作以及任意非线性函数的评估。目前，它提供了比其他任何FHE方案更快的启动操作性能。尽管性能较快，但TFHE在评估同态电路时仍面临显著更高的计算开销。在加密域中的计算速度比其未加密版本慢几个数量级。这一瓶颈阻碍了(T)FHE在保护敏感数据方面的广泛应用。虽然当前最先进的实现专注于加速和外包单个操作，但它们的可扩展性和实用性受到高内存带宽成本的限制。为了解决这一问题，我们提出了一种基于FPGA的硬件加速器，用于同态电路的评估。具体而言，我们设计了一种功能完备的TFHE处理器，能够在FPGA硬件上完全处理数据指令。为了提高我们TFHE处理器的吞吐量，我们实现了一个改进的可编程启动模块，其每秒启动次数比当前最先进的方案高出240%至480%。我们的高效、紧凑且可扩展的设计为实现完整的基于FPGA的TFHE处理器架构奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Fully homomorphic encryption allows the evaluation of arbitrary functions on encrypted data.</div>
</details>
</div>
<div class="card">
<div class="title">A Pairwise Comparison Relation-assisted Multi-objective Evolutionary Neural Architecture Search Method with Multi-population Mechanism</div>
<div class="meta-line">Authors: Yu Xue, Pengcheng Jiang, Chenchen Zhu, MengChu Zhou, Mohamed Wahib, Moncef Gabbouj</div>
<div class="meta-line">First: 2024-07-22T12:46:22+00:00 · Latest: 2025-12-26T17:44:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.15600v3">Abs</a> · <a href="https://arxiv.org/pdf/2407.15600v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural architecture search (NAS) has emerged as a powerful paradigm that enables researchers to automatically explore vast search spaces and discover efficient neural networks. However, NAS suffers from a critical bottleneck, i.e. the evaluation of numerous architectures during the search process demands substantial computing resources and time. In order to improve the efficiency of NAS, a series of methods have been proposed to reduce the evaluation time of neural architectures. However, they are not efficient enough and still only focus on the accuracy of architectures. Beyond classification accuracy, real-world applications increasingly demand more efficient and compact network architectures that balance multiple performance criteria. To address these challenges, we propose the SMEMNAS, a pairwise comparison relation-assisted multi-objective evolutionary algorithm based on a multi-population mechanism. In the SMEMNAS, a surrogate model is constructed based on pairwise comparison relations to predict the accuracy ranking of architectures, rather than the absolute accuracy. Moreover, two populations cooperate with each other in the search process, i.e. a main population that guides the evolutionary process, while a vice population that enhances search diversity. Our method aims to discover high-performance models that simultaneously optimize multiple objectives. We conduct comprehensive experiments on CIFAR-10, CIFAR-100 and ImageNet datasets to validate the effectiveness of our approach. With only a single GPU searching for 0.17 days, competitive architectures can be found by SMEMNAS which achieves 78.91% accuracy with the MAdds of 570M on the ImageNet. This work makes a significant advancement in the field of NAS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于多种群机制的配对比较关系辅助多目标进化神经网络架构搜索方法</div>
<div class="mono" style="margin-top:8px">神经网络架构搜索（NAS）作为一种强大的范式，使研究人员能够自动探索庞大的搜索空间并发现高效的神经网络。然而，NAS在搜索过程中评估大量架构存在关键瓶颈，即需要大量的计算资源和时间。为提高NAS效率，已有多种方法被提出以减少神经网络架构的评估时间。然而，这些方法效率仍不足，且主要关注架构的准确率。在现实应用中，除了分类准确率外，越来越多的应用需求更高效、更紧凑的网络架构，以平衡多个性能指标。为解决这些问题，我们提出SMEMNAS，一种基于多种群机制的配对比较关系辅助多目标进化算法。在SMEMNAS中，通过构建基于配对比较关系的代理模型来预测架构的准确率排名，而非绝对准确率。此外，两个种群在搜索过程中相互协作：一个主种群引导进化过程，一个副种群增强搜索多样性。我们的方法旨在同时优化多个目标以发现高性能模型。我们在CIFAR-10、CIFAR-100和ImageNet数据集上进行了全面实验，验证了我们方法的有效性。仅使用单个GPU搜索0.17天，SMEMNAS便能找到具有竞争力的架构，在ImageNet上实现570M MAdds和78.91%的准确率。本工作在NAS领域取得了显著进展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Neural architecture search (NAS) has emerged as a powerful paradigm that enables researchers to automatically explore vast search spaces and discover efficient neural networks.</div>
</details>
</div>
<div class="card">
<div class="title">Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity</div>
<div class="meta-line">Authors: Cong Qi, Hanzhang Fang, Tianxing Hu, Siqi Jiang, Wei Zhi</div>
<div class="meta-line">First: 2025-04-22T20:34:47+00:00 · Latest: 2025-12-26T17:42:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.16956v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.16956v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of cellular heterogeneity, but its complexity, which is marked by high dimensionality, sparsity, and batch effects, which poses major computational challenges. Transformer-based models have made significant advances in this domain but are often limited by their quadratic complexity and suboptimal handling of long-range dependencies. In this work, we introduce GeneMamba, a scalable and efficient foundation model for single-cell transcriptomics built on state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba captures bidirectional gene context with linear-time complexity, offering substantial computational gains over transformer baselines. The model is pretrained on nearly 30 million cells and incorporates biologically informed objectives, including pathway-aware contrastive loss and rank-based gene encoding. We evaluate GeneMamba across diverse tasks, including multi-batch integration, cell type annotation, and gene-gene correlation, demonstrating strong performance, interpretability, and robustness. These results position GeneMamba as a practical and powerful alternative to transformer-based methods, advancing the development of biologically grounded, scalable tools for large-scale single-cell data analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于单细胞数据的双向Mamba模型：高效上下文学习与生物保真性</div>
<div class="mono" style="margin-top:8px">单细胞RNA测序（scRNA-seq）能够实现对细胞异质性的高分辨率分析，但其复杂性由高维性、稀疏性和批次效应等特征所决定，带来了重大的计算挑战。基于Transformer的模型在该领域取得了显著进展，但往往受限于其二次复杂度和对长程依赖关系处理的不理想性。在本工作中，我们引入了GeneMamba，这是一个基于状态空间建模的可扩展且高效的单细胞转录组学基础模型。通过利用Bi-Mamba架构，GeneMamba以线性时间复杂度捕捉双向基因上下文，相较于Transformer基线模型提供了显著的计算优势。该模型在近3000万个细胞上进行预训练，并结合了生物启发的目标，包括路径感知对比损失和基于排名的基因编码。我们在多种任务上评估了GeneMamba，包括多批次整合、细胞类型注释和基因-基因相关性分析，展示了其强大的性能、可解释性和鲁棒性。这些结果使GeneMamba成为基于Transformer方法的实用且有力的替代方案，推动了生物基础、可扩展工具在大规模单细胞数据分析中的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of cellular heterogeneity, but its complexity, which is marked by high dimensionality, sparsity, and batch effects, which poses major computational challenges.</div>
</details>
</div>
<div class="card">
<div class="title">Prenormal categories</div>
<div class="meta-line">Authors: Sandra Mantovani, Mariano Messora</div>
<div class="meta-line">Venue: Appl. Categor. Struct. 34, 5 (2026)</div>
<div class="meta-line">First: 2025-07-04T10:28:41+00:00 · Latest: 2025-12-26T17:33:06+00:00</div>
<div class="meta-line">Comments: 32 pages. Revised and accepted for publication in Applied Categorical Structures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.03459v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.03459v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper we introduce the notion of (pointed) prenormal category, modelled after regular categories, but with the key notions of coequaliser and kernel pair replaced by those of cokernel and kernel. This framework provides a natural setting for extending certain classical results in algebra. We study the fundamental properties of prenormal categories, including a characterisation in terms of a factorisation system involving normal epimorphisms, and a categorical version of Noether&#x27;s so-called `third isomorphism theorem&#x27;. We also present a range of examples, with the category of commutative monoids constituting a central one. In the second part of the paper we extend prenormality and its related properties to the non-pointed context, using kernels and cokernels defined relative to a distinguished class of trivial objects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>预正规范畴</div>
<div class="mono" style="margin-top:8px">本文引入了（带零对象的）预正规范畴的概念，该概念借鉴了正规范畴的结构，但将核心概念共等化子和核对替换为余核和核。这一框架为扩展代数中某些经典结果提供了自然的环境。我们研究了预正规范畴的基本性质，包括一个涉及正规满射的分解系统特征刻画，以及范畴论中诺特所谓的`第三同构定理`的版本。我们还提供了一系列例子，其中交换幺半群范畴是核心的一个例子。在论文的第二部分，我们将预正规性及其相关性质扩展到非带零对象的上下文中，使用相对于一个区分的平凡对象类定义的核和余核。</div>
</details>
</div>
<div class="card">
<div class="title">Contextually Private Mechanisms</div>
<div class="meta-line">Authors: Andreas Haupt, Zoë Hitzig</div>
<div class="meta-line">First: 2021-12-20T19:16:32+00:00 · Latest: 2025-12-26T17:33:04+00:00</div>
<div class="meta-line">Comments: 52 pages, 8 figures, 1 table. Appeared in the proceedings of the Twenty-Third ACM Conference on Economics and Computation, 2022</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2112.10812v8">Abs</a> · <a href="https://arxiv.org/pdf/2112.10812v8">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce a framework for comparing the privacy of different mechanisms. A mechanism designer employs a dynamic protocol to elicit agents&#x27; private information. Protocols produce a set of contextual privacy violations -- information learned about agents that may be superfluous given the context. A protocol is \emph{maximally contextually private} if there is no protocol that produces a proper subset of the violations it produces, while still implementing the choice rule. Contextual privacy violations arise when a choice rule makes some agents collectively, but not individually, pivotal. In auctions, designing for contextual privacy requires choosing an initial question posed to each agent and the order in which agents are queried. We study a particular maximally contextually private protocol for $k$-item Vickrey auctions -- the ascending-join protocol -- and show that it achieves maximal contextual privacy by delaying queries to bidders whose privacy it protects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>上下文隐私机制</div>
<div class="mono" style="margin-top:8px">我们引入了一个用于比较不同机制隐私性的框架。机制设计者采用动态协议来获取代理的私人信息。协议会产生一组上下文隐私违规信息——这些信息在给定上下文中可能是多余的。如果一个协议产生的隐私违规信息集合不能被任何其他实现相同选择规则的协议产生一个真子集，则该协议被称为\emph{最大上下文隐私协议}。当选择规则使某些代理集体但非个体关键时，就会产生上下文隐私违规。在拍卖中，设计上下文隐私需要选择对每个代理提出的问题以及查询代理的顺序。我们研究了用于$k$-物品维克瑞拍卖的一种特定最大上下文隐私协议——上升合并协议，并证明通过延迟对受其保护隐私的竞标者进行查询，该协议实现了最大上下文隐私。</div>
</details>
</div>
<div class="card">
<div class="title">Modeling Microenvironment Trajectories on Spatial Transcriptomics with NicheFlow</div>
<div class="meta-line">Authors: Kristiyan Sakalyan, Alessandro Palma, Filippo Guerranti, Fabian J. Theis, Stephan Günnemann</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-11-02T15:41:38+00:00 · Latest: 2025-12-26T17:29:14+00:00</div>
<div class="meta-line">Comments: 37 pages, 15 figures, to appear in NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00977v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.00977v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding the evolution of cellular microenvironments in spatiotemporal data is essential for deciphering tissue development and disease progression. While experimental techniques like spatial transcriptomics now enable high-resolution mapping of tissue organization across space and time, current methods that model cellular evolution operate at the single-cell level, overlooking the coordinated development of cellular states in a tissue. We introduce NicheFlow, a flow-based generative model that infers the temporal trajectory of cellular microenvironments across sequential spatial slides. By representing local cell neighborhoods as point clouds, NicheFlow jointly models the evolution of cell states and spatial coordinates using optimal transport and Variational Flow Matching. Our approach successfully recovers both global spatial architecture and local microenvironment composition across diverse spatiotemporal datasets, from embryonic to brain development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用NicheFlow对空间转录组学中的微环境轨迹进行建模</div>
<div class="mono" style="margin-top:8px">理解时空数据中细胞微环境的演变对于解析组织发育和疾病进展至关重要。尽管空间转录组学等实验技术现在能够实现对组织在空间和时间上的高分辨率映射，但当前建模细胞演化的现有方法主要在单细胞层面进行，忽略了组织中细胞状态的协调发育。我们引入了NicheFlow，这是一种基于流的生成模型，能够推断细胞微环境在连续空间切片中的时间轨迹。通过将局部细胞邻域表示为点云，NicheFlow利用最优传输和变分流匹配联合建模细胞状态和空间坐标的演变。我们的方法成功地在多种时空数据集（从胚胎到大脑发育）中恢复了全局空间结构和局部微环境组成。</div>
</details>
</div>
<div class="card">
<div class="title">Unifying Learning Dynamics and Generalization in Transformers Scaling Law</div>
<div class="meta-line">Authors: Chiwun Yang</div>
<div class="meta-line">First: 2025-12-26T17:20:09+00:00 · Latest: 2025-12-26T17:20:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22088v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22088v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The scaling law, a cornerstone of Large Language Model (LLM) development, predicts improvements in model performance with increasing computational resources. Yet, while empirically validated, its theoretical underpinnings remain poorly understood. This work formalizes the learning dynamics of transformer-based language models as an ordinary differential equation (ODE) system, then approximates this process to kernel behaviors. Departing from prior toy-model analyses, we rigorously analyze stochastic gradient descent (SGD) training for multi-layer transformers on sequence-to-sequence data with arbitrary data distribution, closely mirroring real-world conditions. Our analysis characterizes the convergence of generalization error to the irreducible risk as computational resources scale with data, especially during the optimization process.
  We establish a theoretical upper bound on excess risk characterized by a distinct phase transition. In the initial optimization phase, the excess risk decays exponentially relative to the computational cost ${\sf C}$. However, once a specific resource allocation threshold is crossed, the system enters a statistical phase, where the generalization error follows a power-law decay of $Θ(\mathsf{C}^{-1/6})$. Beyond this unified framework, our theory derives isolated scaling laws for model size, training time, and dataset size, elucidating how each variable independently governs the upper bounds of generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将Transformer的扩展定律中的学习动态与泛化统一</div>
<div class="mono" style="margin-top:8px">扩展定律是大型语言模型（LLM）开发的核心，它预测随着计算资源的增加，模型性能会得到提升。然而，尽管在经验上得到了验证，其理论基础仍不明确。本文将基于Transformer的语言模型的学习动态形式化为一个常微分方程（ODE）系统，然后将其近似为核行为。与以往的玩具模型分析不同，我们严格分析了在任意数据分布的序列到序列数据上，多层Transformer模型通过随机梯度下降（SGD）进行训练的过程，这一过程与现实条件高度相似。我们的分析描述了随着计算资源与数据规模的增加，泛化误差收敛到不可约风险的过程。我们建立了一个以独特相变特征为标志的过剩风险理论上界。在初始优化阶段，过剩风险相对于计算成本${\sf C}$呈指数衰减。然而，一旦达到特定的资源分配阈值，系统进入统计相，泛化误差遵循$Θ(\mathsf{C}^{-1/6})$的幂律衰减。在此统一框架之外，我们的理论还推导出针对模型规模、训练时间和数据集规模的独立扩展定律，阐明了每个变量如何独立地控制泛化的上界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The scaling law, a cornerstone of Large Language Model (LLM) development, predicts improvements in model performance with increasing computational resources.</div>
</details>
</div>
<div class="card">
<div class="title">Context as a Tool: Context Management for Long-Horizon SWE-Agents</div>
<div class="meta-line">Authors: Shukai Liu, Jian Yang, Bo Jiang, Yizhi Li, Jinyang Guo, Xianglong Liu, Bryan Dai</div>
<div class="meta-line">First: 2025-12-26T17:15:47+00:00 · Latest: 2025-12-26T17:15:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22087v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22087v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agents based on large language models have recently shown strong potential on real-world software engineering (SWE) tasks that require long-horizon interaction with repository-scale codebases. However, most existing agents rely on append-only context maintenance or passively triggered compression heuristics, which often lead to context explosion, semantic drift, and degraded reasoning in long-running interactions. We propose CAT, a new context management paradigm that elevates context maintenance to a callable tool integrated into the decision-making process of agents. CAT formalizes a structured context workspace consisting of stable task semantics, condensed long-term memory, and high-fidelity short-term interactions, and enables agents to proactively compress historical trajectories into actionable summaries at appropriate milestones. To support context management for SWE-agents, we propose a trajectory-level supervision framework, CAT-GENERATOR, based on an offline data construction pipeline that injects context-management actions into complete interaction trajectories. Using this framework, we train a context-aware model, SWE-Compressor. Experiments on SWE-Bench-Verified demonstrate that SWE-Compressor reaches a 57.6% solved rate and significantly outperforms ReAct-based agents and static compression baselines, while maintaining stable and scalable long-horizon reasoning under a bounded context budget.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>上下文作为工具：面向长交互SWE代理的上下文管理</div>
<div class="mono" style="margin-top:8px">基于大语言模型的代理最近在需要与仓库级代码库进行长交互的现实软件工程（SWE）任务中展现出强大的潜力。然而，大多数现有代理依赖于只追加的上下文维护或被动触发的压缩启发式方法，这常常导致上下文爆炸、语义漂移和长交互中的推理退化。我们提出CAT，一种新的上下文管理范式，将上下文维护提升为一个可调用的工具，集成到代理的决策过程中。CAT形式化了一个结构化的上下文工作空间，包括稳定的任务语义、压缩的长期记忆和高保真的短期交互，并使代理能够在适当的关键节点上主动将历史轨迹压缩为可操作的摘要。为支持SWE代理的上下文管理，我们提出了一种轨迹级监督框架CAT-GENERATOR，该框架基于一个离线数据构建流水线，将上下文管理动作注入完整的交互轨迹中。利用该框架，我们训练了一个上下文感知模型SWE-Compressor。在SWE-Bench-Verified上的实验表明，SWE-Compressor达到了57.6%的解决率，显著优于基于ReAct的代理和静态压缩基线，同时在有限的上下文预算下保持了稳定且可扩展的长交互推理能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Agents based on large language models have recently shown strong potential on real-world software engineering (SWE) tasks that require long-horizon interaction with repository-scale codebases.</div>
</details>
</div>
<div class="card">
<div class="title">Recursive Training Loops in LLMs: How training data properties modulate distribution shift in generated data?</div>
<div class="meta-line">Authors: Grgur Kovač, Jérémy Perez, Rémy Portelas, Peter Ford Dominey, Pierre-Yves Oudeyer</div>
<div class="meta-line">Venue: EMNLP 2025 Oral</div>
<div class="meta-line">First: 2025-04-04T14:41:41+00:00 · Latest: 2025-12-26T17:12:34+00:00</div>
<div class="meta-line">Comments: Accepted to EMNLP 2025 (Oral), Source Code: https://github.com/flowersteam/ce_llms</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.03814v6">Abs</a> · <a href="https://arxiv.org/pdf/2504.03814v6">PDF</a> · <a href="https://github.com/flowersteam/ce_llms">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly used in the creation of online content, creating feedback loops as subsequent generations of models will be trained on this synthetic data. Such loops were shown to lead to distribution shifts - models misrepresenting the true underlying distributions of human data (also called model collapse). However, how human data properties affect such shifts remains poorly understood. In this paper, we provide the first empirical examination of the effect of such properties on the outcome of recursive training. We first confirm that using different human datasets leads to distribution shifts of different magnitudes. Through exhaustive manipulation of dataset properties combined with regression analyses, we then identify a set of properties predicting distribution shift magnitudes. Lexical diversity is found to amplify these shifts, while semantic diversity and data quality mitigate them. Furthermore, we find that these influences are highly modular: data scrapped from a given internet domain has little influence on the content generated for another domain. Finally, experiments on political bias reveal that human data properties affect whether the initial bias will be amplified or reduced. Overall, our results portray a novel view, where different parts of internet may undergo different types of distribution shift.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型中的递归训练循环：训练数据属性如何调节生成数据的分布偏移？</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）越来越多地用于在线内容的生成，从而形成反馈循环，后续模型将基于这些合成数据进行训练。研究表明，这种循环会导致分布偏移——模型对人类数据的真实分布产生误表（也称为模型崩溃）。然而，人类数据属性如何影响这种偏移仍不明确。本文首次实证研究了这些属性对递归训练结果的影响。我们首先确认，使用不同的数据集会导致不同程度的分布偏移。通过全面操控数据集属性并结合回归分析，我们识别出一组预测分布偏移幅度的属性。词汇多样性会加剧这些偏移，而语义多样性和数据质量则有助于缓解。此外，我们发现这些影响具有高度模块化的特点：从特定互联网领域抓取的数据对其他领域生成的内容影响较小。最后，我们在政治偏见方面的实验表明，人类数据属性会影响初始偏见是被放大还是被减弱。总体而言，我们的研究呈现了一种新颖的观点，即互联网的不同部分可能会经历不同类型的分布偏移。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) are increasingly used in the creation of online content, creating feedback loops as subsequent generations of models will be trained on this synthetic data.</div>
</details>
</div>
<div class="card">
<div class="title">A Frobenius-Optimal Projection for Enforcing Linear Conservation in Learned Dynamical Models</div>
<div class="meta-line">Authors: John M. Mango, Ronald Katende</div>
<div class="meta-line">First: 2025-12-26T17:11:16+00:00 · Latest: 2025-12-26T17:11:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22084v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22084v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider the problem of restoring linear conservation laws in data-driven linear dynamical models. Given a learned operator $\widehat{A}$ and a full-rank constraint matrix $C$ encoding one or more invariants, we show that the matrix closest to $\widehat{A}$ in the Frobenius norm and satisfying $C^\top A = 0$ is the orthogonal projection $A^\star = \widehat{A} - C(C^\top C)^{-1}C^\top \widehat{A}$. This correction is uniquely defined, low rank and fully determined by the violation $C^\top \widehat{A}$. In the single-invariant case it reduces to a rank-one update. We prove that $A^\star$ enforces exact conservation while minimally perturbing the dynamics, and we verify these properties numerically on a Markov-type example. The projection provides an elementary and general mechanism for embedding exact invariants into any learned linear model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种用于在学习的动力学模型中强制线性守恒的Frobenius最优投影</div>
<div class="mono" style="margin-top:8px">我们考虑在数据驱动的线性动力学模型中恢复线性守恒定律的问题。给定一个学习到的算子 $\widehat{A}$ 和一个满秩约束矩阵 $C$，该矩阵编码一个或多个不变量，我们证明在Frobenius范数下最接近 $\widehat{A}$ 且满足 $C^\top A = 0$ 的矩阵是正交投影 $A^\star = \widehat{A} - C(C^\top C)^{-1}C^\top \widehat{A}$。这种校正唯一确定，秩低，并完全由违反程度 $C^\top \widehat{A}$ 决定。在单不变量情况下，它简化为一个秩一更新。我们证明 $A^\star$ 能够在最小扰动动力学的情况下强制精确守恒，并在马尔可夫型示例中通过数值验证了这些性质。该投影提供了一种简单且通用的机制，用于将精确不变量嵌入到任何学习到的线性模型中。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We consider the problem of restoring linear conservation laws in data-driven linear dynamical models.</div>
</details>
</div>
<div class="card">
<div class="title">Sparse Hyperparametric Itakura-Saito Nonnegative Matrix Factorization via Bi-Level Optimization</div>
<div class="meta-line">Authors: Laura Selicato, Flavia Esposito, Andersen Ang, Nicoletta Del Buono, Rafal Zdunek</div>
<div class="meta-line">First: 2025-02-24T13:05:01+00:00 · Latest: 2025-12-26T17:10:32+00:00</div>
<div class="meta-line">Comments: 23 pages, 7 figures, 8 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.17123v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.17123v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The selection of penalty hyperparameters is a critical aspect in Nonnegative Matrix Factorization (NMF), since these values control the trade-off between reconstruction accuracy and adherence to desired constraints. In this work, we focus on an NMF problem involving the Itakura-Saito (IS) divergence, which is particularly effective for extracting low spectral density components from spectrograms of mixed signals, and benefits from the introduction of sparsity constraints. We propose a new algorithm called SHINBO, which introduces a bi-level optimization framework to automatically and adaptively tune the row-dependent penalty hyperparameters, enhancing the ability of IS-NMF to isolate sparse, periodic signals in noisy environments. Experimental results demonstrate that SHINBO achieves accurate spectral decompositions and demonstrates superior performance in both synthetic and real-world applications. In the latter case, SHINBO is particularly useful for noninvasive vibration-based fault detection in rolling bearings, where the desired signal components often reside in high-frequency subbands but are obscured by stronger, spectrally broader noise. By addressing the critical issue of hyperparameter selection, SHINBO improves the state-of-the-art in signal recovery for complex, noise-dominated environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过双层优化的稀疏超参数Itakura-Saito非负矩阵分解</div>
<div class="mono" style="margin-top:8px">在非负矩阵分解（NMF）中，惩罚超参数的选择是一个关键方面，因为这些值控制着重构精度与满足期望约束之间的权衡。本文聚焦于涉及Itakura-Saito（IS）散度的NMF问题，该方法在从混合信号的频谱图中提取低频谱密度成分方面特别有效，并且受益于稀疏性约束的引入。我们提出了一种新算法SHINBO，通过双层优化框架自动且自适应地调整行相关的惩罚超参数，从而增强IS-NMF在噪声环境中分离稀疏、周期性信号的能力。实验结果表明，SHINBO实现了精确的频谱分解，并在合成数据和实际应用中均表现出优越的性能。在实际应用中，SHINBO特别适用于滚动轴承的非侵入式振动故障检测，其中期望的信号成分通常位于高频子带中，但被更强、频谱更广的噪声所掩盖。通过解决超参数选择这一关键问题，SHINBO提高了复杂、噪声主导环境中的信号恢复技术水平。</div>
</details>
</div>
<div class="card">
<div class="title">Robust Federated Learning in Unreliable Wireless Networks: A Client Selection Approach</div>
<div class="meta-line">Authors: Yanmeng Wang, Wenkai Ji, Jian Zhou, Fu Xiao, Tsung-Hui Chang</div>
<div class="meta-line">First: 2025-02-24T15:44:02+00:00 · Latest: 2025-12-26T17:08:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.17260v4">Abs</a> · <a href="https://arxiv.org/pdf/2502.17260v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated learning (FL) has emerged as a promising distributed learning paradigm for training deep neural networks (DNNs) at the wireless edge, but its performance can be severely hindered by unreliable wireless transmission and inherent data heterogeneity among clients. Existing solutions primarily address these challenges by incorporating wireless resource optimization strategies, often focusing on uplink resource allocation across clients under the assumption of homogeneous client-server network standards. However, these approaches overlooked the fact that mobile clients may connect to the server via diverse network standards (e.g., 4G, 5G, Wi-Fi) with customized configurations, limiting the flexibility of server-side modifications and restricting applicability in real-world commercial networks. This paper presents a novel theoretical analysis about how transmission failures in unreliable networks distort the effective label distributions of local samples, causing deviations from the global data distribution and introducing convergence bias in FL. Our analysis reveals that a carefully designed client selection strategy can mitigate biases induced by network unreliability and data heterogeneity. Motivated by this insight, we propose FedCote, a client selection approach that optimizes client selection probabilities without relying on wireless resource scheduling. Experimental results demonstrate the robustness of FedCote in DNN-based classification tasks under unreliable networks with frequent transmission failures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在不可靠无线网络中的鲁棒联邦学习：一种客户端选择方法</div>
<div class="mono" style="margin-top:8px">联邦学习（FL）作为一种在无线边缘训练深度神经网络（DNNs）的有前景的分布式学习范式，其性能可能受到不可靠无线传输和客户端固有数据异构性的严重阻碍。现有解决方案主要通过引入无线资源优化策略来应对这些挑战，通常关注客户端上行资源分配，并假设客户端与服务器之间的网络标准是同构的。然而，这些方法忽略了移动客户端可能通过多种网络标准（如4G、5G、Wi-Fi）连接服务器，并具有定制化配置的情况，这限制了服务器端修改的灵活性，并限制了其在现实商业网络中的适用性。本文提出了一种新颖的理论分析，探讨了不可靠网络中的传输失败如何扭曲本地样本的有效标签分布，导致与全局数据分布的偏差，并在联邦学习中引入收敛偏差。我们的分析表明，精心设计的客户端选择策略可以缓解由网络不可靠性和数据异构性引起的偏差。基于这一见解，我们提出FedCote，一种不依赖无线资源调度的客户端选择方法，通过优化客户端选择概率来提升性能。实验结果表明，FedCote在频繁传输失败的不可靠网络中，对于基于DNN的分类任务具有良好的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Laser: Governing Long-Horizon Agentic Search via Structured Protocol and Context Register</div>
<div class="meta-line">Authors: Shuting Wang, Qiaolin Xia, Vich Wang, Herberttli, Bobsimons, Zhicheng Dou</div>
<div class="meta-line">First: 2025-12-23T15:53:33+00:00 · Latest: 2025-12-26T17:05:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20458v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.20458v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in Large Language Models (LLMs) and Large Reasoning Models (LRMs) have enabled agentic search systems that interleave multi-step reasoning with external tool use. However, existing frameworks largely rely on unstructured natural-language reasoning and accumulate raw intermediate traces in the context, which often leads to unstable reasoning trajectories, context overflow, and degraded performance on complex multi-hop queries. In this study, we introduce Laser, a general framework for stabilizing and scaling agentic search. Laser defines a symbolic action protocol that organizes agent behaviors into three spaces: planning, task-solving, and retrospection. Each action is specified with explicit semantics and a deterministic execution format, enabling structured and logical reasoning processes and reliable action parsing. This design makes intermediate decisions interpretable and traceable, enhancing explicit retrospection and fine-grained control over reasoning trajectories. In coordination with parsable actions, Laser further maintains a compact context register that stores only essential states of the reasoning process, allowing the agent to reason over long horizons without uncontrolled context expansion. Experiments on Qwen2.5/3-series models across challenging multi-hop QA datasets show that Laser consistently outperforms existing agentic search baselines under both prompting-only and fine-tuning settings, demonstrating that Laser provides a principled and effective foundation for robust, scalable agentic search.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>激光：通过结构化协议和上下文寄存器治理长时域代理搜索</div>
<div class="mono" style="margin-top:8px">近年来，大语言模型（LLMs）和大推理模型（LRMs）的进展使得能够将多步推理与外部工具使用相结合的代理搜索系统成为可能。然而，现有框架主要依赖于非结构化的自然语言推理，并在上下文中累积原始中间轨迹，这常常导致推理轨迹不稳定、上下文溢出以及在复杂多跳查询上的性能下降。在本研究中，我们引入了Laser，这是一个用于稳定和扩展代理搜索的通用框架。Laser定义了一个符号化动作协议，将代理行为组织为三个空间：规划、任务解决和回顾。每个动作都具有明确的语义和确定性的执行格式，从而实现结构化和逻辑化的推理过程以及可靠的动作解析。这种设计使中间决策可解释且可追溯，增强了显式的回顾能力并实现了对推理轨迹的细粒度控制。与可解析动作协同工作，Laser还维护了一个紧凑的上下文寄存器，仅存储推理过程中的关键状态，使代理能够在长时域内进行推理而不会出现不受控的上下文扩展。在具有挑战性的多跳问答数据集上对Qwen2.5/3系列模型的实验表明，Laser在仅提示和微调两种设置下均优于现有的代理搜索基线，证明了Laser为稳健、可扩展的代理搜索提供了原理性和有效性的基础。</div>
</details>
</div>
<div class="card">
<div class="title">Agent-based simulation of online social networks and disinformation</div>
<div class="meta-line">Authors: Alejandro Buitrago López, Alberto Ortega Pastor, David Montoro Aguilera, Mario Fernández Tárraga, Jesús Verdú Chacón, Javier Pastor-Galindo, José A. Ruipérez-Valiente</div>
<div class="meta-line">First: 2025-12-26T16:56:45+00:00 · Latest: 2025-12-26T16:56:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22082v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.22082v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Research on online social networks (OSNs) is often hindered by platform opacity, limited access to data, and ethical constraints. Simulation offer a valuable alternative, but existing frameworks frequently lack realism and explainability. This paper presents a simulation framework that models synthetic social networks with agents endowed with demographic-based personality traits and finite-state behavioral automata, enabling realistic and interpretable actions. A generative module powered by a large language model (LLM) produces context-aware social media posts consistent with each agent&#x27;s profile and memory. In parallel, a red module implements DISARM-inspired workflows to orchestrate disinformation campaigns executed by malicious agents targeting simulated audiences. A Mastodon-based visualization layer supports real-time inspection and post-hoc validation of agent activity within a familiar interface. We evaluate the resulting synthetic social networks using topological metrics and LLM-based content assessments, demonstrating structural, behavioral, and linguistic realism. Overall, the framework enables the creation of customizable and controllable social network environments for studying information dynamics and the effects of disinformation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于代理的在线社交网络与虚假信息模拟</div>
<div class="mono" style="margin-top:8px">在线社交网络（OSNs）的研究常受平台不透明性、数据访问受限和伦理限制的阻碍。模拟提供了一个有价值的替代方案，但现有框架往往缺乏真实性和可解释性。本文提出了一种模拟框架，该框架通过具有基于人口统计学的人格特征和有限状态行为自动机的代理，建模合成社交网络，从而实现真实且可解释的行为。一个由大型语言模型（LLM）驱动的生成模块能够产生与每个代理的档案和记忆一致的情境感知社交媒体帖子。同时，一个红色模块实现了受DISARM启发的工作流程，以协调由恶意代理执行的针对模拟受众的虚假信息宣传活动。基于Mastodon的可视化层支持在熟悉的界面中对代理活动进行实时检查和事后验证。我们使用拓扑度量和基于LLM的内容评估来评估生成的合成社交网络，展示了其结构、行为和语言的真实性。总体而言，该框架能够创建可定制和可控的社交网络环境，用于研究信息动态和虚假信息的影响。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Multi-turn Task Completion in Task-Oriented Dialog Systems via Prompt Chaining and Fine-Grained Feedback</div>
<div class="meta-line">Authors: Moghis Fereidouni, Md Sajid Ahmed, Adib Mosharrof, A. B. Siddique</div>
<div class="meta-line">First: 2025-02-18T21:36:19+00:00 · Latest: 2025-12-26T16:51:41+00:00</div>
<div class="meta-line">Comments: 7 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.13298v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.13298v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Task-oriented dialog (TOD) systems facilitate users in accomplishing complex, multi-turn tasks through natural language. While instruction-tuned large language models (LLMs) have demonstrated strong performance on a range of single-turn NLP tasks, they often struggle with reliable multi-turn task completion in TOD settings, particularly when generating API calls required to interact with external systems. To address this, we introduce RealTOD, a novel framework that improves LLM-based TOD systems through (1) prompt chaining and (2) fine-grained feedback. Prompt chaining enables zero-shot generalization to new domains by automatically synthesizing a schema-aligned in-context example for the target task. Fine-grained feedback verifies each generated API call against the domain schema, identifies specific errors, and provides targeted correction prompts. To evaluate task completion reliability, we introduce full API Call Accuracy as a robust metric, along with detailed sub-metrics to capture common failure modes. We conduct extensive experiments on the SGD and BiTOD benchmarks using four LLMs. RealTOD improves Full API accuracy, surpassing state-of-the-art AutoTOD by 37.10% on SGD and supervised learning-based baseline SimpleTOD by 10.32% on BiTOD. Human evaluations further confirm that LLMs integrated with RealTOD achieve superior task completion, fluency, and informativeness compared to existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过提示链和细粒度反馈提升任务导向对话系统中的多轮任务完成能力</div>
<div class="mono" style="margin-top:8px">任务导向对话（TOD）系统通过自然语言帮助用户完成复杂且多轮的任务。尽管指令调优的大语言模型（LLMs）在多种单轮NLP任务中表现出色，但在TOD场景中，尤其是在生成与外部系统交互所需的API调用时，往往难以可靠地完成多轮任务。为了解决这一问题，我们引入了RealTOD，这是一种新颖的框架，通过（1）提示链和（2）细粒度反馈来提升基于LLM的TOD系统。提示链通过自动合成与目标任务对齐的上下文示例，实现零样本泛化到新领域。细粒度反馈则通过将每个生成的API调用与领域模式进行比对，识别具体错误并提供针对性的修正提示。为了评估任务完成的可靠性，我们引入了全API调用准确率作为稳健的评估指标，并结合详细的子指标来捕捉常见的失败模式。我们在SGD和BiTOD基准上使用四个LLMs进行了大量实验。RealTOD在全API准确率上有所提升，在SGD上超越了最先进的AutoTOD方法37.10%，在BiTOD上超越了基于监督学习的基线SimpleTOD方法10.32%。人工评估进一步确认，集成RealTOD的LLMs在任务完成度、流畅性和信息丰富性方面优于现有方法。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
