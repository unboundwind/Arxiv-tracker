<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-26 03:56</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260126_0356</div>
    <div class="row"><div class="card">
<div class="title">CamPilot: Improving Camera Control in Video Diffusion Model with Efficient Camera Reward Feedback</div>
<div class="meta-line">Authors: Wenhang Ge, Guibao Shen, Jiawei Feng, Luozhou Wang, Hao Lu, Xingye Tian, Xin Tao, Ying-Cong Chen</div>
<div class="meta-line">First: 2026-01-22T18:59:56+00:00 · Latest: 2026-01-22T18:59:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16214v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16214v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://a-bigbao.github.io/CamPilot/}{CamPilot">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in camera-controlled video diffusion models have significantly improved video-camera alignment. However, the camera controllability still remains limited. In this work, we build upon Reward Feedback Learning and aim to further improve camera controllability. However, directly borrowing existing ReFL approaches faces several challenges. First, current reward models lack the capacity to assess video-camera alignment. Second, decoding latent into RGB videos for reward computation introduces substantial computational overhead. Third, 3D geometric information is typically neglected during video decoding. To address these limitations, we introduce an efficient camera-aware 3D decoder that decodes video latent into 3D representations for reward quantization. Specifically, video latent along with the camera pose are decoded into 3D Gaussians. In this process, the camera pose not only acts as input, but also serves as a projection parameter. Misalignment between the video latent and camera pose will cause geometric distortions in the 3D structure, resulting in blurry renderings. Based on this property, we explicitly optimize pixel-level consistency between the rendered novel views and ground-truth ones as reward. To accommodate the stochastic nature, we further introduce a visibility term that selectively supervises only deterministic regions derived via geometric warping. Extensive experiments conducted on RealEstate10K and WorldScore benchmarks demonstrate the effectiveness of our proposed method. Project page: \href{https://a-bigbao.github.io/CamPilot/}{CamPilot Page}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CamPilot: 通过高效的相机奖励反馈提升视频扩散模型的相机控制能力</div>
<div class="mono" style="margin-top:8px">近期在相机控制视频扩散模型方面的进展显著提升了视频与相机的对齐效果。然而，相机控制能力仍然受到限制。在本工作中，我们基于奖励反馈学习，旨在进一步提升相机控制能力。然而，直接采用现有的ReFL方法面临诸多挑战。首先，当前的奖励模型缺乏评估视频与相机对齐能力的能力。其次，将潜在变量解码为RGB视频以计算奖励会引入显著的计算开销。第三，在视频解码过程中通常忽略了3D几何信息。为了解决这些限制，我们引入了一种高效的相机感知3D解码器，将视频潜在变量解码为3D表示以进行奖励量化。具体而言，视频潜在变量与相机姿态被解码为3D高斯分布。在此过程中，相机姿态不仅作为输入，还作为投影参数。视频潜在变量与相机姿态之间的不匹配会导致3D结构的几何失真，从而产生模糊的渲染结果。基于这一特性，我们明确地将渲染新视角与真实视角之间的像素级一致性作为奖励。为了适应其随机性，我们进一步引入了可见性项，仅对通过几何变形得到的确定性区域进行选择性监督。在RealEstate10K和WorldScore基准上的大量实验验证了我们提出方法的有效性。项目页面：\href{https://a-bigbao.github.io/CamPilot/}{CamPilot 页面}。</div>
</details>
</div>
<div class="card">
<div class="title">Point Bridge: 3D Representations for Cross Domain Policy Learning</div>
<div class="meta-line">Authors: Siddhant Haldar, Lars Johannsmeier, Lerrel Pinto, Abhishek Gupta, Dieter Fox, Yashraj Narang, Ajay Mandlekar</div>
<div class="meta-line">First: 2026-01-22T18:59:24+00:00 · Latest: 2026-01-22T18:59:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16212v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16212v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://pointbridge3d.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robot foundation models are beginning to deliver on the promise of generalist robotic agents, yet progress remains constrained by the scarcity of large-scale real-world manipulation datasets. Simulation and synthetic data generation offer a scalable alternative, but their usefulness is limited by the visual domain gap between simulation and reality. In this work, we present Point Bridge, a framework that leverages unified, domain-agnostic point-based representations to unlock synthetic datasets for zero-shot sim-to-real policy transfer, without explicit visual or object-level alignment. Point Bridge combines automated point-based representation extraction via Vision-Language Models (VLMs), transformer-based policy learning, and efficient inference-time pipelines to train capable real-world manipulation agents using only synthetic data. With additional co-training on small sets of real demonstrations, Point Bridge further improves performance, substantially outperforming prior vision-based sim-and-real co-training methods. It achieves up to 44% gains in zero-shot sim-to-real transfer and up to 66% with limited real data across both single-task and multitask settings. Videos of the robot are best viewed at: https://pointbridge3d.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>点桥：用于跨域策略学习的3D表示</div>
<div class="mono" style="margin-top:8px">机器人基础模型已经开始兑现通用机器人代理的承诺，但进展仍受到大规模真实世界操作数据集稀缺的限制。模拟和合成数据生成提供了可扩展的替代方案，但其效用受到模拟与现实之间视觉域差距的限制。在本工作中，我们提出了Point Bridge，这是一个利用统一的、与领域无关的点基表示框架，以实现零样本模拟到现实的策略迁移，而无需显式的视觉或物体级对齐。Point Bridge结合了通过视觉-语言模型（VLMs）自动提取点基表示、基于Transformer的策略学习以及高效的推理时间管道，仅使用合成数据即可训练出具备实际操作能力的机器人代理。通过在少量真实演示数据上进行联合训练，Point Bridge进一步提升了性能，在零样本模拟到现实迁移和有限真实数据的单任务和多任务设置中，分别实现了最高44%和66%的性能提升。机器人视频建议访问：https://pointbridge3d.github.io/</div>
</details>
</div>
<div class="card">
<div class="title">Why Can&#x27;t I Open My Drawer? Mitigating Object-Driven Shortcuts in Zero-Shot Compositional Action Recognition</div>
<div class="meta-line">Authors: Geo Ahn, Inwoong Lee, Taeoh Kim, Minho Shim, Dongyoon Wee, Jinwoo Choi</div>
<div class="meta-line">First: 2026-01-22T18:59:13+00:00 · Latest: 2026-01-22T18:59:13+00:00</div>
<div class="meta-line">Comments: The code is available at https://github.com/KHU-VLL/RCORE</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16211v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16211v1">PDF</a> · <a href="https://github.com/KHU-VLL/RCORE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations. We find that existing Zero-Shot Compositional Action Recognition (ZS-CAR) models fail primarily due to an overlooked failure mode: object-driven verb shortcuts. Through systematic analysis, we show that this behavior arises from two intertwined factors: severe sparsity and skewness of compositional supervision, and the asymmetric learning difficulty between verbs and objects. As training progresses, the existing ZS-CAR model increasingly ignores visual evidence and overfits to co-occurrence statistics. Consequently, the existing model does not gain the benefit of compositional recognition in unseen verb-object compositions. To address this, we propose RCORE, a simple and effective framework that enforces temporally grounded verb learning. RCORE introduces (i) a composition-aware augmentation that diversifies verb-object combinations without corrupting motion cues, and (ii) a temporal order regularization loss that penalizes shortcut behaviors by explicitly modeling temporal structure. Across two benchmarks, Sth-com and our newly constructed EK100-com, RCORE significantly improves unseen composition accuracy, reduces reliance on co-occurrence bias, and achieves consistently positive compositional gaps. Our findings reveal object-driven shortcuts as a critical limiting factor in ZS-CAR and demonstrate that addressing them is essential for robust compositional video understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>为什么我无法打开抽屉？缓解基于物体的动词捷径问题在零样本组合动作识别中的应用</div>
<div class="mono" style="margin-top:8px">我们研究组合视频理解（CVU），其中模型必须识别动词和物体，并将它们组合起来以泛化到未见过的组合。我们发现现有的零样本组合动作识别（ZS-CAR）模型主要失败的原因是被忽视的一种失败模式：基于物体的动词捷径。通过系统分析，我们表明这种行为源于两个交织的因素：组合监督的严重稀疏性和偏差性，以及动词和物体之间学习难度的不对称性。随着训练的进行，现有ZS-CAR模型越来越忽略视觉证据，过度依赖共现统计。因此，现有模型在未见过的动词-物体组合中无法获得组合识别的优势。为了解决这一问题，我们提出了RCORE，一个简单且有效的框架，强制进行基于时间的动词学习。RCORE引入了（i）一种组合感知的增强方法，能够在不破坏运动线索的情况下多样化动词-物体组合，以及（ii）一种时间顺序正则化损失，通过显式建模时间结构来惩罚捷径行为。在两个基准数据集Sth-com和我们新构建的EK100-com上，RCORE显著提升了未见过组合的识别准确率，减少了对共现偏差的依赖，并实现了持续的正组合差距。我们的研究揭示了基于物体的捷径是ZS-CAR中的关键限制因素，并表明解决这一问题对于实现稳健的组合视频理解至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We study Compositional Video Understanding (CVU), where models must recognize verbs and objects and compose them to generalize to unseen combinations.</div>
</details>
</div>
<div class="card">
<div class="title">PyraTok: Language-Aligned Pyramidal Tokenizer for Video Understanding and Generation</div>
<div class="meta-line">Authors: Onkar Susladkar, Tushar Prakash, Adheesh Juvekar, Kiet A. Nguyen, Dong-Hwan Jang, Inderjit S Dhillon, Ismini Lourentzou</div>
<div class="meta-line">First: 2026-01-22T18:58:55+00:00 · Latest: 2026-01-22T18:58:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16210v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16210v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Discrete video VAEs underpin modern text-to-video generation and video understanding systems, yet existing tokenizers typically learn visual codebooks at a single scale with limited vocabularies and shallow language supervision, leading to poor cross-modal alignment and zero-shot transfer. We introduce PyraTok, a language-aligned pyramidal tokenizer that learns semantically structured discrete latents across multiple spatiotemporal resolutions. PyraTok builds on a pretrained video VAE and a novel Language aligned Pyramidal Quantization (LaPQ) module that discretizes encoder features at several depths using a shared large binary codebook, yielding compact yet expressive video token sequences. To tightly couple visual tokens with language, PyraTok jointly optimizes multi-scale text-guided quantization and a global autoregressive objective over the token hierarchy. Across ten benchmarks, PyraTok delivers state-of-the-art (SOTA) video reconstruction, consistently improves text-to-video quality, and sets new SOTA zero-shot performance on video segmentation, temporal action localization, and video understanding, scaling robustly to up to 4K/8K resolutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PyraTok: 用于视频理解与生成的语言对齐金字塔分词器</div>
<div class="mono" style="margin-top:8px">离散视频变分自编码器（VAE）是现代文本到视频生成和视频理解系统的基础，但现有的分词器通常在单一尺度上学习视觉码本，词汇量有限且语言监督较浅，导致跨模态对齐效果差和零样本迁移能力弱。我们引入了PyraTok，这是一种语言对齐的金字塔分词器，能够在多个时空分辨率上学习语义结构化的离散潜在表示。PyraTok基于预训练的视频VAE，并引入了一种新颖的Language aligned Pyramidal Quantization（LaPQ）模块，通过共享的大二进制码本在多个深度上离散化编码器特征，从而生成紧凑且具有表现力的视频分词序列。为了紧密耦合视觉分词与语言，PyraTok联合优化多尺度文本引导的量化和在分词层次上的全局自回归目标。在十个基准测试中，PyraTok实现了最先进的视频重建效果，持续提升文本到视频生成质量，并在视频分割、时序动作定位和视频理解任务中实现了新的零样本性能最优，可稳健扩展至高达4K/8K分辨率。</div>
</details>
</div>
<div class="card">
<div class="title">GutenOCR: A Grounded Vision-Language Front-End for Documents</div>
<div class="meta-line">Authors: Hunter Heidenreich, Ben Elliott, Olivia Dinica, Yosheb Getachew</div>
<div class="meta-line">First: 2026-01-20T21:26:15+00:00 · Latest: 2026-01-22T18:58:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14490v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.14490v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">GutenOCR is a family of grounded OCR front-ends obtained by fine-tuning Qwen2.5-VL-3B and Qwen2.5-VL-7B. The resulting single-checkpoint vision-language models expose reading, detection, and grounding through a unified, prompt-based interface. Trained on business documents, scientific articles, and synthetic grounding data, the models support full-page and localized reading with line- and paragraph-level bounding boxes and conditional ``where is x?&#x27;&#x27; queries. We introduce a grounded OCR evaluation protocol and show that GutenOCR-7B more than doubles the composite grounded OCR score of its Qwen2.5-VL-7B backbone on 10.5K held-out business and scientific pages (0.40 to 0.82). On Fox and OmniDocBench v1.5, our approach substantially improves region- and line-level OCR as well as text-detection recall, but reveals trade-offs in page-level linearization, color-guided OCR, and formula-heavy layouts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GutenOCR：一种用于文档的基于定位的视觉-语言前端</div>
<div class="mono" style="margin-top:8px">GutenOCR 是通过微调 Qwen2.5-VL-3B 和 Qwen2.5-VL-7B 获得的一系列基于定位的 OCR 前端模型。这些单检查点的视觉-语言模型通过统一的提示接口实现阅读、检测和定位功能。模型在商业文档、科学文章和合成定位数据上进行训练，支持全页和局部阅读，并提供基于行和段落的边界框以及条件性的『x 在哪里？』查询。我们引入了一种基于定位的 OCR 评估协议，并展示在 10.5K 个保留的商业和科学页面上，GutenOCR-7B 的综合基于定位的 OCR 得分比其 Qwen2.5-VL-7B 基础模型高出一倍多（0.40 到 0.82）。在 Fox 和 OmniDocBench v1.5 上，我们的方法显著提升了区域和行级 OCR 以及文本检测的召回率，但也暴露出页面级线性化、颜色引导 OCR 和公式密集布局方面的权衡。</div>
</details>
</div>
<div class="card">
<div class="title">CropCraft: Complete Structural Characterization of Crop Plants From Images</div>
<div class="meta-line">Authors: Albert J. Zhai, Xinlei Wang, Kaiyuan Li, Zhao Jiang, Junxiong Zhou, Sheng Wang, Zhenong Jin, Kaiyu Guan, Shenlong Wang</div>
<div class="meta-line">First: 2024-11-14T18:58:02+00:00 · Latest: 2026-01-22T18:58:18+00:00</div>
<div class="meta-line">Comments: 3DV 2026 (Oral). Project page: https://ajzhai.github.io/CropCraft</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.09693v2">Abs</a> · <a href="https://arxiv.org/pdf/2411.09693v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ajzhai.github.io/CropCraft">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to automatically build 3D digital twins of plants from images has countless applications in agriculture, environmental science, robotics, and other fields. However, current 3D reconstruction methods fail to recover complete shapes of plants due to heavy occlusion and complex geometries. In this work, we present a novel method for 3D modeling of agricultural crops based on optimizing a parametric model of plant morphology via inverse procedural modeling. Our method first estimates depth maps by fitting a neural radiance field and then optimizes a specialized loss to estimate morphological parameters that result in consistent depth renderings. The resulting 3D model is complete and biologically plausible. We validate our method on a dataset of real images of agricultural fields, and demonstrate that the reconstructed canopies can be used for a variety of monitoring and simulation applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CropCraft：从图像中完整表征作物植物的结构</div>
<div class="mono" style="margin-top:8px">从图像中自动构建植物3D数字孪生的能力在农业、环境科学、机器人学等领域有无数应用。然而，当前的3D重建方法由于严重的遮挡和复杂的几何结构，无法恢复植物的完整形状。在本工作中，我们提出了一种基于逆过程建模优化植物形态参数化模型的新方法进行农业作物的3D建模。我们的方法首先通过拟合神经辐射场估计深度图，然后优化专门的损失函数以估计形态参数，从而获得一致的深度渲染结果。所得到的3D模型是完整的且具有生物学合理性。我们在农业田间真实图像数据集上验证了该方法，并展示了重建的冠层可用于多种监测和模拟应用。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Text-to-Image Diffusion Transformers with Representation Autoencoders</div>
<div class="meta-line">Authors: Shengbang Tong, Boyang Zheng, Ziteng Wang, Bingda Tang, Nanye Ma, Ellis Brown, Jihan Yang, Rob Fergus, Yann LeCun, Saining Xie</div>
<div class="meta-line">First: 2026-01-22T18:58:16+00:00 · Latest: 2026-01-22T18:58:16+00:00</div>
<div class="meta-line">Comments: website: https://rae-dit.github.io/scale-rae/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16208v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16208v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://rae-dit.github.io/scale-rae/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Representation Autoencoders (RAEs) have shown distinct advantages in diffusion modeling on ImageNet by training in high-dimensional semantic latent spaces. In this work, we investigate whether this framework can scale to large-scale, freeform text-to-image (T2I) generation. We first scale RAE decoders on the frozen representation encoder (SigLIP-2) beyond ImageNet by training on web, synthetic, and text-rendering data, finding that while scale improves general fidelity, targeted data composition is essential for specific domains like text. We then rigorously stress-test the RAE design choices originally proposed for ImageNet. Our analysis reveals that scaling simplifies the framework: while dimension-dependent noise scheduling remains critical, architectural complexities such as wide diffusion heads and noise-augmented decoding offer negligible benefits at scale Building on this simplified framework, we conduct a controlled comparison of RAE against the state-of-the-art FLUX VAE across diffusion transformer scales from 0.5B to 9.8B parameters. RAEs consistently outperform VAEs during pretraining across all model scales. Further, during finetuning on high-quality datasets, VAE-based models catastrophically overfit after 64 epochs, while RAE models remain stable through 256 epochs and achieve consistently better performance. Across all experiments, RAE-based diffusion models demonstrate faster convergence and better generation quality, establishing RAEs as a simpler and stronger foundation than VAEs for large-scale T2I generation. Additionally, because both visual understanding and generation can operate in a shared representation space, the multimodal model can directly reason over generated latents, opening new possibilities for unified models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用表示自编码器扩展文本到图像扩散变换器</div>
<div class="mono" style="margin-top:8px">表示自编码器（RAEs）在ImageNet上通过在高维语义潜在空间中进行训练，显示出在扩散建模中的显著优势。在本工作中，我们研究该框架是否可以扩展到大规模、自由形式的文本到图像（T2I）生成。我们首先在冻结的表示编码器（SigLIP-2）上扩展RAE解码器，通过在网页、合成和文本渲染数据上训练，发现虽然扩展提高了通用保真度，但针对特定领域（如文本）的数据组成是关键。随后，我们严格测试了最初为ImageNet设计的RAE设计选择。我们的分析表明，扩展简化了该框架：尽管维度依赖的噪声调度仍然关键，但像宽扩散头和噪声增强解码等架构复杂性在扩展后提供微乎其微的优势。基于这一简化框架，我们在从0.5B到9.8B参数的扩散变换器规模上进行受控比较，RAEs在所有模型规模的预训练阶段都优于FLUX VAE。此外，在高质量数据集上的微调过程中，基于VAE的模型在64个epoch后出现灾难性过拟合，而RAE模型在256个epoch内保持稳定并实现更优性能。在所有实验中，基于RAE的扩散模型表现出更快的收敛速度和更好的生成质量，确立了RAEs作为大规模T2I生成比VAEs更简单且更强的基础。此外，由于视觉理解和生成都可以在共享表示空间中进行，多模态模型可以直接对生成的潜在表示进行推理，为统一模型开辟了新的可能性。</div>
</details>
</div>
<div class="card">
<div class="title">LLM-in-Sandbox Elicits General Agentic Intelligence</div>
<div class="meta-line">Authors: Daixuan Cheng, Shaohan Huang, Yuxian Gu, Huatong Song, Guoxin Chen, Li Dong, Wayne Xin Zhao, Ji-Rong Wen, Furu Wei</div>
<div class="meta-line">First: 2026-01-22T18:57:09+00:00 · Latest: 2026-01-22T18:57:09+00:00</div>
<div class="meta-line">Comments: Project Page: https://llm-in-sandbox.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16206v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16206v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://llm-in-sandbox.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce LLM-in-Sandbox, enabling LLMs to explore within a code sandbox (i.e., a virtual computer), to elicit general intelligence in non-code domains. We first demonstrate that strong LLMs, without additional training, exhibit generalization capabilities to leverage the code sandbox for non-code tasks. For example, LLMs spontaneously access external resources to acquire new knowledge, leverage the file system to handle long contexts, and execute scripts to satisfy formatting requirements. We further show that these agentic capabilities can be enhanced through LLM-in-Sandbox Reinforcement Learning (LLM-in-Sandbox-RL), which uses only non-agentic data to train models for sandbox exploration. Experiments demonstrate that LLM-in-Sandbox, in both training-free and post-trained settings, achieves robust generalization spanning mathematics, physics, chemistry, biomedicine, long-context understanding, and instruction following. Finally, we analyze LLM-in-Sandbox&#x27;s efficiency from computational and system perspectives, and open-source it as a Python package to facilitate real-world deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>沙盒中的LLM激发通用代理智能</div>
<div class="mono" style="margin-top:8px">我们引入LLM-in-Sandbox，使大型语言模型能够在代码沙盒（即虚拟计算机）中探索，以在非代码领域激发通用智能。我们首先证明，强大的LLM无需额外训练即可展现出将代码沙盒用于非代码任务的泛化能力。例如，LLM会自发访问外部资源获取新知识，利用文件系统处理长上下文，并执行脚本来满足格式要求。我们进一步表明，这些代理能力可以通过LLM-in-Sandbox强化学习（LLM-in-Sandbox-RL）进行增强，该方法仅使用非代理数据训练模型以实现沙盒探索。实验表明，无论是无训练还是微调后的LLM-in-Sandbox，都能在数学、物理、化学、生物医学、长上下文理解和指令遵循等多个领域实现稳健的泛化。最后，我们从计算和系统角度分析LLM-in-Sandbox的效率，并将其开源为Python包，以促进实际部署。</div>
</details>
</div>
<div class="card">
<div class="title">Counterfactual Training: Teaching Models Plausible and Actionable Explanations</div>
<div class="meta-line">Authors: Patrick Altmeyer, Aleksander Buszydlik, Arie van Deursen, Cynthia C. S. Liem</div>
<div class="meta-line">First: 2026-01-22T18:56:14+00:00 · Latest: 2026-01-22T18:56:14+00:00</div>
<div class="meta-line">Comments: This work has been accepted for publication at the 2026 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). The final version will be available on IEEE Xplore</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16205v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16205v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models. Counterfactual explanations have emerged as a popular post-hoc explanation method for opaque machine learning models: they inform how factual inputs would need to change in order for a model to produce some desired output. To be useful in real-world decision-making systems, counterfactuals should be plausible with respect to the underlying data and actionable with respect to the feature mutability constraints. Much existing research has therefore focused on developing post-hoc methods to generate counterfactuals that meet these desiderata. In this work, we instead hold models directly accountable for the desired end goal: counterfactual training employs counterfactuals during the training phase to minimize the divergence between learned representations and plausible, actionable explanations. We demonstrate empirically and theoretically that our proposed method facilitates training models that deliver inherently desirable counterfactual explanations and additionally exhibit improved adversarial robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>反事实训练：为模型提供合理且可操作的解释</div>
<div class="mono" style="margin-top:8px">我们提出了一种新的训练范式，称为反事实训练，利用反事实解释来增强模型的解释能力。反事实解释已成为一种流行的后验解释方法，用于解释不透明的机器学习模型：它们说明事实输入需要如何改变，才能使模型产生期望的输出。为了在现实世界决策系统中发挥作用，反事实解释应符合数据分布并满足特征可变性约束。因此，大量现有研究致力于开发后验方法，生成符合这些要求的反事实解释。在本工作中，我们直接让模型对期望的目标负责：反事实训练在训练阶段使用反事实解释，以最小化学习到的表示与合理、可操作的解释之间的差异。我们通过实证和理论分析表明，我们的方法有助于训练出能够提供本质上可取的反事实解释，并且具有增强对抗鲁棒性的模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We propose a novel training regime termed counterfactual training that leverages counterfactual explanations to increase the explanatory capacity of models.</div>
</details>
</div>
<div class="card">
<div class="title">Cyclic sunspot activity during the first millennium CE as reconstructed from radiocarbon</div>
<div class="meta-line">Authors: Ilya Usoskin, Sami K. Solanki, Natalie A. Krivova, Theodosis Chatzistergos</div>
<div class="meta-line">First: 2026-01-22T18:55:13+00:00 · Latest: 2026-01-22T18:55:13+00:00</div>
<div class="meta-line">Comments: Submitted to Astron. Astrophys</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16203v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16203v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Context. Solar activity, dominated by the 11-year cyclic evolution, has been observed directly since 1610. Before that, indirect cosmogenic proxy data are used to reconstruct it over millennia. Recently, the precision of radiocarbon measurements has improved sufficiently to allow reconstructing solar activity over millennia. Aims. The first detailed reconstruction of solar activity, represented by annual sunspot numbers, is presented for 1-969 CE. Methods. The reconstruction of sunspot numbers from D14C was performed using a physics-based method involving several steps: using a carbon-cycle box model, the 14C production rate, corrected for the geomagnetic shielding, was computed from the measured data; The open solar magnetic flux was computed using a model of the heliospheric cosmic-ray modulation; Sunspot numbers were calculated using a model of the evolution of the Sun&#x27;s magnetic field. The Markov Chain Monte Carlo approach was used to account for different sources of uncertainty. Results. Annual sunspot numbers were reconstructed for the first millennium CE. This period includes one extreme solar event of 774 CE and one Grand solar minimum of 650-730 CE. We could identify 91 solar cycles, of which 26 were well-defined, while 24 and 41 were reasonably and poorly defined, respectively. The mean cycle length was 10.6 years, but the lengths of individual cycles vary between 8 and 15 years. The existence of empirical Waldmeier&#x27;s relations remains inconclusive. No significant periodicities were found beyond the 11-year cycle. Conclusions. This work fills the gap in the solar cycle statistics between the previously reconstructed first millennium BCE and the second millennium CE, providing vital constraints for the solar dynamo and irradiance models. A consistent 3-millennium-long reconstruction of sunspot numbers, based on a composite multi-proxy cosmogenic record, is pending.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>公元第一千年太阳黑子活动的重建：基于放射性碳测定</div>
<div class="mono" style="margin-top:8px">背景。自1610年以来，太阳活动主要由11年周期演变主导，可以直接观测。在此之前，使用间接宇宙成因代理数据来重建太阳活动。近年来，放射性碳测定的精度已提高到足以重建千年尺度的太阳活动。目标。首次对公元1-969年太阳活动进行详细重建，以年度太阳黑子数为指标。方法。通过基于物理的方法，利用多个步骤从D14C数据重建太阳黑子数：使用碳循环箱模型，从测量数据计算校正了地磁屏蔽效应的14C产生率；利用日球层宇宙射线调制模型计算开放太阳磁场通量；利用太阳磁场演变模型计算太阳黑子数。采用马尔可夫链蒙特卡洛方法来考虑不同来源的不确定性。结果。重建了公元第一千年的年度太阳黑子数。这一时期包括一次极端太阳事件（774年）和一次大太阳极小期（650-730年）。我们识别出91个太阳周期，其中26个定义明确，24个定义较合理，41个定义较差。平均周期长度为10.6年，但个别周期长度在8到15年之间变化。实证的瓦尔德迈尔关系的存在仍不明确。未发现11年周期以外的显著周期性。结论。这项工作填补了之前重建的公元前第一千年与公元第二千年太阳周期统计之间的空白，为太阳动力学和太阳辐射模型提供了关键约束。基于复合多代理宇宙成因记录的长达3千年的太阳黑子数一致重建工作仍在进行中。</div>
</details>
</div>
<div class="card">
<div class="title">Provable Robustness in Multimodal Large Language Models via Feature Space Smoothing</div>
<div class="meta-line">Authors: Song Xia, Meiwen Ding, Chenqi Kong, Wenhan Yang, Xudong Jiang</div>
<div class="meta-line">First: 2026-01-22T18:52:21+00:00 · Latest: 2026-01-22T18:52:21+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16200v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16200v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) exhibit strong capabilities across diverse applications, yet remain vulnerable to adversarial perturbations that distort their feature representations and induce erroneous predictions. To address this vulnerability, we propose the Feature-space Smoothing (FS) and theoretically prove that FS offers certified robustness on the feature representations of MLLMs. Specifically, FS transforms any feature encoder into a smoothed variant that is guaranteed to maintain a certified lower bound on the feature cosine similarity between clean and adversarial representations under $\ell_2$-bounded attacks. Moreover, we indicate that the value of this Feature Cosine Similarity Bound (FCSB) derived from FS can be improved by enlarging the defined Gaussian robustness score on the vanilla encoder. Building upon this, we introduce the Purifier and Smoothness Mapper (PSM), a plug-and-play module that improves the Gaussian robustness score of MLLMs and thus enhances their certified robustness under FS, without requiring any retraining on MLLMs. We demonstrate that the FS with PSM not only provides a strong theoretical robustness guarantee but also exhibits superior empirical performance compared to adversarial training. Extensive experiments across diverse MLLMs and downstream tasks indicate the effectiveness of the FS-PSM, reducing the Attack Success Rate (ASR) of various white-box attacks from nearly 90\% to about 1\%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过特征空间平滑实现多模态大语言模型的可证明鲁棒性</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）在各种应用中展现出强大的能力，但它们仍然容易受到对抗性扰动的影响，这些扰动会破坏其特征表示并导致错误预测。为了解决这一脆弱性，我们提出了特征空间平滑（FS）方法，并在理论上证明了FS能够在$\ell_2$-受限攻击下为MLLMs的特征表示提供可认证的鲁棒性。具体而言，FS将任何特征编码器转换为一个平滑版本，该版本保证在干净和对抗性表示之间保持特征余弦相似度的可认证下界。此外，我们指出，通过在原始编码器上定义更大的高斯鲁棒性得分，可以提升由FS得出的特征余弦相似度下界（FCSB）的值。在此基础上，我们引入了Purifier和Smoothness Mapper（PSM），这是一个即插即用模块，能够提升MLLMs的高斯鲁棒性得分，从而增强其在FS下的可认证鲁棒性，而无需对MLLMs进行重新训练。我们证明了FS结合PSM不仅提供了强大的理论鲁棒性保证，而且在实验表现上优于对抗训练。在多种MLLMs和下游任务上的广泛实验表明FS-PSM的有效性，将各种白盒攻击的攻击成功率（ASR）从近90\%降低到约1\%。</div>
</details>
</div>
<div class="card">
<div class="title">PAL*M: Property Attestation for Large Generative Models</div>
<div class="meta-line">Authors: Prach Chantasantitam, Adam Ilyas Caulfield, Vasisht Duddu, Lachlan J. Gunn, N. Asokan</div>
<div class="meta-line">First: 2026-01-22T18:51:13+00:00 · Latest: 2026-01-22T18:51:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16199v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16199v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning property attestations allow provers (e.g., model providers or owners) to attest properties of their models/datasets to verifiers (e.g., regulators, customers), enabling accountability towards regulations and policies. But, current approaches do not support generative models or large datasets. We present PAL*M, a property attestation framework for large generative models, illustrated using large language models. PAL*M defines properties across training and inference, leverages confidential virtual machines with security-aware GPUs for coverage of CPU-GPU operations, and proposes using incremental multiset hashing over memory-mapped datasets to efficiently track their integrity. We implement PAL*M on Intel TDX and NVIDIA H100, showing it is efficient, scalable, versatile, and secure.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PAL*M：大型生成模型的属性认证</div>
<div class="mono" style="margin-top:8px">机器学习属性认证允许证明者（例如模型提供者或所有者）向验证者（例如监管机构、客户）声明其模型/数据集的属性，从而实现对法规和政策的责任追究。然而，当前方法不支持生成模型或大型数据集。我们提出了PAL*M，这是一个针对大型生成模型的属性认证框架，以大型语言模型为例进行说明。PAL*M在训练和推理过程中定义属性，利用具有安全意识的GPU的保密虚拟机来覆盖CPU-GPU操作，并提出使用内存映射数据集的增量多重集哈希以高效跟踪其完整性。我们在Intel TDX和NVIDIA H100上实现了PAL*M，展示了其高效、可扩展、多功能和安全的特性。</div>
</details>
</div>
<div class="card">
<div class="title">Four Over Six: More Accurate NVFP4 Quantization with Adaptive Block Scaling</div>
<div class="meta-line">Authors: Jack Cook, Junxian Guo, Guangxuan Xiao, Yujun Lin, Song Han</div>
<div class="meta-line">First: 2025-12-01T18:59:45+00:00 · Latest: 2026-01-22T18:49:14+00:00</div>
<div class="meta-line">Comments: 10 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.02010v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.02010v3">PDF</a> · <a href="http://github.com/mit-han-lab/fouroversix">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models have grown larger, interest has grown in low-precision numerical formats such as NVFP4 as a way to improve speed and reduce memory usage. However, quantizing models to NVFP4 remains difficult as the lack of precision generally degrades model performance. In this work, we address this issue with Four Over Six (4/6), a modification to the block-scaled NVFP4 quantization algorithm that yields reduced quantization error. Unlike integer formats, floating point formats have non-uniform step sizes which create larger quantization error on larger values. 4/6 takes advantage of this by adaptively scaling some blocks to smaller FP4 values, making the distribution of representable values more uniform and reducing quantization error for near-maximal values. We show that 4/6 can be implemented efficiently on NVIDIA Blackwell GPUs, resulting in performance gains during both pre-training and inference with minimal computational overhead. In pre-training experiments with the Nemotron 3 Nano 30B-A3B model architecture, we find that 4/6 brings training loss closer to BF16 compared to models trained with current state-of-the-art NVFP4 training recipes. Our code is available at http://github.com/mit-han-lab/fouroversix.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Four Over Six：一种更精确的NVFP4量化方法，采用自适应块缩放</div>
<div class="mono" style="margin-top:8px">随着大语言模型的规模不断扩大，人们对低精度数值格式如NVFP4的兴趣日益增加，以提高计算速度并减少内存使用。然而，将模型量化为NVFP4仍然具有挑战性，因为精度的降低通常会损害模型性能。在本工作中，我们提出了Four Over Six（4/6），这是一种改进的块缩放NVFP4量化算法，能够减少量化误差。与整数格式不同，浮点格式具有非均匀的步长，这会导致较大数值的量化误差更大。4/6利用这一点，通过自适应地将某些块缩放到较小的FP4值，使可表示值的分布更加均匀，从而减少接近最大值的量化误差。我们展示了4/6可以在NVIDIA Blackwell GPU上高效实现，从而在预训练和推理过程中带来显著的性能提升，且计算开销极小。在使用Nemotron 3 Nano 30B-A3B模型架构的预训练实验中，我们发现4/6使训练损失更接近BF16，相较于使用当前最先进的NVFP4训练方案训练的模型。我们的代码可在http://github.com/mit-han-lab/fouroversix获取。</div>
</details>
</div>
<div class="card">
<div class="title">Constraining light dark matter in vector-scalar portals with COSI and AMEGO-X</div>
<div class="meta-line">Authors: Maíra Dutra, Clarissa Siqueira, Tonia M. Venters</div>
<div class="meta-line">First: 2025-08-21T18:00:01+00:00 · Latest: 2026-01-22T18:47:17+00:00</div>
<div class="meta-line">Comments: 45 pages, 1 table, and 8 figures; references and discussion added; typos corrected; new appendix with documentation of our model implementation in the Hazma toolkit, now available at https://github.com/dutramaira/VectorScalarPortal_Hazma</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.15891v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.15891v2">PDF</a> · <a href="https://github.com/dutramaira/VectorScalarPortal_Hazma">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Detecting gamma-ray signals that could be due to dark matter (DM) particles would give us invaluable information about the nature of DM. In particular, gamma-ray lines could provide a way to measure the DM mass. The excellent energy resolution of the upcoming Compton Spectrometer and Imager (COSI) will allow us to probe underexplored regions of the DM parameter space while being sensitive to distinctive spectral features of potential DM signals. In this work, we consider a fermionic sub-GeV DM charged under a new U(1) gauge symmetry. Both the DM and the new gauge boson $Z&#x27;$ acquire mass from a new singlet scalar. The masses of the new particles in this class of vector-scalar portal models are naturally at the MeV scale, enabling detectable gamma-ray lines in the bandpasses of COSI and proposed missions such as the All-sky Medium Energy Gamma-ray Observatory eXplorer (AMEGO-X). We estimate the sensitivities of COSI and AMEGO-X to sub-GeV DM in this context, considering a B-L and a purely axial $Z&#x27;$ as benchmark examples. We find regions of the parameter space where COSI will provide leading constraints, beyond the strong CMB limits. On the other hand, AMEGO-X would probe most of the viable parameter space leading to continuum gamma rays. The implementation of our generic vector-scalar portal model in the Hazma toolkit is available at GitHub.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用COSI和AMEGO-X约束矢量-标量通路中的轻暗物质</div>
<div class="mono" style="margin-top:8px">检测可能由暗物质（DM）粒子引起的伽马射线信号将为我们提供关于暗物质本质的宝贵信息。特别是伽马射线谱线可以提供一种测量暗物质质量的方法。即将投入使用的康普顿光谱仪和成像仪（COSI）具有出色的能量分辨率，使我们能够探测尚未充分研究的暗物质参数空间区域，并对潜在暗物质信号的显著谱线特征敏感。在本工作中，我们考虑了一种带有新U(1)规范对称性的费米子亚GeV暗物质。暗物质和新规范玻色子$Z&#x27;$的质量均来源于一个新的单态标量场。在这一类矢量-标量通路模型中，新粒子的质量自然处于MeV量级，从而在COSI和诸如全天空中能伽马射线天文台探测器（AMEGO-X）等提议任务的波段内产生可探测的伽马射线谱线。我们估算了COSI和AMEGO-X在此背景下对亚GeV暗物质的灵敏度，以B-L和纯轴向$Z&#x27;$作为基准示例。我们发现，在某些参数空间区域中，COSI将提供超越强CMB限制的主导约束。另一方面，AMEGO-X将探测到大部分导致连续伽马射线的可行参数空间。我们通用的矢量-标量通路模型在Hazma工具包中的实现可在GitHub上获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Detecting gamma-ray signals that could be due to dark matter (DM) particles would give us invaluable information about the nature of DM.</div>
</details>
</div>
<div class="card">
<div class="title">Training-Free Geospatial Place Representation Learning from Large-Scale Point-of-Interest Graph Data</div>
<div class="meta-line">Authors: Mohammad Hashemi, Hossein Amiri, Andreas Zufle</div>
<div class="meta-line">First: 2025-06-25T15:10:31+00:00 · Latest: 2026-01-22T18:46:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.02921v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.02921v3">PDF</a> · <a href="https://github.com/mohammadhashemii/PlaceRep">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning effective representations of urban environments requires capturing spatial structure beyond fixed administrative boundaries. Existing geospatial representation learning approaches typically aggregate Points of Interest(POI) into pre-defined administrative regions such as census units or ZIP code areas, assigning a single embedding to each region. However, POIs often form semantically meaningful groups that extend across, within, or beyond these boundaries, defining places that better reflect human activity and urban function. To address this limitation, we propose PlaceRep, a training-free geospatial representation learning method that constructs place-level representations by clustering spatially and semantically related POIs. PlaceRep summarizes large-scale POI graphs from U.S. Foursquare data to produce general-purpose urban region embeddings while automatically identifying places across multiple spatial scales. By eliminating model pre-training, PlaceRep provides a scalable and efficient solution for multi-granular geospatial analysis. Experiments using the tasks of population density estimation and housing price prediction as downstream tasks show that PlaceRep outperforms most state-of-the-art graph-based geospatial representation learning methods and achieves up to a 100x speedup in generating region-level representations on large-scale POI graphs. The implementation of PlaceRep is available at https://github.com/mohammadhashemii/PlaceRep.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无需训练的地理空间地点表示学习：从大规模兴趣点图数据中构建</div>
<div class="mono" style="margin-top:8px">学习有效的城市环境表示需要捕捉超越固定行政边界的地理结构。现有的地理空间表示学习方法通常将兴趣点（POI）聚合到预定义的行政区域（如人口普查单位或邮政编码区域），为每个区域分配一个单一的嵌入向量。然而，兴趣点往往形成具有语义意义的群体，跨越、位于或超出这些边界，定义更符合人类活动和城市功能的地点。为了解决这一局限性，我们提出PlaceRep，一种无需训练的地理空间表示学习方法，通过聚类空间和语义相关的兴趣点来构建地点级别的表示。PlaceRep利用美国Foursquare数据中的大规模兴趣点图，生成通用的城市区域嵌入向量，同时自动识别多个空间尺度上的地点。通过消除模型预训练，PlaceRep为多粒度地理空间分析提供了一种可扩展且高效的方法。在人口密度估计和房价预测任务中进行的实验表明，PlaceRep在大多数基于图的地理空间表示学习方法中表现更优，并在大规模兴趣点图上生成区域级表示时实现了高达100倍的速度提升。PlaceRep的实现可在https://github.com/mohammadhashemii/PlaceRep获取。</div>
</details>
</div>
<div class="card">
<div class="title">A Rolling-Space Branch-and-Price Algorithm for the Multi-Compartment Vehicle Routing Problem with Multiple Time Windows</div>
<div class="meta-line">Authors: El Mehdi Er Raqabi, Kevin Dalmeijer, Pascal Van Hentenryck</div>
<div class="meta-line">First: 2026-01-22T18:46:46+00:00 · Latest: 2026-01-22T18:46:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16194v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16194v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper investigates the multi-compartment vehicle routing problem with multiple time windows (MCVRPMTW), an extension of the classical vehicle routing problem with time windows that considers vehicles equipped with multiple compartments and customers requiring service across several delivery time windows. The problem incorporates three key compartment-related features: (i) compartment flexibility in the number of compartments, (ii) item-to-compartment compatibility, and (iii) item-to-item compatibility. The problem also accommodates practical operational requirements such as driver breaks. To solve the MCVRPMTW, we develop an exact branch-and-price (B&amp;P) algorithm in which the pricing problem is solved using a labeling algorithm. Several acceleration strategies are introduced to limit symmetry during label extensions, improve the stability of dual solutions in column generation, and enhance the branching process. To handle large-scale instances, we propose a rolling-space B&amp;P algorithm that integrates clustering techniques into the solution framework. Extensive computational experiments on instances inspired by a real-world industrial application demonstrate the effectiveness of the proposed approach and provide useful managerial insights for practical implementation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多舱车辆路径问题与多时间窗的滚动空间分支定界算法</div>
<div class="mono" style="margin-top:8px">本文研究了多舱车辆路径问题与多时间窗（MCVRPMTW），这是经典带时间窗车辆路径问题的扩展，考虑了配备多个舱室的车辆和需要在多个配送时间窗内服务的客户。该问题包含三个关键的舱室相关特性：(i) 舱室数量的灵活性，(ii) 物品与舱室的兼容性，(iii) 物品之间的兼容性。此外，该问题还考虑了实际运营需求，如司机休息时间。为了解决MCVRPMTW，我们开发了一种精确的分支定界（B&amp;P）算法，其中定价问题通过标签算法求解。我们引入了多种加速策略，以限制标签扩展过程中的对称性，提高列生成过程中对偶解的稳定性，并优化分支过程。为处理大规模实例，我们提出了一种结合聚类技术的滚动空间B&amp;P算法。基于现实工业应用的实例进行的大量计算实验验证了所提出方法的有效性，并为实际应用提供了有用的管理见解。</div>
</details>
</div>
<div class="card">
<div class="title">360Anything: Geometry-Free Lifting of Images and Videos to 360°</div>
<div class="meta-line">Authors: Ziyi Wu, Daniel Watson, Andrea Tagliasacchi, David J. Fleet, Marcus A. Brubaker, Saurabh Saxena</div>
<div class="meta-line">First: 2026-01-22T18:45:59+00:00 · Latest: 2026-01-22T18:45:59+00:00</div>
<div class="meta-line">Comments: Project page: https://360anything.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16192v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16192v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://360anything.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Lifting perspective images and videos to 360° panoramas enables immersive 3D world generation. Existing approaches often rely on explicit geometric alignment between the perspective and the equirectangular projection (ERP) space. Yet, this requires known camera metadata, obscuring the application to in-the-wild data where such calibration is typically absent or noisy. We propose 360Anything, a geometry-free framework built upon pre-trained diffusion transformers. By treating the perspective input and the panorama target simply as token sequences, 360Anything learns the perspective-to-equirectangular mapping in a purely data-driven way, eliminating the need for camera information. Our approach achieves state-of-the-art performance on both image and video perspective-to-360° generation, outperforming prior works that use ground-truth camera information. We also trace the root cause of the seam artifacts at ERP boundaries to zero-padding in the VAE encoder, and introduce Circular Latent Encoding to facilitate seamless generation. Finally, we show competitive results in zero-shot camera FoV and orientation estimation benchmarks, demonstrating 360Anything&#x27;s deep geometric understanding and broader utility in computer vision tasks. Additional results are available at https://360anything.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>360Anything：无需几何的图像和视频到360°的提升</div>
<div class="mono" style="margin-top:8px">将透视图像和视频提升为360°全景图，可以生成沉浸式的3D世界。现有方法通常依赖于透视空间与球面投影（ERP）空间之间的显式几何对齐。然而，这需要已知的相机元数据，限制了其在野外数据中的应用，因为这些校准信息通常缺失或存在噪声。我们提出360Anything，这是一个基于预训练扩散变换器的无几何框架。通过将透视输入和全景目标简单视为标记序列，360Anything以纯数据驱动的方式学习透视到球面投影的映射，从而无需相机信息。我们的方法在图像和视频的透视到360°生成任务中均取得了最先进的性能，优于使用真实相机信息的先前方法。我们还追踪了ERP边界处接缝伪影的根本原因，发现是VAE编码器中的零填充操作所致，并引入了环形潜在编码以实现无缝生成。最后，我们在零样本相机视场角和方向估计基准测试中展示了具有竞争力的结果，证明了360Anything在计算机视觉任务中对几何的深刻理解和更广泛的应用价值。更多信息请访问 https://360anything.github.io/。</div>
</details>
</div>
<div class="card">
<div class="title">SciArena: An Open Evaluation Platform for Non-Verifiable Scientific Literature-Grounded Tasks</div>
<div class="meta-line">Authors: Yilun Zhao, Kaiyan Zhang, Tiansheng Hu, Sihong Wu, Ronan Le Bras, Charles McGrady, Taira Anderson, Jonathan Bragg, Joseph Chee Chang, Jesse Dodge, Matt Latzke, Yixin Liu, Xiangru Tang, Zihang Wang, Chen Zhao, Hannaneh Hajishirzi, Doug Downey, Arman Cohan</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-07-01T17:51:59+00:00 · Latest: 2026-01-22T18:32:06+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 Datasets &amp; Benchmarks Track (Spotlight)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.01001v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.01001v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present SciArena, an open and collaborative platform for evaluating foundation models on scientific literature-grounded tasks. Unlike traditional benchmarks for scientific literature understanding and synthesis, SciArena engages the research community directly, following the Chatbot Arena evaluation approach of community voting on model comparisons. By leveraging collective intelligence, SciArena offers a community-driven evaluation of model performance on open-ended scientific tasks that demand literature-grounded, long-form responses. The platform currently supports 47 foundation models and has collected over 20,000 votes from human researchers across diverse scientific domains. Our analysis of the data collected so far confirms its high quality. We discuss the results and insights based on the model ranking leaderboard. To further promote research in building model-based automated evaluation systems for literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based on collected preference data. It measures the accuracy of models in judging answer quality by comparing their pairwise assessments with human votes. Our experiments highlight the benchmark&#x27;s challenges and emphasize the need for more reliable automated evaluation methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SciArena：面向非可验证科学文献任务的开放评估平台</div>
<div class="mono" style="margin-top:8px">我们提出了SciArena，这是一个用于评估基础模型在科学文献相关任务上的开放且协作的平台。与传统的科学文献理解和综合基准不同，SciArena直接邀请研究社区参与，采用Chatbot Arena模型比较的社区投票评估方法。通过利用集体智慧，SciArena为需要基于文献、开放式回答的科学任务提供社区驱动的模型性能评估。目前该平台支持47个基础模型，并已从多个科学领域的人类研究人员处收集了超过20,000票。我们对目前收集的数据进行了分析，确认其高质量。我们基于模型排名榜单讨论了结果和洞察。为了进一步推动构建基于模型的科学文献任务自动化评估系统的研究，我们发布了SciArena-Eval，这是一个基于收集偏好数据的元评估基准。它通过将模型的成对评估与人类投票进行比较，衡量模型在判断答案质量方面的准确性。我们的实验突显了该基准的挑战，并强调了需要更可靠自动化评估方法的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Paramanu: Compact and Competitive Monolingual Language Models for Low-Resource Morphologically Rich Indian Languages</div>
<div class="meta-line">Authors: Mitodru Niyogi, Eric Gaussier, Arnab Bhattacharya</div>
<div class="meta-line">First: 2024-01-31T17:58:10+00:00 · Latest: 2026-01-22T18:28:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2401.18034v3">Abs</a> · <a href="https://arxiv.org/pdf/2401.18034v3">PDF</a> · <a href="https://huggingface.co/collections/mitodru/paramanu">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multilingual large language models (LLMs) are expensive to pretrain and often suffer from imbalances across languages and datasets, English-centric bias, tokenizer oversegmentation for morphologically rich low-resource languages, and the curse of multilinguality. We introduce PARAMANU, the first family of Indian-only autoregressive language models trained from scratch on open-source language-specific data for the five most spoken Indian languages: Bengali, Hindi, Marathi, Tamil, and Telugu. All models are designed for affordability and are trained on a single GPU with a budget under $1,000, allowing under-resourced researchers to build competitive language models. To address low-resource challenges, we develop morphology-aligned, low-fertility tokenizers, propose an interpolation-based method for token position indices in RoPE based scaling to train longer sequences efficiently. We also create instruction-tuning datasets in Bangla that are translated to the other four languages. Despite their small size (108M-367M parameters), Paramanu achieves a strong performance-efficiency tradeoff and outperforms most larger multilingual models across all five languages. Our collection is available at https://huggingface.co/collections/mitodru/paramanu.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Paramanu：针对低资源形态丰富的印度语言的紧凑且具有竞争力的单语语言模型</div>
<div class="mono" style="margin-top:8px">多语言大语言模型（LLMs）在预训练时成本高昂，且常面临语言和数据集之间的不平衡、以英语为中心的偏差、对形态丰富的低资源语言的分词过度分割以及多语言的诅咒。我们引入了PARAMANU，这是首个基于印度语言的自回归语言模型家族，使用开源的语言特定数据从头开始训练，涵盖印度最常用的语言：孟加拉语、印地语、马拉地语、泰米尔语和泰卢固语。所有模型均设计为经济实惠，仅需单个GPU和不到1000美元的预算即可训练，使资源有限的研究人员也能构建具有竞争力的语言模型。为应对低资源挑战，我们开发了形态对齐的低生育力分词器，提出了基于插值的方法用于RoPE缩放中的标记位置索引，以高效地训练更长的序列。我们还创建了用孟加拉语编写的指令调优数据集，并将其翻译成其他四种语言。尽管其规模较小（1.08亿至3.67亿参数），Paramanu在性能和效率之间实现了良好的平衡，并在所有五种语言上均优于大多数更大的多语言模型。我们的模型集合可在 https://huggingface.co/collections/mitodru/paramanu 获取。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Discover at Test Time</div>
<div class="meta-line">Authors: Mert Yuksekgonul, Daniel Koceja, Xinhao Li, Federico Bianchi, Jed McCaleb, Xiaolong Wang, Jan Kautz, Yejin Choi, James Zou, Carlos Guestrin, Yu Sun</div>
<div class="meta-line">First: 2026-01-22T18:24:00+00:00 · Latest: 2026-01-22T18:24:00+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/test-time-training/discover</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16175v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16175v1">PDF</a> · <a href="https://github.com/test-time-training/discover">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How can we use AI to discover a new state of the art for a scientific problem? Prior work in test-time scaling, such as AlphaEvolve, performs search by prompting a frozen LLM. We perform reinforcement learning at test time, so the LLM can continue to train, but now with experience specific to the test problem. This form of continual learning is quite special, because its goal is to produce one great solution rather than many good ones on average, and to solve this very problem rather than generalize to other problems. Therefore, our learning objective and search subroutine are designed to prioritize the most promising solutions. We call this method Test-Time Training to Discover (TTT-Discover). Following prior work, we focus on problems with continuous rewards. We report results for every problem we attempted, across mathematics, GPU kernel engineering, algorithm design, and biology. TTT-Discover sets the new state of the art in almost all of them: (i) Erdős&#x27; minimum overlap problem and an autocorrelation inequality; (ii) a GPUMode kernel competition (up to $2\times$ faster than prior art); (iii) past AtCoder algorithm competitions; and (iv) denoising problem in single-cell analysis. Our solutions are reviewed by experts or the organizers. All our results are achieved with an open model, OpenAI gpt-oss-120b, and can be reproduced with our publicly available code, in contrast to previous best results that required closed frontier models. Our test-time training runs are performed using Tinker, an API by Thinking Machines, with a cost of only a few hundred dollars per problem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在测试时学习发现</div>
<div class="mono" style="margin-top:8px">我们如何利用AI为科学问题发现新的前沿？先前的测试时扩展工作，如AlphaEvolve，通过提示一个冻结的LLM进行搜索。我们则在测试时进行强化学习，使LLM能够继续训练，但现在使用与测试问题相关的经验。这种持续学习的形式非常特殊，因为其目标是产生一个卓越的解决方案，而不是平均产生多个良好的解决方案，且是为了解决这个问题而不是泛化到其他问题。因此，我们的学习目标和搜索子程序被设计为优先考虑最有希望的解决方案。我们将这种方法称为测试时训练以发现（TTT-Discover）。与先前工作类似，我们专注于具有连续奖励的问题。我们在数学、GPU内核工程、算法设计和生物学等多个领域报告了所有尝试问题的结果。TTT-Discover在几乎所有这些问题上都设定了新的前沿：(i) 埃尔德什最小重叠问题和自相关不等式；(ii) GPUMode内核竞赛（比之前的方法快至2倍）；(iii) 之前的AtCoder算法竞赛；以及(iv) 单细胞分析中的去噪问题。我们的解决方案由专家或组织者评审。所有结果均使用开源模型OpenAI gpt-oss-120b实现，并且可以通过我们公开的代码进行复现，这与之前最佳结果需要封闭前沿模型的情况不同。我们的测试时训练运行使用了Thinking Machines的API Tinker，每项问题的运行成本仅为几百美元。</div>
</details>
</div>
<div class="card">
<div class="title">Is this chart lying to me? Automating the detection of misleading visualizations</div>
<div class="meta-line">Authors: Jonathan Tonglet, Jan Zimny, Tinne Tuytelaars, Iryna Gurevych</div>
<div class="meta-line">First: 2025-08-29T14:36:45+00:00 · Latest: 2026-01-22T18:23:24+00:00</div>
<div class="meta-line">Comments: Preprint under review. Code and data available at: https://github.com/UKPLab/arxiv2025-misviz</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.21675v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.21675v2">PDF</a> · <a href="https://github.com/UKPLab/arxiv2025-misviz">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Misleading visualizations are a potent driver of misinformation on social media and the web. By violating chart design principles, they distort data and lead readers to draw inaccurate conclusions. Prior work has shown that both humans and multimodal large language models (MLLMs) are frequently deceived by such visualizations. Automatically detecting misleading visualizations and identifying the specific design rules they violate could help protect readers and reduce the spread of misinformation. However, the training and evaluation of AI models has been limited by the absence of large, diverse, and openly available datasets. In this work, we introduce Misviz, a benchmark of 2,604 real-world visualizations annotated with 12 types of misleaders. To support model training, we also create Misviz-synth, a synthetic dataset of 57,665 visualizations generated using Matplotlib and based on real-world data tables. We perform a comprehensive evaluation on both datasets using state-of-the-art MLLMs, rule-based systems, and image-axis classifiers. Our results reveal that the task remains highly challenging. We release Misviz, Misviz-synth, and the accompanying code.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>这张图表在欺骗我吗？自动检测误导性可视化</div>
<div class="mono" style="margin-top:8px">误导性可视化是社交媒体和网络上虚假信息传播的一个强大驱动因素。它们通过违反图表设计原则扭曲数据，使读者得出不准确的结论。已有研究表明，人类和多模态大语言模型（MLLMs）都经常被此类可视化所欺骗。自动检测误导性可视化并识别其违反的具体设计规则，有助于保护读者并减少虚假信息的传播。然而，由于缺乏大规模、多样化且公开可用的数据集，AI模型的训练和评估受到了限制。在本工作中，我们引入了Misviz，这是一个包含2,604个真实世界可视化并标注了12种误导类型的基准数据集。为了支持模型训练，我们还创建了Misviz-synth，一个包含57,665个合成可视化数据集，使用Matplotlib生成，基于真实世界的数据表。我们使用最先进的MLLMs、基于规则的系统和图像轴分类器对这两个数据集进行了全面评估。我们的结果表明，该任务仍然极具挑战性。我们发布了Misviz、Misviz-synth以及相关代码。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Misleading visualizations are a potent driver of misinformation on social media and the web.</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Predictive Uncertainty: Reliable Representation Learning with Structural Constraints</div>
<div class="meta-line">Authors: Yiyao Yang</div>
<div class="meta-line">First: 2026-01-22T18:19:52+00:00 · Latest: 2026-01-22T18:19:52+00:00</div>
<div class="meta-line">Comments: 22 pages, 5 figures, 5 propositions</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16174v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16174v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Uncertainty estimation in machine learning has traditionally focused on the prediction stage, aiming to quantify confidence in model outputs while treating learned representations as deterministic and reliable by default. In this work, we challenge this implicit assumption and argue that reliability should be regarded as a first-class property of learned representations themselves. We propose a principled framework for reliable representation learning that explicitly models representation-level uncertainty and leverages structural constraints as inductive biases to regularize the space of feasible representations. Our approach introduces uncertainty-aware regularization directly in the representation space, encouraging representations that are not only predictive but also stable, well-calibrated, and robust to noise and structural perturbations. Structural constraints, such as sparsity, relational structure, or feature-group dependencies, are incorporated to define meaningful geometry and reduce spurious variability in learned representations, without assuming fully correct or noise-free structure. Importantly, the proposed framework is independent of specific model architectures and can be integrated with a wide range of representation learning methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越预测不确定性：利用结构约束实现可靠的表示学习</div>
<div class="mono" style="margin-top:8px">机器学习中的不确定性估计传统上专注于预测阶段，旨在量化模型输出的置信度，同时默认将学习到的表示视为确定且可靠的。在本工作中，我们挑战这一隐含假设，认为可靠性应被视为学习到的表示本身的首要属性。我们提出了一种原理性的框架，用于可靠的表示学习，该框架明确建模表示层面的不确定性，并利用结构约束作为归纳偏置来正则化可行表示的空间。我们的方法在表示空间中直接引入不确定性感知的正则化，鼓励不仅具有预测能力，而且稳定、校准良好，并且对噪声和结构扰动具有鲁棒性的表示。结构约束，如稀疏性、关系结构或特征组依赖性，被纳入以定义有意义的几何结构，并减少学习到的表示中的虚假变异性，而无需假设结构完全正确或无噪声。重要的是，所提出的框架与特定模型架构无关，可以与各种表示学习方法集成。</div>
</details>
</div>
<div class="card">
<div class="title">Structured Hints for Sample-Efficient Lean Theorem Proving</div>
<div class="meta-line">Authors: Zachary Burton</div>
<div class="meta-line">First: 2026-01-22T18:16:46+00:00 · Latest: 2026-01-22T18:16:46+00:00</div>
<div class="meta-line">Comments: 9 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16172v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16172v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-of-the-art neural theorem provers like DeepSeek-Prover-V1.5 combine large language models with reinforcement learning, achieving impressive results through sophisticated training. We ask: do these highly-trained models still benefit from simple structural guidance at inference time? We evaluate a lightweight intervention -- a fixed prompt schedule over 15 common tactic skeletons -- on the miniF2F benchmark. This simple approach yields 21.7% pass@16 compared to 15.2% for standard sampling from the same model, a 43% relative improvement using the same number of samples (k=16) and same maximum generation length (1024 tokens). Our results suggest that even capable RL-trained provers underutilize structural priors available in the tactic language, and that simple inference-time guidance remains a cheap, complementary boost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结构化提示用于高效低样本量的定理证明</div>
<div class="mono" style="margin-top:8px">像DeepSeek-Prover-V1.5这样的最先进的神经定理证明器结合了大语言模型和强化学习，通过复杂的训练取得了令人印象深刻的结果。我们提出问题：这些高度训练的模型在推理时是否仍然能从简单的结构指导中受益？我们在miniF2F基准上评估了一种轻量级干预方法——在15种常见战术骨架上使用固定提示调度。这种方法相比标准采样方式，在相同采样数量（k=16）和最大生成长度（1024个标记）下，pass@16达到了21.7%，相比15.2%有43%的相对提升。我们的结果表明，即使是有能力的强化学习训练的证明器，也未能充分利用战术语言中可用的结构先验知识，而简单的推理时指导仍是一个低成本且互补的提升手段。</div>
</details>
</div>
<div class="card">
<div class="title">Everybody Prune Now: Structured Pruning of LLMs with only Forward Passes</div>
<div class="meta-line">Authors: Steven Kolawole, Lucio Dery, Jean-François Kagy, Virginia Smith, Graham Neubig, Ameet Talwalkar</div>
<div class="meta-line">First: 2024-02-08T04:48:26+00:00 · Latest: 2026-01-22T18:13:50+00:00</div>
<div class="meta-line">Comments: 19 pages, 6 fiigures, 16 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2402.05406v4">Abs</a> · <a href="https://arxiv.org/pdf/2402.05406v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Structured pruning is a promising approach to create smaller, faster large language models. However, existing methods typically rely on computing the gradient via backward passes, which can inflate memory requirements and compute costs. In this work we introduce Bonsai, a gradient-free structured pruning method that eliminates the need for backpropagation, significantly reducing memory requirements and compute costs while achieving state-of-the-art pruning performance. Bonsai uses forward-pass-only perturbative pruning to enable efficient compression of large models on a broader range of hardware configurations. Unlike existing structured pruning approaches, Bonsai not only achieves better compression with fewer resources but also produces models that are twice as fast as those generated by semi-structured pruning. As a concrete demonstration, we use Bonsai to prune 7B and 8B models to 50% sparsity on a single A6000 GPU -- a task challenging for backprop-based methods in memory-constrained settings, as they require 2-3x the memory. Our results show that removing backprop as a requirement not only enables pruning larger models on constrained hardware but can also lead to state-of-the-art efficiency and performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>现在人人都来修剪：仅使用前向传播的LLM结构化剪枝</div>
<div class="mono" style="margin-top:8px">结构化剪枝是一种有前景的方法，用于创建更小、更快的大型语言模型。然而，现有方法通常依赖于通过反向传播计算梯度，这会增加内存需求和计算成本。在本工作中，我们引入了Bonsai，一种无需梯度的结构化剪枝方法，消除了反向传播的需要，显著降低了内存需求和计算成本，同时实现了最先进的剪枝性能。Bonsai使用仅前向传播的微扰剪枝方法，使大型模型在更广泛的硬件配置上能够高效压缩。与现有的结构化剪枝方法不同，Bonsai不仅在资源较少的情况下实现了更好的压缩效果，还生成的模型速度是半结构化剪枝方法生成模型的两倍。作为具体演示，我们在单块A6000 GPU上使用Bonsai将7B和8B模型剪枝至50%的稀疏度——这在内存受限的环境下对基于反向传播的方法来说是一项具有挑战性的任务，因为它们需要2-3倍的内存。我们的结果表明，去除对反向传播的需求不仅使在受限硬件上剪枝更大的模型成为可能，还能带来最先进的效率和性能。</div>
</details>
</div>
<div class="card">
<div class="title">GRITHopper: Decomposition-Free Multi-Hop Dense Retrieval</div>
<div class="meta-line">Authors: Justus-Jonas Erker, Nils Reimers, Iryna Gurevych</div>
<div class="meta-line">First: 2025-03-10T16:42:48+00:00 · Latest: 2026-01-22T18:12:25+00:00</div>
<div class="meta-line">Comments: Accepted at EACL 2026 Main Conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.07519v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.07519v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Decomposition-based multi-hop retrieval methods rely on many autoregressive steps to break down complex queries, which breaks end-to-end differentiability and is computationally expensive. Decomposition-free methods tackle this, but current decomposition-free approaches struggle with longer multi-hop problems and generalization to out-of-distribution data. To address these challenges, we introduce GRITHopper-7B, a novel multi-hop dense retrieval model that achieves state-of-the-art performance on both in-distribution and out-of-distribution benchmarks. GRITHopper combines generative and representational instruction tuning by integrating causal language modeling with dense retrieval training. Through controlled studies, we find that incorporating additional context after the retrieval process, referred to as post-retrieval language modeling, enhances dense retrieval performance. By including elements such as final answers during training, the model learns to better contextualize and retrieve relevant information. GRITHopper-7B offers a robust, scalable, and generalizable solution for multi-hop dense retrieval, and we release it to the community for future research and applications requiring multi-hop reasoning and retrieval capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GRITHopper：无需分解的多跳密集检索</div>
<div class="mono" style="margin-top:8px">基于分解的多跳检索方法依赖于许多自回归步骤来分解复杂查询，这破坏了端到端可微性和计算成本较高。分解方法可以解决这些问题，但当前的分解方法在处理较长的多跳问题和泛化到分布外数据时表现不佳。为应对这些挑战，我们引入了GRITHopper-7B，这是一种新型的多跳密集检索模型，在分布内和分布外基准测试中均实现了最先进的性能。GRITHopper通过将因果语言建模与密集检索训练相结合，实现了生成式和表示式指令调优。通过受控研究，我们发现，在检索过程后引入额外上下文，即所谓的后检索语言建模，可以提升密集检索性能。在训练过程中包含诸如最终答案等元素，使模型能够更好地进行上下文化和相关信息检索。GRITHopper-7B为多跳密集检索提供了一种稳健、可扩展且可泛化的解决方案，并将其发布给社区，供未来需要多跳推理和检索能力的研究和应用使用。</div>
</details>
</div>
<div class="card">
<div class="title">Cosmos Policy: Fine-Tuning Video Models for Visuomotor Control and Planning</div>
<div class="meta-line">Authors: Moo Jin Kim, Yihuai Gao, Tsung-Yi Lin, Yen-Chen Lin, Yunhao Ge, Grace Lam, Percy Liang, Shuran Song, Ming-Yu Liu, Chelsea Finn, Jinwei Gu</div>
<div class="meta-line">First: 2026-01-22T18:09:30+00:00 · Latest: 2026-01-22T18:09:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16163v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16163v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent video generation models demonstrate remarkable ability to capture complex physical interactions and scene evolution over time. To leverage their spatiotemporal priors, robotics works have adapted video models for policy learning but introduce complexity by requiring multiple stages of post-training and new architectural components for action generation. In this work, we introduce Cosmos Policy, a simple approach for adapting a large pretrained video model (Cosmos-Predict2) into an effective robot policy through a single stage of post-training on the robot demonstration data collected on the target platform, with no architectural modifications. Cosmos Policy learns to directly generate robot actions encoded as latent frames within the video model&#x27;s latent diffusion process, harnessing the model&#x27;s pretrained priors and core learning algorithm to capture complex action distributions. Additionally, Cosmos Policy generates future state images and values (expected cumulative rewards), which are similarly encoded as latent frames, enabling test-time planning of action trajectories with higher likelihood of success. In our evaluations, Cosmos Policy achieves state-of-the-art performance on the LIBERO and RoboCasa simulation benchmarks (98.5% and 67.1% average success rates, respectively) and the highest average score in challenging real-world bimanual manipulation tasks, outperforming strong diffusion policies trained from scratch, video model-based policies, and state-of-the-art vision-language-action models fine-tuned on the same robot demonstrations. Furthermore, given policy rollout data, Cosmos Policy can learn from experience to refine its world model and value function and leverage model-based planning to achieve even higher success rates in challenging tasks. We release code, models, and training data at https://research.nvidia.com/labs/dir/cosmos-policy/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Cosmos Policy：为视觉运动控制与规划微调视频模型</div>
<div class="mono" style="margin-top:8px">近期的视频生成模型展现出捕捉复杂物理交互和场景随时间演变的卓越能力。为了利用其时空先验知识，机器人研究工作已将视频模型应用于策略学习，但这种方法引入了复杂性，因为它需要多个训练后的阶段以及新的架构组件来生成动作。在本工作中，我们提出了Cosmos Policy，这是一种简单的方法，通过在目标平台收集的机器人演示数据上进行单阶段微调，将一个大型预训练视频模型（Cosmos-Predict2）转化为有效的机器人策略，且无需架构修改。Cosmos Policy学习直接生成机器人动作，这些动作被编码为视频模型中的潜在帧，利用模型的预训练先验知识和核心学习算法来捕捉复杂的动作分布。此外，Cosmos Policy还生成未来状态图像和值（预期累计奖励），这些同样被编码为潜在帧，从而在测试时能够进行动作轨迹的规划，提高成功概率。在我们的评估中，Cosmos Policy在LIBERO和RoboCasa模拟基准测试中取得了最先进的性能（分别为98.5%和67.1%的平均成功率），并在具有挑战性的现实世界双臂操作任务中获得了最高的平均得分，优于从头训练的强扩散策略、基于视频模型的策略以及在相同机器人演示上微调的最先进视觉-语言-动作模型。此外，给定策略 rollout 数据，Cosmos Policy可以从经验中学习，优化其世界模型和价值函数，并利用基于模型的规划在复杂任务中实现更高的成功率。我们将在https://research.nvidia.com/labs/dir/cosmos-policy/发布代码、模型和训练数据。</div>
</details>
</div>
<div class="card">
<div class="title">BAH Dataset for Ambivalence/Hesitancy Recognition in Videos for Digital Behavioural Change</div>
<div class="meta-line">Authors: Manuela González-González, Soufiane Belharbi, Muhammad Osama Zeeshan, Masoumeh Sharafi, Muhammad Haseeb Aslam, Marco Pedersoli, Alessandro Lameiras Koerich, Simon L Bacon, Eric Granger</div>
<div class="meta-line">First: 2025-05-25T21:29:00+00:00 · Latest: 2026-01-22T18:06:39+00:00</div>
<div class="meta-line">Comments: 45 pages, 21 figures, under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.19328v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.19328v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ambivalence and hesitancy (A/H), a closely related construct, is the primary reasons why individuals delay, avoid, or abandon health behaviour changes. It is a subtle and conflicting emotion that sets a person in a state between positive and negative orientations, or between acceptance and refusal to do something. It manifests by a discord in affect between multiple modalities or within a modality, such as facial and vocal expressions, and body language. Although experts can be trained to recognize A/H as done for in-person interactions, integrating them into digital health interventions is costly and less effective. Automatic A/H recognition is therefore critical for the personalization and cost-effectiveness of digital behaviour change interventions. However, no datasets currently exists for the design of machine learning models to recognize A/H. This paper introduces the Behavioural Ambivalence/Hesitancy (BAH) dataset collected for multimodal recognition of A/H in videos. It contains 1,427 videos with a total duration of 10.60 hours captured from 300 participants across Canada answering predefined questions to elicit A/H. It is intended to mirror real-world online personalized behaviour change interventions. BAH is annotated by three experts to provide timestamps that indicate where A/H occurs, and frame- and video-level annotations with A/H cues. Video transcripts, cropped and aligned faces, and participants&#x27; meta-data are also provided. Since A and H manifest similarly in practice, we provide a binary annotation indicating the presence or absence of A/H. Additionally, this paper includes benchmarking results using baseline models on BAH for frame- and video-level recognition, zero-shot prediction, and personalization using source-free domain adaptation. The data, code, and pretrained weights are available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于数字行为改变视频中矛盾/犹豫识别的BAH数据集</div>
<div class="mono" style="margin-top:8px">矛盾与犹豫（A/H）是一种密切相关的情感构念，是个人延迟、回避或放弃健康行为改变的主要原因。它是一种微妙且冲突的情绪，使个体处于积极与消极取向之间，或接受与拒绝做某事的状态。这种情绪通过多种模态或同一模态内的情感不协调表现出来，例如面部表情与语音表达之间的不一致，以及肢体语言。尽管专家可以通过训练来识别A/H，如同面对面交流中所做的那样，但将其整合到数字健康干预中成本高昂且效果不佳。因此，自动识别A/H对于数字行为改变干预的个性化和成本效益至关重要。然而，目前尚无可用于设计机器学习模型以识别A/H的数据集。本文介绍了为视频中A/H的多模态识别而收集的Behavioral Ambivalence/Hesitancy（BAH）数据集。该数据集包含来自加拿大300名参与者拍摄的1,427个视频，总时长为10.60小时，参与者回答预定义问题以引发A/H。该数据集旨在模拟现实世界中在线个性化行为改变干预的真实场景。BAH由三位专家进行标注，提供时间戳以指示A/H发生的位置，并提供帧级和视频级的A/H提示标注。此外，还提供了视频转录文本、裁剪并对齐的面部图像以及参与者的元数据。由于在实践中A和H的表现相似，我们提供了一个二元标注，表示A/H的存在与否。此外，本文还包含了使用基线模型在BAH数据集上进行帧级和视频级识别、零样本预测以及使用无源域适应进行个性化方面的基准测试结果。数据、代码和预训练权重均可获取。</div>
</details>
</div>
<div class="card">
<div class="title">From Text to Image: Exploring GPT-4Vision&#x27;s Potential in Advanced Radiological Analysis across Subspecialties</div>
<div class="meta-line">Authors: Felix Busch, Tianyu Han, Marcus Makowski, Daniel Truhn, Keno Bressem, Lisa Adams</div>
<div class="meta-line">Venue: J Med Internet Res 2024;26:e54948</div>
<div class="meta-line">First: 2023-11-24T15:39:29+00:00 · Latest: 2026-01-22T18:06:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2311.14777v2">Abs</a> · <a href="https://arxiv.org/pdf/2311.14777v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The study evaluates and compares GPT-4 and GPT-4Vision for radiological tasks, suggesting GPT-4Vision may recognize radiological features from images, thereby enhancing its diagnostic potential over text-based descriptions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从文本到图像：探索GPT-4Vision在各亚专科高级放射分析中的潜力</div>
<div class="mono" style="margin-top:8px">本研究评估并比较了GPT-4和GPT-4Vision在放射学任务中的表现，建议GPT-4Vision可能能够从图像中识别放射学特征，从而增强其基于文本描述的诊断潜力。</div>
</details>
</div>
<div class="card">
<div class="title">CONTEX-T: Contextual Privacy Exploitation via Transformer Spectral Analysis for IoT Device Fingerprinting</div>
<div class="meta-line">Authors: Nazmul Islam, Mohammad Zulkernine</div>
<div class="meta-line">First: 2026-01-22T18:03:34+00:00 · Latest: 2026-01-22T18:03:34+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16160v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16160v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid expansion of internet of things (IoT) devices have created a pervasive ecosystem where encrypted wireless communications serve as the primary privacy and security protection mechanism. While encryption effectively protects message content, packet metadata and statistics inadvertently expose device identities and user contexts. Various studies have exploited raw packet statistics and their visual representations for device fingerprinting and identification. However, these approaches remain confined to the spatial domain with limited feature representation. Therefore, this paper presents CONTEX-T, a novel framework that exploits contextual privacy vulnerabilities using spectral representation of encrypted wireless traffic for IoT device characterization. The experiments show that spectral analysis provides new and rich feature representation for covert reconnaissance attacks, revealing a complex and expanding threat landscape that would require robust countermeasures for IoT security management. CONTEXT-T first transforms raw packet length sequences into time-frequency spectral representations and then utilizes transformer-based spectral analysis for the device identification. We systematically evaluated multiple spectral representation techniques and transformer-based models across encrypted traffic samples from various IoT devices. CONTEXT-T effectively exploited privacy vulnerabilities and achieved device classification accuracy exceeding 99% across all devices while remaining completely passive and undetectable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CONTEX-T: 通过Transformer频谱分析进行上下文隐私利用的物联网设备指纹识别</div>
<div class="mono" style="margin-top:8px">物联网设备的快速扩展构建了一个广泛存在的生态系统，其中加密无线通信是主要的隐私和安全保护机制。虽然加密有效保护了消息内容，但数据包元数据和统计信息无意中暴露了设备身份和用户上下文。已有多种研究利用原始数据包统计信息及其可视化表示进行设备指纹识别和定位。然而，这些方法仍局限于空间域，特征表示有限。因此，本文提出CONTEX-T，一种新颖的框架，利用加密无线流量的频谱表示来挖掘上下文隐私漏洞，实现物联网设备的特征刻画。实验表明，频谱分析为隐蔽侦察攻击提供了新的、丰富的特征表示，揭示了一个复杂且不断扩展的威胁图景，这将需要强大的对策来保障物联网安全。CONTEX-T首先将原始数据包长度序列转换为时频频谱表示，然后利用基于Transformer的频谱分析进行设备识别。我们系统地评估了多种频谱表示技术及基于Transformer的模型在不同物联网设备加密流量样本上的表现。CONTEX-T有效地利用了隐私漏洞，在所有设备上实现了超过99%的设备分类准确率，同时保持完全被动和不可检测。</div>
</details>
</div>
<div class="card">
<div class="title">Domain-Incremental Continual Learning for Robust and Efficient Keyword Spotting in Resource Constrained Systems</div>
<div class="meta-line">Authors: Prakash Dhungana, Sayed Ahmad Salehi</div>
<div class="meta-line">First: 2026-01-22T17:59:31+00:00 · Latest: 2026-01-22T17:59:31+00:00</div>
<div class="meta-line">Comments: 12 pages, 8 figures, and 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16158v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16158v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Keyword Spotting (KWS) systems with small footprint models deployed on edge devices face significant accuracy and robustness challenges due to domain shifts caused by varying noise and recording conditions. To address this, we propose a comprehensive framework for continual learning designed to adapt to new domains while maintaining computational efficiency. The proposed pipeline integrates a dual-input Convolutional Neural Network, utilizing both Mel Frequency Cepstral Coefficients (MFCC) and Mel-spectrogram features, supported by a multi-stage denoising process, involving discrete wavelet transform and spectral subtraction techniques, plus model and prototype update blocks. Unlike prior methods that restrict updates to specific layers, our approach updates the complete quantized model, made possible due to compact model architecture. A subset of input samples are selected during runtime using class prototypes and confidence-driven filtering, which are then pseudo-labeled and combined with rehearsal buffer for incremental model retraining. Experimental results on noisy test dataset demonstrate the framework&#x27;s effectiveness, achieving 99.63\% accuracy on clean data and maintaining robust performance (exceeding 94\% accuracy) across diverse noisy environments, even at -10 dB Signal-to-Noise Ratio. The proposed framework work confirms that integrating efficient denoising with prototype-based continual learning enables KWS models to operate autonomously and robustly in resource-constrained, dynamic environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向资源受限系统的鲁棒高效关键词识别的领域增量持续学习</div>
<div class="mono" style="margin-top:8px">由于噪声和录音条件的变化导致领域转移，部署在边缘设备上的小型模型关键词识别（KWS）系统面临显著的准确性和鲁棒性挑战。为了解决这一问题，我们提出了一种全面的持续学习框架，旨在适应新领域的同时保持计算效率。该框架包含一个双输入卷积神经网络，利用梅尔频率倒谱系数（MFCC）和梅尔频谱图特征，并结合多阶段降噪过程，包括离散小波变换和频谱减法技术，以及模型和原型更新模块。与之前仅限制更新特定层的方法不同，我们的方法更新完整的量化模型，这得益于紧凑的模型架构。在运行时，通过类原型和置信度驱动的过滤选择一部分输入样本，然后进行伪标签处理，并与回放缓冲区结合进行增量模型再训练。在噪声测试数据集上的实验结果表明，该框架有效，实现了99.63\%的干净数据准确率，并在多样化的噪声环境中保持鲁棒性能（超过94\%准确率），即使在-10 dB信噪比下也能表现良好。所提出的框架验证了将高效的降噪与基于原型的持续学习相结合，能够使KWS模型在资源受限、动态环境中自主且鲁棒地运行。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Keyword Spotting (KWS) systems with small footprint models deployed on edge devices face significant accuracy and robustness challenges due to domain shifts caused by varying noise and recording conditions.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
