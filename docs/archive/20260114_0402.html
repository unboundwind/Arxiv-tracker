<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-14 04:02</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260114_0402</div>
    <div class="row"><div class="card">
<div class="title">SecureCAI: Injection-Resilient LLM Assistants for Cybersecurity Operations</div>
<div class="meta-line">Authors: Mohammed Himayath Ali, Mohammed Aqib Abdullah, Mohammed Mudassir Uddin, Shahnawaz Alam</div>
<div class="meta-line">First: 2026-01-12T18:59:45+00:00 · Latest: 2026-01-12T18:59:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07835v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07835v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models have emerged as transformative tools for Security Operations Centers, enabling automated log analysis, phishing triage, and malware explanation; however, deployment in adversarial cybersecurity environments exposes critical vulnerabilities to prompt injection attacks where malicious instructions embedded in security artifacts manipulate model behavior. This paper introduces SecureCAI, a novel defense framework extending Constitutional AI principles with security-aware guardrails, adaptive constitution evolution, and Direct Preference Optimization for unlearning unsafe response patterns, addressing the unique challenges of high-stakes security contexts where traditional safety mechanisms prove insufficient against sophisticated adversarial manipulation. Experimental evaluation demonstrates that SecureCAI reduces attack success rates by 94.7% compared to baseline models while maintaining 95.1% accuracy on benign security analysis tasks, with the framework incorporating continuous red-teaming feedback loops enabling dynamic adaptation to emerging attack strategies and achieving constitution adherence scores exceeding 0.92 under sustained adversarial pressure, thereby establishing a foundation for trustworthy integration of language model capabilities into operational cybersecurity workflows and addressing a critical gap in current approaches to AI safety within adversarial domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SecureCAI：面向网络安全运营的抗注入LLM助手</div>
<div class="mono" style="margin-top:8px">大型语言模型已成为安全运营中心的变革性工具，能够实现日志分析自动化、钓鱼邮件分类和恶意软件解释；然而，在对抗性网络安全环境中部署时，会暴露于提示注入攻击的严重漏洞，其中恶意指令嵌入安全工件中可操控模型行为。本文介绍了SecureCAI，这是一种新型防御框架，扩展了宪法式AI原则，结合了安全意识的护栏、自适应宪法演化和直接偏好优化，以消除不安全响应模式，应对传统安全机制在高风险安全情境下无法有效抵御复杂对抗性操控的独特挑战。实验评估表明，SecureCAI在与基线模型相比时，将攻击成功率降低了94.7%，同时在良性安全分析任务中保持了95.1%的准确性。该框架通过持续的红队反馈循环实现动态适应新兴攻击策略，并在持续对抗压力下实现了超过0.92的宪法遵循评分，从而为语言模型能力在操作性网络安全流程中的可信集成奠定了基础，并填补了当前对抗领域AI安全方法中的关键空白。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models have emerged as transformative tools for Security Operations Centers, enabling automated log analysis, phishing triage, and malware explanation; however, deployment in adversarial cybersecurity environments exposes critical vulnerabilities to prompt injection attacks where malicious instructions embedded in security artifacts manipulate model behavior.</div>
</details>
</div>
<div class="card">
<div class="title">A Complete Decomposition of Stochastic Differential Equations</div>
<div class="meta-line">Authors: Samuel Duffield</div>
<div class="meta-line">First: 2026-01-12T18:59:36+00:00 · Latest: 2026-01-12T18:59:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07834v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07834v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We show that any stochastic differential equation with prescribed time-dependent marginal distributions admits a decomposition into three components: a unique scalar field governing marginal evolution, a symmetric positive-semidefinite diffusion matrix field and a skew-symmetric matrix field.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随机微分方程的完整分解</div>
<div class="mono" style="margin-top:8px">我们证明，任何具有给定时间依赖边际分布的随机微分方程都可以分解为三个组成部分：一个唯一标量场，用于控制边际演化；一个对称正半定扩散矩阵场；以及一个斜对称矩阵场。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We show that any stochastic differential equation with prescribed time-dependent marginal distributions admits a decomposition into three components: a unique scalar field governing marginal evolution, a symmetric positive-semidefinite diffusion matrix field and a skew-symmetric matrix field.</div>
</details>
</div>
<div class="card">
<div class="title">Tuning-free Visual Effect Transfer across Videos</div>
<div class="meta-line">Authors: Maxwell Jones, Rameen Abdal, Or Patashnik, Ruslan Salakhutdinov, Sergey Tulyakov, Jun-Yan Zhu, Kuan-Chieh Jackson Wang</div>
<div class="meta-line">First: 2026-01-12T18:59:32+00:00 · Latest: 2026-01-12T18:59:32+00:00</div>
<div class="meta-line">Comments: Project Page: $\href{https://tuningfreevisualeffects-maker.github.io/Tuning-free-Visual-Effect-Transfer-across-Videos-Project-Page/}{this\ URL}$</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07833v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07833v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://tuningfreevisualeffects-maker.github.io/Tuning-free-Visual-Effect-Transfer-across-Videos-Project-Page/}{this\">Project1</a> · <a href="https://tuningfreevisualeffects-maker.github.io/Tuning-free-Visual-Effect-Transfer-across-Videos-Project-Page/}{at\">Project2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present RefVFX, a new framework that transfers complex temporal effects from a reference video onto a target video or image in a feed-forward manner. While existing methods excel at prompt-based or keyframe-conditioned editing, they struggle with dynamic temporal effects such as dynamic lighting changes or character transformations, which are difficult to describe via text or static conditions. Transferring a video effect is challenging, as the model must integrate the new temporal dynamics with the input video&#x27;s existing motion and appearance. % To address this, we introduce a large-scale dataset of triplets, where each triplet consists of a reference effect video, an input image or video, and a corresponding output video depicting the transferred effect. Creating this data is non-trivial, especially the video-to-video effect triplets, which do not exist naturally. To generate these, we propose a scalable automated pipeline that creates high-quality paired videos designed to preserve the input&#x27;s motion and structure while transforming it based on some fixed, repeatable effect. We then augment this data with image-to-video effects derived from LoRA adapters and code-based temporal effects generated through programmatic composition. Building on our new dataset, we train our reference-conditioned model using recent text-to-video backbones. Experimental results demonstrate that RefVFX produces visually consistent and temporally coherent edits, generalizes across unseen effect categories, and outperforms prompt-only baselines in both quantitative metrics and human preference. See our website $\href{https://tuningfreevisualeffects-maker.github.io/Tuning-free-Visual-Effect-Transfer-across-Videos-Project-Page/}{at\ this\ URL}$.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无需调参的视频间视觉效果迁移</div>
<div class="mono" style="margin-top:8px">我们提出了RefVFX，一个能够以前馈方式将参考视频中的复杂时序效果迁移到目标视频或图像上的新框架。尽管现有方法在基于提示或关键帧条件的编辑方面表现优异，但它们在处理动态时序效果（如动态光照变化或角色转换）时存在困难，这些效果难以通过文本或静态条件描述。视频效果的迁移具有挑战性，因为模型必须将新的时序动态与输入视频已有的运动和外观进行整合。为了解决这一问题，我们引入了一个大规模的三元组数据集，其中每个三元组包含一个参考效果视频、一个输入图像或视频，以及一个展示迁移效果的输出视频。创建这些数据具有挑战性，尤其是视频到视频的效果三元组，它们在自然界中并不存在。为此，我们提出了一种可扩展的自动化流程，生成高质量的配对视频，旨在保留输入的运动和结构，同时根据某些固定、可重复的效果进行转换。我们还通过LoRA适配器和基于代码的时序效果程序化组合，对这些数据进行了扩展。基于我们新构建的数据集，我们使用最近的文本到视频主干网络训练了我们的参考条件模型。实验结果表明，RefVFX能够生成视觉一致且时间连贯的编辑效果，在未见过的效果类别上具有良好的泛化能力，并在定量指标和人类偏好上优于仅提示的基线方法。详见我们的项目页面 $\href{https://tuningfreevisualeffects-maker.github.io/Tuning-free-Visual-Effect-Transfer-across-Videos-Project-Page/}{在此链接}$。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present RefVFX, a new framework that transfers complex temporal effects from a reference video onto a target video or image in a feed-forward manner.</div>
</details>
</div>
<div class="card">
<div class="title">MHLA: Restoring Expressivity of Linear Attention via Token-Level Multi-Head</div>
<div class="meta-line">Authors: Kewei Zhang, Ye Huang, Yufan Deng, Jincheng Yu, Junsong Chen, Huan Ling, Enze Xie, Daquan Zhou</div>
<div class="meta-line">First: 2026-01-12T18:59:18+00:00 · Latest: 2026-01-12T18:59:18+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/DAGroup-PKU/MHLA/ Project website: https://dagroup-pku.github.io/MHLA/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07832v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07832v1">PDF</a> · <a href="https://github.com/DAGroup-PKU/MHLA/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://dagroup-pku.github.io/MHLA/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications. Linear attention offers an efficient alternative, but its direct application often degrades performance, with existing fixes typically re-introducing computational overhead through extra modules (e.g., depthwise separable convolution) that defeat the original purpose. In this work, we identify a key failure mode in these methods: global context collapse, where the model loses representational diversity. To address this, we propose Multi-Head Linear Attention (MHLA), which preserves this diversity by computing attention within divided heads along the token dimension. We prove that MHLA maintains linear complexity while recovering much of the expressive power of softmax attention, and verify its effectiveness across multiple domains, achieving a 3.6\% improvement on ImageNet classification, a 6.3\% gain on NLP, a 12.6\% improvement on image generation, and a 41\% enhancement on video generation under the same time complexity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MHLA: 通过Token级多头机制恢复线性注意力的表达能力</div>
<div class="mono" style="margin-top:8px">尽管Transformer架构在许多领域占据主导地位，但其二次方的自注意力复杂度阻碍了其在大规模应用中的使用。线性注意力提供了一种高效的替代方案，但其直接应用常常导致性能下降，现有的修复方法通常通过添加额外模块（如深度可分离卷积）重新引入计算开销，这与原始目标相悖。在本工作中，我们识别出这些方法中的一个关键失败模式：全局上下文崩溃，即模型丢失了表示多样性。为了解决这一问题，我们提出了多头线性注意力（MHLA），通过在token维度上划分头并计算注意力，从而保留这种多样性。我们证明了MHLA在保持线性复杂度的同时，恢复了大部分softmax注意力的表达能力，并在多个领域验证了其有效性，包括在相同时间复杂度下，ImageNet分类任务提升3.6%，NLP任务提升6.3%，图像生成任务提升12.6%，视频生成任务提升41%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">While the Transformer architecture dominates many fields, its quadratic self-attention complexity hinders its use in large-scale applications.</div>
</details>
</div>
<div class="card">
<div class="title">Optimal Learning Rate Schedule for Balancing Effort and Performance</div>
<div class="meta-line">Authors: Valentina Njaradi, Rodrigo Carrasco-Davis, Peter E. Latham, Andrew Saxe</div>
<div class="meta-line">First: 2026-01-12T18:59:07+00:00 · Latest: 2026-01-12T18:59:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07830v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07830v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning how to learn efficiently is a fundamental challenge for biological agents and a growing concern for artificial ones. To learn effectively, an agent must regulate its learning speed, balancing the benefits of rapid improvement against the costs of effort, instability, or resource use. We introduce a normative framework that formalizes this problem as an optimal control process in which the agent maximizes cumulative performance while incurring a cost of learning. From this objective, we derive a closed-form solution for the optimal learning rate, which has the form of a closed-loop controller that depends only on the agent&#x27;s current and expected future performance. Under mild assumptions, this solution generalizes across tasks and architectures and reproduces numerically optimized schedules in simulations. In simple learning models, we can mathematically analyze how agent and task parameters shape learning-rate scheduling as an open-loop control solution. Because the optimal policy depends on expectations of future performance, the framework predicts how overconfidence or underconfidence influence engagement and persistence, linking the control of learning speed to theories of self-regulated learning. We further show how a simple episodic memory mechanism can approximate the required performance expectations by recalling similar past learning experiences, providing a biologically plausible route to near-optimal behaviour. Together, these results provide a normative and biologically plausible account of learning speed control, linking self-regulated learning, effort allocation, and episodic memory estimation within a unified and tractable mathematical framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在平衡努力与表现之间实现最优学习率安排</div>
<div class="mono" style="margin-top:8px">学习如何高效学习是生物体的基本挑战，也是人工系统日益关注的问题。为了有效学习，一个智能体必须调节其学习速度，在快速提升的收益与努力、不稳定或资源消耗的成本之间取得平衡。我们引入一个规范性框架，将该问题形式化为一个最优控制过程，其中智能体在最大化累积表现的同时承担学习成本。基于这一目标，我们推导出学习率的闭式解，其形式为一个仅依赖于智能体当前和预期未来表现的闭环控制器。在温和的假设下，该解可以推广到不同任务和架构，并在模拟中再现数值优化的学习率安排。在简单的学习模型中，我们可以数学分析智能体和任务参数如何塑造学习率安排作为开环控制的解决方案。由于最优策略依赖于对未来表现的预期，该框架预测了过度自信或不足自信如何影响参与度和坚持性，将学习速度的控制与自我调节学习理论联系起来。我们进一步展示了如何通过一个简单的事件记忆机制，通过回忆相似的过往学习经验来近似所需的未来表现预期，从而提供了一条生物上合理的途径以实现近似最优行为。这些结果共同提供了一个规范性和生物上合理的学习速度控制解释，将自我调节学习、努力分配和事件记忆估计整合在一个统一且可处理的数学框架中。</div>
</details>
</div>
<div class="card">
<div class="title">Histopathology-centered Computational Evolution of Spatial Omics: Integration, Mapping, and Foundation Models</div>
<div class="meta-line">Authors: Ninghui Hao, Xinxing Yang, Boshen Yan, Dong Li, Junzhou Huang, Xintao Wu, Emily S. Ruiz, Arlene Ruiz de Luzuriaga, Chen Zhao, Guihong Wan</div>
<div class="meta-line">First: 2026-01-12T18:58:28+00:00 · Latest: 2026-01-12T18:58:28+00:00</div>
<div class="meta-line">Comments: 30 pages, 5 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07826v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07826v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial omics (SO) technologies enable spatially resolved molecular profiling, while hematoxylin and eosin (H&amp;E) imaging remains the gold standard for morphological assessment in clinical pathology. Recent computational advances increasingly place H&amp;E images at the center of SO analysis, bridging morphology with transcriptomic, proteomic, and other spatial molecular modalities, and pushing resolution toward the single-cell level. In this survey, we systematically review the computational evolution of SO from a histopathology-centered perspective and organize existing methods into three paradigms: integration, which jointly models paired multimodal data; mapping, which infers molecular profiles from H&amp;E images; and foundation models, which learn generalizable representations from large-scale spatial datasets. We analyze how the role of H&amp;E images evolves across these paradigms from spatial context to predictive anchor and ultimately to representation backbone in response to practical constraints such as limited paired data and increasing resolution demands. We further summarize actionable modeling directions enabled by current architectures and delineate persistent gaps driven by data, biology, and technology that are unlikely to be resolved by model design alone. Together, this survey provides a histopathology-centered roadmap for developing and applying computational frameworks in SO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>以组织病理学为中心的空间组学计算演化：整合、映射与基础模型</div>
<div class="mono" style="margin-top:8px">空间组学（SO）技术能够实现空间分辨的分子谱分析，而苏木精和伊红（H&amp;E）成像仍是临床病理学中形态学评估的金标准。最近的计算进展越来越多地将H&amp;E图像置于SO分析的核心位置，弥合形态学与转录组学、蛋白质组学及其他空间分子模态之间的鸿沟，并推动分辨率向单细胞级别发展。本文系统地从组织病理学视角回顾了空间组学的计算演化，并将现有方法归纳为三种范式：整合，即联合建模配对的多模态数据；映射，即从H&amp;E图像推断分子谱；以及基础模型，即从大规模空间数据集中学习可泛化的表示。我们分析了H&amp;E图像在这些范式中的角色如何从空间上下文演变为预测锚点，最终成为表示主干，以应对诸如配对数据有限和分辨率需求增加等实际约束。此外，我们还总结了当前架构所支持的可操作建模方向，并界定了由数据、生物学和技术创新所驱动的持续存在的研究空白，这些空白无法仅通过模型设计来解决。综上，本文为在空间组学中开发和应用计算框架提供了一个以组织病理学为中心的路线图。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Spatial omics (SO) technologies enable spatially resolved molecular profiling, while hematoxylin and eosin (H&amp;E) imaging remains the gold standard for morphological assessment in clinical pathology.</div>
</details>
</div>
<div class="card">
<div class="title">Mechanical Resonator-based Quantum Computing</div>
<div class="meta-line">Authors: Yu Yang, Igor Kladaric, Martynas Skrabulis, Michael Eichenberger, Stefano Marti, Simon Storz, Jonathan Esche, Raquel Garcia Belles, Max-Emanuel Kern, Andraz Omahen, Arianne Brooks, Marius Bild, Mateo Fadel, Yiwen Chu</div>
<div class="meta-line">First: 2026-01-12T18:58:18+00:00 · Latest: 2026-01-12T18:58:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07825v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07825v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hybrid quantum systems combine the unique advantages of different physical platforms with the goal of realizing more powerful and practical quantum information processing devices. Mechanical systems, such as bulk acoustic wave resonators, feature a large number of highly coherent harmonic modes in a compact footprint, which complements the strong nonlinearities and fast operation times of superconducting quantum circuits. Here, we demonstrate an architecture for mechanical resonator-based quantum computing, in which a superconducting qubit is used to perform quantum gates on a collection of mechanical modes. We show the implementation of a universal gate set, composed of single-qubit gates and controlled arbitrary-phase gates, and showcase their use in the quantum Fourier transform and quantum period finding algorithms. These results pave the way toward using mechanical systems to build crucial components for future quantum technologies, such as quantum random-access memories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于机械谐振器的量子计算</div>
<div class="mono" style="margin-top:8px">混合量子系统结合了不同物理平台的独特优势，旨在实现更强大且实用的量子信息处理设备。机械系统，如体声波谐振器，具有在紧凑结构中实现大量高相干性谐振模式的特性，这与超导量子电路的强非线性和快速操作时间相辅相成。在此，我们展示了一种基于机械谐振器的量子计算架构，其中使用超导量子比特对一组机械模式执行量子门操作。我们实现了由单量子比特门和受控任意相位门组成的通用门集，并展示了它们在量子傅里叶变换和量子周期查找算法中的应用。这些成果为利用机械系统构建未来量子技术的关键组件（如量子随机存取存储器）铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Hybrid quantum systems combine the unique advantages of different physical platforms with the goal of realizing more powerful and practical quantum information processing devices.</div>
</details>
</div>
<div class="card">
<div class="title">Failure-Aware RL: Reliable Offline-to-Online Reinforcement Learning with Self-Recovery for Real-World Manipulation</div>
<div class="meta-line">Authors: Huanyu Li, Kun Lei, Sheng Zang, Kaizhe Hu, Yongyuan Liang, Bo An, Xiaoli Li, Huazhe Xu</div>
<div class="meta-line">First: 2026-01-12T18:53:11+00:00 · Latest: 2026-01-12T18:53:11+00:00</div>
<div class="meta-line">Comments: Project page: https://failure-aware-rl.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07821v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07821v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://failure-aware-rl.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-training algorithms based on deep reinforcement learning can push the limits of robotic models for specific objectives, such as generalizability, accuracy, and robustness. However, Intervention-requiring Failures (IR Failures) (e.g., a robot spilling water or breaking fragile glass) during real-world exploration happen inevitably, hindering the practical deployment of such a paradigm. To tackle this, we introduce Failure-Aware Offline-to-Online Reinforcement Learning (FARL), a new paradigm minimizing failures during real-world reinforcement learning. We create FailureBench, a benchmark that incorporates common failure scenarios requiring human intervention, and propose an algorithm that integrates a world-model-based safety critic and a recovery policy trained offline to prevent failures during online exploration. Extensive simulation and real-world experiments demonstrate the effectiveness of FARL in significantly reducing IR Failures while improving performance and generalization during online reinforcement learning post-training. FARL reduces IR Failures by 73.1% while elevating performance by 11.3% on average during real-world RL post-training. Videos and code are available at https://failure-aware-rl.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>故障感知强化学习：一种面向现实世界操作的可靠离线到在线强化学习范式与自恢复机制</div>
<div class="mono" style="margin-top:8px">基于深度强化学习的后训练算法可以推动机器人模型在特定目标（如泛化能力、精度和鲁棒性）上的性能极限。然而，在现实世界探索过程中，不可避免地会发生需要人工干预的故障（IR故障），例如机器人打翻水或打碎易碎玻璃，这阻碍了该范式的实际部署。为了解决这一问题，我们引入了一种新的范式——故障感知离线到在线强化学习（FARL），旨在减少现实世界强化学习过程中的故障。我们构建了FailureBench基准，该基准包含需要人工干预的常见故障场景，并提出了一种算法，该算法结合了基于世界模型的安全批评者和一个离线训练的恢复策略，以防止在线探索过程中发生故障。大量仿真和现实世界实验验证了FARL在显著减少IR故障的同时，提升了在线强化学习的性能和泛化能力。在现实世界强化学习后训练中，FARL将IR故障减少了73.1%，平均性能提升了11.3%。相关视频和代码可在https://failure-aware-rl.github.io获取。</div>
</details>
</div>
<div class="card">
<div class="title">Reference Games as a Testbed for the Alignment of Model Uncertainty and Clarification Requests</div>
<div class="meta-line">Authors: Manar Ali, Judith Sieker, Sina Zarrieß, Hendrik Buschmeier</div>
<div class="meta-line">First: 2026-01-12T18:53:09+00:00 · Latest: 2026-01-12T18:53:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07820v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07820v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In human conversation, both interlocutors play an active role in maintaining mutual understanding. When addressees are uncertain about what speakers mean, for example, they can request clarification. It is an open question for language models whether they can assume a similar addressee role, recognizing and expressing their own uncertainty through clarification. We argue that reference games are a good testbed to approach this question as they are controlled, self-contained, and make clarification needs explicit and measurable. To test this, we evaluate three vision-language models comparing a baseline reference resolution task to an experiment where the models are instructed to request clarification when uncertain. The results suggest that even in such simple tasks, models often struggle to recognize internal uncertainty and translate it into adequate clarification behavior. This demonstrates the value of reference games as testbeds for interaction qualities of (vision and) language models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>参考游戏作为模型不确定性对齐与澄清请求的测试平台</div>
<div class="mono" style="margin-top:8px">在人类对话中，双方都积极地参与维持相互理解。当接收者不确定说话者的意思时，例如，他们可以请求澄清。语言模型是否能够承担类似的接收者角色，通过澄清来识别和表达自身的不确定性，仍是一个开放性问题。我们认为参考游戏是探讨这一问题的良好测试平台，因为它们是受控的、自包含的，并且能够明确且可衡量地体现澄清需求。为此，我们评估了三种视觉-语言模型，将基线参考解析任务与模型在不确定时被指示请求澄清的实验进行对比。结果表明，即使在如此简单的任务中，模型往往难以识别内部的不确定性，并将其转化为适当的澄清行为。这展示了参考游戏作为（视觉和）语言模型交互质量测试平台的价值。</div>
</details>
</div>
<div class="card">
<div class="title">Learning the Value of Value Learning</div>
<div class="meta-line">Authors: Alex John London, Aydin Mohseni</div>
<div class="meta-line">First: 2025-11-21T19:06:30+00:00 · Latest: 2026-01-12T18:50:10+00:00</div>
<div class="meta-line">Comments: 19 pages, 6 figures, mathematical appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.17714v4">Abs</a> · <a href="https://arxiv.org/pdf/2511.17714v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Standard decision frameworks address uncertainty about facts but assume fixed options and values. We extend the Jeffrey-Bolker framework to model refinements in values and prove a value-of-information theorem for axiological refinement. In multi-agent settings, we establish that mutual refinement will characteristically transform zero-sum games into positive-sum interactions and yield Pareto-improvements in Nash bargaining. These results show that a framework of rational choice can be extended to model value refinement. By unifying epistemic and axiological refinement under a single formalism, we broaden the conceptual foundations of rational choice and illuminate the normative status of ethical deliberation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习价值学习的价值</div>
<div class="mono" style="margin-top:8px">标准决策框架处理对事实的不确定性，但假设选项和价值是固定的。我们扩展了Jeffrey-Bolker框架以建模价值的细化，并证明了对价值细化的信息价值定理。在多智能体环境中，我们确立了相互细化通常会将零和博弈转化为正和互动，并在纳什讨价还价中产生帕累托改进。这些结果表明，理性选择框架可以扩展以建模价值细化。通过在单一形式化框架下统一认识论和价值论的细化，我们拓宽了理性选择的概念基础，并阐明了伦理思考的规范地位。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Standard decision frameworks address uncertainty about facts but assume fixed options and values.</div>
</details>
</div>
<div class="card">
<div class="title">CLAPS: Posterior-Aware Conformal Intervals via Last-Layer Laplace</div>
<div class="meta-line">Authors: Dongseok Kim, Hyoungsun Choi, Mohamed Jismy Aashik Rasool, Gisung Oh</div>
<div class="meta-line">First: 2025-12-01T07:58:21+00:00 · Latest: 2026-01-12T18:49:06+00:00</div>
<div class="meta-line">Comments: Revised for clarity and correctness; improved exposition and fixed minor issues</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.01384v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.01384v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present CLAPS, a posterior-aware conformal regression method that pairs a Last-Layer Laplace Approximation with split-conformal calibration. From the resulting Gaussian posterior, CLAPS defines a simple two-sided posterior CDF score that aligns the conformity metric with the full predictive shape, not just a point estimate. This alignment can yield substantially narrower prediction intervals at a fixed target coverage, particularly on small to medium tabular datasets where data are scarce and uncertainty modeling is informative. We also provide a lightweight diagnostic suite that separates aleatoric and epistemic components and visualizes posterior behavior, helping practitioners assess when and why intervals shrink. Across multiple benchmarks using the same MLP backbone, CLAPS achieves nominal coverage and offers the most efficient intervals on small to medium datasets with mild heterogeneity, while remaining competitive and diagnostically transparent on large-scale heterogeneous data where Normalized-CP and CQR attain the tightest intervals.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CLAPS：通过最后一层拉普拉斯方法实现后验感知的置信区间</div>
<div class="mono" style="margin-top:8px">我们提出了CLAPS，这是一种后验感知的符合回归方法，它将最后一层拉普拉斯近似与分割符合校准相结合。从得到的高斯后验分布中，CLAPS定义了一个简单的双侧后验CDF得分，使符合度度量与完整的预测形状对齐，而不仅仅是点估计。这种对齐可以在固定目标覆盖率下显著缩小预测区间，尤其是在数据稀缺的小到中等表格数据集上，不确定性建模具有重要意义。我们还提供了一套轻量级的诊断工具，用于分离随机性和认识性不确定性，并可视化后验行为，帮助从业者评估区间为何以及何时缩小。在使用相同MLP主干的多个基准测试中，CLAPS实现了名义覆盖率，并在具有轻微异质性的中小数据集上提供了最高效的区间，同时在大规模异质数据上保持竞争力和诊断透明性，其中Normalized-CP和CQR达到最紧的区间。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present CLAPS, a posterior-aware conformal regression method that pairs a Last-Layer Laplace Approximation with split-conformal calibration.</div>
</details>
</div>
<div class="card">
<div class="title">EEG-to-fMRI synthesis of task-evoked and spontaneous brain activity: addressing issues of statistical significance and generalizability</div>
<div class="meta-line">Authors: Neil Mehta, Ines Goncalves, Alberto Montagna, Mathis Fleury, Gustavo Caetano, Ines Esteves, Athanasios Vourvopoulos, Pulkit Grover, Patricia Figueiredo</div>
<div class="meta-line">First: 2025-04-14T22:54:41+00:00 · Latest: 2026-01-12T18:47:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.10752v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.10752v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A growing interest has developed in the problem of training models of EEG features to predict brain activity measured using fMRI, i.e. the problem of EEG-to-fMRI synthesis. Despite some reported success, the statistical significance and generalizability of EEG-to-fMRI predictions remains to be fully demonstrated. Here, we investigate the predictive power of EEG for both task-evoked and spontaneous activity of the somatomotor network measured by fMRI, based on data collected from healthy subjects in two different sessions. We trained subject-specific distributed-lag linear models of time-varying, multi-channel EEG spectral power using Sparse Group LASSO regularization, and we showed that learned models outperformed conventional EEG somatomotor rhythm predictors as well as massive univariate correlation models. Furthermore, we showed that learned models were statistically significantly better than appropriate null models in most subjects and conditions, although less frequently for spontaneous compared to task-evoked activity. Critically, predictions improved significantly when training and testing on data acquired in the same session relative to across sessions, highlighting the importance of temporally separating the collection of train and test data to avoid data leakage and optimistic bias in model generalization. In sum, while we demonstrate that EEG models can provide fMRI predictions with statistical significance, we also show that predictive power is impaired for spontaneous fluctuations in brain activity and for models trained on data acquired in a different session. Our findings highlight the need to explicitly consider these often overlooked issues in the growing literature of EEG-to-fMRI synthesis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EEG到fMRI的合成：任务诱发和自发脑活动的预测，解决统计显著性和泛化性问题</div>
<div class="mono" style="margin-top:8px">人们对训练EEG特征模型以预测通过fMRI测量的脑活动问题（即EEG到fMRI合成问题）产生了日益浓厚的兴趣。尽管已有部分成功报告，但EEG到fMRI预测的统计显著性和泛化性仍需进一步验证。本文基于健康受试者在两个不同会话中收集的数据，研究了EEG对fMRI测量的躯体运动网络任务诱发和自发活动的预测能力。我们使用稀疏组LASSO正则化训练了受试者特异性的时变多通道EEG频谱功率的分布式滞后线性模型，并展示了所学模型在预测性能上优于传统的EEG躯体运动节律预测器以及大规模单变量相关模型。此外，我们还表明，所学模型在大多数受试者和条件下显著优于适当的零模型，尽管自发活动的预测效果不如任务诱发活动频繁。关键的是，当训练和测试数据来自同一会话时，预测性能显著提高，这突显了在模型泛化过程中，将训练和测试数据采集时间分离的重要性，以避免数据泄露和乐观偏差。总之，虽然我们证明了EEG模型可以提供具有统计显著性的fMRI预测，但也表明自发脑活动波动和跨会话数据训练的模型在预测能力上存在缺陷。我们的研究结果强调了在EEG到fMRI合成的日益增长的文献中，需要明确考虑这些常被忽视的问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">A growing interest has developed in the problem of training models of EEG features to predict brain activity measured using fMRI, i.e.</div>
</details>
</div>
<div class="card">
<div class="title">The Power of Iterative Filtering for Supervised Learning with (Heavy) Contamination</div>
<div class="meta-line">Authors: Adam R. Klivans, Konstantinos Stavropoulos, Kevin Tian, Arsen Vasilyan</div>
<div class="meta-line">First: 2025-05-26T16:17:48+00:00 · Latest: 2026-01-12T18:47:51+00:00</div>
<div class="meta-line">Comments: 36 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.20177v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.20177v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inspired by recent work on learning with distribution shift, we give a general outlier removal algorithm called iterative polynomial filtering and show a number of striking applications for supervised learning with contamination: (1) We show that any function class that can be approximated by low-degree polynomials with respect to a hypercontractive distribution can be efficiently learned under bounded contamination (also known as nasty noise). This is a surprising resolution to a longstanding gap between the complexity of agnostic learning and learning with contamination, as it was widely believed that low-degree approximators only implied tolerance to label noise. In particular, it implies the first efficient algorithm for learning halfspaces with $η$-bounded contamination up to error $2η+ε$ with respect to the Gaussian distribution. (2) For any function class that admits the (stronger) notion of sandwiching approximators, we obtain near-optimal learning guarantees even with respect to heavy additive contamination, where far more than $1/2$ of the training set may be added adversarially. Prior related work held only for regression and in a list-decodable setting. (3) We obtain the first efficient algorithms for tolerant testable learning of functions of halfspaces with respect to any fixed log-concave distribution. Even the non-tolerant case for a single halfspace in this setting had remained open. These results significantly advance our understanding of efficient supervised learning under contamination, a setting that has been much less studied than its unsupervised counterpart.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迭代过滤在有（严重）污染的监督学习中的力量</div>
<div class="mono" style="margin-top:8px">受近期关于分布偏移学习工作的启发，我们提出了一种通用的异常值去除算法，称为迭代多项式过滤，并展示了其在有污染的监督学习中的多个显著应用：(1) 我们证明，任何可以相对于超收缩分布用低次多项式近似的函数类，都可以在有界污染（也称为恶劣噪声）下高效学习。这解决了长期存在的一个难题，即在无监督学习与有污染学习之间的复杂度差距，因为此前普遍认为低次近似器只能容忍标签噪声。特别地，这表明了在高斯分布下，第一个针对η-有界污染的半空间学习的高效算法，其误差可达到2η+ε。 (2) 对于任何允许（更强的）夹逼近似器概念的函数类，即使在严重加性污染下，我们也能获得接近最优的学习保证，其中超过1/2的训练集可能被敌对地添加。此前的相关工作仅适用于回归问题，并且是在列表可解码的设定下。 (3) 我们获得了第一个针对任何固定对数凹分布下半空间函数的容忍可测试学习的高效算法。即使在该设定下单个半空间的非容忍情况也一直未被解决。这些结果显著推进了我们对在污染环境下高效监督学习的理解，而这一设定相比其无监督对应问题研究得较少。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Inspired by recent work on learning with distribution shift, we give a general outlier removal algorithm called iterative polynomial filtering and show a number of striking applications for supervised learning with contamination: (1) We show that any function class that can be approximated by low-degree polynomials with respect to a hypercontractive distribution can be efficiently learned under bounded contamination (also known as nasty noise).</div>
</details>
</div>
<div class="card">
<div class="title">ORACLE: Explaining Feature Interactions in Neural Networks with ANOVA</div>
<div class="meta-line">Authors: Dongseok Kim, Hyoungsun Choi, Mohamed Jismy Aashik Rasool, Gisung Oh</div>
<div class="meta-line">First: 2025-09-13T14:44:45+00:00 · Latest: 2026-01-12T18:46:44+00:00</div>
<div class="meta-line">Comments: v4: Revised for clarity and correctness; improved exposition and fixed minor issues</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.10825v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.10825v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce ORACLE, a framework for explaining neural networks on tabular data and scientific factorial designs. ORACLE summarizes a trained network&#x27;s prediction surface with main effects and pairwise interactions by treating the network as a black-box response, discretizing the inputs onto a grid, and fitting an orthogonal factorial (ANOVA-style) surrogate -- the $L^2$ orthogonal projection of the model response onto a finite-dimensional factorial subspace. A simple centering and $μ$-rebalancing step then expresses this surrogate as main- and interaction-effect tables that remain faithful to the original model in the $L^2$ sense. The resulting grid-based interaction maps are easy to visualize, comparable across backbones, and directly aligned with classical design-of-experiments practice. On synthetic factorial benchmarks and low- to medium-dimensional tabular regression tasks, ORACLE more accurately recovers ground-truth interaction structure and hotspots than Monte Carlo SHAP-family interaction methods, as measured by ranking, localization, and cross-backbone stability. We also discuss its scope in latent image and text settings: grid-based factorial surrogates are most effective when features admit an interpretable factorial structure, making ORACLE particularly well-suited to scientific and engineering workflows that require stable DoE-style interaction summaries.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ORACLE：使用方差分析解释神经网络的特征交互</div>
<div class="mono" style="margin-top:8px">我们介绍了ORACLE，一个用于解释表格数据和科学因子设计的神经网络框架。ORACLE通过将网络视为黑盒响应，将输入离散化到网格上，并拟合一个正交因子（ANOVA风格）的替代模型——即模型响应在有限维因子子空间上的$L^2$正交投影，来总结训练后的网络的预测表面。随后通过简单的中心化和$μ$-再平衡步骤，将该替代模型表示为保持$L^2$意义下的主效应和交互效应表格。由此得到的基于网格的交互图易于可视化，可以在不同主干模型之间进行比较，并且与经典实验设计实践直接对齐。在合成因子基准和低至中维表格回归任务中，ORACLE在排名、定位和跨主干稳定性方面，比蒙特卡洛SHAP家族的交互方法更准确地恢复真实交互结构和热点。我们还讨论了其在潜在图像和文本设置中的适用范围：当特征具有可解释的因子结构时，基于网格的因子替代模型效果最佳，这使得ORACLE特别适合需要稳定DoE风格交互总结的科学和工程工作流程。</div>
</details>
</div>
<div class="card">
<div class="title">More Images, More Problems? A Controlled Analysis of VLM Failure Modes</div>
<div class="meta-line">Authors: Anurag Das, Adrian Bulat, Alberto Baldrati, Ioannis Maniadis Metaxas, Bernt Schiele, Georgios Tzimiropoulos, Brais Martinez</div>
<div class="meta-line">First: 2026-01-12T18:45:13+00:00 · Latest: 2026-01-12T18:45:13+00:00</div>
<div class="meta-line">Comments: 19 pages, 16 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07812v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07812v1">PDF</a> · <a href="https://github.com/anurag-198/MIMIC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision Language Models (LVLMs) have demonstrated remarkable capabilities, yet their proficiency in understanding and reasoning over multiple images remains largely unexplored. While existing benchmarks have initiated the evaluation of multi-image models, a comprehensive analysis of their core weaknesses and their causes is still lacking. In this work, we introduce MIMIC (Multi-Image Model Insights and Challenges), a new benchmark designed to rigorously evaluate the multi-image capabilities of LVLMs. Using MIMIC, we conduct a series of diagnostic experiments that reveal pervasive issues: LVLMs often fail to aggregate information across images and struggle to track or attend to multiple concepts simultaneously. To address these failures, we propose two novel complementary remedies. On the data side, we present a procedural data-generation strategy that composes single-image annotations into rich, targeted multi-image training examples. On the optimization side, we analyze layer-wise attention patterns and derive an attention-masking scheme tailored for multi-image inputs. Experiments substantially improved cross-image aggregation, while also enhancing performance on existing multi-image benchmarks, outperforming prior state of the art across tasks. Data and code will be made available at https://github.com/anurag-198/MIMIC.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>更多图像，更多问题？LVLM故障模式的控制分析</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（LVLMs）展现了显著的能力，但它们在理解和推理多张图像方面的熟练程度仍缺乏深入研究。尽管现有基准测试已开始评估多图像模型，但对其核心弱点及其成因的全面分析仍不足。在本工作中，我们引入了MIMIC（多图像模型洞察与挑战），一个旨在严格评估LVLM多图像能力的新基准测试。通过MIMIC，我们进行了一系列诊断实验，揭示了普遍存在的问题：LVLMs常常无法跨图像整合信息，并且难以同时跟踪或关注多个概念。为了解决这些问题，我们提出了两种新颖的互补解决方案。在数据方面，我们提出了一种程序化数据生成策略，将单图像注释组合成丰富且有针对性的多图像训练示例。在优化方面，我们分析了逐层注意力模式，并推导出一种专为多图像输入设计的注意力掩码方案。实验结果显著提升了跨图像信息整合能力，同时在现有多图像基准测试中也提高了性能，优于以往的最先进方法。数据和代码将在https://github.com/anurag-198/MIMIC上公开。</div>
</details>
</div>
<div class="card">
<div class="title">MixtureVitae: Open Web-Scale Pretraining Dataset With High Quality Instruction and Reasoning Data Built from Permissive-First Text Sources</div>
<div class="meta-line">Authors: Huu Nguyen, Victor May, Harsh Raj, Marianna Nezhurina, Yishan Wang, Yanqi Luo, Minh Chien Vu, Taishi Nakamura, Ken Tsui, Van Khue Nguyen, David Salinas, Aleksandra Krasnodębska, Christoph Schuhmann, Mats Leon Richter, Xuan-Son, Vu, Jenia Jitsev</div>
<div class="meta-line">First: 2025-09-29T21:40:10+00:00 · Latest: 2026-01-12T18:44:30+00:00</div>
<div class="meta-line">Comments: Code: \url{https://github.com/ontocord/mixturevitae}</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.25531v5">Abs</a> · <a href="https://arxiv.org/pdf/2509.25531v5">PDF</a> · <a href="https://github.com/ontocord/mixturevitae">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong downstream performance. MixtureVitae follows a permissive-first, risk-mitigated sourcing strategy that combines public-domain and permissively licensed text (e.g., CC-BY/Apache) with carefully justified low-risk additions (e.g., government works and EU TDM-eligible sources). MixtureVitae adopts a simple, single-stage pretraining recipe that integrates a large proportion of permissive synthetic instruction and reasoning data-signals typically introduced during post-training and generally scarce in permissive web corpora. We categorize all sources into a three-tier scheme that reflects varying risk levels and provide shard-level provenance metadata to enable risk-aware usage. In controlled experiments using the open-sci-ref training protocol (fixed architectures and hyperparameters; 50B and 300B token budgets across 130M-1.7B parameters), models trained on MixtureVitae consistently outperform other permissive datasets across a suite of standard benchmarks, and at the 1.7B-parameters/300B-tokens setting, they surpass FineWeb-Edu and approach DCLM late in training. Performance is particularly strong on MMLU and on math and code benchmarks: a 1.7B model pretrained on 300B MixtureVitae tokens matches or exceeds a strong 1.7B instruction-tuned baseline on GSM8K, HumanEval, and MBPP, despite using over 36 times fewer tokens (300B vs. ~11T). Supported by a thorough decontamination analysis, these results show that permissive-first data with high instruction and reasoning density, tiered by licensing and provenance-related risk, can provide a practical and risk-mitigated foundation for training capable LLMs, reducing reliance on broad web scrapes without sacrificing competitiveness. Code: https://github.com/ontocord/mixturevitae</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MixtureVitae：基于宽松许可文本来源构建的高质量指令和推理数据开放网络规模预训练语料库</div>
<div class="mono" style="margin-top:8px">我们提出了MixtureVitae，这是一个开放获取的预训练语料库，旨在最小化法律风险同时提供强大的下游性能。MixtureVitae采用以宽松许可为优先、风险缓解的来源策略，结合公共领域和宽松许可文本（如CC-BY/Apache）以及经过合理论证的低风险补充内容（如政府作品和欧盟TDM合规来源）。MixtureVitae采用简单、单阶段的预训练方案，整合了大量宽松许可的合成指令和推理数据，这些数据通常在微调阶段引入，而在宽松许可网络语料库中通常较为稀缺。我们将所有来源分为三级体系，反映不同风险等级，并提供分片级别的来源元数据，以支持风险感知的使用。在使用开放sci-ref训练协议的受控实验中（固定架构和超参数；130M-1.7B参数下50B和300B token预算），基于MixtureVitae训练的模型在一系列标准基准测试中持续优于其他宽松许可数据集，并在1.7B参数/300B token设置下超越FineWeb-Edu，接近DCLM的训练后期表现。在MMLU以及数学和代码基准测试中表现尤为突出：一个在300B MixtureVitae token上预训练的1.7B模型，在GSM8K、HumanEval和MBPP等强大指令调优基线模型上表现匹配或更优，尽管其使用的token数量仅为后者（约11T）的约36分之一。这些结果得到了详尽的去污染分析支持，表明以宽松许可为优先的数据，结合高密度的指令和推理内容，并按许可和来源相关风险分级，可以为训练有竞争力的LLM提供实用且风险可控的基础，而无需依赖广泛的网络爬取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present MixtureVitae, an open-access pretraining corpus built to minimize legal risk while providing strong downstream performance.</div>
</details>
</div>
<div class="card">
<div class="title">The Confidence Trap: Gender Bias and Predictive Certainty in LLMs</div>
<div class="meta-line">Authors: Ahmed Sabir, Markus Kängsepp, Rajesh Sharma</div>
<div class="meta-line">Venue: AAAI 2026 Oral</div>
<div class="meta-line">First: 2026-01-12T18:38:05+00:00 · Latest: 2026-01-12T18:38:05+00:00</div>
<div class="meta-line">Comments: AAAI 2026 (AISI Track), Oral. Project page: https://bit.ly/4p8OKQD</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07806v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07806v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The increased use of Large Language Models (LLMs) in sensitive domains leads to growing interest in how their confidence scores correspond to fairness and bias. This study examines the alignment between LLM-predicted confidence and human-annotated bias judgments. Focusing on gender bias, the research investigates probability confidence calibration in contexts involving gendered pronoun resolution. The goal is to evaluate if calibration metrics based on predicted confidence scores effectively capture fairness-related disparities in LLMs. The results show that, among the six state-of-the-art models, Gemma-2 demonstrates the worst calibration according to the gender bias benchmark. The primary contribution of this work is a fairness-aware evaluation of LLMs&#x27; confidence calibration, offering guidance for ethical deployment. In addition, we introduce a new calibration metric, Gender-ECE, designed to measure gender disparities in resolution tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>信心陷阱：LLMs中的性别偏见与预测确定性</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在敏感领域中的使用增加，引发了对其信心评分与公平性和偏见之间关系的兴趣。本研究探讨了LLM预测的信心与人类标注的偏见判断之间的对齐情况。重点研究性别偏见，分析涉及性别代词解析情境中的概率信心校准。目标是评估基于预测信心评分的校准指标是否能有效捕捉LLMs中的公平性相关差异。结果表明，在六种最先进的模型中，Gemma-2在性别偏见基准测试中表现出最差的校准效果。本工作的主要贡献是提出一种公平性意识的LLMs信心校准评估方法，为伦理部署提供指导。此外，我们引入了一种新的校准指标Gender-ECE，用于衡量解析任务中的性别差异。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increased use of Large Language Models (LLMs) in sensitive domains leads to growing interest in how their confidence scores correspond to fairness and bias.</div>
</details>
</div>
<div class="card">
<div class="title">Exchange Is All You Need for Remote Sensing Change Detection</div>
<div class="meta-line">Authors: Sijun Dong, Siming Fu, Kaiyu Li, Xiangyong Cao, Xiaoliang Meng, Bo Du</div>
<div class="meta-line">First: 2026-01-12T18:36:51+00:00 · Latest: 2026-01-12T18:36:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07805v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07805v1">PDF</a> · <a href="https://github.com/dyzy41/open-rscd">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Remote sensing change detection fundamentally relies on the effective fusion and discrimination of bi-temporal features. Prevailing paradigms typically utilize Siamese encoders bridged by explicit difference computation modules, such as subtraction or concatenation, to identify changes. In this work, we challenge this complexity with SEED (Siamese Encoder-Exchange-Decoder), a streamlined paradigm that replaces explicit differencing with parameter-free feature exchange. By sharing weights across both Siamese encoders and decoders, SEED effectively operates as a single parameter set model. Theoretically, we formalize feature exchange as an orthogonal permutation operator and prove that, under pixel consistency, this mechanism preserves mutual information and Bayes optimal risk, whereas common arithmetic fusion methods often introduce information loss. Extensive experiments across five benchmarks, including SYSU-CD, LEVIR-CD, PX-CLCD, WaterCD, and CDD, and three backbones, namely SwinT, EfficientNet, and ResNet, demonstrate that SEED matches or surpasses state of the art methods despite its simplicity. Furthermore, we reveal that standard semantic segmentation models can be transformed into competitive change detectors solely by inserting this exchange mechanism, referred to as SEG2CD. The proposed paradigm offers a robust, unified, and interpretable framework for change detection, demonstrating that simple feature exchange is sufficient for high performance information fusion. Code and full training and evaluation protocols will be released at https://github.com/dyzy41/open-rscd.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>交换即一切：遥感变化检测</div>
<div class="mono" style="margin-top:8px">遥感变化检测本质上依赖于双时相特征的有效融合与区分。主流范式通常通过显式的差异计算模块（如减法或拼接）连接双Siamese编码器来识别变化。在本工作中，我们提出SEED（Siamese编码器-交换-解码器）这一简化范式，用无参数的特征交换替代显式差异计算。通过在双Siamese编码器和解码器之间共享权重，SEED实际上等效于一个单一参数集模型。理论上，我们将特征交换形式化为正交排列算子，并证明在像素一致性条件下，该机制能够保持互信息和贝叶斯最优风险，而常见的算术融合方法往往引入信息损失。在五个基准数据集（包括SYSU-CD、LEVIR-CD、PX-CLCD、WaterCD和CDD）以及三个主干网络（SwinT、EfficientNet和ResNet）上的大量实验表明，尽管SEED结构简单，其性能仍可匹配或超越当前最先进的方法。此外，我们发现只需插入该交换机制，标准的语义分割模型即可转化为具有竞争力的变化检测器，这一方法称为SEG2CD。所提出的范式为变化检测提供了一个鲁棒、统一且可解释的框架，证明了简单的特征交换足以实现高性能的信息融合。代码和完整的训练与评估协议将在https://github.com/dyzy41/open-rscd上发布。</div>
</details>
</div>
<div class="card">
<div class="title">Discovering Coordinated Joint Options via Inter-Agent Relative Dynamics</div>
<div class="meta-line">Authors: Raul D. Steleac, Mohan Sridharan, David Abel</div>
<div class="meta-line">First: 2025-12-31T12:39:22+00:00 · Latest: 2026-01-12T18:29:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24827v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.24827v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Temporally extended actions improve the ability to explore and plan in single-agent settings. In multi-agent settings, the exponential growth of the joint state space with the number of agents makes coordinated behaviours even more valuable. Yet, this same exponential growth renders the design of multi-agent options particularly challenging. Existing multi-agent option discovery methods often sacrifice coordination by producing loosely coupled or fully independent behaviours. Toward addressing these limitations, we describe a novel approach for multi-agent option discovery. Specifically, we propose a joint-state abstraction that compresses the state space while preserving the information necessary to discover strongly coordinated behaviours. Our approach builds on the inductive bias that synchronisation over agent states provides a natural foundation for coordination in the absence of explicit objectives. We first approximate a fictitious state of maximal alignment with the team, the \textit{Fermat} state, and use it to define a measure of \textit{spreadness}, capturing team-level misalignment on each individual state dimension. Building on this representation, we then employ a neural graph Laplacian estimator to derive options that capture state synchronisation patterns between agents. We evaluate the resulting options across multiple scenarios in two multi-agent domains, showing that they yield stronger downstream coordination capabilities compared to alternative option discovery methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过智能体间相对动态发现协调联合选项</div>
<div class="mono" style="margin-top:8px">时间扩展动作提高了单智能体环境中的探索和规划能力。在多智能体环境中，随着智能体数量的增加，联合状态空间呈指数增长，这使得协调行为更加有价值。然而，这种指数增长也使得多智能体选项的设计尤为具有挑战性。现有的多智能体选项发现方法通常通过生成松散耦合或完全独立的行为来牺牲协调性。为了解决这些限制，我们提出了一种新颖的多智能体选项发现方法。具体而言，我们提出了一种联合状态抽象方法，该方法在压缩状态空间的同时保留了发现强协调行为所需的信息。我们的方法基于这样一个归纳偏置：在没有显式目标的情况下，智能体状态的同步为协调提供了自然的基础。我们首先近似一个与团队最大对齐的假想状态，即\textit{Fermat}状态，并利用它定义一种\textit{spreadness}度量，捕捉每个个体状态维度上的团队级错位。在此表示基础上，我们进一步采用神经图拉普拉斯估计器来推导能够捕捉智能体间状态同步模式的选项。我们在两个多智能体领域中的多个场景中评估了这些选项，结果表明它们相比其他选项发现方法能够产生更强的下游协调能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Temporally extended actions improve the ability to explore and plan in single-agent settings.</div>
</details>
</div>
<div class="card">
<div class="title">StarFlow: Generating Structured Workflow Outputs From Sketch Images</div>
<div class="meta-line">Authors: Patrice Bechard, Chao Wang, Amirhossein Abaskohi, Juan Rodriguez, Christopher Pal, David Vazquez, Spandana Gella, Sai Rajeswar, Perouz Taslakian</div>
<div class="meta-line">First: 2025-03-27T18:04:05+00:00 · Latest: 2026-01-12T18:27:42+00:00</div>
<div class="meta-line">Comments: To be presented at EACL2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.21889v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.21889v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Workflows are a fundamental component of automation in enterprise platforms, enabling the orchestration of tasks, data processing, and system integrations. Despite being widely used, building workflows can be complex, often requiring manual configuration through low-code platforms or visual programming tools. To simplify this process, we explore the use of generative foundation models, particularly vision-language models (VLMs), to automatically generate structured workflows from visual inputs. Translating hand-drawn sketches or computer-generated diagrams into executable workflows is challenging due to the ambiguity of free-form drawings, variations in diagram styles, and the difficulty of inferring execution logic from visual elements. To address this, we introduce StarFlow, a framework for generating structured workflow outputs from sketches using vision-language models. We curate a diverse dataset of workflow diagrams -- including synthetic, manually annotated, and real-world samples -- to enable robust training and evaluation. We finetune and benchmark multiple vision-language models, conducting a series of ablation studies to analyze the strengths and limitations of our approach. Our results show that finetuning significantly enhances structured workflow generation, outperforming large vision-language models on this task.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StarFlow：从草图图像生成结构化工作流输出</div>
<div class="mono" style="margin-top:8px">工作流是企业平台自动化中的基本组成部分，能够实现任务调度、数据处理和系统集成。尽管广泛使用，但构建工作流通常较为复杂，往往需要通过低代码平台或可视化编程工具手动配置。为简化这一过程，我们探索使用生成式基础模型，特别是视觉-语言模型（VLMs），从视觉输入自动生成结构化工作流。将手绘草图或计算机生成的图表转换为可执行工作流具有挑战性，因为自由形式的绘制存在歧义，图表风格存在差异，且从视觉元素中推断执行逻辑较为困难。为解决这些问题，我们引入StarFlow，这是一个利用视觉-语言模型从草图生成结构化工作流输出的框架。我们整理了一个包含合成数据、人工标注数据和真实世界样本的多样化工作流图表数据集，以支持稳健的训练和评估。我们对多个视觉-语言模型进行了微调和基准测试，并开展了一系列消融实验以分析我们方法的优势和局限性。实验结果表明，微调显著提升了结构化工作流的生成效果，在该任务上优于大型视觉-语言模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Workflows are a fundamental component of automation in enterprise platforms, enabling the orchestration of tasks, data processing, and system integrations.</div>
</details>
</div>
<div class="card">
<div class="title">AgentCompress: Task-Aware Compression for Affordable Large Language Model Agents</div>
<div class="meta-line">Authors: Zuhair Ahmed Khan Taha, Mohammed Mudassir Uddin, Shahnawaz Alam</div>
<div class="meta-line">First: 2026-01-08T18:13:46+00:00 · Latest: 2026-01-12T18:25:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05191v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05191v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models hold considerable promise for various applications, but their computational requirements create a barrier that many institutions cannot overcome. A single session using a 70-billion-parameter model can cost around $127 in cloud computing fees, which puts these tools out of reach for organizations operating on limited budgets. We present AgentCompress, a framework that tackles this problem through task-aware dynamic compression. The idea comes from a simple observation: not all tasks require the same computational effort. Complex reasoning, for example, is far more demanding than text reformatting, yet conventional compression applies the same reduction to both. Our approach uses a lightweight neural controller that looks at the first few tokens of each request, estimates how complex the task will be, and sends it to an appropriately quantized version of the model. This routing step adds only about 12 milliseconds of overhead. We tested the framework on 290 multi-stage workflows from domains including computer science, physics, chemistry, and biology. The results show a 68.3% reduction in computational costs while preserving 96.2% of the original success rate. These findings suggest that routing queries intelligently can make powerful language models substantially more affordable without sacrificing output quality</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AgentCompress：面向任务的压缩技术，使大型语言模型代理更具成本效益</div>
<div class="mono" style="margin-top:8px">大型语言模型在多种应用中展现出巨大潜力，但其计算需求成为许多机构难以逾越的障碍。使用一个700亿参数模型的单次会话费用约为127美元，这使得这些工具对预算有限的组织来说难以负担。我们提出了AgentCompress框架，通过任务感知的动态压缩来解决这一问题。这一想法源于一个简单的观察：并非所有任务都需要相同的计算努力。例如，复杂推理比文本格式转换要耗费更多计算资源，但传统压缩方法对两者采用相同的压缩策略。我们的方法使用一个轻量级的神经控制器，分析每个请求的前几个标记，估计任务的复杂度，并将其路由到适当量化版本的模型。这一路由步骤仅增加约12毫秒的开销。我们在包括计算机科学、物理、化学和生物学在内的多个领域中测试了该框架，涉及290个多阶段工作流。结果表明，计算成本减少了68.3%，同时保持了96.2%的原始成功率。这些发现表明，智能地路由查询可以在不牺牲输出质量的前提下，使强大的语言模型显著更具成本效益。</div>
</details>
</div>
<div class="card">
<div class="title">Rigorous Anderson-type lower bounds on the ground-state energy of the pyrochlore Heisenberg antiferromagnet</div>
<div class="meta-line">Authors: Péter Kránitz, Karlo Penc</div>
<div class="meta-line">First: 2026-01-12T18:21:50+00:00 · Latest: 2026-01-12T18:21:50+00:00</div>
<div class="meta-line">Comments: This work is dedicated to the memory of Johannes Richter (20 pages, 8 figures)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07800v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07800v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We construct rigorous Anderson-type lower bounds on the ground-state energy of the spin-$S$ Heisenberg antiferromagnet on the pyrochlore lattice. By formulating and optimizing a hierarchy of local cluster motifs ordered by size, we generate a sequence of increasingly tight bounds. A seven-site &quot;hourglass&quot; cluster composed of two corner-sharing tetrahedra furnishes an optimal lower bound that admits a closed-form expression for arbitrary spin $S$. We also derive exact lower bounds for generalized models with further-neighbor exchange, ring exchange, and scalar spin-chirality interactions. For $S=1/2$ and $S=1$, numerical optimization of an 18-site &quot;crown&quot; cluster containing a hexagonal loop yields rigorous lower bounds on the ground-state energy per site of the nearest-neighbor Heisenberg model with unit exchange, $e_\mathrm{GS} \geq -0.549832$ and $e_\mathrm{GS} \geq -1.632985$, respectively. We compare the resulting bounds with numerical ground-state energy estimates from the literature.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于层状菱面体海森堡反铁磁体基态能量的严格安德森型下限</div>
<div class="mono" style="margin-top:8px">我们构建了关于自旋-$S$ 海森堡反铁磁体在菱面体晶格上的基态能量的严格安德森型下限。通过构建并优化按大小排序的局部簇模式层次结构，我们生成了一系列越来越紧的下限。由两个共角四面体组成的七点&quot;沙漏&quot;簇提供了适用于任意自旋$S$的闭式表达的最优下限。我们还推导了包含远邻交换、环交换和标量自旋-手性相互作用的广义模型的精确下限。对于$S=1/2$和$S=1$，通过数值优化一个包含六边形环的18点&quot;王冠&quot;簇，我们得到了最近邻海森堡模型中单位交换的基态能量每点下限，分别为$e_\mathrm{GS} \geq -0.549832$和$e_\mathrm{GS} \geq -1.632985$。我们将所得下限与文献中的数值基态能量估计进行比较。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We construct rigorous Anderson-type lower bounds on the ground-state energy of the spin-$S$ Heisenberg antiferromagnet on the pyrochlore lattice.</div>
</details>
</div>
<div class="card">
<div class="title">Near-Real-Time Resource Slicing for QoS Optimization in 5G O-RAN using Deep Reinforcement Learning</div>
<div class="meta-line">Authors: Peihao Yan, Jie Lu, Huacheng Zeng, Y. Thomas Hou</div>
<div class="meta-line">Venue: P. Yan, J. Lu, H. Zeng and Y. Thomas Hou, &quot;Near-Real-Time Resource Slicing for QoS Optimization in 5G O-RAN Using Deep Reinforcement Learning,&quot; in IEEE Transactions on Networking, vol. 34, pp. 1596-1611, 2026</div>
<div class="meta-line">First: 2025-09-17T18:20:04+00:00 · Latest: 2026-01-12T18:11:14+00:00</div>
<div class="meta-line">Comments: Published in: IEEE Transactions on Networking</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.14343v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.14343v2">PDF</a> · <a href="https://github.com/xslice-5G/code">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-Radio Access Network (O-RAN) has become an important paradigm for 5G and beyond radio access networks. This paper presents an xApp called xSlice for the Near-Real-Time (Near-RT) RAN Intelligent Controller (RIC) of 5G O-RANs. xSlice is an online learning algorithm that adaptively adjusts MAC-layer resource allocation in response to dynamic network states, including time-varying wireless channel conditions, user mobility, traffic fluctuations, and changes in user demand. To address these network dynamics, we first formulate the Quality-of-Service (QoS) optimization problem as a regret minimization problem by quantifying the QoS demands of all traffic sessions through weighting their throughput, latency, and reliability. We then develop a deep reinforcement learning (DRL) framework that utilizes an actor-critic model to combine the advantages of both value-based and policy-based updating methods. A graph convolutional network (GCN) is incorporated as a component of the DRL framework for graph embedding of RAN data, enabling xSlice to handle a dynamic number of traffic sessions. We have implemented xSlice on an O-RAN testbed with 10 smartphones and conducted extensive experiments to evaluate its performance in realistic scenarios. Experimental results show that xSlice can reduce performance regret by 67% compared to the state-of-the-art solutions. Source code is available at https://github.com/xslice-5G/code.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度强化学习的5G O-RAN近实时资源切片以优化QoS</div>
<div class="mono" style="margin-top:8px">开放无线接入网络（O-RAN）已成为5G及未来无线接入网络的重要范式。本文提出了一种名为xSlice的xApp，用于5G O-RAN的近实时（Near-RT）无线接入网智能控制器（RIC）。xSlice是一种在线学习算法，能够根据动态网络状态自适应调整MAC层资源分配，包括时变无线信道条件、用户移动性、流量波动以及用户需求变化。为应对这些网络动态，我们首先通过量化所有流量会话的QoS需求（包括吞吐量、延迟和可靠性），将QoS优化问题建模为一个遗憾最小化问题。随后，我们开发了一个深度强化学习（DRL）框架，利用actor-critic模型结合基于价值和基于策略的更新方法的优势。我们还引入了图卷积网络（GCN）作为DRL框架的一部分，用于对RAN数据进行图嵌入，使xSlice能够处理动态变化的流量会话数量。我们在一个包含10部智能手机的O-RAN测试平台上实现了xSlice，并进行了大量实验以评估其在现实场景中的性能。实验结果表明，与最先进的解决方案相比，xSlice可将性能遗憾降低67%。源代码可在https://github.com/xslice-5G/code获取。</div>
</details>
</div>
<div class="card">
<div class="title">Vision-Language Model for Accurate Crater Detection</div>
<div class="meta-line">Authors: Patrick Bauer, Marius Schwinning, Florian Renk, Andreas Weinmann, Hichem Snoussi</div>
<div class="meta-line">First: 2026-01-12T18:08:17+00:00 · Latest: 2026-01-12T18:08:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07795v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07795v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The European Space Agency (ESA), driven by its ambitions on planned lunar missions with the Argonaut lander, has a profound interest in reliable crater detection, since craters pose a risk to safe lunar landings. This task is usually addressed with automated crater detection algorithms (CDA) based on deep learning techniques. It is non-trivial due to the vast amount of craters of various sizes and shapes, as well as challenging conditions such as varying illumination and rugged terrain. Therefore, we propose a deep-learning CDA based on the OWLv2 model, which is built on a Vision Transformer, that has proven highly effective in various computer vision tasks. For fine-tuning, we utilize a manually labeled dataset fom the IMPACT project, that provides crater annotations on high-resolution Lunar Reconnaissance Orbiter Camera Calibrated Data Record images. We insert trainable parameters using a parameter-efficient fine-tuning strategy with Low-Rank Adaptation, and optimize a combined loss function consisting of Complete Intersection over Union (CIoU) for localization and a contrastive loss for classification. We achieve satisfactory visual results, along with a maximum recall of 94.0% and a maximum precision of 73.1% on a test dataset from IMPACT. Our method achieves reliable crater detection across challenging lunar imaging conditions, paving the way for robust crater analysis in future lunar exploration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于精确陨石坑检测的视觉-语言模型</div>
<div class="mono" style="margin-top:8px">欧洲航天局（ESA）因其计划中的阿耳戈纳乌特着陆器月球任务，对可靠的陨石坑检测有浓厚兴趣，因为陨石坑对安全着陆构成威胁。该任务通常通过基于深度学习技术的自动陨石坑检测算法（CDA）来解决。由于陨石坑数量庞大且形态各异，以及照明变化和崎岖地形等复杂条件，该任务具有挑战性。因此，我们提出了一种基于 OWLv2 模型的深度学习 CDA，该模型建立在视觉 Transformer 之上，在各种计算机视觉任务中已被证明非常有效。在微调过程中，我们使用了来自 IMPACT 项目的手动标注数据集，该数据集提供了高分辨率月球勘测轨道器相机校准数据记录图像的陨石坑标注。我们采用参数高效微调策略（低秩适应）插入可训练参数，并优化由完全交并比（CIoU）定位损失和对比损失组成的联合损失函数。我们在 IMPACT 项目的一个测试数据集上取得了令人满意的视觉结果，最大召回率为 94.0%，最大精确率为 73.1%。我们的方法在复杂的月球成像条件下实现了可靠的陨石坑检测，为未来月球探测中的稳健陨石坑分析铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The European Space Agency (ESA), driven by its ambitions on planned lunar missions with the Argonaut lander, has a profound interest in reliable crater detection, since craters pose a risk to safe lunar landings.</div>
</details>
</div>
<div class="card">
<div class="title">Metaphors are a Source of Cross-Domain Misalignment of Large Reasoning Models</div>
<div class="meta-line">Authors: Zhibo Hu, Chen Wang, Yanfeng Shu, Hye-young Paik, Liming Zhu</div>
<div class="meta-line">First: 2026-01-06T19:50:58+00:00 · Latest: 2026-01-12T18:08:06+00:00</div>
<div class="meta-line">Comments: 17 pages, 7 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03388v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03388v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Earlier research has shown that metaphors influence human&#x27;s decision making, which raises the question of whether metaphors also influence large language models (LLMs)&#x27; reasoning pathways, considering their training data contain a large number of metaphors. In this work, we investigate the problem in the scope of the emergent misalignment problem where LLMs can generalize patterns learned from misaligned content in one domain to another domain. We discover a strong causal relationship between metaphors in training data and the misalignment degree of LLMs&#x27; reasoning contents. With interventions using metaphors in pre-training, fine-tuning and re-alignment phases, models&#x27; cross-domain misalignment degrees change significantly. As we delve deeper into the causes behind this phenomenon, we observe that there is a connection between metaphors and the activation of global and local latent features of large reasoning models. By monitoring these latent features, we design a detector that predict misaligned content with high accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>隐喻是大语言模型跨领域推理偏差的来源</div>
<div class="mono" style="margin-top:8px">早期研究表明隐喻会影响人类的决策过程，这引发了关于隐喻是否也会影响大语言模型（LLMs）推理路径的疑问，尤其是考虑到它们的训练数据中包含大量隐喻。在本研究中，我们从新兴的跨领域偏差问题出发，探讨LLMs如何将一个领域中从偏差内容中学习到的模式泛化到另一个领域。我们发现训练数据中的隐喻与LLMs推理内容的偏差程度之间存在强因果关系。通过在预训练、微调和重新对齐阶段使用隐喻进行干预，模型的跨领域偏差程度发生显著变化。随着我们深入研究这一现象的原因，我们观察到隐喻与大语言模型全局和局部潜在特征的激活之间存在联系。通过监控这些潜在特征，我们设计了一种能够高精度预测偏差内容的检测器。</div>
</details>
</div>
<div class="card">
<div class="title">Kinship Data Benchmark for Multi-hop Reasoning</div>
<div class="meta-line">Authors: Tianda Sun, Dimitar Kazakov</div>
<div class="meta-line">First: 2026-01-12T18:07:41+00:00 · Latest: 2026-01-12T18:07:41+00:00</div>
<div class="meta-line">Comments: 11 pages, 2 figures, 9 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07794v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07794v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly evaluated on their ability to perform multi-hop reasoning, i.e., to combine multiple pieces of information into a coherent inference. We introduce KinshipQA, a benchmark designed to probe this capability through reasoning over kinship relations. The central contribution of our work is a generative pipeline that produces, on demand, large-scale, realistic, and culture-specific genealogical data: collections of interconnected family trees that satisfy explicit marriage constraints associated with different kinship systems. This allows task difficulty, cultural assumptions, and relational depth to be systematically controlled and varied. From these genealogies, we derive textual inference tasks that require reasoning over implicit relational chains. We evaluate the resulting benchmark using six state-of-the-art LLMs, spanning both open-source and closed-source models, under a uniform zero-shot protocol with deterministic decoding. Performance is measured using exact-match and set-based metrics. Our results demonstrate that KinshipQA yields a wide spread of outcomes and exposes systematic differences in multi-hop reasoning across models and cultural settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多跳推理的亲属关系数据基准</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）越来越多地被评估其进行多跳推理的能力，即通过结合多个信息片段进行连贯推理。我们引入了KinshipQA，这是一个通过推理亲属关系来探测该能力的基准。我们工作的核心贡献是一个生成管道，能够按需生成大规模、现实且具有文化特性的家谱数据：满足不同亲属关系系统显式婚姻约束的相互关联的家庭树集合。这使得任务难度、文化假设和关系深度可以被系统地控制和调整。我们从这些家谱中推导出文本推理任务，这些任务需要推理隐含的关系链。我们使用六个最先进的LLMs，在统一的零样本协议下，通过确定性解码对生成的基准进行评估。性能通过精确匹配和基于集合的指标进行衡量。我们的结果表明，KinshipQA能够产生广泛的结果，并揭示模型和文化背景之间在多跳推理上的系统性差异。</div>
</details>
</div>
<div class="card">
<div class="title">Benchmarking Small Language Models and Small Reasoning Language Models on System Log Severity Classification</div>
<div class="meta-line">Authors: Yahya Masri, Emily Ma, Zifu Wang, Joseph Rogers, Chaowei Yang</div>
<div class="meta-line">First: 2026-01-12T18:02:33+00:00 · Latest: 2026-01-12T18:02:33+00:00</div>
<div class="meta-line">Comments: 28 pages, 5 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07790v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07790v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">System logs are crucial for monitoring and diagnosing modern computing infrastructure, but their scale and complexity require reliable and efficient automated interpretation. Since severity levels are predefined metadata in system log messages, having a model merely classify them offers limited standalone practical value, revealing little about its underlying ability to interpret system logs. We argue that severity classification is more informative when treated as a benchmark for probing runtime log comprehension rather than as an end task. Using real-world journalctl data from Linux production servers, we evaluate nine small language models (SLMs) and small reasoning language models (SRLMs) under zero-shot, few-shot, and retrieval-augmented generation (RAG) prompting. The results reveal strong stratification. Qwen3-4B achieves the highest accuracy at 95.64% with RAG, while Gemma3-1B improves from 20.25% under few-shot prompting to 85.28% with RAG. Notably, the tiny Qwen3-0.6B reaches 88.12% accuracy despite weak performance without retrieval. In contrast, several SRLMs, including Qwen3-1.7B and DeepSeek-R1-Distill-Qwen-1.5B, degrade substantially when paired with RAG. Efficiency measurements further separate models: most Gemma and Llama variants complete inference in under 1.2 seconds per log, whereas Phi-4-Mini-Reasoning exceeds 228 seconds per log while achieving &lt;10% accuracy. These findings suggest that (1) architectural design, (2) training objectives, and (3) the ability to integrate retrieved context under strict output constraints jointly determine performance. By emphasizing small, deployable models, this benchmark aligns with real-time requirements of digital twin (DT) systems and shows that severity classification serves as a lens for evaluating model competence and real-time deployability, with implications for root cause analysis (RCA) and broader DT integration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在系统日志严重性分类任务中基准测试小型语言模型和小型推理语言模型</div>
<div class="mono" style="margin-top:8px">系统日志对于监控和诊断现代计算基础设施至关重要，但其规模和复杂性需要可靠且高效的自动化解析。由于严重性等级是系统日志消息中的预定义元数据，仅让模型进行分类的实用价值有限，无法充分反映其对系统日志的理解能力。我们认为，将严重性分类作为评估运行时日志理解能力的基准任务比作为最终任务更有信息量。我们使用来自Linux生产服务器的真实journalctl数据，对九个小型语言模型（SLMs）和小型推理语言模型（SRLMs）在零样本、少样本和检索增强生成（RAG）提示下进行了评估。结果显示出显著的性能分层。Qwen3-4B在RAG提示下达到95.64%的最高准确率，而Gemma3-1B在少样本提示下准确率为20.25%，在RAG提示下提升至85.28%。值得注意的是，尽管在无检索情况下表现较弱，但极小的Qwen3-0.6B仍达到了88.12%的准确率。相比之下，一些SRLMs，如Qwen3-1.7B和DeepSeek-R1-Distill-Qwen-1.5B，在与RAG结合时性能显著下降。效率测量进一步区分了模型：大多数Gemma和Llama变体在每条日志的推理时间低于1.2秒，而Phi-4-Mini-Reasoning每条日志的推理时间超过228秒，准确率低于10%。这些发现表明，（1）模型架构设计、（2）训练目标以及（3）在严格输出约束下整合检索上下文的能力共同决定了模型性能。通过强调小型、可部署的模型，该基准测试符合数字孪生（DT）系统的实时需求，并表明严重性分类可作为评估模型能力与实时部署性的工具，对根本原因分析（RCA）和更广泛的DT集成具有重要意义。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Single-Shot: Multi-step Tool Retrieval via Query Planning</div>
<div class="meta-line">Authors: Wei Fang, James Glass</div>
<div class="meta-line">First: 2026-01-12T17:58:39+00:00 · Latest: 2026-01-12T17:58:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07782v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07782v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests. These failures primarily stem from the disconnect between abstract user goals and technical documentation, and the limited capacity of fixed-size embeddings to model combinatorial tool compositions. To address these challenges, we propose TOOLQP, a lightweight framework that models retrieval as iterative query planning. Instead of single-shot matching, TOOLQP decomposes instructions into sub-tasks and dynamically generates queries to interact with the retriever, effectively bridging the semantic gap by targeting the specific sub-tasks required for composition. We train TOOLQP using synthetic query trajectories followed by optimization via Reinforcement Learning with Verifiable Rewards (RLVR). Experiments demonstrate that TOOLQP achieves state-of-the-art performance, exhibiting superior zero-shot generalization, robustness across diverse retrievers, and significant improvements in downstream agentic execution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越单次检索：通过查询规划实现多步骤工具检索</div>
<div class="mono" style="margin-top:8px">在大规模、动态工具库上运行的LLM代理依赖于有效的检索，但标准的单次密集检索器在处理复杂请求时表现不佳。这些失败主要源于抽象用户目标与技术文档之间的脱节，以及固定大小嵌入向量在建模组合工具使用时的局限性。为了解决这些问题，我们提出了TOOLQP，一个轻量级框架，将检索建模为迭代的查询规划过程。与单次匹配不同，TOOLQP将指令分解为子任务，并动态生成查询以与检索器交互，从而有效弥合语义鸿沟，针对组合所需的特定子任务进行优化。我们使用合成查询轨迹训练TOOLQP，并通过可验证奖励的强化学习（RLVR）进行优化。实验表明，TOOLQP实现了最先进的性能，在零样本泛化能力、跨多样化检索器的鲁棒性以及下游代理执行任务的改进方面均表现出色。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">LLM agents operating over massive, dynamic tool libraries rely on effective retrieval, yet standard single-shot dense retrievers struggle with complex requests.</div>
</details>
</div>
<div class="card">
<div class="title">OS-Symphony: A Holistic Framework for Robust and Generalist Computer-Using Agent</div>
<div class="meta-line">Authors: Bowen Yang, Kaiming Jin, Zhenyu Wu, Zhaoyang Liu, Qiushi Sun, Zehao Li, JingJing Xie, Zhoumianze Liu, Fangzhi Xu, Kanzhi Cheng, Qingyun Li, Yian Wang, Yu Qiao, Zun Wang, Zichen Ding</div>
<div class="meta-line">First: 2026-01-12T17:55:51+00:00 · Latest: 2026-01-12T17:55:51+00:00</div>
<div class="meta-line">Comments: 31 pages, 11 figures, 12 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07779v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07779v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains. These limitations stem from a lack of granular control over historical visual context curation and the absence of visual-aware tutorial retrieval. To bridge these gaps, we introduce OS-Symphony, a holistic framework that comprises an Orchestrator coordinating two key innovations for robust automation: (1) a Reflection-Memory Agent that utilizes milestone-driven long-term memory to enable trajectory-level self-correction, effectively mitigating visual context loss in long-horizon tasks; (2) Versatile Tool Agents featuring a Multimodal Searcher that adopts a SeeAct paradigm to navigate a browser-based sandbox to synthesize live, visually aligned tutorials, thereby resolving fidelity issues in unseen scenarios. Experimental results demonstrate that OS-Symphony delivers substantial performance gains across varying model scales, establishing new state-of-the-art results on three online benchmarks, notably achieving 65.84% on OSWorld.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OS-Symphony：面向鲁棒性和通用性计算机使用代理的综合框架</div>
<div class="mono" style="margin-top:8px">尽管视觉语言模型（VLMs）显著推动了计算机使用代理（CUAs）的发展，但当前框架在长流程任务中缺乏鲁棒性，并且在新领域中泛化能力不足。这些限制源于对历史视觉上下文管理缺乏细粒度控制，以及缺乏视觉感知的教程检索机制。为了解决这些问题，我们提出了OS-Symphony，一个综合框架，包含一个协调器，用于实现两个关键创新以实现鲁棒自动化：（1）一个基于里程碑的长期记忆反思代理，能够进行轨迹级自我修正，有效缓解长流程任务中的视觉上下文丢失问题；（2）多功能工具代理，采用SeeAct范式在基于浏览器的沙箱中导航，合成实时、视觉对齐的教程，从而解决未见过场景中的保真度问题。实验结果表明，OS-Symphony在不同模型规模下均实现了显著的性能提升，并在三个在线基准测试中建立了新的最先进结果，特别是在OSWorld上达到了65.84%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">While Vision-Language Models (VLMs) have significantly advanced Computer-Using Agents (CUAs), current frameworks struggle with robustness in long-horizon workflows and generalization in novel domains.</div>
</details>
</div>
<div class="card">
<div class="title">DT-ICU: Towards Explainable Digital Twins for ICU Patient Monitoring via Multi-Modal and Multi-Task Iterative Inference</div>
<div class="meta-line">Authors: Wen Guo</div>
<div class="meta-line">First: 2026-01-12T17:54:19+00:00 · Latest: 2026-01-12T17:54:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07778v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.07778v1">PDF</a> · <a href="https://github.com/GUO-W/DT-ICU-release">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce DT-ICU, a multimodal digital twin framework for continuous risk estimation in intensive care. DT-ICU integrates variable-length clinical time series with static patient information in a unified multitask architecture, enabling predictions to be updated as new observations accumulate over the ICU stay. We evaluate DT-ICU on the large, publicly available MIMIC-IV dataset, where it consistently outperforms established baseline models under different evaluation settings. Our test-length analysis shows that meaningful discrimination is achieved shortly after admission, while longer observation windows further improve the ranking of high-risk patients in highly imbalanced cohorts. To examine how the model leverages heterogeneous data sources, we perform systematic modality ablations, revealing that the model learnt a reasonable structured reliance on interventions, physiological response observations, and contextual information. These analyses provide interpretable insights into how multimodal signals are combined and how trade-offs between sensitivity and precision emerge. Together, these results demonstrate that DT-ICU delivers accurate, temporally robust, and interpretable predictions, supporting its potential as a practical digital twin framework for continuous patient monitoring in critical care. The source code and trained model weights for DT-ICU are publicly available at https://github.com/GUO-W/DT-ICU-release.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DT-ICU：通过多模态和多任务迭代推理实现ICU患者监测的可解释数字孪生</div>
<div class="mono" style="margin-top:8px">我们引入了DT-ICU，这是一个用于重症监护中持续风险评估的多模态数字孪生框架。DT-ICU在一个统一的多任务架构中整合了可变长度的临床时间序列和静态患者信息，使得预测能够随着ICU住院期间的新观察数据不断更新。我们在大型公开数据集MIMIC-IV上评估DT-ICU，结果表明在不同的评估设置下，它始终优于现有的基线模型。我们的测试长度分析显示，在入院后不久就能实现有意义的区分，而更长的观察窗口进一步提高了高风险患者在高度不平衡队列中的排序效果。为了研究模型如何利用异构数据源，我们进行了系统的模态消融实验，发现模型合理地依赖了干预措施、生理反应观察和上下文信息。这些分析提供了可解释的见解，说明多模态信号是如何结合的，以及敏感性和精确性之间的权衡是如何产生的。总体而言，这些结果表明DT-ICU能够提供准确、时间鲁棒且可解释的预测，支持其作为重症监护中持续患者监测的实用数字孪生框架的潜力。DT-ICU的源代码和训练模型权重已公开在https://github.com/GUO-W/DT-ICU-release。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
