<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-17 04:10</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260217_0410</div>
    <div class="row"><div class="card">
<div class="title">$\texttt{GPUmonty}$: A GPU-accelerated relativistic Monte Carlo radiative transfer code</div>
<div class="meta-line">Authors: Pedro Naethe Motta, Rodrigo Nemmen, Abhishek V. Joshi</div>
<div class="meta-line">First: 2026-02-13T18:59:59+00:00 · Latest: 2026-02-13T18:59:59+00:00</div>
<div class="meta-line">Comments: 12 pages, 6 figures and 1 table. Comments are welcome! Submitted to ApJ</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13198v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13198v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce $\texttt{GPUmonty}$, a CUDA/C-based Monte Carlo radiative transfer code accelerated using graphics processing units (GPUs). $\texttt{GPUmonty}$ derives from the CPU-based code $\texttt{grmonty}$ and offloads the most computationally expensive stages of the calculation -- superphoton generation, sampling, tracking, and scattering -- to the GPU. Whereas $\texttt{grmonty}$ handles photons sequentially, $\texttt{GPUmonty}$ processes large numbers of superphotons concurrently, leveraging the single-instruction, multiple-thread (SIMT) execution model of modern GPUs. Benchmarks demonstrate a speedup of about $12\times$ relative to the original CPU implementation on a single GPU, with runtime limited primarily by register pressure rather than compute or memory bandwidth saturation. We validate the implementation through analytic tests for a optically thin synchrotron sphere, as well as comparisons with $\texttt{igrmonty}$ for scattering synchrotron sphere and GRMHD simulation data. Relative errors remain below a percent level and convergence is consistent with the expected $N_{\rm s}^{-1/2}$ Monte Carlo scaling. By significantly reducing computational costs, GPUmonty enables the extensive parameter space surveys and faster spectra modeling required to interpret horizon-scale observations of supermassive black holes. $\texttt{GPUmonty}$ is publicly available under the GNU General Public License.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GPUmonty：一种基于GPU加速的相对论蒙特卡洛辐射传输代码</div>
<div class="mono" style="margin-top:8px">我们介绍了GPUmonty，这是一种基于CUDA/C的蒙特卡洛辐射传输代码，利用图形处理单元（GPU）进行加速。GPUmonty源自CPU版本的grmonty代码，将计算最昂贵的阶段——超光子生成、采样、追踪和散射——转移到GPU上。与grmonty按顺序处理光子不同，GPUmonty通过现代GPU的单指令多线程（SIMT）执行模型，能够并行处理大量超光子。基准测试表明，在单个GPU上，其速度比原始CPU实现快约12倍，运行时间主要受限于寄存器压力，而非计算或内存带宽饱和。我们通过解析测试验证了该实现，测试对象为光学薄的同步辐射球，同时与igrmonty进行了同步辐射球和GRMHD模拟数据的比较。相对误差保持在百分之一以下，收敛性与预期的蒙特卡洛缩放 $N_{\rm s}^{-1/2}$ 一致。通过显著降低计算成本，GPUmonty使得对超大质量黑洞视界尺度观测的广泛参数空间调查和更快的光谱建模成为可能。GPUmonty在GNU通用公共许可证下公开可用。</div>
</details>
</div>
<div class="card">
<div class="title">Imitating What Works: Simulation-Filtered Modular Policy Learning from Human Videos</div>
<div class="meta-line">Authors: Albert J. Zhai, Kuo-Hao Zeng, Jiasen Lu, Ali Farhadi, Shenlong Wang, Wei-Chiu Ma</div>
<div class="meta-line">First: 2026-02-13T18:59:10+00:00 · Latest: 2026-02-13T18:59:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13197v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13197v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The ability to learn manipulation skills by watching videos of humans has the potential to unlock a new source of highly scalable data for robot learning. Here, we tackle prehensile manipulation, in which tasks involve grasping an object before performing various post-grasp motions. Human videos offer strong signals for learning the post-grasp motions, but they are less useful for learning the prerequisite grasping behaviors, especially for robots without human-like hands. A promising way forward is to use a modular policy design, leveraging a dedicated grasp generator to produce stable grasps. However, arbitrary stable grasps are often not task-compatible, hindering the robot&#x27;s ability to perform the desired downstream motion. To address this challenge, we present Perceive-Simulate-Imitate (PSI), a framework for training a modular manipulation policy using human video motion data processed by paired grasp-trajectory filtering in simulation. This simulation step extends the trajectory data with grasp suitability labels, which allows for supervised learning of task-oriented grasping capabilities. We show through real-world experiments that our framework can be used to learn precise manipulation skills efficiently without any robot data, resulting in significantly more robust performance than using a grasp generator naively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模仿有效行为：从人类视频中通过模拟过滤的模块化策略学习操作技能</div>
<div class="mono" style="margin-top:8px">通过观看人类视频来学习操作技能的能力，有潜力为机器人学习解锁一种高度可扩展的数据新来源。在此，我们处理抓取操作，其中任务包括在执行各种抓取后动作之前抓取物体。人类视频对学习抓取后动作提供了强信号，但对于学习抓取前的必要行为则不太有用，尤其是对于没有类人手的机器人。一种有前景的方法是使用模块化策略设计，利用专门的抓取生成器来产生稳定的抓取动作。然而，任意稳定的抓取动作通常并不与任务兼容，这会阻碍机器人执行所需的下游动作。为了解决这一挑战，我们提出了感知-模拟-模仿（Perceive-Simulate-Imitate，PSI）框架，该框架通过在模拟中使用配对的抓取-轨迹过滤处理人类视频运动数据来训练模块化操作策略。这一模拟步骤扩展了轨迹数据，添加了抓取适宜性标签，从而允许监督学习任务导向的抓取能力。我们通过现实世界实验表明，我们的框架可以在不使用任何机器人数据的情况下高效地学习精确的操作技能，从而实现比简单使用抓取生成器显著更鲁棒的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The ability to learn manipulation skills by watching videos of humans has the potential to unlock a new source of highly scalable data for robot learning.</div>
</details>
</div>
<div class="card">
<div class="title">Conversational Image Segmentation: Grounding Abstract Concepts with Scalable Supervision</div>
<div class="meta-line">Authors: Aadarsh Sahoo, Georgia Gkioxari</div>
<div class="meta-line">First: 2026-02-13T18:58:30+00:00 · Latest: 2026-02-13T18:58:30+00:00</div>
<div class="meta-line">Comments: Project webpage: https://glab-caltech.github.io/converseg/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13195v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13195v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://glab-caltech.github.io/converseg/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conversational image segmentation grounds abstract, intent-driven concepts into pixel-accurate masks. Prior work on referring image grounding focuses on categorical and spatial queries (e.g., &quot;left-most apple&quot;) and overlooks functional and physical reasoning (e.g., &quot;where can I safely store the knife?&quot;). We address this gap and introduce Conversational Image Segmentation (CIS) and ConverSeg, a benchmark spanning entities, spatial relations, intent, affordances, functions, safety, and physical reasoning. We also present ConverSeg-Net, which fuses strong segmentation priors with language understanding, and an AI-powered data engine that generates prompt-mask pairs without human supervision. We show that current language-guided segmentation models are inadequate for CIS, while ConverSeg-Net trained on our data engine achieves significant gains on ConverSeg and maintains strong performance on existing language-guided segmentation benchmarks. Project webpage: https://glab-caltech.github.io/converseg/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对话式图像分割：通过可扩展监督将抽象概念具象化</div>
<div class="mono" style="margin-top:8px">对话式图像分割将抽象、意图驱动的概念转化为像素精确的掩码。以往的指称图像定位研究主要关注类别和空间查询（例如：&quot;最左边的苹果&quot;），而忽略了功能和物理推理（例如：&quot;我应该在哪里安全地存放刀具？&quot;）。我们填补了这一空白，提出了对话式图像分割（CIS）以及ConverSeg基准，涵盖实体、空间关系、意图、可操作性、功能、安全性和物理推理。我们还提出了ConverSeg-Net，该模型融合了强大的分割先验知识与语言理解能力，并引入了一种AI驱动的数据引擎，可在无人监督的情况下生成提示-掩码对。我们表明，当前的语言引导分割模型在CIS任务上表现不足，而基于我们数据引擎训练的ConverSeg-Net在ConverSeg基准上取得了显著提升，并在现有的语言引导分割基准上保持了强劲的表现。项目网页：https://glab-caltech.github.io/converseg/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Conversational image segmentation grounds abstract, intent-driven concepts into pixel-accurate masks.</div>
</details>
</div>
<div class="card">
<div class="title">Semantic Chunking and the Entropy of Natural Language</div>
<div class="meta-line">Authors: Weishun Zhong, Doron Sivan, Tankut Can, Mikhail Katkov, Misha Tsodyks</div>
<div class="meta-line">First: 2026-02-13T18:58:10+00:00 · Latest: 2026-02-13T18:58:10+00:00</div>
<div class="meta-line">Comments: 29 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13194v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13194v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per character expected for random text. We introduce a statistical model that attempts to capture the intricate multi-scale structure of natural language, providing a first-principles account of this redundancy level. Our model describes a procedure of self-similarly segmenting text into semantically coherent chunks down to the single-word level. The semantic structure of the text can then be hierarchically decomposed, allowing for analytical treatment. Numerical experiments with modern LLMs and open datasets suggest that our model quantitatively captures the structure of real texts at different levels of the semantic hierarchy. The entropy rate predicted by our model agrees with the estimated entropy rate of printed English. Moreover, our theory further reveals that the entropy rate of natural language is not fixed but should increase systematically with the semantic complexity of corpora, which are captured by the only free parameter in our model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语义分块与自然语言的熵</div>
<div class="mono" style="margin-top:8px">印刷英语的熵率著名地估计为每字符约1比特，这一基准直到最近现代大语言模型（LLMs）才接近。这一熵率意味着英语相对于随机文本每字符预期的5比特而言，具有近80%的冗余性。我们引入了一个统计模型，试图捕捉自然语言的复杂多尺度结构，从而提供这一冗余水平的第一性原理解释。我们的模型描述了一种自相似地将文本分割为语义连贯的块的过程，直至单个单词级别。随后，文本的语义结构可以被分层分解，从而便于分析处理。使用现代LLMs和公开数据集进行的数值实验表明，我们的模型在语义层次的不同级别上定量地捕捉了真实文本的结构。我们模型预测的熵率与印刷英语的估计熵率一致。此外，我们的理论进一步揭示，自然语言的熵率并非固定，而是应随着语料库的语义复杂性系统性地增加，而这种复杂性仅由我们模型中的唯一自由参数所捕捉。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached.</div>
</details>
</div>
<div class="card">
<div class="title">Steerable Vision-Language-Action Policies for Embodied Reasoning and Hierarchical Control</div>
<div class="meta-line">Authors: William Chen, Jagdeep Singh Bhatia, Catherine Glossop, Nikhil Mathihalli, Ria Doshi, Andy Tang, Danny Driess, Karl Pertsch, Sergey Levine</div>
<div class="meta-line">First: 2026-02-13T18:57:56+00:00 · Latest: 2026-02-13T18:57:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13193v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13193v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pretrained vision-language models (VLMs) can make semantic and visual inferences across diverse settings, providing valuable common-sense priors for robotic control. However, effectively grounding this knowledge in robot behaviors remains an open challenge. Prior methods often employ a hierarchical approach where VLMs reason over high-level commands to be executed by separate low-level policies, e.g., vision-language-action models (VLAs). The interface between VLMs and VLAs is usually natural language task instructions, which fundamentally limits how much VLM reasoning can steer low-level behavior. We thus introduce Steerable Policies: VLAs trained on rich synthetic commands at various levels of abstraction, like subtasks, motions, and grounded pixel coordinates. By improving low-level controllability, Steerable Policies can unlock pretrained knowledge in VLMs, enabling improved task generalization. We demonstrate this benefit by controlling our Steerable Policies with both a learned high-level embodied reasoner and an off-the-shelf VLM prompted to reason over command abstractions via in-context learning. Across extensive real-world manipulation experiments, these two novel methods outperform prior embodied reasoning VLAs and VLM-based hierarchical baselines, including on challenging generalization and long-horizon tasks.
  Website: steerable-policies.github.io</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于具身推理和分层控制的可引导视觉-语言-动作策略</div>
<div class="mono" style="margin-top:8px">预训练的视觉-语言模型（VLMs）可以在各种场景中进行语义和视觉推理，为机器人控制提供有价值的常识先验。然而，如何有效地将这些知识融入机器人行为仍然是一个开放性挑战。先前的方法通常采用分层方法，其中VLMs在高层指令上进行推理，由独立的低层策略执行，例如视觉-语言-动作模型（VLAs）。VLMs与VLAs之间的接口通常是自然语言任务指令，这从根本上限制了VLM推理对低层行为的引导能力。因此，我们引入了可引导策略：这些策略是在各种抽象层次的丰富合成指令上训练的，例如子任务、动作和基于像素的定位坐标。通过提升低层的可控性，可引导策略可以解锁预训练VLM中的知识，从而实现任务泛化的提升。我们通过使用一个学习的高层具身推理器和一个基于上下文学习的现成VLM来控制这些可引导策略，以推理指令的抽象层次。在大量真实世界操作实验中，这两种新方法在具身推理的VLAs和基于VLM的分层基线方法上表现更优，包括在具有挑战性的泛化任务和长时序任务上的表现。</div>
</details>
</div>
<div class="card">
<div class="title">CoPE-VideoLM: Codec Primitives For Efficient Video Language Models</div>
<div class="meta-line">Authors: Sayan Deb Sarkar, Rémi Pautrat, Ondrej Miksik, Marc Pollefeys, Iro Armeni, Mahdi Rad, Mihai Dusmanu</div>
<div class="meta-line">First: 2026-02-13T18:57:31+00:00 · Latest: 2026-02-13T18:57:31+00:00</div>
<div class="meta-line">Comments: Project Page: https://sayands.github.io/cope/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13191v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13191v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sayands.github.io/cope/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos. To fit to the maximum context window constraint, current methods use keyframe sampling which can miss both macro-level events and micro-level details due to the sparse temporal coverage. Furthermore, processing full images and their tokens for each frame incurs substantial computational overhead. To address these limitations, we propose to leverage video codec primitives (specifically motion vectors and residuals) which natively encode video redundancy and sparsity without requiring expensive full-image encoding for most frames. To this end, we introduce lightweight transformer-based encoders that aggregate codec primitives and align their representations with image encoder embeddings through a pre-training strategy that accelerates convergence during end-to-end fine-tuning. Our approach reduces the time-to-first-token by up to $86\%$ and token usage by up to $93\%$ compared to standard VideoLMs. Moreover, by varying the keyframe and codec primitive densities we are able to maintain or exceed performance on $14$ diverse video understanding benchmarks spanning general question answering, temporal reasoning, long-form understanding, and spatial scene understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoPE-VideoLM：用于高效视频语言模型的编解码器基本单元</div>
<div class="mono" style="margin-top:8px">视频语言模型（VideoLMs）使人工智能系统能够理解视频中的时间动态。为了适应最大上下文窗口的限制，当前方法使用关键帧采样，但由于时间覆盖稀疏，可能会遗漏宏观事件和微观细节。此外，对每一帧进行完整图像和其标记的处理会带来显著的计算开销。为了解决这些限制，我们提出利用视频编解码器基本单元（特别是运动向量和残差），这些基本单元原生地编码视频的冗余和稀疏性，而无需对大多数帧进行昂贵的完整图像编码。为此，我们引入了轻量级的基于Transformer的编码器，用于聚合编解码器基本单元，并通过一种预训练策略将它们的表示与图像编码器嵌入对齐，从而加速端到端微调过程中的收敛。与标准的VideoLMs相比，我们的方法将首次标记生成时间减少了高达86%，并将标记使用量减少了高达93%。此外，通过调整关键帧和编解码器基本单元的密度，我们能够在14个多样化的视频理解基准测试中保持或超越性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Video Language Models (VideoLMs) empower AI systems to understand temporal dynamics in videos.</div>
</details>
</div>
<div class="card">
<div class="title">DRL-Based Beam Positioning for LEO Satellite Constellations with Weighted Least Squares</div>
<div class="meta-line">Authors: Po-Heng Chou, Chiapin Wang, Kuan-Hao Chen, Wei-Chen Hsiao</div>
<div class="meta-line">First: 2025-11-12T00:14:10+00:00 · Latest: 2026-02-13T18:56:52+00:00</div>
<div class="meta-line">Comments: 6 pages, 3 figures, 1 table, and submitted to 2026 IEEE ICC Workshops</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.08852v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.08852v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we propose a reinforcement learning based beam weighting framework that couples a policy network with an augmented weighted least squares (WLS) estimator for accurate and low-complexity positioning in multi-beam LEO constellations. Unlike conventional geometry or CSI-dependent approaches, the policy learns directly from uplink pilot responses and geometry features, enabling robust localization without explicit CSI estimation. An augmented WLS jointly estimates position and receiver clock bias, improving numerical stability under dynamic beam geometry. Across representative scenarios, the proposed method reduces the mean positioning error by 99.3% compared with the geometry-based baseline, achieving 0.395 m RMSE with near real-time inference.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度强化学习的LEO卫星星座波束定位方法</div>
<div class="mono" style="margin-top:8px">本文提出了一种基于强化学习的波束加权框架，结合策略网络与增强加权最小二乘（WLS）估计器，以实现多波束LEO星座中的高精度、低复杂度定位。与传统的几何或CSI依赖方法不同，该策略直接从上行链路导频响应和几何特征中学习，无需显式的CSI估计即可实现鲁棒定位。增强的WLS同时估计位置和接收机时钟偏差，提高了在动态波束几何下的数值稳定性。在代表性场景中，所提方法相比基于几何的基准方法将平均定位误差降低了99.3%，实现了接近实时推理的0.395米RMSE。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this paper, we propose a reinforcement learning based beam weighting framework that couples a policy network with an augmented weighted least squares (WLS) estimator for accurate and low-complexity positioning in multi-beam LEO constellations.</div>
</details>
</div>
<div class="card">
<div class="title">Addressing the Hubble tension with Sterile Neutrino Dark Matter</div>
<div class="meta-line">Authors: Debtosh Chowdhury, Md Sariful Islam</div>
<div class="meta-line">First: 2026-02-13T18:55:39+00:00 · Latest: 2026-02-13T18:55:39+00:00</div>
<div class="meta-line">Comments: 8 pages, 5 captioned figures, Comments are welcome</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13189v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13189v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One of the promising dark matter (DM) candidates is a keV scale sterile neutrino. In the early universe the observed relic of the sterile neutrino DM is generated via the \textit{Dodelson-Widrow} mechanism. However, this production scenario is severely constraint by various astrophysical observations. Many non-standard interactions between active ($ν_a$) and sterile ($ν_s$) neutrino have been proposed to evade these astrophysical bounds. Here, we study sterile neutrino in the context of a mass-varying scenario by coupling both active and sterile neutrino to a scalar field. This novel mechanism opens up a new parameter space that generates the observed DM relic and alleviates the \textit{Hubble tension}. We find that the resulting parameter space can be fully probed by future X-ray missions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过惰性中微子暗物质缓解哈勃张力</div>
<div class="mono" style="margin-top:8px">一个有前景的暗物质（DM）候选者是keV尺度的惰性中微子。在早期宇宙中，惰性中微子暗物质的观测残余是通过\textit{Dodelson-Widrow}机制产生的。然而，这种产生机制受到各种天体物理观测的严重限制。许多非标准的主动中微子（ν_a）与惰性中微子（ν_s）之间的相互作用已被提出以规避这些天体物理限制。在此，我们通过将主动和惰性中微子都耦合到标量场的背景下，研究惰性中微子在质量变化情景中的特性。这种新颖的机制开辟了一个新的参数空间，能够生成观测到的暗物质残余并缓解\textit{哈勃张力}。我们发现，由此产生的参数空间可以通过未来的X射线任务完全探测。</div>
</details>
</div>
<div class="card">
<div class="title">Learning-based Radio Link Failure Prediction Based on Measurement Dataset in Railway Environments</div>
<div class="meta-line">Authors: Po-Heng Chou, Da-Chih Lin, Hung-Yu Wei, Walid Saad, Yu Tsao</div>
<div class="meta-line">First: 2025-11-12T00:13:37+00:00 · Latest: 2026-02-13T18:53:36+00:00</div>
<div class="meta-line">Comments: 6 pages, 3 figures, 2 tables, and submitted to 2026 IEEE ICC Workshops</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.08851v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.08851v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a measurement-driven case study on early radio link failure (RLF) warning as device-side network sensing and analytics for proactive mobility management in 5G non-standalone (NSA) railway environments. Using 10~Hz metro-train measurement traces with serving- and neighbor-cell indicators, we benchmark six representative learning models, including CNN, LSTM, XGBoost, Anomaly Transformer, PatchTST, and TimesNet, under multiple observation windows and prediction horizons. Rather than proposing a new prediction architecture, this study focuses on quantifying the feasibility of early warning and the trade-offs among observation context, prediction horizon, and alarm reliability under real railway mobility. Experimental results show that learning models can anticipate RLF-related reliability degradation seconds in advance using lightweight features available on commercial devices. The presented benchmark provides practical insights for sensing-assisted communication control, such as proactive redundancy activation and adaptive handover strategies, aligning with the 6G vision of integrating sensing and analytics into mobility control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于测量数据集的铁路环境中基于学习的无线链路故障预测</div>
<div class="mono" style="margin-top:8px">本文提出了一项基于测量的案例研究，探讨在5G非独立组网（NSA）铁路环境中，设备侧网络感知与分析用于前瞻性移动性管理的早期无线链路故障（RLF）预警。利用10~Hz地铁列车测量轨迹及服务小区和邻近小区指示器，我们在多种观测窗口和预测时间范围内对六个代表性学习模型（包括CNN、LSTM、XGBoost、Anomaly Transformer、PatchTST和TimesNet）进行了基准测试。本研究不提出新的预测架构，而是聚焦于在真实铁路移动性条件下，量化早期预警的可行性以及观测上下文、预测时间范围和告警可靠性之间的权衡。实验结果表明，学习模型可以利用商用设备上轻量级特征，提前数秒预测与RLF相关的可靠性下降。所提出的基准测试为感知辅助通信控制提供了实用见解，例如前瞻性冗余激活和自适应切换策略，符合6G将感知与分析集成到移动性控制的愿景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper presents a measurement-driven case study on early radio link failure (RLF) warning as device-side network sensing and analytics for proactive mobility management in 5G non-standalone (NSA) railway environments.</div>
</details>
</div>
<div class="card">
<div class="title">R-Zero: Self-Evolving Reasoning LLM from Zero Data</div>
<div class="meta-line">Authors: Chengsong Huang, Wenhao Yu, Xiaoyang Wang, Hongming Zhang, Zongxia Li, Ruosen Li, Jiaxin Huang, Haitao Mi, Dong Yu</div>
<div class="meta-line">First: 2025-08-07T03:38:16+00:00 · Latest: 2026-02-13T18:53:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.05004v4">Abs</a> · <a href="https://arxiv.org/pdf/2508.05004v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Self-evolving Large Language Models (LLMs) offer a scalable path toward super-intelligence by autonomously generating, refining, and learning from their own experiences. However, existing methods for training such models still rely heavily on vast human-curated tasks and labels, typically via fine-tuning or reinforcement learning, which poses a fundamental bottleneck to advancing AI systems toward capabilities beyond human intelligence. To overcome this limitation, we introduce R-Zero, a fully autonomous framework that generates its own training data from scratch. Starting from a single base LLM, R-Zero initializes two independent models with distinct roles, a Challenger and a Solver. These models are optimized separately and co-evolve through interaction: the Challenger is rewarded for proposing tasks near the edge of the Solver capability, and the Solver is rewarded for solving increasingly challenging tasks posed by the Challenger. This process yields a targeted, self-improving curriculum without any pre-existing tasks and labels. Empirically, R-Zero substantially improves reasoning capability across different backbone LLMs, e.g., boosting the Qwen3-4B-Base by +6.49 on math-reasoning benchmarks and +7.54 on general-domain reasoning benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>R-Zero：从零数据自进化的推理大语言模型</div>
<div class="mono" style="margin-top:8px">自进化的大型语言模型（LLMs）通过自主生成、优化和从自身经验中学习，为实现超智能提供了一条可扩展的路径。然而，现有方法在训练此类模型时仍严重依赖大量人工标注的任务和标签，通常通过微调或强化学习实现，这成为推动AI系统超越人类智能能力的根本瓶颈。为克服这一限制，我们引入了R-Zero，一个完全自主的框架，能够从零开始生成自己的训练数据。R-Zero从一个基础LLM出发，初始化两个独立的模型，分别承担挑战者和求解者的角色。这两个模型通过交互共同进化：挑战者因提出接近求解者能力边缘的任务而获得奖励，求解者则因解决挑战者提出的越来越困难的任务而获得奖励。这一过程无需任何预设任务和标签，即可生成一个有针对性的、自我提升的课程体系。实验表明，R-Zero在不同主干LLMs上显著提升了推理能力，例如在数学推理基准上将Qwen3-4B-Base提升+6.49，在通用领域推理基准上提升+7.54。</div>
</details>
</div>
<div class="card">
<div class="title">FlexAM: Flexible Appearance-Motion Decomposition for Versatile Video Generation Control</div>
<div class="meta-line">Authors: Mingzhi Sheng, Zekai Gu, Peng Li, Cheng Lin, Hao-Xiang Guo, Ying-Cong Chen, Yuan Liu</div>
<div class="meta-line">First: 2026-02-13T18:52:11+00:00 · Latest: 2026-02-13T18:52:11+00:00</div>
<div class="meta-line">Comments: Codes: https://github.com/IGL-HKUST/FlexAM</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13185v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13185v1">PDF</a> · <a href="https://github.com/IGL-HKUST/FlexAM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effective and generalizable control in video generation remains a significant challenge. While many methods rely on ambiguous or task-specific signals, we argue that a fundamental disentanglement of &quot;appearance&quot; and &quot;motion&quot; provides a more robust and scalable pathway. We propose FlexAM, a unified framework built upon a novel 3D control signal. This signal represents video dynamics as a point cloud, introducing three key enhancements: multi-frequency positional encoding to distinguish fine-grained motion, depth-aware positional encoding, and a flexible control signal for balancing precision and generative quality. This representation allows FlexAM to effectively disentangle appearance and motion, enabling a wide range of tasks including I2V/V2V editing, camera control, and spatial object editing. Extensive experiments demonstrate that FlexAM achieves superior performance across all evaluated tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlexAM：用于多功能视频生成控制的灵活外观-运动分解</div>
<div class="mono" style="margin-top:8px">视频生成中的有效且可泛化的控制仍然是一个重大挑战。尽管许多方法依赖于模糊或任务特定的信号，我们认为对&quot;外观&quot;和&quot;运动&quot;进行根本性的解耦提供了一种更稳健且可扩展的途径。我们提出FlexAM，这是一个基于新颖3D控制信号的统一框架。该信号将视频动态表示为点云，引入了三个关键增强：多频率位置编码以区分细粒度运动、深度感知位置编码以及用于平衡精度和生成质量的灵活控制信号。这种表示使FlexAM能够有效解耦外观和运动，从而支持包括I2V/V2V编辑、相机控制和空间物体编辑在内的广泛任务。大量实验表明，FlexAM在所有评估任务中均实现了优越的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Effective and generalizable control in video generation remains a significant challenge.</div>
</details>
</div>
<div class="card">
<div class="title">The Fuzzy Front Ends: Reflections on the Never-Ending Story of Visualization Co-Design</div>
<div class="meta-line">Authors: Wei Wei, Foroozan Daneshzand, Zezhong Wang, Erica Mattson, Charles Perin, Sheelagh Carpendale</div>
<div class="meta-line">First: 2026-02-13T18:43:23+00:00 · Latest: 2026-02-13T18:43:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13182v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13182v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Co-design is an increasingly popular approach in HCI and visualization, yet there is little guidance on how to effectively apply this method in visualization contexts. In this paper, we visually present our experience of a two-and-a-half-year co-design project with the local arts community. Focusing on facilitating community exploration and sense-making around arts funding distribution, the project involved a series of co-design sessions between visualization researchers and members of the arts community. Through these iterative sessions, we built shared understanding and developed visualization prototypes tailored to community needs. However, the practice is far from complete, and we found ourselves continually returning to the &quot;fuzzy front end&quot; of the co-design process. We share this ongoing story through comic-style visuals and reflect on three fuzzy front ends that we encountered during the project. By sharing these experiences with the visualization community, we hope to offer insights that others can draw on in their own community-engaged co-design work.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模糊前端：关于可视化协同设计永无止境故事的反思</div>
<div class="mono" style="margin-top:8px">协同设计在人机交互和可视化领域日益流行，但在可视化情境中如何有效应用这一方法的指导却很少。本文通过视觉方式呈现我们与本地艺术社区合作进行的为期两年半的协同设计项目经验。项目聚焦于促进社区对艺术资金分配的探索与意义建构，涉及可视化研究者与艺术社区成员之间的一系列协同设计会议。通过这些迭代会议，我们建立了共同的理解，并开发出符合社区需求的可视化原型。然而，这一实践远未完成，我们发现自己不断回到协同设计过程的&quot;模糊前端&quot;。我们通过漫画风格的视觉呈现这一持续进行的故事，并反思在项目中遇到的三个模糊前端问题。通过与可视化社区分享这些经验，我们希望为其他从事社区参与式协同设计工作的人提供参考见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Co-design is an increasingly popular approach in HCI and visualization, yet there is little guidance on how to effectively apply this method in visualization contexts.</div>
</details>
</div>
<div class="card">
<div class="title">Selection of CMIP6 Models for Regional Precipitation Projection and Climate Change Assessment in the Jhelum and Chenab River Basins</div>
<div class="meta-line">Authors: Saad Ahmed Jamal, Ammara Nusrat, Muhammad Azmat, Muhammad Osama Nusrat</div>
<div class="meta-line">First: 2026-02-13T18:41:40+00:00 · Latest: 2026-02-13T18:41:40+00:00</div>
<div class="meta-line">Comments: 28 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13181v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13181v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Effective water resource management depends on accurate projections of flows in water channels. For projected climate data, use of different General Circulation Models (GCM) simulates contrasting results. This study shows selection of GCM for the latest generation CMIP6 for hydroclimate change impact studies. Envelope based method was used for the selection, which includes components based on machine learning techniques, allowing the selection of GCMs without the need for in-situ reference data. According to our knowledge, for the first time, such a comparison was performed for the CMIP6 Shared Socioeconomic Pathway (SSP) scenarios data. In addition, the effect of climate change under SSP scenarios was studied, along with the calculation of extreme indices. Finally, GCMs were compared to quantify spatiotemporal differences between CMIP5 and CMIP6 data. Results provide NorESM2 LM, FGOALS g3 as selected models for the Jhelum and Chenab River. Highly vulnerable regions under the effect of climate change were highlighted through spatial maps, which included parts of Punjab, Jammu, and Kashmir. Upon comparison of CMIP5 and CMIP6, no discernible difference was found between the RCP and SSP scenarios precipitation projections. In the future, more detailed statistical comparisons could further reinforce the proposition.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>杰赫勒姆河和切纳布河流域CMIP6模型的选择用于区域降水预测与气候变化评估</div>
<div class="mono" style="margin-top:8px">有效的水资源管理依赖于对水道流量的准确预测。对于预测的气候数据，使用不同的通用环流模型（GCM）会模拟出不同的结果。本研究展示了在最新一代CMIP6中选择GCM用于水文气候变迁影响研究的方法。该方法基于包络分析，包含基于机器学习技术的组件，使得在无需现场参考数据的情况下也能选择GCM。据我们所知，这是首次对CMIP6共享社会经济路径（SSP）情景数据进行此类比较。此外，还研究了SSP情景下的气候变化影响，并计算了极端指数。最后，通过比较GCM来量化CMIP5和CMIP6数据在时空上的差异。结果表明，杰赫勒姆河和切纳布河流域选定的模型为NorESM2 LM和FGOALS g3。通过空间地图突出了受气候变化影响高度脆弱的地区，包括旁遮普邦、查谟和克什米尔的部分地区。在比较CMIP5和CMIP6时，未发现RCP和SSP情景降水预测之间有明显的差异。未来更详细的统计比较可能会进一步支持这一观点。</div>
</details>
</div>
<div class="card">
<div class="title">Improved Regret Guarantees for Online Mirror Descent using a Portfolio of Mirror Maps</div>
<div class="meta-line">Authors: Swati Gupta, Jai Moondra, Mohit Singh</div>
<div class="meta-line">First: 2026-02-13T18:37:26+00:00 · Latest: 2026-02-13T18:37:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13177v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13177v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">OMD and its variants give a flexible framework for OCO where the performance depends crucially on the choice of the mirror map. While the geometries underlying OPGD and OEG, both special cases of OMD, are well understood, it remains a challenging open question on how to construct an optimal mirror map for any given constrained set and a general family of loss functions, e.g., sparse losses. Motivated by parameterizing a near-optimal set of mirror maps, we consider a simpler question: is it even possible to obtain polynomial gains in regret by using mirror maps for geometries that interpolate between $L_1$ and $L_2$, which may not be possible by restricting to only OEG ($L_1$) or OPGD ($L_2$).
  Our main result answers this question positively. We show that mirror maps based on block norms adapt better to the sparsity of loss functions, compared to previous $L_p$ (for $p \in [1, 2]$) interpolations. In particular, we construct a family of online convex optimization instances in $\mathbb{R}^d$, where block norm-based mirror maps achieve a provable polynomial (in $d$) improvement in regret over OEG and OPGD for sparse loss functions. We then turn to the setting in which the sparsity level of the loss functions is unknown. In this case, the choice of geometry itself becomes an online decision problem. We first show that naively switching between OEG and OPGD can incur linear regret, highlighting the intrinsic difficulty of geometry selection. To overcome this issue, we propose a meta-algorithm based on multiplicative weights that dynamically selects among a family of uniform block norms. We show that this approach effectively tunes OMD to the sparsity of the losses, yielding adaptive regret guarantees. Overall, our results demonstrate that online mirror-map selection can significantly enhance the ability of OMD to exploit sparsity in online convex optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用镜像映射组合的在线镜像下降算法的改进遗憾保证</div>
<div class="mono" style="margin-top:8px">OMD及其变体为OCO提供了一个灵活的框架，其性能在很大程度上依赖于镜像映射的选择。尽管OPGD和OEG（均为OMD的特例）所基于的几何结构已被很好地理解，但如何为任意给定的约束集和一般的损失函数族（例如稀疏损失函数）构造最优镜像映射仍然是一个具有挑战性的开放问题。受参数化近似最优镜像映射集的启发，我们考虑一个更简单的问题：是否可以通过在$ L_1 $和$ L_2 $之间插值的几何结构使用镜像映射来获得多项式级别的遗憾改进，这在仅限制于OEG（$ L_1 $）或OPGD（$ L_2 $）的情况下可能无法实现。
我们的主要结果回答了这一问题，表明基于块范数的镜像映射比之前$ L_p $（$ p \in [1, 2] $）的插值方法更能适应损失函数的稀疏性。具体而言，我们构造了一类在线凸优化问题，其中基于块范数的镜像映射在稀疏损失函数下相比OEG和OPGD实现了可证明的多项式（关于$ d $）级别的遗憾改进。随后，我们考虑损失函数稀疏性未知的情况。在这种情况下，几何结构的选择本身成为一个在线决策问题。我们首先证明，简单地在OEG和OPGD之间切换可能导致线性级别的遗憾，突显了几何选择的内在困难。为了解决这一问题，我们提出了一种基于乘法权重的元算法，动态地在一系列均匀块范数中进行选择。我们证明，这种方法能够有效地将OMD调整到损失函数的稀疏性，从而获得自适应的遗憾保证。总体而言，我们的结果表明，在线镜像映射选择可以显著增强OMD在在线凸优化中利用稀疏性的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Monocular Markerless Motion Capture Enables Quantitative Assessment of Upper Extremity Reachable Workspace</div>
<div class="meta-line">Authors: Seth Donahue, J. D. Peiffer, R. Tyler Richardson, Yishan Zhong, Shaun Q. Y. Tan, Benoit Marteau, Stephanie R. Russo, May D. Wang, R. James Cotton, Ross Chafetz</div>
<div class="meta-line">First: 2026-02-13T18:36:27+00:00 · Latest: 2026-02-13T18:36:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13176v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13176v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To validate a clinically accessible approach for quantifying the Upper Extremity Reachable Workspace (UERW) using a single (monocular) camera and Artificial Intelligence (AI)-driven Markerless Motion Capture (MMC) for biomechanical analysis. Objective assessment and validation of these techniques for specific clinically oriented tasks are crucial for their adoption in clinical motion analysis. AI-driven monocular MMC reduces the barriers to adoption in the clinic and has the potential to reduce the overhead for analysis of this common clinical assessment. Nine adult participants with no impairments performed the standardized UERW task, which entails reaching targets distributed across a virtual sphere centered on the torso, with targets displayed in a VR headset. Movements were simultaneously captured using a marker-based motion capture system and a set of eight FLIR cameras. We performed monocular video analysis on two of these video camera views to compare a frontal and offset camera configurations. The frontal camera orientation demonstrated strong agreement with the marker-based reference, exhibiting a minimal mean bias of $0.61 \pm 0.12$ \% reachspace reached per octanct (mean $\pm$ standard deviation). In contrast, the offset camera view underestimated the percent workspace reached ($-5.66 \pm 0.45$ \% reachspace reached). Conclusion: The findings support the feasibility of a frontal monocular camera configuration for UERW assessment, particularly for anterior workspace evaluation where agreement with marker-based motion capture was highest. The overall performance demonstrates clinical potential for practical, single-camera assessments. This study provides the first validation of monocular MMC system for the assessment of the UERW task. By reducing technical complexity, this approach enables broader implementation of quantitative upper extremity mobility assessment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>单目无标记运动捕捉可实现上肢可达工作空间的定量评估</div>
<div class="mono" style="margin-top:8px">为验证一种临床可访问的方法，使用单目相机和人工智能驱动的无标记运动捕捉（MMC）技术对上肢可达工作空间（UERW）进行量化分析。这些技术在特定临床任务中的客观评估和验证对于其在临床运动分析中的应用至关重要。人工智能驱动的单目MMC技术降低了在临床环境中采用的门槛，并有潜力减少对这种常见临床评估的分析负担。九名无功能障碍的成年参与者执行了标准化的UERW任务，该任务包括在以躯干为中心的虚拟球面上达到分布的多个目标，目标通过VR头显显示。运动同时使用基于标记的运动捕捉系统和一组八台FLIR相机进行记录。我们对其中两个视频相机视角进行了单目视频分析，以比较正面和偏移视角的配置效果。正面视角与基于标记的参考系统表现出强一致性，每个八分之一空间的平均偏差为0.61 ± 0.12%。相比之下，偏移视角低估了达到的百分比工作空间（-5.66 ± 0.45%）。结论：研究结果支持正面单目相机配置在UERW评估中的可行性，尤其是在前部工作空间评估中，与基于标记的运动捕捉的一致性最高。整体性能表明，该方法在临床实践中具有应用潜力。本研究首次验证了单目MMC系统在UERW任务评估中的有效性。通过降低技术复杂性，这种方法有助于更广泛地实施上肢运动能力的定量评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">To validate a clinically accessible approach for quantifying the Upper Extremity Reachable Workspace (UERW) using a single (monocular) camera and Artificial Intelligence (AI)-driven Markerless Motion Capture (MMC) for biomechanical analysis.</div>
</details>
</div>
<div class="card">
<div class="title">Privacy-Preserving Federated Learning with Verifiable Fairness Guarantees</div>
<div class="meta-line">Authors: Mohammed Himayath Ali, Mohammed Aqib Abdullah, Syed Muneer Hussain, Mohammed Mudassir Uddin, Shahnawaz Alam</div>
<div class="meta-line">First: 2026-01-18T15:06:30+00:00 · Latest: 2026-02-13T18:35:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.12447v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.12447v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated learning enables collaborative model training across distributed institutions without centralizing sensitive data; however, ensuring algorithmic fairness across heterogeneous data distributions while preserving privacy remains fundamentally unresolved. This paper introduces CryptoFair-FL, a novel cryptographic framework providing the first verifiable fairness guarantees for federated learning systems under formal security definitions. The proposed approach combines additively homomorphic encryption with secure multi-party computation to enable privacy-preserving verification of demographic parity and equalized odds metrics without revealing protected attribute distributions or individual predictions. A novel batched verification protocol reduces computational complexity from BigO(n^2) to BigO(n \log n) while maintaining (\dparam, \deltap)-differential privacy with dparam = 0.5 and deltap = 10^{-6}. Theoretical analysis establishes information-theoretic lower bounds on the privacy cost of fairness verification, demonstrating that the proposed protocol achieves near-optimal privacy-fairness tradeoffs. Comprehensive experiments across four benchmark datasets (MIMIC-IV healthcare records, Adult Income, CelebA, and a novel FedFair-100 benchmark) demonstrate that CryptoFair-FL reduces fairness violations from 0.231 to 0.031 demographic parity difference while incurring only 2.3 times computational overhead compared to standard federated averaging. The framework successfully defends against attribute inference attacks, maintaining adversarial success probability below 0.05 across all tested configurations. These results establish a practical pathway for deploying fairness-aware federated learning in regulated industries requiring both privacy protection and algorithmic accountability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有可验证公平性保证的隐私保护联邦学习</div>
<div class="mono" style="margin-top:8px">联邦学习能够在不集中敏感数据的情况下，使分布式机构协作训练模型；然而，在保持隐私的同时确保算法公平性，特别是在异构数据分布中，仍然是一个根本性未解决的问题。本文提出了一种名为CryptoFair-FL的新颖密码学框架，为联邦学习系统在形式化安全定义下提供了首个可验证的公平性保证。所提出的方法结合了可加同态加密与安全多方计算，能够在不泄露受保护属性分布或个体预测的情况下实现隐私保护的公平性验证。一种新颖的批量验证协议将计算复杂度从BigO(n^2)降低到BigO(n \log n)，同时保持(\dparam, \deltap)-差分隐私，其中dparam = 0.5，deltap = 10^{-6}。理论分析建立了公平性验证的隐私成本信息论下限，证明了所提出的协议在隐私与公平性之间的权衡接近最优。在四个基准数据集（MIMIC-IV医疗记录、Adult Income、CelebA以及新提出的FedFair-100基准）上的全面实验表明，CryptoFair-FL将公平性违规从0.231降低到0.031的群体差异，同时仅产生比标准联邦平均2.3倍的计算开销。该框架成功抵御了属性推断攻击，在所有测试配置中保持对抗成功率低于0.05。这些结果为在需要隐私保护和算法问责的监管行业部署公平感知的联邦学习提供了实际路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Federated learning enables collaborative model training across distributed institutions without centralizing sensitive data; however, ensuring algorithmic fairness across heterogeneous data distributions while preserving privacy remains fundamentally unresolved.</div>
</details>
</div>
<div class="card">
<div class="title">tLoRA: Efficient Multi-LoRA Training with Elastic Shared Super-Models</div>
<div class="meta-line">Authors: Kevin Li, Dibyadeep Saha, Avni Kanodia, Fan Lai</div>
<div class="meta-line">First: 2026-02-06T23:26:02+00:00 · Latest: 2026-02-13T18:35:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07263v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.07263v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As Low-Rank Adaptation (LoRA) becomes the standard approach for efficiently fine-tuning large language models (LLMs), shared clusters increasingly execute many concurrent LoRA training jobs over the same frozen backbone. While recent advances enable batching (co-locating) multiple adapters during serving, efficient training-time co-location of heterogeneous LoRA adapters presents unique challenges. Jobs often differ in adapter rank, batch size, and resource allocation, and naïve batching can introduce synchronization stalls, communication overheads, and per-job slowdowns that are worse than executing independently. We introduce tLoRA, a framework that enables efficient batch training of multiple LoRA jobs. tLoRA fuses adapters that share the same base model into an elastic shared super-model, exploiting existing distributed training frameworks to derive parallelism plans that share resources effectively. At the kernel level, tLoRA employs a fused LoRA kernel that adaptively reconstructs low-rank computation tiles and schedules rank-aware nano-batches to maximize overlap between computation and communication across adapters. At the scheduling layer, tLoRA incorporates an online, residual-capacity-aware scheduler that adaptively groups jobs to maximize collective throughput. Evaluations using real-world cluster traces demonstrate that tLoRA improves training throughput by 1.2--1.8x, job training completion time by 2.3--5.4x, and GPU utilization by 37%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>tLoRA: 基于弹性共享超模型的高效多LoRA训练</div>
<div class="mono" style="margin-top:8px">随着低秩适应（LoRA）成为高效微调大语言模型（LLMs）的标准方法，共享集群越来越多地在同一个冻结主干模型上执行多个并发的LoRA训练任务。尽管近期进展使得在服务时可以将多个适配器进行批量处理（共存），但在训练时对异构LoRA适配器进行高效批量处理仍面临独特挑战。任务通常在适配器秩、批量大小和资源分配上存在差异，而简单的批量处理可能会引入同步停滞、通信开销以及每个任务的性能下降，其效果甚至不如独立执行。我们提出tLoRA框架，用于高效地批量训练多个LoRA任务。tLoRA将共享相同基础模型的适配器融合为一个弹性共享超模型，利用现有的分布式训练框架生成有效的并行计划，实现资源的高效共享。在内核层面，tLoRA采用融合的LoRA内核，自适应地重构低秩计算块，并调度秩感知的纳米批次，以最大化不同适配器之间计算与通信的重叠。在调度层，tLoRA引入了一个在线、剩余容量感知的调度器，自适应地将任务分组以最大化整体吞吐量。使用真实集群轨迹进行的评估表明，tLoRA可将训练吞吐量提升1.2至1.8倍，任务训练完成时间缩短2.3至5.4倍，并将GPU利用率提升37%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">As Low-Rank Adaptation (LoRA) becomes the standard approach for efficiently fine-tuning large language models (LLMs), shared clusters increasingly execute many concurrent LoRA training jobs over the same frozen backbone.</div>
</details>
</div>
<div class="card">
<div class="title">Solving Conic Programs over Sparse Graphs using a Variational Quantum Approach: The Case of the Optimal Power Flow</div>
<div class="meta-line">Authors: Thinh Viet Le, Mark M. Wilde, Vassilis Kekatos</div>
<div class="meta-line">First: 2025-08-30T03:47:52+00:00 · Latest: 2026-02-13T18:33:19+00:00</div>
<div class="meta-line">Comments: 21 pages, 7 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.00341v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.00341v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conic programs arise broadly in physics, quantum information, machine learning, and engineering, many of which are defined over sparse graphs. Although such problems can be solved in polynomial time using classical interior-point solvers, the computational complexity scales unfavorably with graph size. In this context, this work proposes a variational quantum paradigm for solving conic programs, including quadratically constrained quadratic programs (QCQPs) and semidefinite programs (SDPs). We encode primal variables via the state of a parameterized quantum circuit (PQC), and dual variables via the probability mass function of a second PQC. The Lagrangian function can thus be expressed as scaled expectations of quantum observables. A primal-dual solution can be found by minimizing/maximizing the Lagrangian over the parameters of the first/second PQC. We pursue saddle points of the Lagrangian in a hybrid fashion. Gradients of the Lagrangian are estimated using the two PQCs, while PQC parameters are updated classically using a primal-dual method. We propose permuting the primal variables so that related observables are expressed in a banded form, enabling efficient measurement. The proposed framework is applied to the OPF problem, a large-scale optimization problem central to the operation of electric power systems. Numerical tests on the IEEE 57-node power system using Pennylane&#x27;s simulator corroborate that the proposed doubly variational quantum framework can find high-quality OPF solutions. Although showcased for the OPF, this framework features a broader scope, including conic programs with numerous variables and constraints, problems defined over sparse graphs, and training quantum machine learning models to satisfy constraints.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用变分量子方法在稀疏图上求解锥规划问题：以最优潮流为例</div>
<div class="mono" style="margin-top:8px">锥规划问题广泛出现在物理、量子信息、机器学习和工程领域，其中许多问题是在稀疏图上定义的。尽管可以使用经典内点求解器在多项式时间内求解此类问题，但计算复杂度随着图的规模增长而不利地增加。在此背景下，本文提出了一种变分量子范式来求解锥规划问题，包括二次约束二次规划（QCQPs）和半正定规划（SDPs）。我们通过参数化量子电路（PQC）的状态来编码原始变量，通过第二个PQC的概率质量函数来编码对偶变量。因此，拉格朗日函数可以表示为量子可观测量的缩放期望值。通过在第一个和第二个PQC的参数上进行最小化/最大化，可以找到原始-对偶解。我们采用混合方式寻找拉格朗日函数的鞍点。利用两个PQC估计拉格朗日函数的梯度，同时使用原始-对偶方法在经典计算机上更新PQC参数。我们提出对原始变量进行排列，使得相关的可观测量以带状形式表达，从而实现高效的测量。所提出的框架应用于最优潮流（OPF）问题，这是一个电力系统运行中的大规模优化问题。使用Pennylane模拟器在IEEE 57节点电力系统上进行的数值测试证实了所提出的双重变分量子框架能够找到高质量的OPF解。尽管本文以OPF为例展示该框架，但其应用范围更广，包括具有大量变量和约束的锥规划问题、在稀疏图上定义的问题，以及训练满足约束条件的量子机器学习模型。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Conic programs arise broadly in physics, quantum information, machine learning, and engineering, many of which are defined over sparse graphs.</div>
</details>
</div>
<div class="card">
<div class="title">Learning functional components of PDEs from data using neural networks</div>
<div class="meta-line">Authors: Torkel E. Loman, Yurij Salmaniw, Antonio Leon Villares, Jose A. Carrillo, Ruth E. Baker</div>
<div class="meta-line">First: 2026-02-13T18:32:33+00:00 · Latest: 2026-02-13T18:32:33+00:00</div>
<div class="meta-line">Comments: 16 pages with 6 figures. Additional 24 pages and 19 figures supplementary information</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13174v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13174v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Partial differential equations often contain unknown functions that are difficult or impossible to measure directly, hampering our ability to derive predictions from the model. Workflows for recovering scalar PDE parameters from data are well studied: here we show how similar workflows can be used to recover functions from data. Specifically, we embed neural networks into the PDE and show how, as they are trained on data, they can approximate unknown functions with arbitrary accuracy. Using nonlocal aggregation-diffusion equations as a case study, we recover interaction kernels and external potentials from steady state data. Specifically, we investigate how a wide range of factors, such as the number of available solutions, their properties, sampling density, and measurement noise, affect our ability to successfully recover functions. Our approach is advantageous because it can utilise standard parameter-fitting workflows, and in that the trained PDE can be treated as a normal PDE for purposes such as generating system predictions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用神经网络从数据中学习PDE的功能组件</div>
<div class="mono" style="margin-top:8px">偏微分方程通常包含难以或无法直接测量的未知函数，这阻碍了我们从模型中推导预测的能力。恢复标量PDE参数的流程已被广泛研究：在这里，我们展示了如何利用类似的流程从数据中恢复函数。具体而言，我们将神经网络嵌入到PDE中，并展示它们在训练过程中如何以任意精度近似未知函数。以非局部聚合扩散方程为例，我们从稳态数据中恢复相互作用核和外部势场。我们具体研究了诸如可用解的数量、其特性、采样密度和测量噪声等因素如何影响我们成功恢复函数的能力。我们的方法具有优势，因为它可以利用标准的参数拟合流程，并且训练后的PDE可以被视为普通PDE，用于生成系统预测等目的。</div>
</details>
</div>
<div class="card">
<div class="title">LongStream: Long-Sequence Streaming Autoregressive Visual Geometry</div>
<div class="meta-line">Authors: Chong Cheng, Xianda Chen, Tao Xie, Wei Yin, Weiqiang Ren, Qian Zhang, Xiaoyuang Guo, Hao Wang</div>
<div class="meta-line">First: 2026-02-13T18:30:51+00:00 · Latest: 2026-02-13T18:30:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13172v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13172v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://3dagentworld.github.io/longstream/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Long-sequence streaming 3D reconstruction remains a significant open challenge. Existing autoregressive models often fail when processing long sequences. They typically anchor poses to the first frame, which leads to attention decay, scale drift, and extrapolation errors. We introduce LongStream, a novel gauge-decoupled streaming visual geometry model for metric-scale scene reconstruction across thousands of frames. Our approach is threefold. First, we discard the first-frame anchor and predict keyframe-relative poses. This reformulates long-range extrapolation into a constant-difficulty local task. Second, we introduce orthogonal scale learning. This method fully disentangles geometry from scale estimation to suppress drift. Finally, we solve Transformer cache issues such as attention-sink reliance and long-term KV-cache contamination. We propose cache-consistent training combined with periodic cache refresh. This approach suppresses attention degradation over ultra-long sequences and reduces the gap between training and inference. Experiments show LongStream achieves state-of-the-art performance. It delivers stable, metric-scale reconstruction over kilometer-scale sequences at 18 FPS. Project Page: https://3dagentworld.github.io/longstream/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LongStream: 长序列流式自回归视觉几何</div>
<div class="mono" style="margin-top:8px">长序列流式3D重建仍然是一个重要的开放性挑战。现有的自回归模型在处理长序列时往往表现不佳。它们通常将姿态锚定在第一帧，导致注意力衰减、尺度漂移和外推误差。我们引入LongStream，这是一种新型的解耦流式视觉几何模型，用于在数千帧下实现度量尺度的场景重建。我们的方法包含三个部分。首先，我们摒弃第一帧锚定，预测关键帧相对姿态。这将长距离外推问题转化为恒定难度的局部任务。其次，我们引入正交尺度学习。该方法完全解耦几何与尺度估计，以抑制漂移。最后，我们解决Transformer缓存问题，如注意力沉降依赖和长期KV缓存污染。我们提出结合定期缓存刷新的缓存一致性训练方法。这种方法抑制了在超长序列上的注意力退化，并缩小了训练与推理之间的差距。实验表明，LongStream实现了最先进的性能。它在千米级序列上以18 FPS的速度提供稳定、度量尺度的重建。项目页面：https://3dagentworld.github.io/longstream/</div>
</details>
</div>
<div class="card">
<div class="title">MissionHD: Hyperdimensional Refinement of Distribution-Deficient Reasoning Graphs for Video Anomaly Detection</div>
<div class="meta-line">Authors: Sanggeon Yun, Raheeb Hassan, Ryozo Masukawa, Nathaniel D. Bastian, Mohsen Imani</div>
<div class="meta-line">First: 2025-08-20T14:43:04+00:00 · Latest: 2026-02-13T18:30:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.14746v4">Abs</a> · <a href="https://arxiv.org/pdf/2508.14746v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-generated reasoning graphs, referred to as mission-specific graphs (MSGs), are increasingly used for video anomaly detection (VAD) and recognition (VAR). However, they are typically treated as fixed despite being generic and distribution-deficient. Conventional graph structure refinement (GSR) methods are ill-suited to this setting, as they rely on learning structural distributions that are absent in LLM-generated graphs. We propose HDC-constrained Graph Structure Refinement (HDC-GSR), a new paradigm that directly optimizes a decodable, task-aligned graph representation in a single hyperdimensional space without distribution modeling. Leveraging Hyperdimensional Computing (HDC), our framework encodes graphs via binding and bundling operations, aligns the resulting graph code with downstream loss, and decodes edge contributions to refine the structure. We instantiate this approach as MissionHD for weakly supervised VAD/VAR and demonstrate consistent performance gains on benchmark datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MissionHD：用于视频异常检测的分布不足推理图的超维度精炼</div>
<div class="mono" style="margin-top:8px">由大语言模型生成的推理图，称为任务特定图（MSGs），越来越多地用于视频异常检测（VAD）和识别（VAR）。然而，这些图通常被视为固定结构，尽管它们是通用且分布不足的。传统的图结构精炼（GSR）方法在这种情况下并不适用，因为它们依赖于学习结构分布，而这些分布在LLM生成的图中并不存在。我们提出了一种基于超维度约束的图结构精炼方法（HDC-GSR），这是一种新的范式，它在单一超维度空间中直接优化可解码、任务对齐的图表示，无需分布建模。通过利用超维度计算（HDC），我们的框架通过绑定和捆绑操作对图进行编码，将生成的图代码与下游损失对齐，并解码边的贡献以精炼图结构。我们将这种方法实例化为MissionHD，用于弱监督的VAD/VAR任务，并在基准数据集上展示了其一致的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">LLM-generated reasoning graphs, referred to as mission-specific graphs (MSGs), are increasingly used for video anomaly detection (VAD) and recognition (VAR).</div>
</details>
</div>
<div class="card">
<div class="title">Realistic Face Reconstruction from Facial Embeddings via Diffusion Models</div>
<div class="meta-line">Authors: Dong Han, Yong Li, Joachim Denzler</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2026-02-13T18:28:24+00:00 · Latest: 2026-02-13T18:28:24+00:00</div>
<div class="meta-line">Comments: Accepted to AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13168v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13168v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the advancement of face recognition (FR) systems, privacy-preserving face recognition (PPFR) systems have gained popularity for their accurate recognition, enhanced facial privacy protection, and robustness to various attacks. However, there are limited studies to further verify privacy risks by reconstructing realistic high-resolution face images from embeddings of these systems, especially for PPFR. In this work, we propose the face embedding mapping (FEM), a general framework that explores Kolmogorov-Arnold Network (KAN) for conducting the embedding-to-face attack by leveraging pre-trained Identity-Preserving diffusion model against state-of-the-art (SOTA) FR and PPFR systems. Based on extensive experiments, we verify that reconstructed faces can be used for accessing other real-word FR systems. Besides, the proposed method shows the robustness in reconstructing faces from the partial and protected face embeddings. Moreover, FEM can be utilized as a tool for evaluating safety of FR and PPFR systems in terms of privacy leakage. All images used in this work are from public datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过扩散模型从面部嵌入进行真实面部重建</div>
<div class="mono" style="margin-top:8px">随着面部识别（FR）系统的进步，隐私保护面部识别（PPFR）系统因其准确的识别能力、增强的面部隐私保护以及对各种攻击的鲁棒性而受到欢迎。然而，目前针对通过这些系统嵌入重建高分辨率真实面部图像以进一步验证隐私风险的研究仍有限，尤其是在PPFR领域。在本文中，我们提出了一种名为面部嵌入映射（FEM）的通用框架，利用预训练的身份保持扩散模型，探索Kolmogorov-Arnold网络（KAN）来实施嵌入到面部的攻击，针对最先进的（SOTA）FR和PPFR系统。基于大量实验，我们验证了重建的面部可用于访问其他实际的FR系统。此外，所提出的方法在从部分和受保护的面部嵌入中重建面部方面表现出鲁棒性。而且，FEM可以作为评估FR和PPFR系统隐私泄露安全性的工具。本文中使用的所有图像均来自公开数据集。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">With the advancement of face recognition (FR) systems, privacy-preserving face recognition (PPFR) systems have gained popularity for their accurate recognition, enhanced facial privacy protection, and robustness to various attacks.</div>
</details>
</div>
<div class="card">
<div class="title">Optimal Take-off under Fuzzy Clearances</div>
<div class="meta-line">Authors: Hugo Henry, Arthur Tsai, Kelly Cohen</div>
<div class="meta-line">First: 2026-02-13T18:25:24+00:00 · Latest: 2026-02-13T18:25:24+00:00</div>
<div class="meta-line">Comments: 12 pages, 12 figures, conference paper</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13166v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13166v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a hybrid obstacle avoidance architecture that integrates Optimal Control under clearance with a Fuzzy Rule Based System (FRBS) to enable adaptive constraint handling for unmanned aircraft. Motivated by the limitations of classical optimal control under uncertainty and the need for interpretable decision making in safety critical aviation systems, we design a three stage Takagi Sugeno Kang fuzzy layer that modulates constraint radii, urgency levels, and activation decisions based on regulatory separation minima and airworthiness guidelines from FAA and EASA. These fuzzy-derived clearances are then incorporated as soft constraints into an optimal control problem solved using the FALCON toolbox and IPOPT. The framework aims to reduce unnecessary recomputations by selectively activating obstacle avoidance updates while maintaining compliance with aviation procedures. A proof of concept implementation using a simplified aircraft model demonstrates that the approach can generate optimal trajectories with computation times of 2,3 seconds per iteration in a single threaded MATLAB environment, suggesting feasibility for near real time applications. However, our experiments revealed a critical software incompatibility in the latest versions of FALCON and IPOPT, in which the Lagrangian penalty term remained identically zero, preventing proper constraint enforcement. This behavior was consistent across scenarios and indicates a solver toolbox regression rather than a modeling flaw. Future work includes validating this effect by reverting to earlier software versions, optimizing the fuzzy membership functions using evolutionary methods, and extending the system to higher fidelity aircraft models and stochastic obstacle environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模糊间隔下的最优起飞</div>
<div class="mono" style="margin-top:8px">本文提出了一种混合障碍规避架构，将基于清除的最优控制与模糊规则系统（FRBS）相结合，以实现无人机的自适应约束处理。受经典最优控制在不确定性下的局限性以及安全关键航空系统中可解释决策的需要启发，我们设计了一个三阶段的Takagi-Sugeno-Kang模糊层，根据FAA和EASA的监管分离最小值和适航指南调节约束半径、紧迫程度和激活决策。这些由模糊推导出的间隔随后被作为软约束纳入使用FALCON工具箱和IPOPT求解的最优控制问题中。该框架旨在通过选择性激活障碍规避更新来减少不必要的重新计算，同时保持符合航空程序。使用简化飞机模型进行的概念验证实现表明，该方法可以在单线程MATLAB环境中每迭代生成最优轨迹，计算时间为2.3秒，表明其在近实时应用中的可行性。然而，我们的实验发现FALCON和IPOPT最新版本中存在关键的软件不兼容问题，其中拉格朗日惩罚项始终为零，导致无法正确执行约束。这种行为在各种场景中保持一致，表明是求解器工具箱的回归问题，而非建模错误。未来的工作包括通过回退到早期软件版本验证这一影响，使用进化方法优化模糊隶属函数，并将系统扩展到更高保真度的飞机模型和随机障碍环境中。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper presents a hybrid obstacle avoidance architecture that integrates Optimal Control under clearance with a Fuzzy Rule Based System (FRBS) to enable adaptive constraint handling for unmanned aircraft.</div>
</details>
</div>
<div class="card">
<div class="title">Asynchronous Verified Semantic Caching for Tiered LLM Architectures</div>
<div class="meta-line">Authors: Asmit Kumar Singh, Haozhe Wang, Laxmi Naga Santosh Attaluri, Tak Chiam, Weihua Zhu</div>
<div class="meta-line">First: 2026-02-13T18:25:00+00:00 · Latest: 2026-02-13T18:25:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13165v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13165v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) now sit in the critical path of search, assistance, and agentic workflows, making semantic caching essential for reducing inference cost and latency. Production deployments typically use a tiered static-dynamic design: a static cache of curated, offline vetted responses mined from logs, backed by a dynamic cache populated online. In practice, both tiers are commonly governed by a single embedding similarity threshold, which induces a hard tradeoff: conservative thresholds miss safe reuse opportunities, while aggressive thresholds risk serving semantically incorrect responses. We introduce \textbf{Krites}, an asynchronous, LLM-judged caching policy that expands static coverage without changing serving decisions. On the critical path, Krites behaves exactly like a standard static threshold policy. When the nearest static neighbor of the prompt falls just below the static threshold, Krites asynchronously invokes an LLM judge to verify whether the static response is acceptable for the new prompt. Approved matches are promoted into the dynamic cache, allowing future repeats and paraphrases to reuse curated static answers and expanding static reach over time. In trace-driven simulations on conversational and search workloads, Krites increases the fraction of requests served with curated static answers (direct static hits plus verified promotions) by up to $\textbf{3.9}$ times for conversational traffic and search-style queries relative to tuned baselines, with unchanged critical path latency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分层LLM架构中的异步验证语义缓存</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）现在处于搜索、帮助和代理工作流的关键路径上，使语义缓存成为降低推理成本和延迟的关键。生产部署通常采用分层静态-动态设计：静态缓存包含从日志中挖掘的经过筛选和离线验证的响应，由动态缓存在线填充支持。实际上，两个层级通常由单一的嵌入相似性阈值管理，这导致了一个硬性权衡：保守的阈值会错过安全的重用机会，而激进的阈值则可能提供语义不正确的响应。我们引入了\textbf{Krites}，一种异步的、由LLM判断的缓存策略，可以在不改变服务决策的前提下扩展静态缓存的覆盖范围。在关键路径上，Krites的行为与标准静态阈值策略完全一致。当提示的最近静态邻居仅略低于静态阈值时，Krites会异步调用LLM判断器，验证静态响应是否适用于新提示。通过批准的匹配，Krites将响应提升至动态缓存，使未来的重复和改写提示能够重用经过筛选的静态答案，并随时间扩展静态缓存的覆盖范围。在基于追踪的模拟中，对于对话和搜索工作负载，Krites相较于调优基线，将由经过筛选的静态答案服务的请求数量比例提高了高达\textbf{3.9}倍，且关键路径延迟保持不变。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) now sit in the critical path of search, assistance, and agentic workflows, making semantic caching essential for reducing inference cost and latency.</div>
</details>
</div>
<div class="card">
<div class="title">Fast and Flexible Neutrino Decoupling Part I: The Standard Model</div>
<div class="meta-line">Authors: M. Escudero, G. Jackson, M. Laine, S. Sandner</div>
<div class="meta-line">Venue: JCAP 02 (2026) 046</div>
<div class="meta-line">First: 2025-11-06T19:00:05+00:00 · Latest: 2026-02-13T18:20:33+00:00</div>
<div class="meta-line">Comments: 34 pages, 4 figures, code available from https://github.com/MiguelEA/nudec_BSM v2: clarifications added</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.04747v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.04747v2">PDF</a> · <a href="https://github.com/MiguelEA/nudec_BSM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cosmological determinations of the number of relativistic neutrino species, $N^{ }_{\rm eff}$, are becoming increasingly accurate, and further improvements are expected both from CMB and BBN data. Given this context, we update the evaluation of $N^{ }_{\rm eff}$ and the current entropy density via the momentum-averaged approach. This allows for a numerically fast description of neutrino decoupling, easily portable to an array of new physics scenarios. We revisit all aspects of this approach, including collision terms with full electron mass dependence, finite temperature QED corrections to the equation of state, neutrino oscillations, and the modelling of neutrino ensembles with effective chemical potentials. For integrated observables, our results differ by less than $0.04\%$ from the solution of the momentum-dependent evolution equation. We outline how to extend the approach to BSM settings, and will highlight its power in Part II. To facilitate the practical implementation, we release a Mathematica and Python code within nudec_BSM_v2, easily linkable to BBN codes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>快速且灵活的中微子解耦 I：标准模型</div>
<div class="mono" style="margin-top:8px">宇宙学中对相对论中微子物种数 $N^{ }_{\rm eff}$ 的确定正变得越来越精确，未来从CMB和BBN数据中有望进一步提升精度。在这一背景下，我们通过动量平均方法更新了 $N^{ }_{\rm eff}$ 和当前熵密度的评估。这使得中微子解耦的数值描述更加高效，并且易于应用于各种新物理场景。我们重新审视了该方法的各个方面，包括具有完整电子质量依赖的碰撞项、有限温度QED对状态方程的修正、中微子振荡，以及使用有效化学势建模中微子集合。对于积分可观测量，我们的结果与动量依赖演化方程的解相比误差小于0.04%。我们概述了如何将该方法扩展到BSM（超出标准模型）场景，并将在第二部分突出其优势。为便于实际应用，我们在nudec_BSM_v2中发布了Mathematica和Python代码，易于与BBN代码链接。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Cosmological determinations of the number of relativistic neutrino species, $N^{ }_{\rm eff}$, are becoming increasingly accurate, and further improvements are expected both from CMB and BBN data.</div>
</details>
</div>
<div class="card">
<div class="title">Learnable Chernoff Baselines for Inference-Time Alignment</div>
<div class="meta-line">Authors: Sunil Madhow, Yuchen Liang, Ness Shroff, Yingbin Liang, Yu-Xiang Wang</div>
<div class="meta-line">First: 2026-02-08T00:09:40+00:00 · Latest: 2026-02-13T18:15:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.07738v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.07738v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study inference-time reward-guided alignment for generative models. Existing methods often rely on either architecture-specific adaptations or computationally costly inference procedures. We introduce Learnable Chernoff Baselines (LCBs) as a method for efficiently and approximately sampling from the exponentially tilted kernels that arise from KL-regularized reward alignment. Using only black-box sampling access to the pretrained model, LCBs implement a form of rejection sampling with adaptively selected acceptance probabilities, which allows fine-grained control over inference-compute scaling. We establish total-variation guarantees to the ideal aligned model, and demonstrate in both continuous and discrete diffusion settings that LCB sampling closely matches ideal rejection sampling while using substantially fewer queries to the pretrained model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于推理时对齐的可学习切尔诺夫基线</div>
<div class="mono" style="margin-top:8px">我们研究生成模型的推理时奖励引导对齐方法。现有方法通常依赖于架构特定的调整或计算成本高昂的推理过程。我们引入可学习切尔诺夫基线（LCBs）作为高效且近似地从KL正则化奖励对齐产生的指数倾斜核中进行采样的方法。通过仅使用预训练模型的黑盒采样访问，LCBs实现了一种具有自适应选择接受概率的拒绝采样形式，从而允许对推理计算缩放进行精细控制。我们建立了与理想对齐模型的总变差保证，并在连续和离散扩散设置中展示了LCBs采样与理想拒绝采样高度匹配，同时大幅减少了对预训练模型的查询次数。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We study inference-time reward-guided alignment for generative models.</div>
</details>
</div>
<div class="card">
<div class="title">In-Context Autonomous Network Incident Response: An End-to-End Large Language Model Agent Approach</div>
<div class="meta-line">Authors: Yiran Gao, Kim Hammar, Tao Li</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2026-02-13T18:09:30+00:00 · Latest: 2026-02-13T18:09:30+00:00</div>
<div class="meta-line">Comments: 2026 AAAI Summer Symposium on Human-Aware AI Agents for the Cyber Battlefield</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13156v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13156v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Rapidly evolving cyberattacks demand incident response systems that can autonomously learn and adapt to changing threats. Prior work has extensively explored the reinforcement learning approach, which involves learning response strategies through extensive simulation of the incident. While this approach can be effective, it requires handcrafted modeling of the simulator and suppresses useful semantics from raw system logs and alerts. To address these limitations, we propose to leverage large language models&#x27; (LLM) pre-trained security knowledge and in-context learning to create an end-to-end agentic solution for incident response planning. Specifically, our agent integrates four functionalities, perception, reasoning, planning, and action, into one lightweight LLM (14b model). Through fine-tuning and chain-of-thought reasoning, our LLM agent is capable of processing system logs and inferring the underlying network state (perception), updating its conjecture of attack models (reasoning), simulating consequences under different response strategies (planning), and generating an effective response (action). By comparing LLM-simulated outcomes with actual observations, the LLM agent repeatedly refines its attack conjecture and corresponding response, thereby demonstrating in-context adaptation. Our agentic approach is free of modeling and can run on commodity hardware. When evaluated on incident logs reported in the literature, our agent achieves recovery up to 23% faster than those of frontier LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>上下文感知自主网络事件响应：一种端到端大语言模型代理方法</div>
<div class="mono" style="margin-top:8px">快速演变的网络攻击要求事件响应系统能够自主学习并适应变化的威胁。先前的工作广泛探讨了基于强化学习的方法，该方法通过大量模拟事件来学习响应策略。虽然这种方法可以有效，但它需要人工构建模拟器模型，并且从原始系统日志和警报中抑制了有用的语义信息。为了解决这些限制，我们提出利用大语言模型（LLM）预训练的安全知识和上下文学习，构建一种端到端的代理解决方案用于事件响应规划。具体而言，我们的代理将感知、推理、规划和行动四项功能整合到一个轻量级的LLM（14b模型）中。通过微调和思维链推理，我们的LLM代理能够处理系统日志并推断底层网络状态（感知），更新其对攻击模型的假设（推理），在不同响应策略下模拟后果（规划），并生成有效的响应（行动）。通过将LLM模拟的结果与实际观察进行比较，LLM代理不断优化其攻击假设和相应的响应，从而实现上下文适应。我们的代理方法无需建模，可在普通硬件上运行。在评估文献中报告的事件日志时，我们的代理比前沿LLM的恢复速度提高了最多23%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Rapidly evolving cyberattacks demand incident response systems that can autonomously learn and adapt to changing threats.</div>
</details>
</div>
<div class="card">
<div class="title">Choose Your Agent: Tradeoffs in Adopting AI Advisors, Coaches, and Delegates in Multi-Party Negotiation</div>
<div class="meta-line">Authors: Kehang Zhu, Nithum Thain, Vivian Tsai, James Wexler, Crystal Qian</div>
<div class="meta-line">First: 2026-02-12T15:41:57+00:00 · Latest: 2026-02-13T18:08:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12089v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.12089v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As AI usage becomes more prevalent in social contexts, understanding agent-user interaction is critical to designing systems that improve both individual and group outcomes. We present an online behavioral experiment (N = 243) in which participants play three multi-turn bargaining games in groups of three. Each game, presented in randomized order, grants access to a single LLM assistance modality: proactive recommendations from an Advisor, reactive feedback from a Coach, or autonomous execution by a Delegate; all modalities are powered by an underlying LLM that achieves superhuman performance in an all-agent environment. On each turn, participants privately decide whether to act manually or use the AI modality available in that game. Despite preferring the Advisor modality, participants achieve the highest mean individual gains with the Delegate, demonstrating a preference-performance misalignment. Moreover, delegation generates positive externalities; even non-adopting users in access-to-delegate treatment groups benefit by receiving higher-quality offers. Mechanism analysis reveals that the Delegate agent acts as a market maker, injecting rational, Pareto-improving proposals that restructure the trading environment. Our research reveals a gap between agent capabilities and realized group welfare. While autonomous agents can exhibit super-human strategic performance, their impact on realized welfare gains can be constrained by interfaces, user perceptions, and adoption barriers. Assistance modalities should be designed as mechanisms with endogenous participation; adoption-compatible interaction rules are a prerequisite to improving human welfare with automated assistance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>选择你的代理：在多方谈判中采用AI顾问、教练和代理的权衡</div>
<div class="mono" style="margin-top:8px">随着AI在社会情境中的使用日益普遍，理解代理与用户之间的互动对于设计能够提升个人和群体结果的系统至关重要。我们进行了一项在线行为实验（N = 243），参与者以三人小组的形式进行三轮多回合讨价还价游戏。每轮游戏以随机顺序呈现，提供一种单一的LLM辅助模式：顾问提供的主动建议、教练提供的反应性反馈，或代理的自主执行；所有模式均基于一个在全代理环境中表现超越人类的底层LLM。在每一轮中，参与者私下决定是手动操作还是使用该轮游戏中提供的AI模式。尽管参与者更倾向于使用顾问模式，但他们在代理模式下实现了最高的平均个人收益，这表明存在偏好与表现之间的不匹配。此外，代理的委托行为产生了正外部性；即使在无法使用代理的组别中，参与者也能因获得更高质量的提议而受益。机制分析表明，代理在行为中充当市场制造者，注入理性和帕累托改进的提议，从而重构交易环境。我们的研究揭示了代理能力与实际群体福利之间的差距。虽然自主代理可以表现出超越人类的战略性能，但其对实际福利提升的影响可能受到界面、用户认知和采用障碍的限制。辅助模式应被设计为具有内生参与机制的机制，采用兼容的交互规则是提升人类福利的先决条件。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">As AI usage becomes more prevalent in social contexts, understanding agent-user interaction is critical to designing systems that improve both individual and group outcomes.</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Approximate Uniform Facility Location via Graph Neural Networks</div>
<div class="meta-line">Authors: Chendi Qian, Christopher Morris, Stefanie Jegelka, Christian Sohler</div>
<div class="meta-line">First: 2026-02-13T18:08:23+00:00 · Latest: 2026-02-13T18:08:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13155v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13155v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">There has been a growing interest in using neural networks, especially message-passing neural networks (MPNNs), to solve hard combinatorial optimization problems heuristically. However, existing learning-based approaches for hard combinatorial optimization tasks often rely on supervised training data, reinforcement learning, or gradient estimators, leading to significant computational overhead, unstable training, or a lack of provable performance guarantees. In contrast, classical approximation algorithms offer such performance guarantees under worst-case inputs but are non-differentiable and unable to adaptively exploit structural regularities in natural input distributions. We address this dichotomy with the fundamental example of Uniform Facility Location (UniFL), a variant of the combinatorial facility location problem with applications in clustering, data summarization, logistics, and supply chain design. We develop a fully differentiable MPNN model that embeds approximation-algorithmic principles while avoiding the need for solver supervision or discrete relaxations. Our approach admits provable approximation and size generalization guarantees to much larger instances than seen during training. Empirically, we show that our approach outperforms standard non-learned approximation algorithms in terms of solution quality, closing the gap with computationally intensive integer linear programming approaches. Overall, this work provides a step toward bridging learning-based methods and approximation algorithms for discrete optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过图神经网络学习近似求解均匀设施选址问题</div>
<div class="mono" style="margin-top:8px">近年来，人们越来越关注使用神经网络，尤其是消息传递神经网络（MPNNs），来启发式地解决困难的组合优化问题。然而，现有的基于学习的方法在处理困难组合优化任务时通常依赖于监督训练数据、强化学习或梯度估计器，导致显著的计算开销、训练不稳定或缺乏可证明的性能保证。相比之下，经典近似算法在最坏情况输入下提供性能保证，但不可微且无法适应性地利用自然输入分布中的结构规律。我们以均匀设施选址（UniFL）这一组合设施选址问题的变种为例，探讨这一二元对立问题。UniFL在聚类、数据摘要、物流和供应链设计等领域有广泛应用。我们开发了一个完全可微的MPNN模型，该模型嵌入了近似算法的原则，同时避免了求解器监督或离散松弛的需要。我们的方法在训练过程中未见过的更大实例上，提供了可证明的近似和规模泛化保证。实验表明，我们的方法在解的质量上优于标准的非学习近似算法，缩小了与计算密集型整数线性规划方法之间的差距。总体而言，本工作为将基于学习的方法与近似算法结合用于离散优化提供了一个重要步骤。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">There has been a growing interest in using neural networks, especially message-passing neural networks (MPNNs), to solve hard combinatorial optimization problems heuristically.</div>
</details>
</div>
<div class="card">
<div class="title">Quantization-Robust LLM Unlearning via Low-Rank Adaptation</div>
<div class="meta-line">Authors: João Vitor Boer Abitante, Joana Meneguzzo Pasquali, Luan Fonseca Garcia, Ewerton de Oliveira, Thomas da Silva Paula, Rodrigo C. Barros, Lucas S. Kupssinskü</div>
<div class="meta-line">First: 2026-02-13T18:01:40+00:00 · Latest: 2026-02-13T18:01:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13151v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.13151v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Model (LLM) unlearning aims to remove targeted knowledge from a trained model, but practical deployments often require post-training quantization (PTQ) for efficient inference. However, aggressive low-bit PTQ can mask or erase unlearning updates, causing quantized models to revert to pre-unlearning behavior. We show that standard full-parameter fine-tuning often induce parameter changes that are too small to survive 4-bit quantization. We propose quantization-robust unlearning via low-rank adaptation (LoRA): we freeze the base model and concentrate unlearning into trainable adapters so that the effective update is preserved after quantization. On Llama-2-7B evaluated with MUSE dataset (BOOKS and NEWS), LoRA improves 4-bit utility by up to 7.93 points (NPO+GDR on BOOKS: 50.17 to 58.10) and yields higher 4-bit utility on NEWS for GA+GDR (40.06 to 44.82, increase of 4.76). LoRA also substantially reduces privacy leakage under 4-bit PTQ, e.g., for GA+KLR on BOOKS, PrivLeak moves from -25.68 to -5.86 (closer to ideal 0), while maintaining strong forgetting (VerMem and KnowMem near 0). Thus, using LoRA for Machine Unlearning is beneficial for scenarios where quantization is necessary for model deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过低秩适应实现量化鲁棒的LLM去学习</div>
<div class="mono" style="margin-top:8px">大语言模型（LLM）去学习旨在从训练后的模型中移除特定知识，但实际部署中通常需要在训练后进行量化（PTQ）以提高推理效率。然而，激进的低比特量化可能会掩盖或擦除去学习的更新，导致量化模型恢复到去学习前的行为。我们发现标准的全参数微调通常会导致参数变化过小，无法在4位量化中保留。因此，我们提出了一种通过低秩适应（LoRA）实现的量化鲁棒去学习方法：我们冻结基础模型，并将去学习的更新集中在可训练的适配器中，以确保量化后仍能保留有效的更新。在使用MUSE数据集（BOOKS和NEWS）评估的Llama-2-7B模型上，LoRA使4位量化模型的性能提升了最多7.93个点（BOOKS上的NPO+GDR：50.17到58.10），在NEWS上GA+GDR的4位性能也有所提升（40.06到44.82，提升4.76）。此外，LoRA在4位PTQ下显著减少了隐私泄露，例如在BOOKS上GA+KLR的PrivLeak从-25.68降至-5.86（更接近理想的0），同时保持了强大的遗忘能力（VerMem和KnowMem接近0）。因此，在需要量化以进行模型部署的场景中，使用LoRA进行机器去学习是有益的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Model (LLM) unlearning aims to remove targeted knowledge from a trained model, but practical deployments often require post-training quantization (PTQ) for efficient inference.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260216_0403.html">20260216_0403</a>
<a href="archive/20260215_0401.html">20260215_0401</a>
<a href="archive/20260213_0421.html">20260213_0421</a>
<a href="archive/20260212_0425.html">20260212_0425</a>
<a href="archive/20260210_0435.html">20260210_0435</a>
<a href="archive/20260209_0404.html">20260209_0404</a>
<a href="archive/20260208_0353.html">20260208_0353</a>
<a href="archive/20260207_0410.html">20260207_0410</a>
<a href="archive/20260206_0412.html">20260206_0412</a>
<a href="archive/20260205_0417.html">20260205_0417</a>
<a href="archive/20260204_0421.html">20260204_0421</a>
<a href="archive/20260202_0402.html">20260202_0402</a>
<a href="archive/20260201_0354.html">20260201_0354</a>
<a href="archive/20260131_0408.html">20260131_0408</a>
<a href="archive/20260130_0406.html">20260130_0406</a>
<a href="archive/20260129_0407.html">20260129_0407</a>
<a href="archive/20260128_0407.html">20260128_0407</a>
<a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
