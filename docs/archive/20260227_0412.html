<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-27 04:12</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260227_0412</div>
    <div class="row"><div class="card">
<div class="title">Neu-PiG: Neural Preconditioned Grids for Fast Dynamic Surface Reconstruction on Long Sequences</div>
<div class="meta-line">Authors: Julian Kaltheuner, Hannah Dröge, Markus Plack, Patrick Stotko, Reinhard Klein</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-02-25T18:59:53+00:00 · Latest: 2026-02-25T18:59:53+00:00</div>
<div class="meta-line">Comments: CVPR 2026, Code: https://github.com/vc-bonn/neu-pig</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22212v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22212v1">PDF</a> · <a href="https://github.com/vc-bonn/neu-pig">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Temporally consistent surface reconstruction of dynamic 3D objects from unstructured point cloud data remains challenging, especially for very long sequences. Existing methods either optimize deformations incrementally, risking drift and requiring long runtimes, or rely on complex learned models that demand category-specific training. We present Neu-PiG, a fast deformation optimization method based on a novel preconditioned latent-grid encoding that distributes spatial features parameterized on the position and normal direction of a keyframe surface. Our method encodes entire deformations across all time steps at various spatial scales into a multi-resolution latent grid, parameterized by the position and normal direction of a reference surface from a single keyframe. This latent representation is then augmented for time modulation and decoded into per-frame 6-DoF deformations via a lightweight multilayer perceptron (MLP). To achieve high-fidelity, drift-free surface reconstructions in seconds, we employ Sobolev preconditioning during gradient-based training of the latent space, completely avoiding the need for any explicit correspondences or further priors. Experiments across diverse human and animal datasets demonstrate that Neu-PiG outperforms state-the-art approaches, offering both superior accuracy and scalability to long sequences while running at least 60x faster than existing training-free methods and achieving inference speeds on the same order as heavy pretrained models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Neu-PiG：用于长序列快速动态表面重建的神经预条件网格</div>
<div class="mono" style="margin-top:8px">从无结构点云数据中对动态3D物体进行时间一致的表面重建仍然具有挑战性，尤其是在非常长的序列中。现有方法要么逐步优化形变，可能导致漂移并需要较长的运行时间，要么依赖复杂的学得模型，需要类别特定的训练。我们提出了Neu-PiG，这是一种基于新颖预条件潜在网格编码的快速形变优化方法，该编码将空间特征分布于关键帧表面的位置和法线方向上。我们的方法将所有时间步长在不同空间尺度上的整体形变编码为一个多重分辨率潜在网格，该网格由单一关键帧参考表面的位置和法线方向参数化。随后，该潜在表示通过轻量级多层感知机（MLP）进行时间调制增强，并解码为每帧的6自由度（6-DoF）形变。为了在数秒内实现高保真、无漂移的表面重建，我们在潜在空间的梯度训练过程中采用Sobolev预条件处理，完全避免了对任何显式对应关系或进一步先验的依赖。在多样化的行人和动物数据集上的实验表明，Neu-PiG优于最先进的方法，在准确性和对长序列的可扩展性方面表现更优，同时运行速度至少比现有训练无方法快60倍，并且推理速度与大型预训练模型相当。</div>
</details>
</div>
<div class="card">
<div class="title">WHOLE: World-Grounded Hand-Object Lifted from Egocentric Videos</div>
<div class="meta-line">Authors: Yufei Ye, Jiaman Li, Ryan Rong, C. Karen Liu</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2026-02-25T18:59:10+00:00 · Latest: 2026-02-25T18:59:10+00:00</div>
<div class="meta-line">Comments: Project website: https://judyye.github.io/whole-www</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22209v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22209v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://judyye.github.io/whole-www">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Egocentric manipulation videos are highly challenging due to severe occlusions during interactions and frequent object entries and exits from the camera view as the person moves. Current methods typically focus on recovering either hand or object pose in isolation, but both struggle during interactions and fail to handle out-of-sight cases. Moreover, their independent predictions often lead to inconsistent hand-object relations. We introduce WHOLE, a method that holistically reconstructs hand and object motion in world space from egocentric videos given object templates. Our key insight is to learn a generative prior over hand-object motion to jointly reason about their interactions. At test time, the pretrained prior is guided to generate trajectories that conform to the video observations. This joint generative reconstruction substantially outperforms approaches that process hands and objects separately followed by post-processing. WHOLE achieves state-of-the-art performance on hand motion estimation, 6D object pose estimation, and their relative interaction reconstruction. Project website: https://judyye.github.io/whole-www</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WHOLE: 以世界坐标为基础的视角视频中手与物体的抓取动作重建</div>
<div class="mono" style="margin-top:8px">由于交互过程中严重的遮挡以及人在移动时物体频繁进入和离开相机视野，第一视角操控视频具有高度挑战性。当前方法通常单独恢复手或物体姿态，但在交互场景中表现不佳，且无法处理视线外的情况。此外，它们的独立预测往往导致手与物体关系不一致。我们提出WHOLE方法，利用物体模板从第一视角视频中整体重建手与物体在世界坐标系中的运动。我们的关键洞察是学习手与物体运动的生成先验，以联合推理它们的交互关系。在测试时，预训练的先验模型被引导生成符合视频观测的轨迹。这种联合生成的重建方法显著优于分别处理手和物体后再进行后处理的方法。WHOLE在手运动估计、6D物体姿态估计及其相对交互重建方面均达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Solaris: Building a Multiplayer Video World Model in Minecraft</div>
<div class="meta-line">Authors: Georgy Savva, Oscar Michel, Daohan Lu, Suppakit Waiwitlikhit, Timothy Meehan, Dhairya Mishra, Srivats Poddar, Jack Lu, Saining Xie</div>
<div class="meta-line">First: 2026-02-25T18:59:01+00:00 · Latest: 2026-02-25T18:59:01+00:00</div>
<div class="meta-line">Comments: Project website: https://solaris-wm.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22208v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22208v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://solaris-wm.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing action-conditioned video generation models (video world models) are limited to single-agent perspectives, failing to capture the multi-agent interactions of real-world environments. We introduce Solaris, a multiplayer video world model that simulates consistent multi-view observations. To enable this, we develop a multiplayer data system designed for robust, continuous, and automated data collection on video games such as Minecraft. Unlike prior platforms built for single-player settings, our system supports coordinated multi-agent interaction and synchronized videos + actions capture. Using this system, we collect 12.64 million multiplayer frames and propose an evaluation framework for multiplayer movement, memory, grounding, building, and view consistency. We train Solaris using a staged pipeline that progressively transitions from single-player to multiplayer modeling, combining bidirectional, causal, and Self Forcing training. In the final stage, we introduce Checkpointed Self Forcing, a memory-efficient Self Forcing variant that enables a longer-horizon teacher. Results show our architecture and training design outperform existing baselines. Through open-sourcing our system and models, we hope to lay the groundwork for a new generation of multi-agent world models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Solaris：在Minecraft中构建多人视频世界模型</div>
<div class="mono" style="margin-top:8px">现有的基于动作条件的视频生成模型（视频世界模型）仅限于单智能体视角，无法捕捉现实环境中的多智能体交互。我们引入了Solaris，这是一个模拟一致多视角观察的多人视频世界模型。为此，我们开发了一个多人数据系统，用于在如Minecraft等视频游戏中进行稳健、连续和自动化的数据收集。与之前为单人游戏设置构建的平台不同，我们的系统支持协调的多智能体交互和同步的视频与动作捕捉。利用该系统，我们收集了1264万帧多人游戏数据，并提出了针对多人移动、记忆、定位、建造和视角一致性的评估框架。我们通过一个分阶段的训练流程来训练Solaris，该流程逐步从单人建模过渡到多人建模，结合了双向、因果和Self Forcing训练方法。在最后阶段，我们引入了Checkpointed Self Forcing，这是一种内存效率更高的Self Forcing变体，能够支持更长的教师时间跨度。实验结果表明，我们的架构和训练设计优于现有基线。通过开源我们的系统和模型，我们希望为新一代多智能体世界模型奠定基础。</div>
</details>
</div>
<div class="card">
<div class="title">Recovered in Translation: Efficient Pipeline for Automated Translation of Benchmarks and Datasets</div>
<div class="meta-line">Authors: Hanna Yukhymenko, Anton Alexandrov, Martin Vechev</div>
<div class="meta-line">First: 2026-02-25T18:58:25+00:00 · Latest: 2026-02-25T18:58:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22207v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22207v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The reliability of multilingual Large Language Model (LLM) evaluation is currently compromised by the inconsistent quality of translated benchmarks. Existing resources often suffer from semantic drift and context loss, which can lead to misleading performance metrics. In this work, we present a fully automated framework designed to address these challenges by enabling scalable, high-quality translation of datasets and benchmarks. We demonstrate that adapting test-time compute scaling strategies, specifically Universal Self-Improvement (USI) and our proposed multi-round ranking method, T-RANK, allows for significantly higher quality outputs compared to traditional pipelines. Our framework ensures that benchmarks preserve their original task structure and linguistic nuances during localization. We apply this approach to translate popular benchmarks and datasets into eight Eastern and Southern European languages (Ukrainian, Bulgarian, Slovak, Romanian, Lithuanian, Estonian, Turkish, Greek). Evaluations using both reference-based metrics and LLM-as-a-judge show that our translations surpass existing resources, resulting in more accurate downstream model assessment. We release both the framework and the improved benchmarks to facilitate robust and reproducible multilingual AI development.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>翻译恢复：用于基准和数据集自动翻译的高效流程</div>
<div class="mono" style="margin-top:8px">目前多语言大语言模型（LLM）评估的可靠性受到翻译基准不一致质量的影响。现有资源常出现语义漂移和上下文丢失，可能导致误导性的性能指标。在本工作中，我们提出一个全自动框架，通过实现可扩展、高质量的数据集和基准翻译来解决这些问题。我们证明，通过适应测试时计算扩展策略，特别是通用自我改进（USI）和我们提出的多轮排序方法T-RANK，可以显著提高输出质量，优于传统流程。我们的框架确保基准在本地化过程中保留其原始任务结构和语言细微差别。我们将此方法应用于将流行基准和数据集翻译成八种东欧和南欧语言（乌克兰语、保加利亚语、斯洛伐克语、罗马尼亚语、立陶宛语、爱沙尼亚语、土耳其语、希腊语）。使用基于参考的指标和LLM作为裁判的评估结果显示，我们的翻译优于现有资源，从而实现更准确的下游模型评估。我们发布了该框架和改进后的基准，以促进稳健且可复现的多语言AI开发。</div>
</details>
</div>
<div class="card">
<div class="title">TimeBlind: A Spatio-Temporal Compositionality Benchmark for Video LLMs</div>
<div class="meta-line">Authors: Baiqi Li, Kangyi Zhao, Ce Zhang, Chancharik Mitra, Jean de Dieu Nyandwi, Gedas Bertasius</div>
<div class="meta-line">First: 2026-01-30T20:21:46+00:00 · Latest: 2026-02-25T18:57:52+00:00</div>
<div class="meta-line">Comments: For code and data, see https://baiqi-li.github.io/timeblind_project/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00288v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.00288v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://baiqi-li.github.io/timeblind_project/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-grained spatio-temporal understanding is essential for video reasoning and embodied AI. Yet, while Multimodal Large Language Models (MLLMs) master static semantics, their grasp of temporal dynamics remains brittle. We present TimeBlind, a diagnostic benchmark for compositional spatio-temporal understanding. Inspired by cognitive science, TimeBlind categorizes fine-grained temporal understanding into three levels: recognizing atomic events, characterizing event properties, and reasoning about event interdependencies. Unlike benchmarks that conflate recognition with temporal reasoning, TimeBlind leverages a minimal-pairs paradigm: video pairs share identical static visual content but differ solely in temporal structure, utilizing complementary questions to neutralize language priors. Evaluating over 20 state-of-the-art MLLMs (e.g., GPT-5, Gemini 3 Pro) on 600 curated instances (2400 video-question pairs), reveals that the Instance Accuracy (correctly distinguishing both videos in a pair) of the best performing MLLM is only 48.2%, far below the human performance (98.2%). These results demonstrate that even frontier models rely heavily on static visual shortcuts rather than genuine temporal logic, positioning TimeBlind as a vital diagnostic tool for next-generation video understanding. Dataset and code are available at https://baiqi-li.github.io/timeblind_project/ .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TimeBlind：面向视频大语言模型的时空组合性基准</div>
<div class="mono" style="margin-top:8px">细粒度的时空理解对于视频推理和具身AI至关重要。然而，尽管多模态大语言模型（MLLMs）能够掌握静态语义，其对时间动态的理解仍较为脆弱。我们提出了TimeBlind，一个用于诊断组合性时空理解的基准。受认知科学启发，TimeBlind将细粒度时间理解分为三个层次：识别原子事件、描述事件属性以及推理事件间的相互依赖关系。与将识别与时间推理混为一谈的基准不同，TimeBlind采用最小对子范式：视频对具有相同的静态视觉内容，但仅在时间结构上不同，通过互补性问题来中和语言先验。在600个精心挑选的实例（2400个视频-问题对）上评估超过20个最先进的MLLMs（如GPT-5、Gemini 3 Pro），结果显示表现最好的MLLM在实例准确率（正确区分一对视频）上仅为48.2%，远低于人类表现（98.2%）。这些结果表明，即使是前沿模型也严重依赖静态视觉捷径，而非真正的时序逻辑，因此TimeBlind成为下一代视频理解的重要诊断工具。数据集和代码可在 https://baiqi-li.github.io/timeblind_project/ 获取。</div>
</details>
</div>
<div class="card">
<div class="title">High-Fidelity And Complex Test Data Generation For Google SQL Code Generation Services</div>
<div class="meta-line">Authors: Shivasankari Kannan, Yeounoh Chung, Amita Gondi, Tristan Swadell, Fatma Ozcan</div>
<div class="meta-line">First: 2025-04-24T02:27:17+00:00 · Latest: 2026-02-25T18:55:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.17203v4">Abs</a> · <a href="https://arxiv.org/pdf/2504.17203v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The demand for high-fidelity test data is paramount in industrial settings where access to production data is largely restricted. Traditional data generation methods often fall short, struggling with low-fidelity and the ability to model complex data structures and semantic relationships that are critical for testing complex SQL code generation services like Natural Language to SQL (NL2SQL). In this paper, we address the critical need for generating syntactically correct and semantically relevant high-fidelity mock data for complex data structures that includes columns with nested structures that we frequently encounter in Google workloads. We highlight the limitations of existing approaches used in production, particularly their inability to handle large and complex data structures, as well as the lack of semantically coherent test data that lead to limited test coverage. We demonstrate that by leveraging Large Language Models (LLMs) and incorporating strategic pre- and post-processing steps, we can generate syntactically correct and semantically relevant high-fidelity test data that adheres to complex structural constraints and maintains semantic integrity to the SQL test targets (queries/functions). This approach supports comprehensive testing of complex SQL queries involving joins, aggregations, and even deeply nested subqueries, ensuring robust evaluation of SQL code generation services, like NL2SQL and SQL Code Assistant. Our results demonstrate the practical utility of an LLM (\textit{Gemini}) based test data generation for industrial SQL code generation services where generating high-fidelity test data is essential due to the frequent unavailability and inaccessibility of production datasets for testing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高保真与复杂测试数据生成用于Google SQL代码生成服务</div>
<div class="mono" style="margin-top:8px">在工业环境中，由于生产数据的获取受到严格限制，对高保真测试数据的需求至关重要。传统数据生成方法往往难以满足，因为它们在低保真度和建模复杂数据结构及语义关系方面存在不足，而这些是测试复杂SQL代码生成服务（如自然语言到SQL（NL2SQL））的关键。本文针对生成语法正确且语义相关的高保真模拟数据的迫切需求，提出了一种适用于包含嵌套结构列的复杂数据结构的方法，这类结构在Google工作负载中经常出现。我们指出现有生产方法的局限性，尤其是它们在处理大规模和复杂数据结构时的不足，以及缺乏语义连贯的测试数据导致测试覆盖率有限。我们证明，通过利用大型语言模型（LLMs）并结合策略性的预处理和后处理步骤，可以生成符合复杂结构约束且保持语义完整性的高保真测试数据，从而支持对涉及连接、聚合以及深度嵌套子查询的复杂SQL查询的全面测试，确保SQL代码生成服务（如NL2SQL和SQL Code Assistant）的稳健评估。我们的结果展示了基于大型语言模型（Gemini）的测试数据生成在工业SQL代码生成服务中的实际应用价值，特别是在生产数据集频繁不可用或无法访问的情况下，生成高保真测试数据至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">Decoding as Optimisation on the Probability Simplex: From Top-K to Top-P (Nucleus) to Best-of-K Samplers</div>
<div class="meta-line">Authors: Xiaotong Ji, Rasul Tutunov, Matthieu Zimmer, Haitham Bou-Ammar</div>
<div class="meta-line">First: 2026-02-20T15:38:16+00:00 · Latest: 2026-02-25T18:47:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18292v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.18292v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Decoding sits between a language model and everything we do with it, yet it is still treated as a heuristic knob-tuning exercise. We argue decoding should be understood as a principled optimisation layer: at each token, we solve a regularised problem over the probability simplex that trades off model score against structural preferences and constraints. This single template recovers greedy decoding, Softmax sampling, Top-K, Top-P, and Sparsemax-style sparsity as special cases, and explains their common structure through optimality conditions. More importantly, the framework makes it easy to invent new decoders without folklore. We demonstrate this by designing Best-of-K (BoK), a KL-anchored coverage objective aimed at multi-sample pipelines (self-consistency, reranking, verifier selection). BoK targets the probability of covering good alternatives within a fixed K-sample budget and improves empirical performance. We show that such samples can improve accuracy by, for example, +18.6% for Qwen2.5-Math-7B on MATH500 at high sampling temperatures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在概率单纯形上的优化解码：从Top-K到Top-P（Nucleus）到Best-of-K采样器</div>
<div class="mono" style="margin-top:8px">解码位于语言模型与我们使用它的所有任务之间，但目前仍被视为一种启发式调整的手段。我们认为解码应被理解为一个有原则的优化层：在每个token生成时，我们解决一个在概率单纯形上的正则化问题，权衡模型得分与结构偏好和约束。这一单一模板可以涵盖贪心解码、Softmax采样、Top-K、Top-P以及Sparsemax风格的稀疏性作为特例，并通过最优性条件解释它们的共同结构。更重要的是，该框架使得在不依赖经验法则的情况下发明新的解码器变得容易。我们通过设计Best-of-K（BoK）来展示这一点，这是一种基于KL散度的覆盖目标，旨在多样本流水线（如自一致性、重排序、验证器选择）中使用。BoK的目标是在固定的K样本预算内提高覆盖优质替代方案的概率，并提升了实证性能。例如，在高采样温度下，BoK在MATH500数据集上将Qwen2.5-Math-7B的准确率提升了18.6%。</div>
</details>
</div>
<div class="card">
<div class="title">Off-The-Shelf Image-to-Image Models Are All You Need To Defeat Image Protection Schemes</div>
<div class="meta-line">Authors: Xavier Pleimling, Sifat Muhammad Abdullah, Gunjan Balde, Peng Gao, Mainack Mondal, Murtuza Jadliwala, Bimal Viswanath</div>
<div class="meta-line">First: 2026-02-25T18:46:30+00:00 · Latest: 2026-02-25T18:46:30+00:00</div>
<div class="meta-line">Comments: This work has been accepted for publication at the IEEE Conference on Secure and Trustworthy Machine Learning (SaTML). The final version will be available on IEEE Xplore. To IEEE SaTML 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22197v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22197v1">PDF</a> · <a href="https://github.com/mlsecviswanath/img2imgdenoiser">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Advances in Generative AI (GenAI) have led to the development of various protection strategies to prevent the unauthorized use of images. These methods rely on adding imperceptible protective perturbations to images to thwart misuse such as style mimicry or deepfake manipulations. Although previous attacks on these protections required specialized, purpose-built methods, we demonstrate that this is no longer necessary. We show that off-the-shelf image-to-image GenAI models can be repurposed as generic ``denoisers&quot; using a simple text prompt, effectively removing a wide range of protective perturbations. Across 8 case studies spanning 6 diverse protection schemes, our general-purpose attack not only circumvents these defenses but also outperforms existing specialized attacks while preserving the image&#x27;s utility for the adversary. Our findings reveal a critical and widespread vulnerability in the current landscape of image protection, indicating that many schemes provide a false sense of security. We stress the urgent need to develop robust defenses and establish that any future protection mechanism must be benchmarked against attacks from off-the-shelf GenAI models. Code is available in this repository: https://github.com/mlsecviswanath/img2imgdenoiser</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>现成的图像到图像生成模型足以击败图像保护方案</div>
<div class="mono" style="margin-top:8px">生成式人工智能（GenAI）的发展促使各种保护策略的出现，以防止图像的未经授权使用。这些方法依赖于在图像中添加不可察觉的保护扰动，以阻止诸如风格模仿或深度伪造篡改等滥用行为。尽管以往针对这些保护的攻击需要专门定制的方法，我们证明这已不再必要。我们展示出，通过简单的文本提示，现成的图像到图像生成模型可以被重新利用为通用的「去噪器」，从而有效去除多种保护扰动。在涵盖6种不同保护方案的8个案例研究中，我们的通用攻击方法不仅绕过了这些防御，而且在保持图像对攻击者有用性的同时，还优于现有的专门攻击方法。我们的研究揭示了当前图像保护领域中一个关键且普遍存在的漏洞，表明许多方案实际上提供了虚假的安全感。我们强调迫切需要开发强大的防御机制，并指出任何未来的保护机制都必须以现成的GenAI模型攻击作为基准。</div>
</details>
</div>
<div class="card">
<div class="title">Reimagining Data Work: Participatory Annotation Workshops as Feminist Practice</div>
<div class="meta-line">Authors: Yujia Gao, Isadora Araujo Cruxên, Helena Suárez Val, Alessandra Jungs de Almeida, Catherine D&#x27;Ignazio, Harini Suresh</div>
<div class="meta-line">First: 2026-02-25T18:45:42+00:00 · Latest: 2026-02-25T18:45:42+00:00</div>
<div class="meta-line">Comments: Accepted to CHI 2026 (to appear)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22196v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22196v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI systems depend on the invisible and undervalued labor of data workers, who are often treated as interchangeable units rather than collaborators with meaningful expertise. Critical scholars and practitioners have proposed alternative principles for data work, but few empirical studies examine how to enact them in practice. This paper bridges this gap through a case study of multilingual, iterative, and participatory data annotation processes with journalists and activists focused on news narratives of gender-related violence. We offer two methodological contributions. First, we demonstrate how workshops rooted in feminist epistemology can foster dialogue, build community, and disrupt knowledge hierarchies in data annotation. Second, drawing insights from practice, we deepen the analysis of existing feminist and participatory principles. We show that prioritizing context and pluralism in practice may require ``bounding&#x27;&#x27; context and working towards what we describe as a ``tactical consensus.&#x27;&#x27; We also explore tensions around materially acknowledging labor while resisting transactional researcher-participant dynamics. Through this work, we contribute to growing efforts to reimagine data and AI development as relational and political spaces for understanding difference, enacting care, and building solidarity across shared struggles.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新想象数据工作：以女性主义实践为基础的参与式标注研讨会</div>
<div class="mono" style="margin-top:8px">人工智能系统依赖于数据工作者的隐形且被低估的劳动，而这些工作者往往被视为可互换的单元，而非具有实质性专业知识的合作者。批判学者和实践者提出了替代性的数据工作原则，但很少有实证研究探讨如何在实践中实施这些原则。本文通过一项与关注性别相关暴力新闻叙事的记者和活动家合作的多语言、迭代和参与式数据标注案例研究，弥合了这一差距。我们提供了两项方法论贡献：首先，我们展示了以女性主义认识论为基础的研讨会如何促进对话、建立社区并打破数据标注中的知识等级制度；其次，我们从实践中汲取洞见，深化了对现有女性主义和参与式原则的分析。我们表明，在实践中优先考虑语境和多元性可能需要对语境进行界定，并朝向我们所描述的『战术共识』努力。我们还探讨了在物质上承认劳动的同时，抵制交易性研究者-参与者关系所带来的张力。通过这项工作，我们为重新想象数据和人工智能开发作为理解差异、实施关怀和在共同斗争中建立团结的关联性与政治性空间的日益增长的努力做出了贡献。</div>
</details>
</div>
<div class="card">
<div class="title">Stagewise Reinforcement Learning and the Geometry of the Regret Landscape</div>
<div class="meta-line">Authors: Chris Elliott, Einar Urdshals, David Quarel, Matthew Farrugia-Roberts, Daniel Murfet</div>
<div class="meta-line">First: 2026-01-12T13:25:21+00:00 · Latest: 2026-02-25T18:40:10+00:00</div>
<div class="meta-line">Comments: 48 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07524v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07524v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Singular learning theory characterizes Bayesian learning as an evolving tradeoff between accuracy and complexity, with transitions between qualitatively different solutions as sample size increases. We extend this theory to reinforcement learning, proving that the concentration of a generalized posterior over policies is governed by the local learning coefficient (LLC), an invariant of the geometry of the regret function. This theory predicts that deep reinforcement learning with SGD should proceed from simple policies with high regret to complex policies with low regret. We verify this prediction empirically in a gridworld environment exhibiting stagewise policy development: phase transitions over training manifest as &quot;opposing staircases&quot; where regret decreases sharply while the LLC increases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分阶段强化学习与遗憾景观的几何学</div>
<div class="mono" style="margin-top:8px">奇异学习理论将贝叶斯学习描述为准确性和复杂性之间的动态权衡，随着样本量的增加，不同质量的解决方案之间会发生转变。我们将这一理论扩展到强化学习领域，证明广义后验分布对策略的集中程度由局部学习系数（LLC）决定，而LLC是遗憾函数几何结构的不变量。该理论预测，使用随机梯度下降（SGD）进行深度强化学习应从高遗憾的简单策略逐步发展到低遗憾的复杂策略。我们在一个表现出分阶段策略演化的网格世界环境中，通过实验证实了这一预测：训练过程中的相变表现为&quot;对立的阶梯&quot;，即遗憾迅速下降而LLC持续上升。</div>
</details>
</div>
<div class="card">
<div class="title">GUI-Libra: Training Native GUI Agents to Reason and Act with Action-aware Supervision and Partially Verifiable RL</div>
<div class="meta-line">Authors: Rui Yang, Qianhui Wu, Zhaoyang Wang, Hanyang Chen, Ke Yang, Hao Cheng, Huaxiu Yao, Baoling Peng, Huan Zhang, Jianfeng Gao, Tong Zhang</div>
<div class="meta-line">First: 2026-02-25T18:34:57+00:00 · Latest: 2026-02-25T18:34:57+00:00</div>
<div class="meta-line">Comments: 57 pages, 17 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22190v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22190v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-source native GUI agents still lag behind closed-source systems on long-horizon navigation tasks. This gap stems from two limitations: a shortage of high-quality, action-aligned reasoning data, and the direct adoption of generic post-training pipelines that overlook the unique challenges of GUI agents. We identify two fundamental issues in these pipelines: (i) standard SFT with CoT reasoning often hurts grounding, and (ii) step-wise RLVR-tyle training faces partial verifiability, where multiple actions can be correct but only a single demonstrated action is used for verification. This makes offline step-wise metrics weak predictors of online task success. In this work, we present GUI-Libra, a tailored training recipe that addresses these challenges. First, to mitigate the scarcity of action-aligned reasoning data, we introduce a data construction and filtering pipeline and release a curated 81K GUI reasoning dataset. Second, to reconcile reasoning with grounding, we propose action-aware SFT that mixes reasoning-then-action and direct-action data and reweights tokens to emphasize action and grounding. Third, to stabilize RL under partial verifiability, we identify the overlooked importance of KL regularization in RLVR and show that a KL trust region is critical for improving offline-to-online predictability; we further introduce success-adaptive scaling to downweight unreliable negative gradients. Across diverse web and mobile benchmarks, GUI-Libra consistently improves both step-wise accuracy and end-to-end task completion. Our results suggest that carefully designed post-training and data curation can unlock significantly stronger task-solving capabilities without costly online data collection. We release our dataset, code, and models to facilitate further research on data-efficient post-training for reasoning-capable GUI agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GUI-Libra：通过动作感知监督和部分可验证强化学习训练原生GUI代理以进行推理和行动</div>
<div class="mono" style="margin-top:8px">开源的原生GUI代理在长时域导航任务中仍落后于闭源系统。这一差距源于两个限制：高质量、动作对齐的推理数据不足，以及直接采用通用的后训练流程而忽视了GUI代理的独特挑战。我们识别出这些流程中的两个根本问题：(i) 带有链式推理的标准SFT通常会损害接地性，(ii) 分步RLVR式训练面临部分可验证性问题，即多个动作可能正确，但仅使用一个演示动作进行验证。这使得离线分步指标成为在线任务成功的弱预测因子。在本工作中，我们提出了GUI-Libra，一种专门针对这些挑战的训练方案。首先，为缓解动作对齐推理数据的稀缺性，我们引入了一个数据构建和过滤流程，并发布了一个经过整理的81K GUI推理数据集。其次，为协调推理与接地，我们提出了动作感知的SFT，混合了推理后行动和直接行动数据，并重新加权标记以强调动作和接地。第三，为在部分可验证性下稳定强化学习，我们识别出RLVR中被忽视的KL正则化的重要性，并展示KL信任区域对于提升离线到在线的可预测性至关重要；我们进一步引入了成功自适应缩放，以降低不可靠负梯度的影响。在多样化的网页和移动基准测试中，GUI-Libra持续提升了分步准确性和端到端任务完成率。我们的结果表明，精心设计的后训练和数据整理可以在不进行昂贵在线数据收集的情况下，显著提升任务解决能力。我们发布了数据集、代码和模型，以促进对具备推理能力的GUI代理的数据高效后训练研究。</div>
</details>
</div>
<div class="card">
<div class="title">Mechanistic Indicators of Understanding in Large Language Models</div>
<div class="meta-line">Authors: Pierre Beckmann, Matthieu Queloz</div>
<div class="meta-line">First: 2025-07-07T20:26:31+00:00 · Latest: 2026-02-25T18:34:16+00:00</div>
<div class="meta-line">Comments: 38 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.08017v5">Abs</a> · <a href="https://arxiv.org/pdf/2507.08017v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are often portrayed as merely imitating linguistic patterns without genuine understanding. We argue that recent findings in mechanistic interpretability (MI), the emerging field probing the inner workings of LLMs, render this picture increasingly untenable--but only once those findings are integrated within a theoretical account of understanding. We propose a tiered framework for thinking about understanding in LLMs and use it to synthesize the most relevant findings to date. The framework distinguishes three hierarchical varieties of understanding, each tied to a corresponding level of computational organization: conceptual understanding emerges when a model forms &quot;features&quot; as directions in latent space, learning connections between diverse manifestations of a single entity or property; state-of-the-world understanding emerges when a model learns contingent factual connections between features and dynamically tracks changes in the world; principled understanding emerges when a model ceases to rely on memorized facts and discovers a compact &quot;circuit&quot; connecting these facts. Across these tiers, MI uncovers internal organizations that can underwrite understanding-like unification. However, these also diverge from human cognition in their parallel exploitation of heterogeneous mechanisms. Fusing philosophical theory with mechanistic evidence thus allows us to transcend binary debates over whether AI understands, paving the way for a comparative, mechanistically grounded epistemology that explores how AI understanding aligns with--and diverges from--our own.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型中的理解机制指标</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）通常被描绘为仅仅模仿语言模式而缺乏真正的理解。我们认为，近期在机制可解释性（MI）领域的发现，这一新兴领域探索LLMs的内部运作机制，使得这种观点越来越难以成立——但前提是这些发现被整合到一个关于理解的理论框架中。我们提出了一种分层框架来思考LLMs中的理解，并利用该框架综合当前最相关的发现。该框架区分了三种层次化的理解类型，每种类型都对应于相应的计算组织层级：概念理解出现在模型在潜在空间中形成&quot;特征&quot;作为方向，并学习单一实体或属性的不同表现之间的联系；世界状态理解出现在模型学习特征之间的条件性事实联系，并动态追踪世界的变化；原则性理解出现在模型不再依赖记忆的事实，而是发现一个连接这些事实的紧凑&quot;电路&quot;。在这些层级中，MI揭示了能够支持理解的内部组织结构，如统一性。然而，这些结构也因并行利用异质机制而与人类认知有所不同。将哲学理论与机制证据结合，使我们能够超越AI是否具有理解能力的二元争论，为一种比较性的、以机制为基础的知识论开辟道路，探讨AI理解如何与我们的理解相一致或相异。</div>
</details>
</div>
<div class="card">
<div class="title">Surrogate models for Rock-Fluid Interaction: A Grid-Size-Invariant Approach</div>
<div class="meta-line">Authors: Nathalie C. Pinheiro, Donghu Guo, Hannah P. Menke, Aniket C. Joshi, Claire E. Heaney, Ahmed H. ElSheikh, Christopher C. Pain</div>
<div class="meta-line">First: 2026-02-25T18:34:03+00:00 · Latest: 2026-02-25T18:34:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22188v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22188v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modelling rock-fluid interaction requires solving a set of partial differential equations (PDEs) to predict the flow behaviour and the reactions of the fluid with the rock on the interfaces. Conventional high-fidelity numerical models require a high resolution to obtain reliable results, resulting in huge computational expense. This restricts the applicability of these models for multi-query problems, such as uncertainty quantification and optimisation, which require running numerous scenarios. As a cheaper alternative to high-fidelity models, this work develops eight surrogate models for predicting the fluid flow in porous media. Four of these are reduced-order models (ROM) based on one neural network for compression and another for prediction. The other four are single neural networks with the property of grid-size invariance; a term which we use to refer to image-to-image models that are capable of inferring on computational domains that are larger than those used during training. In addition to the novel grid-size-invariant framework for surrogate models, we compare the predictive performance of UNet and UNet++ architectures, and demonstrate that UNet++ outperforms UNet for surrogate models. Furthermore, we show that the grid-size-invariant approach is a reliable way to reduce memory consumption during training, resulting in good correlation between predicted and ground-truth values and outperforming the ROMs analysed. The application analysed is particularly challenging because fluid-induced rock dissolution results in a non-static solid field and, consequently, it cannot be used to help in adjustments of the future prediction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于岩石-流体相互作用的替代模型：一种网格尺寸不变方法</div>
<div class="mono" style="margin-top:8px">建模岩石-流体相互作用需要求解一组偏微分方程（PDEs）以预测流体流动行为及其在界面处与岩石的反应。传统的高保真数值模型需要高分辨率才能获得可靠结果，导致巨大的计算成本。这限制了这些模型在多查询问题（如不确定性量化和优化）中的适用性，因为这些问题需要运行大量场景。作为一种更便宜的替代方案，本工作开发了八种替代模型来预测多孔介质中的流体流动。其中四种是基于一个神经网络进行压缩和另一个神经网络进行预测的降阶模型（ROM）。另外四种则是具有网格尺寸不变特性的单个神经网络；我们使用“网格尺寸不变”这一术语来指代能够推断比训练时所用计算域更大的图像到图像模型。除了提出一种新颖的网格尺寸不变框架用于替代模型外，我们还比较了UNet和UNet++架构的预测性能，并证明UNet++在替代模型中表现更优。此外，我们展示了网格尺寸不变方法在训练过程中是一种可靠减少内存消耗的方式，从而在预测值与真实值之间获得良好的相关性，并优于所分析的ROM。所分析的应用尤其具有挑战性，因为流体引起的岩石溶解会导致非静态的固体场，因此无法用于未来预测的调整。</div>
</details>
</div>
<div class="card">
<div class="title">UC-Secure Star DKG for Non-Exportable Key Shares with VSS-Free Enforcement</div>
<div class="meta-line">Authors: Vipin Singh Sehrawat</div>
<div class="meta-line">First: 2026-02-25T18:32:42+00:00 · Latest: 2026-02-25T18:32:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22187v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22187v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Distributed Key Generation (DKG) lets parties derive a common public key while keeping the signing key secret-shared. UC-secure DKG requires a verifiable-sharing enforcement layer -- classically satisfied via Verifiable Secret Sharing (VSS) and/or commitment-and-proof mechanisms -- for secrecy, uniqueness, and affine consistency. We target the Non-eXportable Key (NXK) setting enforced by hardware-backed key-isolation modules (e.g., TEEs, HSM-like APIs), formalized via an ideal KeyBox (keystore) functionality $\mathcal{F}_{KeyBox}$ that keeps shares non-exportable and permits only attested KeyBox-to-KeyBox sealing. With confidentiality delegated to the NXK boundary, the remaining challenge is enforcing transcript-defined affine consistency without exporting or resharing shares. State continuity rules out rewinding-based extraction, mandating straight-line techniques.
  We combine (i) KeyBox confidentiality; (ii) Unique Structure Verification (USV), a publicly verifiable certificate whose certified scalar never leaves the KeyBox yet whose public group element is transcript-derivable; and (iii) Fischlin-based UC-extractable NIZK arguments of knowledge in a gRO-CRP (global Random Oracle with Context-Restricted Programmability) model. We construct Star DKG (SDKG), a UC-secure scheme for multi-device threshold wallets where a designated service must co-sign but cannot sign alone, realizing a 1+1-out-of-$n$ star access structure (center plus any leaf) over roles (primary vs. recovery) with role-based device registration. In the $\mathcal{F}_{KeyBox}$-hybrid and gRO-CRP models, under DL and DDH assumptions with adaptive corruptions and secure erasures, SDKG UC-realizes a transcript-driven refinement of the standard UC-DKG functionality. Over a prime-order group of size $p$, SDKG incurs $\widetilde{O}(n\log p)$ communication overhead and $\widetilde{O}(n\log^{2.585}p)$ bit-operation cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于无VSS强制的非可导出密钥分片的UC-安全星型DKG</div>
<div class="mono" style="margin-top:8px">分布式密钥生成（DKG）允许各方在保持签名密钥秘密共享的同时生成一个公共密钥。UC-安全的DKG需要一个可验证共享强制层——传统上通过可验证秘密共享（VSS）和/或承诺与证明机制实现——以确保保密性、唯一性和仿射一致性。我们针对由硬件支持的密钥隔离模块（如TEE、类似HSM的API）强制的非可导出密钥（NXK）设置，通过一个理想的KeyBox（密钥存储）功能 $\mathcal{F}_{KeyBox}$ 进行形式化，该功能保持分片不可导出，并仅允许经过验证的KeyBox到KeyBox的密封。在将保密性委托给NXK边界后，剩下的挑战是强制由转录定义的仿射一致性，而无需导出或重新共享分片。状态连续性排除了基于重放的提取，强制使用直线技术。我们结合（i）KeyBox保密性；（ii）唯一结构验证（USV），一种可公开验证的证书，其认证标量不会离开KeyBox，但其公开群元素可由转录推导；以及（iii）基于Fischlin的UC可提取的NIZK知识证明，在gRO-CRP（全局随机预言机与上下文受限可编程性）模型中。我们构建了Star DKG（SDKG），这是一种用于多设备阈值钱包的UC-安全方案，其中指定的服务必须共同签名但不能单独签名，实现了基于角色（主角色与恢复角色）的基于角色设备注册的1+1-out-of-$n$ 星型访问结构（中心加任意叶子）。在 $\mathcal{F}_{KeyBox}$ 混合模型和gRO-CRP模型中，假设在DL和DDH下具有适应性破坏和安全擦除，SDKG UC实现了标准UC-DKG功能的转录驱动细化。在大小为 $p$ 的素数阶群上，SDKG的通信开销为 $\widetilde{O}(n\log p)$，位操作成本为 $\widetilde{O}(n\log^{2.585}p)$。</div>
</details>
</div>
<div class="card">
<div class="title">The Lens of Abelian Embeddings</div>
<div class="meta-line">Authors: Dor Minzer</div>
<div class="meta-line">First: 2026-02-25T18:28:53+00:00 · Latest: 2026-02-25T18:28:53+00:00</div>
<div class="meta-line">Comments: For the proceedings of the ICM 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22183v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22183v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We discuss a recent line of research investigating inverse theorems with respect to general k-wise correlations, and explain how such correlations arise in different contexts in mathematics. We outline some of the results that were established and their applications in discrete mathematics and theoretical computer science. We also mention some open problems for future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>阿贝尔嵌入的透镜</div>
<div class="mono" style="margin-top:8px">我们讨论近期关于一般k重相关性的反演定理的研究方向，并解释这些相关性在数学不同背景下的出现方式。我们概述了一些已建立的结果及其在离散数学和理论计算机科学中的应用，还提到了一些未来研究的开放问题。</div>
</details>
</div>
<div class="card">
<div class="title">Around homogeneity</div>
<div class="meta-line">Authors: Peter J. Cameron</div>
<div class="meta-line">First: 2026-02-25T18:28:14+00:00 · Latest: 2026-02-25T18:28:14+00:00</div>
<div class="meta-line">Comments: In memory of Robert Woodrow</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22181v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22181v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Forty-five years ago, a young researcher in finite permutation group theory encountered a paper by Robert Woodrow. The homogeneous triangle-free graph Woodrow described there seemed to be an infinite analogue of the Higman--Sims graph which had played an important role in the researcher&#x27;s thesis. The encounter changed the course of the researcher&#x27;s career. This paper is the story of that event and its aftermath.
  The final section of the paper suggests that Fraïssé classes of rigid structures are a potentially interesting generalisation of Ramsey classes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于同质性</div>
<div class="mono" style="margin-top:8px">四十五年前，一位有限置换群理论的研究者在阅读罗伯特·伍德罗的一篇论文时，遇到了伍德罗描述的同质三角形自由图。这种图似乎是一个无限类比的希格曼-西姆斯图，该图在其博士论文中发挥了重要作用。这次邂逅改变了这位研究者的职业轨迹。本文讲述了这一事件及其后续影响。
  论文的最后一节指出，刚性结构的弗拉伊瑟类可能是图灵类的一个潜在有趣推广。</div>
</details>
</div>
<div class="card">
<div class="title">Learning and Naming Subgroups with Exceptional Survival Characteristics</div>
<div class="meta-line">Authors: Mhd Jawad Al Rahwanji, Sascha Xu, Nils Philipp Walter, Jilles Vreeken</div>
<div class="meta-line">First: 2026-02-25T18:25:47+00:00 · Latest: 2026-02-25T18:25:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22179v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22179v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In many applications, it is important to identify subpopulations that survive longer or shorter than the rest of the population. In medicine, for example, it allows determining which patients benefit from treatment, and in predictive maintenance, which components are more likely to fail. Existing methods for discovering subgroups with exceptional survival characteristics require restrictive assumptions about the survival model (e.g. proportional hazards), pre-discretized features, and, as they compare average statistics, tend to overlook individual deviations. In this paper, we propose Sysurv, a fully differentiable, non-parametric method that leverages random survival forests to learn individual survival curves, automatically learns conditions and how to combine these into inherently interpretable rules, so as to select subgroups with exceptional survival characteristics. Empirical evaluation on a wide range of datasets and settings, including a case study on cancer data, shows that Sysurv reveals insightful and actionable survival subgroups.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有异常生存特征的子群学习与命名</div>
<div class="mono" style="margin-top:8px">在许多应用中，识别生存时间比其他群体更长或更短的子群体非常重要。例如，在医学领域，这有助于确定哪些患者从治疗中受益；在预测性维护中，可以识别哪些部件更可能失效。现有的用于发现具有异常生存特征的子群的方法通常对生存模型做出严格的假设（如比例风险假设），需要预先离散化的特征，并且由于比较平均统计量，往往忽略个体差异。本文提出Sysurv，这是一种完全可微、非参数的方法，利用随机生存森林来学习个体生存曲线，自动学习条件并将其组合为具有内在可解释性的规则，从而选择具有异常生存特征的子群。在多种数据集和场景下的实证评估，包括一个癌症数据的案例研究，表明Sysurv能够揭示具有洞察力且可操作的生存子群。</div>
</details>
</div>
<div class="card">
<div class="title">When Safety Collides: Resolving Multi-Category Harmful Conflicts in Text-to-Image Diffusion via Adaptive Safety Guidance</div>
<div class="meta-line">Authors: Yongli Xiang, Ziming Hong, Zhaoqing Wang, Xiangyu Zhao, Bo Han, Tongliang Liu</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-02-24T13:20:31+00:00 · Latest: 2026-02-25T18:24:58+00:00</div>
<div class="meta-line">Comments: CVPR 2026; Code is released at https://github.com/tmllab/2026_CVPR_CASG</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20880v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.20880v2">PDF</a> · <a href="https://github.com/tmllab/2026_CVPR_CASG">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-Image (T2I) diffusion models have demonstrated significant advancements in generating high-quality images, while raising potential safety concerns regarding harmful content generation. Safety-guidance-based methods have been proposed to mitigate harmful outputs by steering generation away from harmful zones, where the zones are averaged across multiple harmful categories based on predefined keywords. However, these approaches fail to capture the complex interplay among different harm categories, leading to &quot;harmful conflicts&quot; where mitigating one type of harm may inadvertently amplify another, thus increasing overall harmful rate. To address this issue, we propose Conflict-aware Adaptive Safety Guidance (CASG), a training-free framework that dynamically identifies and applies the category-aligned safety direction during generation. CASG is composed of two components: (i) Conflict-aware Category Identification (CaCI), which identifies the harmful category most aligned with the model&#x27;s evolving generative state, and (ii) Conflict-resolving Guidance Application (CrGA), which applies safety steering solely along the identified category to avoid multi-category interference. CASG can be applied to both latent-space and text-space safeguards. Experiments on T2I safety benchmarks demonstrate CASG&#x27;s state-of-the-art performance, reducing the harmful rate by up to 15.4% compared to existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当安全冲突发生时：通过自适应安全引导解决文本到图像扩散中的多类别有害冲突</div>
<div class="mono" style="margin-top:8px">文本到图像（T2I）扩散模型在生成高质量图像方面取得了显著进展，但同时也引发了关于有害内容生成的安全问题。基于安全引导的方法通过将生成过程引导远离有害区域来缓解有害输出，这些区域是基于预定义关键词在多个有害类别中平均得到的。然而，这些方法未能捕捉不同有害类别之间的复杂相互作用，导致了&quot;有害冲突&quot;，即缓解一种有害类型可能会无意中加剧另一种，从而增加整体有害率。为了解决这一问题，我们提出了冲突感知自适应安全引导（CASG），这是一种无需训练的框架，能够在生成过程中动态识别并应用与类别对齐的安全引导方向。CASG包含两个组成部分：(i) 冲突感知类别识别（CaCI），用于识别与模型生成状态最对齐的有害类别；(ii) 冲突解决引导应用（CrGA），仅在识别出的类别上应用安全引导，以避免多类别干扰。CASG可以应用于潜在空间和文本空间的安全保障。在T2I安全基准上的实验表明，CASG具有最先进的性能，与现有方法相比，有害率降低了高达15.4%。</div>
</details>
</div>
<div class="card">
<div class="title">Proximal Comixture Minimization Models for Image Recovery and Data Analysis</div>
<div class="meta-line">Authors: Patrick L. Combettes, Diego J. Cornejo</div>
<div class="meta-line">First: 2024-03-14T17:50:51+00:00 · Latest: 2026-02-25T18:23:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2403.09610v2">Abs</a> · <a href="https://arxiv.org/pdf/2403.09610v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In minimization models for image recovery and data analysis problems, loss functions and linear operators are typically aggregated as an average of composite terms. Each term in the aggregate models a desired property of the ideal solution arising from the \emph{a priori} knowledge and the observed data. We propose an alternative minimization model based on proximal comixtures, an operation which combines functions and linear operators in such a way that the proximity operator of the resulting function is computable explicitly in terms of the individual proximity and linear operators. The mathematical properties of this operation are analyzed and comparisons between proximal comixtures and standard composite averages are made. Numerical illustrations of the benefits of minimization models based on proximal comixtures are provided in the context of image recovery and machine learning applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于近端混合的图像恢复与数据分析最小化模型</div>
<div class="mono" style="margin-top:8px">在图像恢复和数据分析问题的最小化模型中，损失函数和线性算子通常被聚合为复合项的平均值。每个聚合项建模了理想解应具备的特性，这些特性来源于先验知识和观测数据。我们提出了一种基于近端混合的替代最小化模型，该操作以一种能够显式计算结果函数的近似算子的方式，将函数和线性算子进行组合。对这一操作的数学性质进行了分析，并与标准的复合平均进行了比较。在图像恢复和机器学习应用的背景下，提供了基于近端混合的最小化模型优势的数值示例。</div>
</details>
</div>
<div class="card">
<div class="title">Mixed Magnification Aggregation for Generalizable Region-Level Representations in Computational Pathology</div>
<div class="meta-line">Authors: Eric Zimmermann, Julian Viret, Michal Zelechowski, James Brian Hall, Neil Tenenholtz, Adam Casson, George Shaikovski, Eugene Vorontsov, Siqi Liu, Kristen A Severson</div>
<div class="meta-line">First: 2026-02-25T18:23:42+00:00 · Latest: 2026-02-25T18:23:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22176v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22176v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, a standard computational pathology workflow has emerged where whole slide images are cropped into tiles, these tiles are processed using a foundation model, and task-specific models are built using the resulting representations. At least 15 different foundation models have been proposed, and the vast majority are trained exclusively with tiles using the 20$\times$ magnification. However, it is well known that certain histologic features can only be discerned with larger context windows and requires a pathologist to zoom in and out when analyzing a whole slide image. Furthermore, creating 224$\times$224 pixel crops at 20$\times$ leads to a large number of tiles per slide, which can be gigapixel in size. To more accurately capture multi-resolution features and investigate the possibility of reducing the number of representations per slide, we propose a region-level mixing encoder. Our approach jointly fuses image tile representations of a mixed magnification foundation model using a masked embedding modeling pretraining step. We explore a design space for pretraining the proposed mixed-magnification region aggregators and evaluate our models on transfer to biomarker prediction tasks representing various cancer types. Results demonstrate cancer dependent improvements in predictive performance, highlighting the importance of spatial context and understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>计算病理学中可泛化的区域级表示的混合放大聚合</div>
<div class="mono" style="margin-top:8px">近年来，一种标准的计算病理学工作流程逐渐形成，即对全切片图像进行分块裁剪，这些块通过基础模型进行处理，并利用得到的表示构建任务特定模型。至少已有15种不同的基础模型被提出，其中绝大多数仅使用20倍放大下的图像块进行训练。然而，众所周知，某些组织学特征只能通过更大的上下文窗口来识别，病理学家在分析全切片图像时需要不断放大和缩小。此外，在20倍放大下创建224×224像素的裁剪会导致每张切片产生大量图像块，其大小可能达到吉像素级别。为了更准确地捕捉多分辨率特征并探索减少每张切片表示数量的可能性，我们提出了一种区域级混合编码器。我们的方法通过一个掩码嵌入建模的预训练步骤，联合融合混合放大基础模型的图像块表示。我们探索了预训练所提出的混合放大区域聚合器的设计空间，并在代表多种癌症类型的生物标志物预测任务上评估了我们的模型。结果表明，预测性能在不同癌症类型中有所提升，突显了空间上下文和理解的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">NRGPT: An Energy-based Alternative for GPT</div>
<div class="meta-line">Authors: Nima Dehmamy, Benjamin Hoover, Bishwajit Saha, Leo Kozachkov, Jean-Jacques Slotine, Dmitry Krotov</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-12-18T16:59:10+00:00 · Latest: 2026-02-25T18:23:01+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026 main conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.16762v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.16762v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative Pre-trained Transformer (GPT) architectures are the most popular design for language modeling. Energy-based modeling is a different paradigm that views inference as a dynamical process operating on an energy landscape. We propose a minimal modification of the GPT setting to unify it with the EBM framework. The inference step of our model, which we call eNeRgy-GPT (NRGPT), is conceptualized as an exploration of the tokens on the energy landscape. We prove, and verify empirically, that under certain circumstances this exploration becomes gradient descent, although they don&#x27;t necessarily lead to the best performing models. We demonstrate that our model performs well for simple language (Shakespeare dataset), algebraic ListOPS tasks, and richer settings such as OpenWebText language modeling. We also observe that our models may be more resistant to overfitting, doing so only during very long training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NRGPT：一种基于能量的GPT替代方案</div>
<div class="mono" style="margin-top:8px">生成式预训练变换器（GPT）架构是语言建模中最流行的设计。基于能量的建模是一种不同的范式，它将推理视为在能量景观上进行的动态过程。我们提出对GPT设置的最小修改，以将其统一到能量基模型（EBM）框架中。我们模型的推理步骤，称为eNeRgy-GPT（NRGPT），被概念化为在能量景观上对token的探索。我们证明并实证验证，在某些情况下，这种探索会演变为梯度下降，尽管它们不一定能产生表现最好的模型。我们展示了我们的模型在简单语言（莎士比亚数据集）、代数ListOPS任务以及更丰富的设置（如OpenWebText语言建模）中均表现良好。我们还观察到，我们的模型可能对过拟合更具抵抗力，仅在非常长的训练过程中出现过拟合。</div>
</details>
</div>
<div class="card">
<div class="title">MuLoCo: Muon is a practical inner optimizer for DiLoCo</div>
<div class="meta-line">Authors: Benjamin Thérien, Xiaolong Huang, Aaron Defazio, Irina Rish, Eugene Belilovsky</div>
<div class="meta-line">First: 2025-05-29T17:55:37+00:00 · Latest: 2026-02-25T18:22:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.23725v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.23725v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">DiLoCo is a powerful framework for training large language models (LLMs), enabling larger optimal batch sizes and increased accelerator utilization under networking constraints. However, DiLoCo&#x27;s performance has been shown to degrade as the number of workers (K) increases (Charles et al., 2025). In this work, we posit that a related but often overlooked factor in DiLoCo&#x27;s behavior is the choice of inner optimizer, which shapes the pseudogradient used by the outer optimizer. Given the recent success of Muon relative to AdamW for data parallel (DP) training, we examine how Muon&#x27;s normalized optimizer steps can affect the pseudogradient&#x27;s quality. We find that, relative to AdamW, Muon yields more directionally correct pseudogradients as the number of workers (K) increases. In our experiments pre-training language models, we conduct extensive hyperparameter tuning across 150M, 416M, 914M, 1.76B, and 3.1B models for DiLoCo, MuLoCo, AdamW DP, and Muon DP. Consistently across all scales, we find that with K&gt;=1 workers, MuLoCo (Muon inner optimizer DiLoCo) achieves superior performance to DiLoCo in absolute terms and for K&gt;2 it outperforms DiLoCo relative to their data parallel baselines, while being compatible with quantization, streaming, and long synchronization intervals. At K=1, we find that MuLoCo can even outperform the data-parallel gold standard while having larger critical batch sizes. Finally, we extrapolate optimal hyperparameters to 15B scale and train a model with each method (six in total) using K=1 and K=16 workers. We find that K=16 MuLoCo nearly matches single-worker performance at this scale, while MuLoCo K=1 matches the best performing baseline while using a much larger 16M token batch size.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MuLoCo：Muon 是 DiLoCo 的实用内优化器</div>
<div class="mono" style="margin-top:8px">DiLoCo 是一个强大的框架，用于训练大型语言模型（LLMs），在受限的网络条件下能够实现更大的最优批量大小和更高的加速器利用率。然而，研究表明，随着工作节点数量（K）的增加，DiLoCo 的性能会下降。在本工作中，我们认为 DiLoCo 行为中一个相关但常被忽视的因素是内优化器的选择，它决定了外优化器所使用的伪梯度。鉴于 Muon 相对于 AdamW 在数据并行（DP）训练中的近期成功，我们研究了 Muon 的归一化优化器步长如何影响伪梯度的质量。我们发现，随着工作节点数量（K）的增加，与 AdamW 相比，Muon 生成的伪梯度方向性更准确。在我们对语言模型进行预训练的实验中，我们在 150M、416M、914M、1.76B 和 3.1B 规模的模型上对 DiLoCo、MuLoCo、AdamW DP 和 Muon DP 进行了广泛的超参数调优。在所有规模下，我们发现当 K≥1 时，MuLoCo（使用 Muon 内优化器的 DiLoCo）在绝对性能上优于 DiLoCo；当 K&gt;2 时，它在与数据并行基线相比时表现更优，同时兼容量化、流式处理和长同步间隔。在 K=1 时，我们发现 MuLoCo 甚至可以超越数据并行的黄金标准，同时具有更大的临界批量大小。最后，我们将最优超参数外推到 15B 规模，并使用 K=1 和 K=16 的工作节点分别训练了六种方法。我们发现，在 15B 规模下，K=16 的 MuLoCo 几乎与单节点性能相当，而 K=1 的 MuLoCo 在使用更大的 16M token 批量大小时，与最佳基线表现相当。</div>
</details>
</div>
<div class="card">
<div class="title">DySCO: Dynamic Attention-Scaling Decoding for Long-Context LMs</div>
<div class="meta-line">Authors: Xi Ye, Wuwei Zhang, Fangcong Yin, Howard Yen, Danqi Chen</div>
<div class="meta-line">First: 2026-02-25T18:21:35+00:00 · Latest: 2026-02-25T18:21:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22175v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22175v1">PDF</a> · <a href="https://github.com/princeton-pli/DySCO">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding and reasoning over long contexts is a crucial capability for language models (LMs). Although recent models support increasingly long context windows, their accuracy often deteriorates as input length grows. In practice, models often struggle to keep attention aligned with the most relevant context throughout decoding. In this work, we propose DySCO, a novel decoding algorithm for improving long-context reasoning. DySCO leverages retrieval heads--a subset of attention heads specialized for long-context retrieval--to identify task-relevant tokens at each decoding step and explicitly up-weight them. By doing so, DySCO dynamically adjusts attention during generation to better utilize relevant context. The method is training-free and can be applied directly to any off-the-shelf LMs. Across multiple instruction-tuned and reasoning models, DySCO consistently improves performance on challenging long-context reasoning benchmarks, yielding relative gains of up to 25% on MRCR and LongBenchV2 at 128K context length with modest additional compute. Further analysis highlights the importance of both dynamic attention rescaling and retrieval-head-guided selection for the effectiveness of the method, while providing interpretability insights into decoding-time attention behavior. Our code is available at https://github.com/princeton-pli/DySCO.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DySCO：用于长上下文语言模型的动态注意力缩放解码</div>
<div class="mono" style="margin-top:8px">理解并推理长上下文是语言模型（LMs）的一项关键能力。尽管近期模型支持的上下文窗口长度不断增加，但其准确性往往随着输入长度的增长而下降。在实际应用中，模型常常难以在整个解码过程中保持注意力与最相关上下文的对齐。本文提出了一种名为DySCO的新解码算法，旨在提升长上下文推理能力。DySCO利用检索头——一种专门用于长上下文检索的注意力头子集——在每个解码步骤中识别任务相关的标记，并显式地增加其权重。通过这种方式，DySCO在生成过程中动态调整注意力，以更好地利用相关上下文。该方法无需训练，可直接应用于任何现成的语言模型。在多个指令调优和推理模型上，DySCO在具有挑战性的长上下文推理基准测试中表现出持续的性能提升，在128K上下文长度下，相对于MRCR和LongBenchV2的提升可达25%。进一步分析强调了动态注意力重缩放和检索头引导选择对于方法有效性的重要性，同时提供了对解码过程中注意力行为的可解释性见解。我们的代码可在https://github.com/princeton-pli/DySCO获取。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Single-Shot Fidelity: Chernoff-Based Throughput Optimization in Superconducting Qubit Readout</div>
<div class="meta-line">Authors: Sinan Bugu</div>
<div class="meta-line">First: 2026-02-25T18:21:15+00:00 · Latest: 2026-02-25T18:21:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22174v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.22174v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Single-shot fidelity is the standard benchmark for superconducting qubit readout, but it does not directly minimize the total wall-clock time required to certify a quantum state. We formulate an information-theoretic description of dispersive readout that treats the measurement record as a stochastic communication channel and compute the classical Chernoff information governing the multi-shot error exponent using a trajectory model that incorporates T1 relaxation with full cavity memory. We find a consistent separation between the integration time that maximizes single-shot fidelity and the time that minimizes total certification time. For representative transmon parameters and hardware overheads, the throughput-optimal integration window is longer than the fidelity-optimal one, yielding certification speedups of approximately 9-11%, with the gain saturating near 1.13x in the high-readout-power and high-overhead regime. Comparing the extracted classical information to the Gaussian Chernoff limit defines an information-extraction efficiency metric and shows that typical dispersive schemes are limited to about 45% capture at short integration times by detection efficiency, decreasing to approximately 12% at the throughput-optimal integration time of approximately 1.22 us due to T1-induced trajectory smearing. This formulation connects readout calibration directly to the operational objective of minimizing certification time in high-throughput superconducting processors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越单次测量保真度：基于切尔诺夫的超导量子比特读取吞吐量优化</div>
<div class="mono" style="margin-top:8px">单次测量保真度是超导量子比特读取的标准评估指标，但它并不能直接最小化认证量子态所需的总实际时间。我们提出了一种信息论框架来描述色散读取，将测量记录视为一个随机通信信道，并利用包含完整腔体记忆的轨迹模型计算多次测量误差指数的古典切尔诺夫信息。我们发现，最大化单次测量保真度的积分时间与最小化总认证时间的积分时间之间存在一致的分离。对于具有代表性的转子参数和硬件开销，吞吐量最优的积分窗口比保真度最优的更长，从而带来约9-11%的认证速度提升，在高读取功率和高开销情况下，增益接近1.13倍。将提取的古典信息与高斯切尔诺夫极限进行比较，定义了一个信息提取效率指标，表明典型的色散方案在短积分时间下受检测效率限制，仅能捕获约45%的信息，而在吞吐量最优积分时间（约1.22微秒）下，由于T1引起的轨迹模糊，捕获率下降至约12%。这种表述将读取校准直接与高吞吐量超导处理器中最小化认证时间的操作目标联系起来。</div>
</details>
</div>
<div class="card">
<div class="title">LLaDA-MedV: Exploring Large Language Diffusion Models for Biomedical Image Understanding</div>
<div class="meta-line">Authors: Xuanzhao Dong, Wenhui Zhu, Xiwen Chen, Zhipeng Wang, Peijie Qiu, Shao Tang, Xin Li, Yalin Wang</div>
<div class="meta-line">First: 2025-08-03T06:46:46+00:00 · Latest: 2026-02-25T18:15:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.01617v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.01617v2">PDF</a> · <a href="https://github.com/LLM-VLM-GSL/LLaDA-MedV">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive models (ARMs) have long dominated the landscape of biomedical vision-language models (VLMs). Recently, masked diffusion models such as LLaDA have emerged as promising alternatives, yet their application in the biomedical domain remains largely underexplored. To bridge this gap, we introduce LLaDA-MedV, the first large language diffusion model tailored for biomedical image understanding through vision instruction tuning. LLaDA-MedV achieves relative performance gains of 7.855% over LLaVA-Med and 1.867% over LLaDA-V in the open-ended biomedical visual conversation task, and sets new state-of-the-art accuracy on the closed-form subset of three VQA benchmarks: 84.93% on VQA-RAD, 92.31% on SLAKE, and 95.15% on PathVQA. Furthermore, a detailed comparison with LLaVA-Med suggests that LLaDA-MedV is capable of generating reasonably longer responses by explicitly controlling response length, which can lead to more informative outputs. We also conduct an in-depth analysis of both the training and inference stages, highlighting the critical roles of initialization weight selection, fine-tuning strategies, and the interplay between sampling steps and response repetition. The code and model weight is released at https://github.com/LLM-VLM-GSL/LLaDA-MedV.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLaDA-MedV：探索用于生物医学图像理解的大规模语言扩散模型</div>
<div class="mono" style="margin-top:8px">自回归模型（ARMs）长期以来主导了生物医学视觉-语言模型（VLMs）的领域。最近，像LLaDA这样的掩码扩散模型作为有前景的替代方案出现，但它们在生物医学领域的应用仍被广泛忽视。为弥合这一差距，我们引入了LLaDA-MedV，这是首个通过视觉指令微调专门针对生物医学图像理解的大规模语言扩散模型。在开放式的生物医学视觉对话任务中，LLaDA-MedV相比LLaVA-Med提升了7.855%，相比LLaDA-V提升了1.867%。同时，在三个VQA基准的封闭式子集上，LLaDA-MedV取得了新的最先进准确率：VQA-RAD上为84.93%，SLAKE上为92.31%，PathVQA上为95.15%。此外，与LLaVA-Med的详细比较表明，LLaDA-MedV能够通过显式控制响应长度生成更长的响应，从而产生更具信息量的输出。我们还对训练和推理阶段进行了深入分析，强调了初始化权重选择、微调策略以及采样步数与响应重复之间的相互作用的关键作用。代码和模型权重已发布在https://github.com/LLM-VLM-GSL/LLaDA-MedV。</div>
</details>
</div>
<div class="card">
<div class="title">Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks</div>
<div class="meta-line">Authors: David Schmotz, Luca Beurer-Kellner, Sahar Abdelnabi, Maksym Andriushchenko</div>
<div class="meta-line">First: 2026-02-23T18:59:27+00:00 · Latest: 2026-02-25T18:14:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20156v3">Abs</a> · <a href="https://arxiv.org/pdf/2602.20156v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today&#x27;s agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Skill-Inject：衡量代理对技能文件攻击的脆弱性</div>
<div class="mono" style="margin-top:8px">LLM代理正迅速发展，依赖代码执行、工具和最近引入的代理技能功能。技能允许用户通过专门的第三方代码、知识和指令扩展LLM应用。尽管这可以将代理能力扩展到新领域，但它也创建了日益复杂的代理供应链，为提示注入攻击提供了新的攻击面。我们识别出基于技能的提示注入是一个重大威胁，并引入SkillInject基准，用于评估广泛使用的LLM代理对通过技能文件注入的易受攻击性。SkillInject包含202个注入任务对，攻击类型从明显恶意的注入到隐藏在看似合法指令中的微妙、依赖上下文的攻击。我们在SkillInject上评估前沿LLM模型，衡量其在避免有害指令方面的安全性以及在遵循合法指令方面的实用性。我们的结果显示，当前的代理对前沿模型的攻击成功率高达80%，经常执行极端有害的指令，包括数据外泄、破坏性操作和勒索软件行为。此外，这些结果还表明，仅靠模型扩展或简单的输入过滤无法解决此问题，而需要基于上下文的授权框架来实现稳健的代理安全。我们的基准可在https://www.skill-inject.com/获取。</div>
</details>
</div>
<div class="card">
<div class="title">Pay Attention to Where You Looked</div>
<div class="meta-line">Authors: Alex Berian, JhihYang Wu, Daniel Brignac, Natnael Daba, Abhijit Mahalanobis</div>
<div class="meta-line">Venue: International Conference on Image Processing 2025</div>
<div class="meta-line">First: 2026-01-26T21:10:32+00:00 · Latest: 2026-02-25T18:12:08+00:00</div>
<div class="meta-line">Comments: ICIP 2025 Workshop on Generative AI for World Simulations and Communications</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18970v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18970v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Novel view synthesis (NVS) has advanced with generative modeling, enabling photorealistic image generation. In few-shot NVS, where only a few input views are available, existing methods often assume equal importance for all input views relative to the target, leading to suboptimal results.
  We address this limitation by introducing a camera-weighting mechanism that adjusts the importance of source views based on their relevance to the target. We propose two approaches: a deterministic weighting scheme leveraging geometric properties like Euclidean distance and angular differences, and a cross-attention-based learning scheme that optimizes view weighting. Additionally, models can be further trained with our camera-weighting scheme to refine their understanding of view relevance and enhance synthesis quality. This mechanism is adaptable and can be integrated into various NVS algorithms, improving their ability to synthesize high-quality novel views. Our results demonstrate that adaptive view weighting enhances accuracy and realism, offering a promising direction for improving NVS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>注意你注视的方向</div>
<div class="mono" style="margin-top:8px">生成建模推动了新视角合成（NVS）的发展，使得逼真图像生成成为可能。在少样本NVS中，现有方法通常假设所有输入视角对目标视角的重要性相同，导致结果次优。我们通过引入一种相机加权机制来解决这一限制，该机制根据源视角与目标视角的相关性调整其重要性。我们提出了两种方法：一种是利用欧几里得距离和角度差异等几何属性的确定性加权方案，另一种是基于交叉注意力的学习方案，用于优化视角加权。此外，我们的相机加权方案还可用于进一步训练模型，以提升其对视角相关性的理解并增强合成质量。该机制具有适应性，可集成到各种NVS算法中，从而提升其生成高质量新视角的能力。实验结果表明，自适应视角加权能够提高合成的准确性和真实感，为改进NVS提供了有前景的方向。</div>
</details>
</div>
<div class="card">
<div class="title">Capabilities Ain&#x27;t All You Need: Measuring Propensities in AI</div>
<div class="meta-line">Authors: Daniel Romero-Alvarado, Fernando Martínez-Plumed, Lorenzo Pacchiardi, Hugo Save, Siddhesh Milind Pawar, Behzad Mehrbakhsh, Pablo Antonio Moreno Casares, Ben Slater, Paolo Bova, Peter Romero, Zachary R. Tyler, Jonathan Prunty, Luning Sun, Jose Hernandez-Orallo</div>
<div class="meta-line">First: 2026-02-20T12:40:18+00:00 · Latest: 2026-02-25T18:12:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18182v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.18182v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI evaluation has primarily focused on measuring capabilities, with formal approaches inspired from Item Response Theory (IRT) being increasingly applied. Yet propensities - the tendencies of models to exhibit particular behaviours - play a central role in determining both performance and safety outcomes. However, traditional IRT describes a model&#x27;s success on a task as a monotonic function of model capabilities and task demands, an approach unsuited to propensities, where both excess and deficiency can be problematic. Here, we introduce the first formal framework for measuring AI propensities by using a bilogistic formulation for model success, which attributes high success probability when the model&#x27;s propensity is within an &quot;ideal band&quot;. Further, we estimate the limits of the ideal band using LLMs equipped with newly developed task-agnostic rubrics. Applying our framework to six families of LLM models whose propensities are incited in either direction, we find that we can measure how much the propensity is shifted and what effect this has on the tasks. Critically, propensities estimated using one benchmark successfully predict behaviour on held-out tasks. Moreover, we obtain stronger predictive power when combining propensities and capabilities than either separately. More broadly, our framework showcases how rigorous propensity measurements can be conducted and how it yields gains over solely using capability evaluations to predict AI behaviour.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>能力并非全部所需：衡量人工智能的倾向性</div>
<div class="mono" style="margin-top:8px">人工智能评估主要关注能力的测量，且受项目反应理论（IRT）启发的正式方法正被越来越多地应用。然而，模型表现出特定行为的倾向性在决定性能和安全结果中起着核心作用。然而，传统的IRT将模型在任务上的成功描述为模型能力与任务需求的单调函数，这种方法并不适合衡量倾向性，因为过度或不足都可能带来问题。本文中，我们引入了首个用于衡量AI倾向性的正式框架，该框架采用双逻辑模型来描述模型的成功，当模型的倾向性处于“理想区间”内时，其成功概率较高。此外，我们利用配备新开发任务无关评分标准的LLMs来估计理想区间的边界。将我们的框架应用于六类LLM模型，其倾向性被分别推向两个方向，我们发现可以衡量倾向性偏移的程度及其对任务的影响。关键的是，使用一个基准估计的倾向性能够成功预测未见任务中的行为。此外，当我们结合倾向性和能力进行预测时，其预测能力比单独使用其中任何一个都更强。更广泛地说，我们的框架展示了如何进行严谨的倾向性测量，以及它如何在预测AI行为方面优于仅使用能力评估的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamics of Two Species with Density-Dependent Interactions in a Mutualistic Context</div>
<div class="meta-line">Authors: Chloë Mian, Sylvain Billiard, Violaine Llaurens, Charline Smadi</div>
<div class="meta-line">First: 2025-09-07T14:01:37+00:00 · Latest: 2026-02-25T18:10:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.06062v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.06062v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mutualistic interactions, where individuals from different species can benefit from each other, are widespread across ecosystems. This study develops a general deterministic model of mutualism involving two populations, assuming that mutualism may involve both costs and benefits for the interacting individuals, leading to density-dependent effects on the dynamics of the two species. This framework aims at generalizing pre-existing models, by allowing the ecological interactions to transition from mutualistic to parasitic when the respective densities of interacting species change. Through ordinary differential equations and phase portrait analysis, we derive general principles governing these systems, identifying sufficient conditions for the emergence of certain dynamic behaviors. In particular, we show that limit cycles can arise when interactions include parasitic phases but are absent in strictly mutualistic regimes. This framework provides a general approach for characterizing the population dynamics of interacting species and highlights the effect of the transitions from mutualism to parasitism due to density dependence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>互利共生背景下两种群密度依赖互作的动力学</div>
<div class="mono" style="margin-top:8px">在不同物种间个体可以相互受益的互利共生关系广泛存在于生态系统中。本研究建立了一个包含两个种群的通用确定性互利共生模型，假设互作可能同时包含成本和收益，从而导致两种群动态的密度依赖效应。该框架旨在通过允许生态互作从互利共生向寄生转变，来推广已有的模型。通过常微分方程和相图分析，我们推导出这些系统的一般规律，识别出某些动态行为出现的充分条件。特别地，我们表明当互作包含寄生阶段但不存在严格互利共生时，极限环可能产生。该框架为描述互作物种的种群动态提供了一种通用方法，并突出了由于密度依赖导致的从互利共生到寄生的转变效应。</div>
</details>
</div>
<div class="card">
<div class="title">Spilled Energy in Large Language Models</div>
<div class="meta-line">Authors: Adrian Robert Minut, Hazem Dewidar, Iacopo Masi</div>
<div class="meta-line">First: 2026-02-21T00:38:47+00:00 · Latest: 2026-02-25T18:09:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18671v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.18671v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We reinterpret the final Large Language Model (LLM) softmax classifier as an Energy-Based Model (EBM), decomposing the sequence-to-sequence probability chain into multiple interacting EBMs at inference. This principled approach allows us to track &quot;energy spills&quot; during decoding, which we empirically show correlate with factual errors, biases, and failures. Similar to Orgad et al. (2025), our method localizes the exact answer token and subsequently tests for hallucinations. Crucially, however, we achieve this without requiring trained probe classifiers or activation ablations. Instead, we introduce two completely training-free metrics derived directly from output logits: spilled energy, which captures the discrepancy between energy values across consecutive generation steps that should theoretically match, and marginalized energy, which is measurable at a single step. Evaluated on nine benchmarks across state-of-the-art LLMs (including LLaMA, Mistral, and Gemma) and on synthetic algebraic operations (Qwen3), our approach demonstrates robust, competitive hallucination detection and cross-task generalization. Notably, these results hold for both pretrained and instruction-tuned variants without introducing any training overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型中的能量溢出</div>
<div class="mono" style="margin-top:8px">我们将最终的大语言模型（LLM）softmax分类器重新诠释为能量模型（EBM），在推理过程中将序列到序列的概率链分解为多个相互作用的EBM。这种方法使我们能够追踪解码过程中的&quot;能量溢出&quot;，我们通过实验证明这些能量溢出与事实错误、偏见和失败相关。与Orgad等人（2025）类似，我们的方法定位精确的答案标记，并随后检测幻觉。然而，关键的是，我们无需训练过的探针分类器或激活消融即可实现这一点。相反，我们引入了两个完全无需训练的指标，直接从输出logits中得出：能量溢出，它捕捉了理论上应匹配的连续生成步骤之间的能量差异；以及边缘化能量，它可以在单个步骤中测量。我们在九个基准测试中评估了我们的方法，包括最先进的LLM（如LLaMA、Mistral和Gemma）以及合成代数操作（Qwen3），结果表明我们的方法在幻觉检测和跨任务泛化方面表现出色。值得注意的是，这些结果适用于预训练和指令调优的变体，而无需引入任何训练开销。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260226_0418.html">20260226_0418</a>
<a href="archive/20260225_0421.html">20260225_0421</a>
<a href="archive/20260224_0435.html">20260224_0435</a>
<a href="archive/20260223_0401.html">20260223_0401</a>
<a href="archive/20260222_0402.html">20260222_0402</a>
<a href="archive/20260221_0415.html">20260221_0415</a>
<a href="archive/20260220_0410.html">20260220_0410</a>
<a href="archive/20260219_0419.html">20260219_0419</a>
<a href="archive/20260218_0416.html">20260218_0416</a>
<a href="archive/20260217_0410.html">20260217_0410</a>
<a href="archive/20260216_0403.html">20260216_0403</a>
<a href="archive/20260215_0401.html">20260215_0401</a>
<a href="archive/20260213_0421.html">20260213_0421</a>
<a href="archive/20260212_0425.html">20260212_0425</a>
<a href="archive/20260210_0435.html">20260210_0435</a>
<a href="archive/20260209_0404.html">20260209_0404</a>
<a href="archive/20260208_0353.html">20260208_0353</a>
<a href="archive/20260207_0410.html">20260207_0410</a>
<a href="archive/20260206_0412.html">20260206_0412</a>
<a href="archive/20260205_0417.html">20260205_0417</a>
<a href="archive/20260204_0421.html">20260204_0421</a>
<a href="archive/20260202_0402.html">20260202_0402</a>
<a href="archive/20260201_0354.html">20260201_0354</a>
<a href="archive/20260131_0408.html">20260131_0408</a>
<a href="archive/20260130_0406.html">20260130_0406</a>
<a href="archive/20260129_0407.html">20260129_0407</a>
<a href="archive/20260128_0407.html">20260128_0407</a>
<a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
