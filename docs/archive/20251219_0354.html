<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-19 03:54</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251219_0354</div>
    <div class="row"><div class="card">
<div class="title">Spatia: Video Generation with Updatable Spatial Memory</div>
<div class="meta-line">Authors: Jinjing Zhao, Fangyun Wei, Zhening Liu, Hongyang Zhang, Chang Xu, Yan Lu</div>
<div class="meta-line">First: 2025-12-17T18:59:59+00:00 · Latest: 2025-12-17T18:59:59+00:00</div>
<div class="meta-line">Comments: Project page: https://zhaojingjing713.github.io/Spatia/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15716v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15716v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://zhaojingjing713.github.io/Spatia/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing video generation models struggle to maintain long-term spatial and temporal consistency due to the dense, high-dimensional nature of video signals. To overcome this limitation, we propose Spatia, a spatial memory-aware video generation framework that explicitly preserves a 3D scene point cloud as persistent spatial memory. Spatia iteratively generates video clips conditioned on this spatial memory and continuously updates it through visual SLAM. This dynamic-static disentanglement design enhances spatial consistency throughout the generation process while preserving the model&#x27;s ability to produce realistic dynamic entities. Furthermore, Spatia enables applications such as explicit camera control and 3D-aware interactive editing, providing a geometrically grounded framework for scalable, memory-driven video generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Spatia: 带有可更新空间记忆的视频生成</div>
<div class="mono" style="margin-top:8px">现有的视频生成模型由于视频信号的密集性和高维特性，难以维持长期的空间和时间一致性。为克服这一限制，我们提出了Spatia，一个具有空间记忆感知的视频生成框架，它显式地将3D场景点云作为持久空间记忆进行保留。Spatia通过视觉SLAM持续更新该空间记忆，并基于此空间记忆迭代生成视频片段。这种动态-静态解耦设计在生成过程中增强了空间一致性，同时保留了模型生成逼真动态实体的能力。此外，Spatia还支持显式的相机控制和3D感知的交互式编辑，为可扩展、基于记忆的视频生成提供了一个几何基础框架。</div>
</details>
</div>
<div class="card">
<div class="title">In Pursuit of Pixel Supervision for Visual Pre-training</div>
<div class="meta-line">Authors: Lihe Yang, Shang-Wen Li, Yang Li, Xinjie Lei, Dong Wang, Abdelrahman Mohamed, Hengshuang Zhao, Hu Xu</div>
<div class="meta-line">First: 2025-12-17T18:59:58+00:00 · Latest: 2025-12-17T18:59:58+00:00</div>
<div class="meta-line">Comments: Project page: https://github.com/facebookresearch/pixio</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15715v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15715v1">PDF</a> · <a href="https://github.com/facebookresearch/pixio">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">At the most basic level, pixels are the source of the visual information through which we perceive the world. Pixels contain information at all levels, ranging from low-level attributes to high-level concepts. Autoencoders represent a classical and long-standing paradigm for learning representations from pixels or other raw inputs. In this work, we demonstrate that autoencoder-based self-supervised learning remains competitive today and can produce strong representations for downstream tasks, while remaining simple, stable, and efficient. Our model, codenamed &quot;Pixio&quot;, is an enhanced masked autoencoder (MAE) with more challenging pre-training tasks and more capable architectures. The model is trained on 2B web-crawled images with a self-curation strategy with minimal human curation. Pixio performs competitively across a wide range of downstream tasks in the wild, including monocular depth estimation (e.g., Depth Anything), feed-forward 3D reconstruction (i.e., MapAnything), semantic segmentation, and robot learning, outperforming or matching DINOv3 trained at similar scales. Our results suggest that pixel-space self-supervised learning can serve as a promising alternative and a complement to latent-space approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>追寻像素监督的视觉预训练</div>
<div class="mono" style="margin-top:8px">在最基本的层面上，像素是我们感知世界所依赖的视觉信息的来源。像素包含从低级属性到高级概念的多层次信息。自编码器是一种经典且长期存在的范式，用于从像素或其他原始输入中学习表示。在本工作中，我们证明了基于自编码器的自监督学习在当今仍然具有竞争力，能够为下游任务生成强大的表示，同时保持简单、稳定和高效。我们的模型代号为 &quot;Pixio&quot;，是一个改进的掩码自编码器（MAE），具有更具挑战性的预训练任务和更强大的架构。该模型通过自整理策略，在约20亿个网络爬取图像上进行训练，仅需极少的人工整理。Pixio在各种实际下游任务中表现优异，包括单目深度估计（如Depth Anything）、前馈3D重建（即MapAnything）、语义分割和机器人学习，其性能优于或至少与在相似规模上训练的DINOv3相当。我们的结果表明，像素空间的自监督学习可以作为潜在空间方法的一种有前景的替代方案和补充。</div>
</details>
</div>
<div class="card">
<div class="title">DiffusionVL: Translating Any Autoregressive Models into Diffusion Vision Language Models</div>
<div class="meta-line">Authors: Lunbin Zeng, Jingfeng Yao, Bencheng Liao, Hongyuan Tao, Wenyu Liu, Xinggang Wang</div>
<div class="meta-line">First: 2025-12-17T18:59:55+00:00 · Latest: 2025-12-17T18:59:55+00:00</div>
<div class="meta-line">Comments: 11 pages, 5 figures, conference or other essential info</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15713v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15713v1">PDF</a> · <a href="https://github.com/hustvl/DiffusionVL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent multimodal research, the diffusion paradigm has emerged as a promising alternative to the autoregressive paradigm (AR), owing to its unique decoding advantages. However, due to the capability limitations of the base diffusion language model, the performance of the diffusion vision language model (dVLM) still lags significantly behind that of mainstream models. This leads to a simple yet fundamental question: Is it possible to construct dVLMs based on existing powerful AR models? In response, we propose DiffusionVL, a dVLM family that could be translated from any powerful AR models. Through simple fine-tuning, we successfully adapt AR pre-trained models into the diffusion paradigm. This approach yields two key observations: (1) The paradigm shift from AR-based multimodal models to diffusion is remarkably effective. (2) Direct conversion of an AR language model to a dVLM is also feasible, achieving performance competitive with LLaVA-style visual-instruction-tuning. Further, we introduce a block-decoding design into dVLMs that supports arbitrary-length generation and KV cache reuse, achieving a significant inference speedup. We conduct a large number of experiments. Despite training with less than 5% of the data required by prior methods, DiffusionVL achieves a comprehensive performance improvement-a 34.4% gain on the MMMU-Pro (vision) bench and 37.5% gain on the MME (Cog.) bench-alongside a 2x inference speedup. The model and code are released at https://github.com/hustvl/DiffusionVL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiffusionVL：将任意自回归模型转换为扩散视觉语言模型</div>
<div class="mono" style="margin-top:8px">在近期的多模态研究中，扩散范式因其独特的解码优势，成为自回归范式（AR）的有前景替代方案。然而，由于基础扩散语言模型的能力限制，扩散视觉语言模型（dVLM）的性能仍显著落后于主流模型。这引发了一个简单但根本的问题：是否可以基于现有的强大自回归模型构建dVLM？为此，我们提出了DiffusionVL，这是一个可以从任意强大自回归模型转换而来的dVLM家族。通过简单的微调，我们成功地将自回归预训练模型适配到扩散范式中。这种方法带来了两个关键观察：(1) 从基于自回归的多模态模型转向扩散范式具有显著效果；(2) 将自回归语言模型直接转换为dVLM也是可行的，其性能可与LLaVA风格的视觉指令微调模型相媲美。此外，我们还在dVLM中引入了块解码设计，支持任意长度的生成和KV缓存重用，从而实现了显著的推理加速。我们进行了大量实验。尽管仅使用了先前方法所需数据的不到5%，DiffusionVL在MMMU-Pro（视觉）基准上实现了34.4%的性能提升，在MME（Cog.）基准上实现了37.5%的性能提升，同时推理速度提升了2倍。模型和代码已发布在https://github.com/hustvl/DiffusionVL。</div>
</details>
</div>
<div class="card">
<div class="title">Predictive Concept Decoders: Training Scalable End-to-End Interpretability Assistants</div>
<div class="meta-line">Authors: Vincent Huang, Dami Choi, Daniel D. Johnson, Sarah Schwettmann, Jacob Steinhardt</div>
<div class="meta-line">First: 2025-12-17T18:59:48+00:00 · Latest: 2025-12-17T18:59:48+00:00</div>
<div class="meta-line">Comments: 28 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15712v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15712v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Interpreting the internal activations of neural networks can produce more faithful explanations of their behavior, but is difficult due to the complex structure of activation space. Existing approaches to scalable interpretability use hand-designed agents that make and test hypotheses about how internal activations relate to external behavior. We propose to instead turn this task into an end-to-end training objective, by training interpretability assistants to accurately predict model behavior from activations through a communication bottleneck. Specifically, an encoder compresses activations to a sparse list of concepts, and a decoder reads this list and answers a natural language question about the model. We show how to pretrain this assistant on large unstructured data, then finetune it to answer questions. The resulting architecture, which we call a Predictive Concept Decoder, enjoys favorable scaling properties: the auto-interp score of the bottleneck concepts improves with data, as does the performance on downstream applications. Specifically, PCDs can detect jailbreaks, secret hints, and implanted latent concepts, and are able to accurately surface latent user attributes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>预测性概念解码器：训练可扩展的端到端可解释性助手</div>
<div class="mono" style="margin-top:8px">解释神经网络的内部激活可以产生更忠实的行为解释，但由于激活空间的复杂结构，这一任务具有挑战性。现有的可扩展可解释性方法使用人工设计的代理来提出并测试内部激活与外部行为之间的假设。我们提出将这一任务转化为端到端的训练目标，通过训练可解释性助手，在通信瓶颈下从激活中准确预测模型行为。具体而言，一个编码器将激活压缩为稀疏的概念列表，一个解码器读取该列表并回答关于模型的自然语言问题。我们展示了如何在大规模非结构化数据上预训练这种助手，然后微调其回答问题。所得到的架构，我们称之为预测性概念解码器（PCD），具有良好的可扩展性：瓶颈概念的自动解释得分随着数据量的增加而提高，下游应用的性能也随之提升。具体来说，PCD 可以检测到 jailbreak、秘密提示和植入的潜在概念，并能够准确揭示潜在的用户属性。</div>
</details>
</div>
<div class="card">
<div class="title">Gaussian Pixel Codec Avatars: A Hybrid Representation for Efficient Rendering</div>
<div class="meta-line">Authors: Divam Gupta, Anuj Pahuja, Nemanja Bartolovic, Tomas Simon, Forrest Iandola, Giljoo Nam</div>
<div class="meta-line">First: 2025-12-17T18:58:50+00:00 · Latest: 2025-12-17T18:58:50+00:00</div>
<div class="meta-line">Comments: Tech report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15711v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15711v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Gaussian Pixel Codec Avatars (GPiCA), photorealistic head avatars that can be generated from multi-view images and efficiently rendered on mobile devices. GPiCA utilizes a unique hybrid representation that combines a triangle mesh and anisotropic 3D Gaussians. This combination maximizes memory and rendering efficiency while maintaining a photorealistic appearance. The triangle mesh is highly efficient in representing surface areas like facial skin, while the 3D Gaussians effectively handle non-surface areas such as hair and beard. To this end, we develop a unified differentiable rendering pipeline that treats the mesh as a semi-transparent layer within the volumetric rendering paradigm of 3D Gaussian Splatting. We train neural networks to decode a facial expression code into three components: a 3D face mesh, an RGBA texture, and a set of 3D Gaussians. These components are rendered simultaneously in a unified rendering engine. The networks are trained using multi-view image supervision. Our results demonstrate that GPiCA achieves the realism of purely Gaussian-based avatars while matching the rendering performance of mesh-based avatars.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高斯像素编码头像：一种用于高效渲染的混合表示方法</div>
<div class="mono" style="margin-top:8px">我们提出了高斯像素编码头像（GPiCA），这是一种可以从多视角图像生成并能在移动设备上高效渲染的逼真头部头像。GPiCA采用了一种独特的混合表示方法，结合了三角网格和各向异性3D高斯分布。这种结合在保持逼真外观的同时，最大化了内存和渲染效率。三角网格在表示面部皮肤等表面区域方面非常高效，而3D高斯分布则能有效处理非表面区域，如头发和胡须。为此，我们开发了一个统一的可微渲染流程，将网格视为体积渲染范式中3D高斯点云的半透明层。我们训练神经网络将面部表情代码解码为三个组成部分：3D面部网格、RGBA纹理和一组3D高斯分布。这些组件在一个统一的渲染引擎中同时进行渲染。网络通过多视角图像监督进行训练。我们的实验结果表明，GPiCA在保持纯高斯头像真实感的同时，实现了与网格头像相当的渲染性能。</div>
</details>
</div>
<div class="card">
<div class="title">Artism: AI-Driven Dual-Engine System for Art Generation and Critique</div>
<div class="meta-line">Authors: Shuai Liu, Yiqing Tian, Yang Chen, Mar Canet Sola</div>
<div class="meta-line">First: 2025-12-17T18:58:42+00:00 · Latest: 2025-12-17T18:58:42+00:00</div>
<div class="meta-line">Comments: 7 pages, 3 figures, 36 references, appendix with support material and 1 introduction video</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15710v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15710v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes a dual-engine AI architectural method designed to address the complex problem of exploring potential trajectories in the evolution of art. We present two interconnected components: AIDA (an artificial artist social network) and the Ismism Machine, a system for critical analysis. The core innovation lies in leveraging deep learning and multi-agent collaboration to enable multidimensional simulations of art historical developments and conceptual innovation patterns. The framework explores a shift from traditional unidirectional critique toward an intelligent, interactive mode of reflexive practice. We are currently applying this method in experimental studies on contemporary art concepts. This study introduces a general methodology based on AI-driven critical loops, offering new possibilities for computational analysis of art.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Artism：一种用于艺术生成与评价的AI双引擎系统</div>
<div class="mono" style="margin-top:8px">本文提出了一种双引擎AI架构方法，旨在解决探索艺术演变潜在轨迹这一复杂问题。我们介绍了两个相互关联的组件：AIDA（人工艺术家社交网络）和Ismism机器，后者是一个用于批判性分析的系统。核心创新在于利用深度学习和多智能体协作，实现对艺术历史发展和概念创新模式的多维模拟。该框架探索了从传统单向评价向智能、互动的反思性实践模式的转变。目前，我们正在将此方法应用于当代艺术概念的实验研究。本研究引入了一种基于AI驱动批判循环的一般方法论，为艺术的计算分析提供了新的可能性。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-View Foundation Models</div>
<div class="meta-line">Authors: Leo Segre, Or Hirschorn, Shai Avidan</div>
<div class="meta-line">First: 2025-12-17T18:58:03+00:00 · Latest: 2025-12-17T18:58:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15708v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15708v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Foundation models are vital tools in various Computer Vision applications. They take as input a single RGB image and output a deep feature representation that is useful for various applications. However, in case we have multiple views of the same 3D scene, they operate on each image independently and do not always produce consistent features for the same 3D point. We propose a way to convert a Foundation Model into a Multi-View Foundation Model. Such a model takes as input a set of images and outputs a feature map for each image such that the features of corresponding points are as consistent as possible. This approach bypasses the need to build a consistent 3D model of the features and allows direct manipulation in the image space. Specifically, we show how to augment Transformers-based foundation models (i.e., DINO, SAM, CLIP) with intermediate 3D-aware attention layers that help match features across different views. As leading examples, we show surface normal estimation and multi-view segmentation tasks. Quantitative experiments show that our method improves feature matching considerably compared to current foundation models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多视角基础模型</div>
<div class="mono" style="margin-top:8px">基础模型是各种计算机视觉应用中的重要工具。它们以单个RGB图像作为输入，并输出可用于多种应用的深度特征表示。然而，当存在同一3D场景的多个视角时，它们对每个图像独立处理，且对同一3D点的特征表示往往不一致。我们提出了一种将基础模型转换为多视角基础模型的方法。此类模型以一组图像作为输入，并为每张图像输出一个特征图，使得对应点的特征尽可能一致。这种方法绕过了构建一致的3D特征模型的需求，允许直接在图像空间中进行操作。具体而言，我们展示了如何通过添加中间的3D感知注意力层来增强基于Transformer的基础模型（如DINO、SAM、CLIP），以帮助在不同视角之间匹配特征。作为主要示例，我们展示了表面法线估计和多视角分割任务。定量实验表明，与当前基础模型相比，我们的方法在特征匹配方面有显著提升。</div>
</details>
</div>
<div class="card">
<div class="title">GateFusion: Hierarchical Gated Cross-Modal Fusion for Active Speaker Detection</div>
<div class="meta-line">Authors: Yu Wang, Juhyung Ha, Frangil M. Ramirez, Yuchen Wang, David J. Crandall</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2025-12-17T18:56:52+00:00 · Latest: 2025-12-17T18:56:52+00:00</div>
<div class="meta-line">Comments: accepted by WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15707v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15707v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Active Speaker Detection (ASD) aims to identify who is currently speaking in each frame of a video. Most state-of-the-art approaches rely on late fusion to combine visual and audio features, but late fusion often fails to capture fine-grained cross-modal interactions, which can be critical for robust performance in unconstrained scenarios. In this paper, we introduce GateFusion, a novel architecture that combines strong pretrained unimodal encoders with a Hierarchical Gated Fusion Decoder (HiGate). HiGate enables progressive, multi-depth fusion by adaptively injecting contextual features from one modality into the other at multiple layers of the Transformer backbone, guided by learnable, bimodally-conditioned gates. To further strengthen multimodal learning, we propose two auxiliary objectives: Masked Alignment Loss (MAL) to align unimodal outputs with multimodal predictions, and Over-Positive Penalty (OPP) to suppress spurious video-only activations. GateFusion establishes new state-of-the-art results on several challenging ASD benchmarks, achieving 77.8% mAP (+9.4%), 86.1% mAP (+2.9%), and 96.1% mAP (+0.5%) on Ego4D-ASD, UniTalk, and WASD benchmarks, respectively, and delivering competitive performance on AVA-ActiveSpeaker. Out-of-domain experiments demonstrate the generalization of our model, while comprehensive ablations show the complementary benefits of each component.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GateFusion: 用于主动说话人检测的分层门控跨模态融合</div>
<div class="mono" style="margin-top:8px">主动说话人检测（ASD）旨在识别视频每一帧中当前正在说话的人。大多数最先进的方法依赖于晚期融合来结合视觉和音频特征，但晚期融合通常无法捕捉细粒度的跨模态交互，这对于在无约束场景中实现鲁棒性能至关重要。本文提出了一种新的架构GateFusion，它结合了强大的预训练单模态编码器与分层门控融合解码器（HiGate）。HiGate通过在Transformer主干网络的多个层中，利用可学习的双模态条件门控，自适应地将一种模态的上下文特征注入到另一种模态中，从而实现渐进式的多深度融合。为了进一步增强多模态学习，我们提出了两个辅助目标：掩码对齐损失（MAL）用于对齐单模态输出与多模态预测，以及过积极惩罚（OPP）用于抑制虚假的视频单模态激活。GateFusion在多个具有挑战性的ASD基准测试中建立了新的最先进结果，在Ego4D-ASD、UniTalk和WASD基准测试中分别达到了77.8% mAP（+9.4%）、86.1% mAP（+2.9%）和96.1% mAP（+0.5%），并在AVA-ActiveSpeaker上表现出竞争力。领域外实验展示了模型的泛化能力，而全面的消融实验则展示了各个组件的互补优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Active Speaker Detection (ASD) aims to identify who is currently speaking in each frame of a video.</div>
</details>
</div>
<div class="card">
<div class="title">Learning Model Parameter Dynamics in a Combination Therapy for Bladder Cancer from Sparse Biological Data</div>
<div class="meta-line">Authors: Kayode Olumoyin, Lamees El Naqa, Katarzyna Rejniak</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-12-17T18:55:49+00:00 · Latest: 2025-12-17T18:55:49+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 Workshop on Learning from Time Series for Health</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15706v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15706v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In a mathematical model of interacting biological organisms, where external interventions may alter behavior over time, traditional models that assume fixed parameters usually do not capture the evolving dynamics. In oncology, this is further exacerbated by the fact that experimental data are often sparse and sometimes are composed of a few time points of tumor volume. In this paper, we propose to learn time-varying interactions between cells, such as those of bladder cancer tumors and immune cells, and their response to a combination of anticancer treatments in a limited data scenario. We employ the physics-informed neural network (PINN) approach to predict possible subpopulation trajectories at time points where no observed data are available. We demonstrate that our approach is consistent with the biological explanation of subpopulation trajectories. Our method provides a framework for learning evolving interactions among biological organisms when external interventions are applied to their environment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从稀疏生物数据中学习膀胱癌联合治疗模型参数动态</div>
<div class="mono" style="margin-top:8px">在描述相互作用的生物体的数学模型中，外部干预可能会随时间改变行为，传统假设参数固定的模型通常无法捕捉动态变化。在肿瘤学中，这一问题更加严重，因为实验数据往往稀疏，有时仅包含肿瘤体积的几个时间点。本文提出在数据有限的情况下，学习细胞间随时间变化的相互作用，例如膀胱癌肿瘤与免疫细胞之间的相互作用及其对联合抗肿瘤治疗的反应。我们采用物理信息神经网络（PINN）方法，预测在无观测数据的时间点上可能的亚群轨迹。我们证明了我们的方法与亚群轨迹的生物学解释是一致的。我们的方法为在外部干预其环境的情况下学习生物体之间动态相互作用提供了框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In a mathematical model of interacting biological organisms, where external interventions may alter behavior over time, traditional models that assume fixed parameters usually do not capture the evolving dynamics.</div>
</details>
</div>
<div class="card">
<div class="title">Dynamic Rebatching for Efficient Early-Exit Inference with DREX</div>
<div class="meta-line">Authors: Xuting Liu, Daniel Alexander, Siva Kesava Reddy Kakarla, Behnaz Arzani, Vincent Liu</div>
<div class="meta-line">First: 2025-12-17T18:55:45+00:00 · Latest: 2025-12-17T18:55:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15705v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15705v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model&#x27;s layers. However, traditional batching frameworks are ill-suited for EE LLMs, as not all requests in a batch may be ready to exit at the same time. Existing solutions either force a uniform decision on the batch, which overlooks EE opportunities, or degrade output quality by forcing premature exits. We propose Dynamic Rebatching, a solution where we dynamically reorganize the batch at each early-exit point. Requests that meet the exit criteria are immediately processed, while those that continue are held in a buffer, re-grouped into a new batch, and forwarded to deeper layers. We introduce DREX, an early-exit inference system that implements Dynamic Rebatching with two key optimizations: 1) a copy-free rebatching buffer that avoids physical data movement, and 2) an EE and SLA-aware scheduler that analytically predicts whether a given rebatching operation will be profitable. DREX also efficiently handles the missing KV cache from skipped layers using memory-efficient state-copying. Our evaluation shows that DREX improves throughput by 2-12% compared to baseline approaches while maintaining output quality. Crucially, DREX completely eliminates involuntary exits, providing a key guarantee for preserving the output quality intended by the EE model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DREX：用于高效提前退出推理的动态重批处理</div>
<div class="mono" style="margin-top:8px">提前退出（EE）是一种大型语言模型（LLM）架构，通过仅使用模型部分层生成较简单的token来加速推理。然而，传统的批处理框架并不适合EE LLM，因为一个批次中的请求可能不会在同一时间准备好退出。现有解决方案要么强制对整个批次做出统一决策，忽略了EE的机会，要么通过强制提前退出而降低输出质量。我们提出动态重批处理，该方案在每次提前退出点动态重组批次。满足退出条件的请求立即被处理，而继续处理的请求则被暂存到缓冲区，重新分组为新批次并转发到更深层的模型中。我们引入了DREX，一个实现动态重批处理的提前退出推理系统，其包含两个关键优化：1）无复制的重批处理缓冲区，避免物理数据移动；2）一种同时考虑EE和SLA的调度器，能够分析预测特定的重批处理操作是否有利可图。DREX还通过内存高效的态复制机制，高效处理跳过层缺失的KV缓存。我们的评估表明，与基线方法相比，DREX在保持输出质量的同时，吞吐量提高了2-12%。关键的是，DREX完全消除了非自愿退出，为保持EE模型预期的输出质量提供了关键保证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Early-Exit (EE) is a Large Language Model (LLM) architecture that accelerates inference by allowing easier tokens to be generated using only a subset of the model&#x27;s layers.</div>
</details>
</div>
<div class="card">
<div class="title">Time integration of quantized tensor trains using the interpolative dynamical low-rank approximation</div>
<div class="meta-line">Authors: Erika Ye, Chao Yang</div>
<div class="meta-line">First: 2025-12-17T18:55:25+00:00 · Latest: 2025-12-17T18:55:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15703v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15703v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quantized tensor trains (QTTs) are a low-rank and multiscale framework that allows for efficient approximation and manipulation of multi-dimensional, high resolution data. One area of active research is their use in numerical simulation of hyperbolic systems such as the Navier-Stokes equations and the Vlasov equations. One popular time integration scheme is the dynamical low-rank approximation (DLRA), in which the time integration is constrained to a low-rank manifold. However, until recently, DLRA has typically used orthogonal projectors to project the original dynamical system into a reduced space, which is only well-suited for linear systems. DLRA has also mostly been investigated in the context of non-quantized tensor trains. This work investigates interpolative DLRA schemes in which the low-rank manifold is constructed from aptly chosen interpolation points and interpolating polynomials, in the context of QTTs. Through various examples, its performance is compared to its orthogonal counterpart. This work demonstrates how interpolative DLRA is suitable for nonlinear systems and time integrators requiring nonlinear element-wise operations, such as upwind time integration schemes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用插值动态低秩近似对量化张量列车进行时间积分</div>
<div class="mono" style="margin-top:8px">量化张量列车（QTTs）是一种低秩且多尺度框架，能够高效地近似和处理多维高分辨率数据。目前，其在超流系统（如纳维-斯托克斯方程和弗拉索夫方程）的数值模拟中是一个活跃的研究领域。一种流行的时序积分方案是动态低秩近似（DLRA），其中时间积分被限制在低秩流形上。然而，直到最近，DLRA通常使用正交投影器将原始动力系统投影到降维空间，这仅适用于线性系统。DLRA也主要在非量化张量列车的背景下进行研究。本文研究了在QTTs框架下基于插值的DLRA方案，其中低秩流形由适当选择的插值点和插值多项式构建。通过多个实例，将其性能与正交DLRA进行比较。本文展示了插值DLRA如何适用于非线性系统以及需要非线性逐元素运算的时间积分方案，如迎风时间积分方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Quantized tensor trains (QTTs) are a low-rank and multiscale framework that allows for efficient approximation and manipulation of multi-dimensional, high resolution data.</div>
</details>
</div>
<div class="card">
<div class="title">End-to-End Training for Autoregressive Video Diffusion via Self-Resampling</div>
<div class="meta-line">Authors: Yuwei Guo, Ceyuan Yang, Hao He, Yang Zhao, Meng Wei, Zhenheng Yang, Weilin Huang, Dahua Lin</div>
<div class="meta-line">First: 2025-12-17T18:53:29+00:00 · Latest: 2025-12-17T18:53:29+00:00</div>
<div class="meta-line">Comments: Project Page: https://guoyww.github.io/projects/resampling-forcing/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15702v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15702v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://guoyww.github.io/projects/resampling-forcing/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive video diffusion models hold promise for world simulation but are vulnerable to exposure bias arising from the train-test mismatch. While recent works address this via post-training, they typically rely on a bidirectional teacher model or online discriminator. To achieve an end-to-end solution, we introduce Resampling Forcing, a teacher-free framework that enables training autoregressive video models from scratch and at scale. Central to our approach is a self-resampling scheme that simulates inference-time model errors on history frames during training. Conditioned on these degraded histories, a sparse causal mask enforces temporal causality while enabling parallel training with frame-level diffusion loss. To facilitate efficient long-horizon generation, we further introduce history routing, a parameter-free mechanism that dynamically retrieves the top-k most relevant history frames for each query. Experiments demonstrate that our approach achieves performance comparable to distillation-based baselines while exhibiting superior temporal consistency on longer videos owing to native-length training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过自重采样实现自回归视频扩散的端到端训练</div>
<div class="mono" style="margin-top:8px">自回归视频扩散模型在世界模拟方面具有前景，但容易受到训练与测试不匹配导致的暴露偏差的影响。尽管近期的研究通过后训练方法解决了这一问题，但通常依赖于双向教师模型或在线判别器。为了实现端到端的解决方案，我们引入了无教师框架Resampling Forcing，使得可以从头开始大规模训练自回归视频模型。我们的方法核心是一种自重采样方案，它在训练过程中模拟推理时模型在历史帧上的误差。基于这些退化的历史帧，稀疏因果掩码强制执行时间因果性，同时允许以帧级扩散损失进行并行训练。为了促进高效长时序生成，我们进一步引入了无参数机制History Routing，该机制动态地为每个查询检索最相关的top-k历史帧。实验表明，我们的方法在性能上与基于蒸馏的基线相当，同时在更长的视频中表现出更优的时间一致性，这是由于原生长度的训练所带来的优势。</div>
</details>
</div>
<div class="card">
<div class="title">VLIC: Vision-Language Models As Perceptual Judges for Human-Aligned Image Compression</div>
<div class="meta-line">Authors: Kyle Sargent, Ruiqi Gao, Philipp Henzler, Charles Herrmann, Aleksander Holynski, Li Fei-Fei, Jiajun Wu, Jason Zhang</div>
<div class="meta-line">First: 2025-12-17T18:52:55+00:00 · Latest: 2025-12-17T18:52:55+00:00</div>
<div class="meta-line">Comments: 14 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15701v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15701v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://kylesargent.github.io/vlic">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluations of image compression performance which include human preferences have generally found that naive distortion functions such as MSE are insufficiently aligned to human perception. In order to align compression models to human perception, prior work has employed differentiable perceptual losses consisting of neural networks calibrated on large-scale datasets of human psycho-visual judgments. We show that, surprisingly, state-of-the-art vision-language models (VLMs) can replicate binary human two-alternative forced choice (2AFC) judgments zero-shot when asked to reason about the differences between pairs of images. Motivated to exploit the powerful zero-shot visual reasoning capabilities of VLMs, we propose Vision-Language Models for Image Compression (VLIC), a diffusion-based image compression system designed to be post-trained with binary VLM judgments. VLIC leverages existing techniques for diffusion model post-training with preferences, rather than distilling the VLM judgments into a separate perceptual loss network. We show that calibrating this system on VLM judgments produces competitive or state-of-the-art performance on human-aligned visual compression depending on the dataset, according to perceptual metrics and large-scale user studies. We additionally conduct an extensive analysis of the VLM-based reward design and training procedure and share important insights. More visuals are available at https://kylesargent.github.io/vlic</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLIC: 以视觉-语言模型作为对齐人类感知的图像压缩感知器</div>
<div class="mono" style="margin-top:8px">包含人类偏好的图像压缩性能评估通常发现，简单的失真函数如均方误差（MSE）不足以对齐人类感知。为了使压缩模型对齐人类感知，先前的工作采用了基于大规模人类心理视觉判断数据集校准的可微感知损失函数。我们发现，令人惊讶的是，最先进的视觉-语言模型（VLMs）在被要求推理两幅图像之间的差异时，可以零样本复现二元人类两两选择（2AFC）判断。受此启发，我们提出了基于视觉-语言模型的图像压缩方法（VLIC），这是一种基于扩散模型的图像压缩系统，设计为可以使用二元VLM判断进行微调。VLIC利用了现有的基于偏好进行扩散模型微调的技术，而不是将VLM判断蒸馏到独立的感知损失网络中。我们展示了在VLM判断上校准该系统，可以在不同数据集上根据感知指标和大规模用户研究，实现与人类对齐的视觉压缩的竞争力或最先进的性能。我们还对基于VLM的奖励设计和训练过程进行了广泛分析，并分享了重要的见解。更多视觉内容请访问 https://kylesargent.github.io/vlic</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Evaluations of image compression performance which include human preferences have generally found that naive distortion functions such as MSE are insufficiently aligned to human perception.</div>
</details>
</div>
<div class="card">
<div class="title">FrontierCS: Evolving Challenges for Evolving Intelligence</div>
<div class="meta-line">Authors: Qiuyang Mang, Wenhao Chai, Zhifei Li, Huanzhi Mao, Shang Zhou, Alexander Du, Hanchen Li, Shu Liu, Edwin Chen, Yichuan Wang, Xieting Chu, Zerui Cheng, Yuan Xu, Tian Xia, Zirui Wang, Tianneng Shi, Jianzhu Yao, Yilong Zhao, Qizheng Zhang, Charlie Ruan, Zeyu Shen, Kaiyuan Liu, Runyuan He, Dong Xing, Zerui Li, Zirong Zeng, Yige Jiang, Lufeng Cheng, Ziyi Zhao, Youran Sun, Wesley Zheng, Meiyuwang Zhang, Ruyi Ji, Xuechang Tu, Zihan Zheng, Zexing Chen, Kangyang Zhou, Zhaozi Wang, Jingbang Chen, Aleksandra Korolova, Peter Henderson, Pramod Viswanath, Vijay Ganesh, Saining Xie, Zhuang Liu, Dawn Song, Sewon Min, Ion Stoica, Joseph E. Gonzalez, Jingbo Shang, Alvin Cheung</div>
<div class="meta-line">First: 2025-12-17T18:52:45+00:00 · Latest: 2025-12-17T18:52:45+00:00</div>
<div class="meta-line">Comments: Code with instruction: https://github.com/FrontierCS/Frontier-CS</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15699v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15699v1">PDF</a> · <a href="https://github.com/FrontierCS/Frontier-CS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce FrontierCS, a benchmark of 156 open-ended problems across diverse areas of computer science, designed and reviewed by experts, including CS PhDs and top-tier competitive programming participants and problem setters. Unlike existing benchmarks that focus on tasks with known optimal solutions, FrontierCS targets problems where the optimal solution is unknown, but the quality of a solution can be objectively evaluated. Models solve these tasks by implementing executable programs rather than outputting a direct answer. FrontierCS includes algorithmic problems, which are often NP-hard variants of competitive programming problems with objective partial scoring, and research problems with the same property. For each problem we provide an expert reference solution and an automatic evaluator. Combining open-ended design, measurable progress, and expert curation, FrontierCS provides a benchmark at the frontier of computer-science difficulty. Empirically, we find that frontier reasoning models still lag far behind human experts on both the algorithmic and research tracks, that increasing reasoning budgets alone does not close this gap, and that models often over-optimize for generating merely workable code instead of discovering high-quality algorithms and system designs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FrontierCS：面向演进智能的挑战性问题集</div>
<div class="mono" style="margin-top:8px">我们介绍了FrontierCS，这是一个包含156个开放性问题的计算机科学基准测试集，由专家设计和评审，包括计算机科学博士和顶级编程竞赛的参赛者与出题人。与现有专注于已知最优解任务的基准测试不同，FrontierCS关注的是最优解未知但可以客观评估解决方案质量的问题。模型通过实现可执行程序来解决这些问题，而不是直接输出答案。FrontierCS包含算法问题，这些问题通常是编程竞赛中NP难变种问题，具有客观的部分评分机制，同时也包含具有相同特性的研究问题。对于每个问题，我们提供了专家参考解决方案和自动评估器。结合开放性设计、可衡量的进展和专家整理，FrontierCS提供了一个处于计算机科学难度前沿的基准测试。实证研究表明，前沿推理模型在算法和研究两个方向上仍远逊于人类专家，仅增加推理预算无法弥合这一差距，且模型往往过度优化以生成仅能运行的代码，而非发现高质量的算法和系统设计。</div>
</details>
</div>
<div class="card">
<div class="title">An introduction to nonlinear fiber optics and optical analogues to gravitational phenomena</div>
<div class="meta-line">Authors: Dimitrios Kranas, Andleeb Zahra, Friedrich König</div>
<div class="meta-line">First: 2025-12-17T18:50:47+00:00 · Latest: 2025-12-17T18:50:47+00:00</div>
<div class="meta-line">Comments: Lecture notes based on the course given by FK at &quot;Analogue Gravity 2023&quot;, Benasque. 34 pages, 17 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15695v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15695v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The optical fiber is a revolutionary technology of the past century. It enables us to manipulate single modes in nonlinear interactions with precision at the quantum level without involved setups. This setting is useful in the field of analogue gravity (AG), where gravitational phenomena are investigated in accessible analogue lab setups. These lecture notes provide an account of this AG framework and applications. Although light in nonlinear dielectrics is discussed in textbooks, the involved modelling often includes many assumptions that are directed at optical communications, some of which are rarely detailed. Here, we provide a self-contained and sufficiently detailed description of the propagation of light in fibers, with a minimal set of assumptions, which is relevant in the context of AG. Starting with the structure of a step-index fiber, we derive linear-optics propagating modes and show that the transverse electric field of the fundamental mode is well approximated as linearly polarized and of a Gaussian profile. We then incorporate a cubic nonlinearity and derive a general wave envelope propagation equation. With further simplifying assumptions, we arrive at the famous nonlinear Schrödinger equation, which governs fundamental effects in nonlinear fibers, such as solitons. As a first application in AG, we show how intense light in the medium creates an effective background spacetime for probe light akin to the propagation of a scalar field in a black hole spacetime. We introduce optical horizons and particle production in this effective spacetime, giving rise to the optical Hawking effect. Furthermore, we discuss two related light emission mechanisms. Finally, we present a second optical analogue model for the oscillations of black holes, the quasinormal modes, which are important in the program of black hole spectroscopy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非线性光纤与引力现象光学类比导论</div>
<div class="mono" style="margin-top:8px">光纤是一种过去一个世纪的革命性技术。它使我们能够在量子层面精确操控单模在非线性相互作用中的行为，而无需复杂的实验装置。这种设置在类比引力（AG）领域非常有用，该领域通过可访问的类比实验室装置研究引力现象。这些讲义介绍了这一AG框架及其应用。尽管非线性介质中的光在教科书中有所讨论，但涉及的建模通常包含许多假设，这些假设主要针对光学通信，其中一些很少被详细说明。在这里，我们提供了一个自洽且足够详细的描述，说明光在光纤中的传播，仅包含最少的假设，这在类比引力的背景下是相关的。我们从阶跃折射率光纤的结构开始，推导出线性光学传播模式，并展示基模的横向电场可以很好地近似为线性极化且具有高斯分布的场。随后，我们引入三次非线性并推导出一般的波包传播方程。通过进一步简化假设，我们得到著名的非线性薛定谔方程，该方程支配非线性光纤中的基本效应，如孤子。作为类比引力的第一个应用，我们展示了在介质中强光如何产生一个有效的背景时空，使探测光的传播类似于标量场在黑洞时空中的传播。我们引入了光学视界和粒子产生，从而产生了光学霍金效应。此外，我们还讨论了两种相关的光发射机制。最后，我们提出了一种第二种光学类比模型，用于描述黑洞的振荡，即准正常模式，这对于黑洞光谱学项目非常重要。</div>
</details>
</div>
<div class="card">
<div class="title">Skyra: AI-Generated Video Detection via Grounded Artifact Reasoning</div>
<div class="meta-line">Authors: Yifei Li, Wenzhao Zheng, Yanran Zhang, Runze Sun, Yu Zheng, Lei Chen, Jie Zhou, Jiwen Lu</div>
<div class="meta-line">First: 2025-12-17T18:48:26+00:00 · Latest: 2025-12-17T18:48:26+00:00</div>
<div class="meta-line">Comments: Project Page: https://github.com/JoeLeelyf/Skyra</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15693v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15693v1">PDF</a> · <a href="https://github.com/JoeLeelyf/Skyra">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The misuse of AI-driven video generation technologies has raised serious social concerns, highlighting the urgent need for reliable AI-generated video detectors. However, most existing methods are limited to binary classification and lack the necessary explanations for human interpretation. In this paper, we present Skyra, a specialized multimodal large language model (MLLM) that identifies human-perceivable visual artifacts in AI-generated videos and leverages them as grounded evidence for both detection and explanation. To support this objective, we construct ViF-CoT-4K for Supervised Fine-Tuning (SFT), which represents the first large-scale AI-generated video artifact dataset with fine-grained human annotations. We then develop a two-stage training strategy that systematically enhances our model&#x27;s spatio-temporal artifact perception, explanation capability, and detection accuracy. To comprehensively evaluate Skyra, we introduce ViF-Bench, a benchmark comprising 3K high-quality samples generated by over ten state-of-the-art video generators. Extensive experiments demonstrate that Skyra surpasses existing methods across multiple benchmarks, while our evaluation yields valuable insights for advancing explainable AI-generated video detection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Skyra：基于基础性艺术瑕疵推理的AI生成视频检测</div>
<div class="mono" style="margin-top:8px">AI驱动的视频生成技术的滥用引发了严重的社会关注，凸显了对可靠AI生成视频检测器的迫切需求。然而，大多数现有方法仅限于二分类任务，缺乏对人类理解必要的解释。本文提出Skyra，一种专门的多模态大语言模型（MLLM），用于识别AI生成视频中人类可感知的视觉瑕疵，并利用这些瑕疵作为检测和解释的基础证据。为支持这一目标，我们构建了ViF-CoT-4K用于监督微调（SFT），这是首个包含细粒度人类标注的大规模AI生成视频瑕疵数据集。随后，我们开发了一种两阶段训练策略，系统性地提升模型在时空艺术瑕疵感知、解释能力和检测精度方面的表现。为了全面评估Skyra，我们引入了ViF-Bench，这是一个包含由超过十种最先进的视频生成器生成的3000个高质量样本的基准数据集。大量实验表明，Skyra在多个基准测试中超越了现有方法，而我们的评估结果也为推动可解释的AI生成视频检测研究提供了有价值的见解。</div>
</details>
</div>
<div class="card">
<div class="title">mimic-video: Video-Action Models for Generalizable Robot Control Beyond VLAs</div>
<div class="meta-line">Authors: Jonas Pai, Liam Achenbach, Victoriano Montesinos, Benedek Forrai, Oier Mees, Elvis Nava</div>
<div class="meta-line">First: 2025-12-17T18:47:31+00:00 · Latest: 2025-12-17T18:47:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15692v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15692v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prevailing Vision-Language-Action Models (VLAs) for robotic manipulation are built upon vision-language backbones pretrained on large-scale, but disconnected static web data. As a result, despite improved semantic generalization, the policy must implicitly infer complex physical dynamics and temporal dependencies solely from robot trajectories. This reliance creates an unsustainable data burden, necessitating continuous, large-scale expert data collection to compensate for the lack of innate physical understanding. We contend that while vision-language pretraining effectively captures semantic priors, it remains blind to physical causality. A more effective paradigm leverages video to jointly capture semantics and visual dynamics during pretraining, thereby isolating the remaining task of low-level control. To this end, we introduce \model, a novel Video-Action Model (VAM) that pairs a pretrained Internet-scale video model with a flow matching-based action decoder conditioned on its latent representations. The decoder serves as an Inverse Dynamics Model (IDM), generating low-level robot actions from the latent representation of video-space action plans. Our extensive evaluation shows that our approach achieves state-of-the-art performance on simulated and real-world robotic manipulation tasks, improving sample efficiency by 10x and convergence speed by 2x compared to traditional VLA architectures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>mimic-video: 超越视觉-语言-动作模型（VLAs）的视频-动作模型用于可泛化的机器人控制</div>
<div class="mono" style="margin-top:8px">目前用于机器人操作的视觉-语言-动作模型（VLAs）基于在大规模、不连贯的静态网络数据上预训练的视觉-语言主干网络。因此，尽管在语义泛化方面有所提升，策略仍必须隐式地从机器人轨迹中推断复杂的物理动态和时间依赖性。这种依赖性导致了不可持续的数据负担，需要持续的大规模专家数据收集来弥补缺乏固有的物理理解。我们认为，虽然视觉-语言预训练可以有效捕捉语义先验，但它对物理因果关系是盲目的。一种更有效的范式是在预训练过程中利用视频同时捕捉语义和视觉动态，从而将剩余的低级控制任务孤立出来。为此，我们引入了\model，一种新颖的视频-动作模型（VAM），它将一个在互联网规模视频上预训练的模型与一个基于流匹配的动作解码器相结合，该解码器根据其潜在表示进行条件化。解码器作为逆动力学模型（IDM），从视频空间动作计划的潜在表示中生成低级机器人动作。我们的广泛评估表明，我们的方法在模拟和现实世界机器人操作任务中实现了最先进的性能，与传统VLA架构相比，提高了采样效率10倍，收敛速度提高了2倍。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Modal Semantic Communication</div>
<div class="meta-line">Authors: Matin Mortaheb, Erciyes Karakaya, Sennur Ulukus</div>
<div class="meta-line">First: 2025-12-17T18:47:22+00:00 · Latest: 2025-12-17T18:47:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15691v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15691v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Semantic communication aims to transmit information most relevant to a task rather than raw data, offering significant gains in communication efficiency for applications such as telepresence, augmented reality, and remote sensing. Recent transformer-based approaches have used self-attention maps to identify informative regions within images, but they often struggle in complex scenes with multiple objects, where self-attention lacks explicit task guidance. To address this, we propose a novel Multi-Modal Semantic Communication framework that integrates text-based user queries to guide the information extraction process. Our proposed system employs a cross-modal attention mechanism that fuses visual features with language embeddings to produce soft relevance scores over the visual data. Based on these scores and the instantaneous channel bandwidth, we use an algorithm to transmit image patches at adaptive resolutions using independently trained encoder-decoder pairs, with total bitrate matching the channel capacity. At the receiver, the patches are reconstructed and combined to preserve task-critical information. This flexible and goal-driven design enables efficient semantic communication in complex and bandwidth-constrained environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多模态语义通信</div>
<div class="mono" style="margin-top:8px">语义通信旨在传输与任务最相关的信息，而非原始数据，这在诸如远程临场、增强现实和遥感等应用中能显著提升通信效率。近期基于Transformer的方法利用自注意力图来识别图像中的信息区域，但在包含多个物体的复杂场景中，自注意力往往缺乏明确的任务指导。为了解决这一问题，我们提出了一种新颖的多模态语义通信框架，通过文本用户查询来引导信息提取过程。我们的系统采用跨模态注意力机制，将视觉特征与语言嵌入融合，以生成对视觉数据的软相关性评分。基于这些评分和瞬时信道带宽，我们使用一种算法通过独立训练的编码器-解码器对以自适应分辨率传输图像块，总比特率匹配信道容量。在接收端，图像块被重建并组合以保留任务关键信息。这种灵活且以目标为导向的设计使得在复杂和带宽受限的环境中能够实现高效的语义通信。</div>
</details>
</div>
<div class="card">
<div class="title">BashArena: A Control Setting for Highly Privileged AI Agents</div>
<div class="meta-line">Authors: Adam Kaufman, James Lucassen, Tyler Tracy, Cody Rushing, Aryan Bhatt</div>
<div class="meta-line">First: 2025-12-17T18:45:25+00:00 · Latest: 2025-12-17T18:45:25+00:00</div>
<div class="meta-line">Comments: The task generation pipeline can be found here: https://github.com/redwoodresearch/basharena_public</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15688v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15688v1">PDF</a> · <a href="https://github.com/redwoodresearch/basharena_public">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Future AI agents might run autonomously with elevated privileges. If these agents are misaligned, they might abuse these privileges to cause serious damage. The field of AI control develops techniques that make it harder for misaligned AIs to cause such damage, while preserving their usefulness. We introduce BashArena, a setting for studying AI control techniques in security-critical environments. BashArena contains 637 Linux system administration and infrastructure engineering tasks in complex, realistic environments, along with four sabotage objectives (execute malware, exfiltrate secrets, escalate privileges, and disable firewall) for a red team to target. We evaluate multiple frontier LLMs on their ability to complete tasks, perform sabotage undetected, and detect sabotage attempts. Claude Sonnet 4.5 successfully executes sabotage while evading monitoring by GPT-4.1 mini 26% of the time, at 4% trajectory-wise FPR. Our findings provide a baseline for designing more effective control protocols in BashArena. We release the dataset as a ControlArena setting and share our task generation pipeline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BashArena：一个用于高度特权AI代理的控制环境</div>
<div class="mono" style="margin-top:8px">未来的AI代理可能会以提升的权限自主运行。如果这些代理与人类目标不一致，它们可能会滥用这些权限造成严重损害。AI控制领域开发了技术，使不一致的AI更难以造成此类损害，同时保持其可用性。我们引入了BashArena，这是一个用于在安全关键环境中研究AI控制技术的设置。BashArena包含637个Linux系统管理与基础设施工程任务，这些任务在复杂、现实的环境中运行，并包含四个破坏目标（执行恶意软件、泄露机密、提升权限、禁用防火墙），供红队攻击。我们评估了多个前沿LLMs在完成任务、执行破坏行为而不被检测以及检测破坏尝试方面的能力。Claude Sonnet 4.5在4%的轨迹误报率下，有26%的时间成功执行破坏行为并规避GPT-4.1 mini的监控。我们的研究结果为设计更有效的控制协议提供了基准。我们发布了该数据集作为ControlArena设置，并分享了我们的任务生成流程。</div>
</details>
</div>
<div class="card">
<div class="title">Can LLMs Guide Their Own Exploration? Gradient-Guided Reinforcement Learning for LLM Reasoning</div>
<div class="meta-line">Authors: Zhenwen Liang, Sidi Lu, Wenhao Yu, Kishan Panaganti, Yujun Zhou, Haitao Mi, Dong Yu</div>
<div class="meta-line">First: 2025-12-17T18:44:45+00:00 · Latest: 2025-12-17T18:44:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15687v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15687v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning has become essential for strengthening the reasoning abilities of large language models, yet current exploration mechanisms remain fundamentally misaligned with how these models actually learn. Entropy bonuses and external semantic comparators encourage surface level variation but offer no guarantee that sampled trajectories differ in the update directions that shape optimization. We propose G2RL, a gradient guided reinforcement learning framework in which exploration is driven not by external heuristics but by the model own first order update geometry. For each response, G2RL constructs a sequence level feature from the model final layer sensitivity, obtainable at negligible cost from a standard forward pass, and measures how each trajectory would reshape the policy by comparing these features within a sampled group. Trajectories that introduce novel gradient directions receive a bounded multiplicative reward scaler, while redundant or off manifold updates are deemphasized, yielding a self referential exploration signal that is naturally aligned with PPO style stability and KL control. Across math and general reasoning benchmarks (MATH500, AMC, AIME24, AIME25, GPQA, MMLUpro) on Qwen3 base 1.7B and 4B models, G2RL consistently improves pass@1, maj@16, and pass@k over entropy based GRPO and external embedding methods. Analyzing the induced geometry, we find that G2RL expands exploration into substantially more orthogonal and often opposing gradient directions while maintaining semantic coherence, revealing that a policy own update space provides a far more faithful and effective basis for guiding exploration in large language model reinforcement learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型能否自主引导探索？基于梯度引导的强化学习用于大语言模型推理</div>
<div class="mono" style="margin-top:8px">强化学习已成为增强大语言模型推理能力的关键方法，但当前的探索机制与这些模型实际学习的方式存在根本性偏差。熵奖励和外部语义比较器鼓励表面变化，但无法保证采样的轨迹在塑造优化的方向上有所不同。我们提出了G2RL，一种基于梯度引导的强化学习框架，其中探索由模型自身的首次更新几何结构驱动，而非外部启发式方法。对于每个响应，G2RL从模型最后一层的敏感性中构建序列级特征，该特征可通过标准前向传播以微小成本获得，并通过在采样组内比较这些特征来衡量每条轨迹如何重塑策略。引入新颖梯度方向的轨迹会获得有界的乘法奖励缩放器，而冗余或离模型流形的更新则被弱化，从而产生一种自然与PPO风格的稳定性及KL控制对齐的自指探索信号。在Qwen3基础模型（1.7B和4B参数）上，针对数学和通用推理基准（MATH500、AMC、AIME24、AIME25、GPQA、MMLUpro），G2RL在基于熵的GRPO和外部嵌入方法上持续提升了pass@1、maj@16和pass@k指标。通过分析诱导的几何结构，我们发现G2RL将探索扩展到大量正交且常常对立的梯度方向，同时保持语义连贯性，揭示了模型自身的更新空间为大语言模型强化学习中的探索引导提供了更忠实且有效的基础。</div>
</details>
</div>
<div class="card">
<div class="title">MMGR: Multi-Modal Generative Reasoning</div>
<div class="meta-line">Authors: Zefan Cai, Haoyi Qiu, Tianyi Ma, Haozhe Zhao, Gengze Zhou, Kung-Hsiang Huang, Parisa Kordjamshidi, Minjia Zhang, Wen Xiao, Jiuxiang Gu, Nanyun Peng, Junjie Hu</div>
<div class="meta-line">First: 2025-12-16T18:58:04+00:00 · Latest: 2025-12-17T18:42:37+00:00</div>
<div class="meta-line">Comments: work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14691v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.14691v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MMGR：多模态生成推理</div>
<div class="mono" style="margin-top:8px">视频基础模型可以生成视觉上逼真且时间上连贯的内容，但其作为世界模拟器的可靠性取决于是否捕捉了物理、逻辑和空间约束。现有的度量标准如Frechet Video Distance（FVD）强调感知质量，却忽略了推理失败，包括因果关系、物理规律和全局一致性方面的违反。我们引入MMGR（多模态生成推理评估与基准），这是一个基于五种推理能力的评估框架：物理推理、逻辑推理、三维空间推理、二维空间推理和时间推理。MMGR在三个领域评估生成推理：抽象推理（ARC-AGI、数独）、具身导航（现实世界三维导航与定位）和物理常识（体育和组合交互）。MMGR应用细粒度度量标准，要求视频和图像生成的全面正确性。我们对领先的视频模型（Veo-3、Sora-2、Wan-2.2）和图像模型（Nano-banana、Nano-banana Pro、GPT-4o-image、Qwen-image）进行了基准测试，揭示了不同领域之间显著的性能差距。模型在物理常识任务上表现出中等水平的成功，但在抽象推理（在ARC-AGI上准确率低于10%）和具身环境中的长视野空间规划方面表现不佳。我们的分析指出了当前模型的关键局限性，包括过度依赖感知数据、全局状态一致性较弱以及目标奖励视觉合理性而非因果正确性。MMGR提供了一个统一的诊断基准，并为具有推理能力的生成世界模型指明了发展方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints.</div>
</details>
</div>
<div class="card">
<div class="title">Improving Recursive Transformers with Mixture of LoRAs</div>
<div class="meta-line">Authors: Mohammadmahdi Nouriborji, Morteza Rohanian, Omid Rohanian</div>
<div class="meta-line">First: 2025-12-14T23:39:30+00:00 · Latest: 2025-12-17T18:41:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.12880v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.12880v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Parameter sharing in recursive transformers reduces model size but collapses layer-wise expressivity. We propose Mixture of LoRAs (MoL), a lightweight conditional-computation mechanism that inserts Low-Rank Adaptation (LoRA) experts inside a shared feed-forward network (FFN). MoL enables token-conditional weight-space modulation of the shared FFN without untying backbone parameters, unlike prior approaches that add fixed or externally attached adapters. We pretrain a modernised recursive architecture, ModernALBERT, integrating rotary embeddings, GeGLU, FlashAttention, and a distillation-based initialisation. Across GLUE, SQuAD-v2, and BEIR, ModernALBERT (50M--120M) achieves state-of-the-art performance among compact models and surpasses larger fully parameterised baselines. We also propose an expert-merging procedure that compresses MoL into a single adapter at inference while preserving accuracy, enabling efficient deployment. Our results show that conditional weight-space modulation effectively restores the expressivity lost under aggressive parameter sharing in recursive transformers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过LoRAs混合改进递归Transformer</div>
<div class="mono" style="margin-top:8px">递归Transformer中的参数共享减少了模型规模，但导致层间表达能力的崩溃。我们提出了一种轻量级的条件计算机制，称为Mixture of LoRAs (MoL)，它在共享的前馈网络(FFN)中插入低秩适应(LoRA)专家。MoL能够在不解除主干参数的情况下，实现对共享FFN的标记条件权重空间调制，与之前添加固定或外部适配器的方法不同。我们预训练了一个现代化的递归架构ModernALBERT，集成了旋转嵌入、GeGLU、FlashAttention和基于蒸馏的初始化。在GLUE、SQuAD-v2和BEIR数据集上，ModernALBERT（50M--120M）在紧凑模型中取得了最先进的性能，并超越了更大的完全参数化基线。我们还提出了一种专家合并过程，该过程在推理时将MoL压缩为单个适配器，同时保持准确性，从而实现高效部署。我们的结果表明，条件权重空间调制有效地恢复了递归Transformer在激进参数共享下丢失的表达能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Parameter sharing in recursive transformers reduces model size but collapses layer-wise expressivity.</div>
</details>
</div>
<div class="card">
<div class="title">A Multivariate Statistical Framework for Detection, Classification and Pre-localization of Anomalies in Water Distribution Networks</div>
<div class="meta-line">Authors: Oleg Melnikov, Yurii Dorofieiev, Yurii Shakhnovskiy, Huy Truong, Victoria Degeler</div>
<div class="meta-line">First: 2025-12-17T18:38:37+00:00 · Latest: 2025-12-17T18:38:37+00:00</div>
<div class="meta-line">Comments: 48 pages, 18 figures, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15685v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15685v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a unified framework, for the detection, classification, and preliminary localization of anomalies in water distribution networks using multivariate statistical analysis. The approach, termed SICAMS (Statistical Identification and Classification of Anomalies in Mahalanobis Space), processes heterogeneous pressure and flow sensor data through a whitening transformation to eliminate spatial correlations among measurements. Based on the transformed data, the Hotelling&#x27;s $T^2$ statistic is constructed, enabling the formulation of anomaly detection as a statistical hypothesis test of network conformity to normal operating conditions. It is shown that Hotelling&#x27;s $T^2$ statistic can serve as an integral indicator of the overall &quot;health&quot; of the system, exhibiting correlation with total leakage volume, and thereby enabling approximate estimation of water losses via a regression model. A heuristic algorithm is developed to analyze the $T^2$ time series and classify detected anomalies into abrupt leaks, incipient leaks, and sensor malfunctions. Furthermore, a coarse leak localization method is proposed, which ranks sensors according to their statistical contribution and employs Laplacian interpolation to approximate the affected region within the network. Application of the proposed framework to the BattLeDIM L-Town benchmark dataset demonstrates high sensitivity and reliability in leak detection, maintaining robust performance even under multiple leaks. These capabilities make the method applicable to real-world operational environments without the need for a calibrated hydraulic model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种用于水管网异常检测、分类和初步定位的多元统计框架</div>
<div class="mono" style="margin-top:8px">本文提出了一种统一框架，利用多元统计分析对水管网中的异常进行检测、分类和初步定位。该方法称为SICAMS（马哈拉诺比斯空间中异常的统计识别与分类），通过白化变换处理异构的压力和流量传感器数据，以消除测量值之间的空间相关性。基于变换后的数据构建霍特林T²统计量，从而将异常检测问题转化为对网络是否符合正常运行状态的统计假设检验。研究表明，霍特林T²统计量可作为系统整体&quot;健康&quot;的综合指标，与总泄漏量相关，从而通过回归模型实现对水损的近似估计。还开发了一种启发式算法，用于分析T²时间序列，并将检测到的异常分类为突发泄漏、初期泄漏和传感器故障。此外，提出了一种粗略泄漏定位方法，根据传感器的统计贡献进行排序，并利用拉普拉斯插值法近似估计网络中受影响的区域。将该框架应用于BattLeDIM L-Town基准数据集，展示了其在泄漏检测中的高灵敏度和可靠性，即使在多泄漏情况下也能保持良好的性能。这些能力使得该方法无需校准水力模型即可应用于实际运行环境。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper presents a unified framework, for the detection, classification, and preliminary localization of anomalies in water distribution networks using multivariate statistical analysis.</div>
</details>
</div>
<div class="card">
<div class="title">High-Dimensional Partial Least Squares: Spectral Analysis and Fundamental Limitations</div>
<div class="meta-line">Authors: Victor Léger, Florent Chatelain</div>
<div class="meta-line">First: 2025-12-17T18:38:01+00:00 · Latest: 2025-12-17T18:38:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15684v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15684v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Partial Least Squares (PLS) is a widely used method for data integration, designed to extract latent components shared across paired high-dimensional datasets. Despite decades of practical success, a precise theoretical understanding of its behavior in high-dimensional regimes remains limited. In this paper, we study a data integration model in which two high-dimensional data matrices share a low-rank common latent structure while also containing individual-specific components. We analyze the singular vectors of the associated cross-covariance matrix using tools from random matrix theory and derive asymptotic characterizations of the alignment between estimated and true latent directions. These results provide a quantitative explanation of the reconstruction performance of the PLS variant based on Singular Value Decomposition (PLS-SVD) and identify regimes where the method exhibits counter-intuitive or limiting behavior. Building on this analysis, we compare PLS-SVD with principal component analysis applied separately to each dataset and show its asymptotic superiority in detecting the common latent subspace. Overall, our results offer a comprehensive theoretical understanding of high-dimensional PLS-SVD, clarifying both its advantages and fundamental limitations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高维偏最小二乘法：谱分析与基本局限性</div>
<div class="mono" style="margin-top:8px">偏最小二乘法（PLS）是一种广泛用于数据整合的方法，旨在提取成对高维数据集之间共享的潜在成分。尽管在实践中取得了数十年的成功，但对其在高维情况下的行为仍缺乏精确的理论理解。本文研究了一种数据整合模型，其中两个高维数据矩阵共享一个低秩的公共潜在结构，同时也包含特定于个体的成分。我们利用随机矩阵理论的工具分析相关交叉协方差矩阵的奇异向量，并推导出估计潜在方向与真实潜在方向之间对齐的渐近特性。这些结果为基于奇异值分解的PLS变体（PLS-SVD）的重建性能提供了定量解释，并识别了该方法表现出反直觉或限制性行为的场景。在此基础上，我们将PLS-SVD与分别应用于每个数据集的主成分分析进行比较，展示了其在检测公共潜在子空间方面的渐近优越性。总体而言，我们的结果为高维PLS-SVD提供了全面的理论理解，阐明了其优势和基本局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Partial Least Squares (PLS) is a widely used method for data integration, designed to extract latent components shared across paired high-dimensional datasets.</div>
</details>
</div>
<div class="card">
<div class="title">Expressibility and inexpressibility in propositional team logics</div>
<div class="meta-line">Authors: Matilda Häggblom, Minna Hirvonen, Jouko Väänänen</div>
<div class="meta-line">First: 2025-12-17T18:32:29+00:00 · Latest: 2025-12-17T18:32:29+00:00</div>
<div class="meta-line">Comments: 38 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15680v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15680v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We develop dimension theoretic methods for propositional team based logics. Such quantitative methods were defined for team based first-order logic in a recent paper by Hella, Luosto and the third author and were used to obtain strong hierarchy results in the first-order logic context. We show that in propositional logic and in several important cases, a team theoretical atom can be expressed in terms of atoms of lower arity. We estimate the `price&#x27; of such a reduction of arity, i.e. how much more complicated the new expression is. Our estimates involve as parameters the arity of the atoms involved, as well as the number of times the atom occurs in a formula. We also consider new variants of atoms and propositional operations, inspired by our work. We believe that our quantitative analysis leads to a deeper understanding of the scope and limits of propositional team based logic.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>命题团队逻辑中的可表达性与不可表达性</div>
<div class="mono" style="margin-top:8px">我们发展了用于命题团队逻辑的维度理论方法。这类量化方法最近由Hella、Luosto和第三作者在一篇论文中定义，并被用于获得一阶逻辑中的强层次结果。我们证明，在命题逻辑以及一些重要情况下，一个团队理论原子可以被表示为低 arity 原子的组合。我们估计这种 arity 减少的`代价&#x27;，即新表达式复杂度的增加程度。我们的估计参数包括所涉及原子的 arity 以及原子在公式中出现的次数。我们还考虑了受本工作启发的新原子和命题运算变体。我们认为，我们的量化分析有助于更深入地理解命题团队逻辑的范围和限制。</div>
</details>
</div>
<div class="card">
<div class="title">A High-level Synthesis Toolchain for the Julia Language</div>
<div class="meta-line">Authors: Benedict Short, Ian McInerney, John Wickerson</div>
<div class="meta-line">First: 2025-12-17T18:32:06+00:00 · Latest: 2025-12-17T18:32:06+00:00</div>
<div class="meta-line">Comments: Extended version of poster abstract accepted for presentation at ISFPGA&#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15679v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15679v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the push towards Exascale computing and data-driven methods, problem sizes have increased dramatically, increasing the computational requirements of the underlying algorithms. This has led to a push to offload computations to general purpose hardware accelerators such as GPUs and TPUs, and a renewed interest in designing problem-specific accelerators using FPGAs. However, the development process of these problem-specific accelerators currently suffers from the &quot;two-language problem&quot;: algorithms are developed in one (usually higher-level) language, but the kernels are implemented in another language at a completely different level of abstraction and requiring fundamentally different expertise. To address this problem, we propose a new MLIR-based compiler toolchain that unifies the development process by automatically compiling kernels written in the Julia programming language into SystemVerilog without the need for any additional directives or language customisations. Our toolchain supports both dynamic and static scheduling, directly integrates with the AXI4-Stream protocol to interface with subsystems like on- and off-chip memory, and generates vendor-agnostic RTL. This prototype toolchain is able to synthesize a set of signal processing/mathematical benchmarks that can operate at 100MHz on real FPGA devices, achieving between 59.71% and 82.6% of the throughput of designs generated by state-of-the-art toolchains that only compile from low-level languages like C or C++. Overall, this toolchain allows domain experts to write compute kernels in Julia as they normally would, and then retarget them to an FPGA without additional pragmas or modifications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种用于Julia语言的高层次综合工具链</div>
<div class="mono" style="margin-top:8px">随着向Exascale计算和数据驱动方法的推进，问题规模显著增加，提高了底层算法的计算需求。这促使人们将计算卸载到通用硬件加速器（如GPU和TPU）上，并重新关注使用FPGA设计问题专用加速器。然而，当前这些问题专用加速器的开发过程面临所谓的“双语言问题”：算法通常在一个（通常是高级语言）中开发，而内核则需要在另一个抽象层次完全不同的语言中实现，这需要根本不同的专业知识。为了解决这一问题，我们提出了一种基于MLIR的新编译工具链，通过自动将Julia编程语言编写的内核编译为SystemVerilog，统一了开发过程，无需任何额外的指令或语言定制。我们的工具链支持动态和静态调度，直接集成AXI4-Stream协议以与片上和片外内存等子系统接口，并生成与供应商无关的RTL。该原型工具链能够综合一组信号处理/数学基准，这些基准可在实际的FPGA设备上以100MHz运行，其吞吐量达到由当前最先进的工具链（仅从C或C++等低级语言编译）生成设计的59.71%至82.6%。总体而言，该工具链使领域专家能够像平常一样用Julia编写计算内核，然后无需额外的编译指令或修改即可将其重新定位到FPGA上。</div>
</details>
</div>
<div class="card">
<div class="title">Stylized Synthetic Augmentation further improves Corruption Robustness</div>
<div class="meta-line">Authors: Georg Siedel, Rojan Regmi, Abhirami Anand, Weijia Shao, Silvia Vock, Andrey Morozov</div>
<div class="meta-line">First: 2025-12-17T18:28:04+00:00 · Latest: 2025-12-17T18:28:04+00:00</div>
<div class="meta-line">Comments: Accepted at VISAPP 2026 conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15675v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15675v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes a training data augmentation pipeline that combines synthetic image data with neural style transfer in order to address the vulnerability of deep vision models to common corruptions. We show that although applying style transfer on synthetic images degrades their quality with respect to the common FID metric, these images are surprisingly beneficial for model training. We conduct a systematic empirical analysis of the effects of both augmentations and their key hyperparameters on the performance of image classifiers. Our results demonstrate that stylization and synthetic data complement each other well and can be combined with popular rule-based data augmentation techniques such as TrivialAugment, while not working with others. Our method achieves state-of-the-art corruption robustness on several small-scale image classification benchmarks, reaching 93.54%, 74.9% and 50.86% robust accuracy on CIFAR-10-C, CIFAR-100-C and TinyImageNet-C, respectively</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>风格化合成增强进一步提升图像分类模型的抗损坏能力</div>
<div class="mono" style="margin-top:8px">本文提出了一种结合合成图像数据与神经风格迁移的训练数据增强流程，以解决深度视觉模型对常见损坏的脆弱性。我们表明，尽管对合成图像应用风格迁移会降低其在常见FID指标下的质量，但这些图像对模型训练却出人意料地有益。我们系统地分析了这两种增强方法及其关键超参数对图像分类器性能的影响。我们的结果表明，风格化和合成数据能够很好地互补，并可与TrivialAugment等流行的基于规则的数据增强技术结合使用，但与其他方法不兼容。我们的方法在多个小规模图像分类基准测试中达到了最先进的抗损坏性能，分别在CIFAR-10-C、CIFAR-100-C和TinyImageNet-C上实现了93.54%、74.9%和50.86%的鲁棒准确率。</div>
</details>
</div>
<div class="card">
<div class="title">Activation Oracles: Training and Evaluating LLMs as General-Purpose Activation Explainers</div>
<div class="meta-line">Authors: Adam Karvonen, James Chua, Clément Dumas, Kit Fraser-Taliente, Subhash Kantamneni, Julian Minder, Euan Ong, Arnab Sen Sharma, Daniel Wen, Owain Evans, Samuel Marks</div>
<div class="meta-line">First: 2025-12-17T18:26:28+00:00 · Latest: 2025-12-17T18:26:28+00:00</div>
<div class="meta-line">Comments: 36 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15674v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15674v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM) activations are notoriously difficult to understand, with most existing techniques using complex, specialized methods for interpreting them. Recent work has proposed a simpler approach known as LatentQA: training LLMs to directly accept LLM activations as inputs and answer arbitrary questions about them in natural language. However, prior work has focused on narrow task settings for both training and evaluation. In this paper, we instead take a generalist perspective. We evaluate LatentQA-trained models, which we call Activation Oracles (AOs), in far out-of-distribution settings and examine how performance scales with training data diversity. We find that AOs can recover information fine-tuned into a model (e.g., biographical knowledge or malign propensities) that does not appear in the input text, despite never being trained with activations from a fine-tuned model. Our main evaluations are four downstream tasks where we can compare to prior white- and black-box techniques. We find that even narrowly-trained LatentQA models can generalize well, and that adding additional training datasets (such as classification tasks and a self-supervised context prediction task) yields consistent further improvements. Overall, our best AOs match or exceed prior white-box baselines on all four tasks and are the best method on 3 out of 4. These results suggest that diversified training to answer natural-language queries imparts a general capability to verbalize information about LLM activations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>激活预言机：训练和评估作为通用激活解释器的LLM</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）的激活过程历来难以理解，大多数现有技术使用复杂且专门的方法进行解释。最近的研究提出了一种更简单的方法，称为LatentQA：训练LLM直接接受LLM激活作为输入，并用自然语言回答关于这些激活的任意问题。然而，以往的工作在训练和评估时都集中在狭窄的任务设置上。本文则采取了更通用的视角。我们在远超出分布的设置中评估了通过LatentQA训练的模型，我们称之为激活预言机（AOs），并研究了训练数据多样性如何影响性能。我们发现，即使从未使用微调模型的激活进行训练，AOs仍能恢复模型中微调的信息（例如生物知识或恶意倾向），这些信息在输入文本中并不存在。我们的主要评估包括四个下游任务，我们可以与以往的白盒和黑盒技术进行比较。我们发现，即使是狭窄训练的LatentQA模型也能很好地泛化，而添加额外的训练数据集（如分类任务和自监督上下文预测任务）则能带来持续的性能提升。总体而言，我们的最佳AOs在所有四个任务中均达到或超过了以往的白盒基线，并在其中三个任务中是最佳方法。这些结果表明，通过多样化训练来回答自然语言查询，能够赋予LLM激活信息的表达能力。</div>
</details>
</div>
<div class="card">
<div class="title">Explaining the Reasoning of Large Language Models Using Attribution Graphs</div>
<div class="meta-line">Authors: Chase Walker, Rickard Ewetz</div>
<div class="meta-line">First: 2025-12-17T18:15:26+00:00 · Latest: 2025-12-17T18:15:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15663v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15663v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) exhibit remarkable capabilities, yet their reasoning remains opaque, raising safety and trust concerns. Attribution methods, which assign credit to input features, have proven effective for explaining the decision making of computer vision models. From these, context attributions have emerged as a promising approach for explaining the behavior of autoregressive LLMs. However, current context attributions produce incomplete explanations by directly relating generated tokens to the prompt, discarding inter-generational influence in the process. To overcome these shortcomings, we introduce the Context Attribution via Graph Explanations (CAGE) framework. CAGE introduces an attribution graph: a directed graph that quantifies how each generation is influenced by both the prompt and all prior generations. The graph is constructed to preserve two properties-causality and row stochasticity. The attribution graph allows context attributions to be computed by marginalizing intermediate contributions along paths in the graph. Across multiple models, datasets, metrics, and methods, CAGE improves context attribution faithfulness, achieving average gains of up to 40%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用归因图解释大型语言模型的推理过程</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）展现出卓越的能力，但其推理过程仍然不透明，引发了安全性和信任度方面的担忧。归因方法通过将信用分配给输入特征，已被证明在解释计算机视觉模型的决策过程中非常有效。由此，上下文归因方法作为一种解释自回归LLMs行为的有前景方法逐渐出现。然而，当前的上下文归因方法通过直接将生成的标记与提示相关联，从而在过程中忽略了代际间的相互影响，导致解释不完整。为克服这些缺陷，我们引入了基于图解释的上下文归因（CAGE）框架。CAGE引入了一个归因图：一个有向图，用于量化每个生成步骤如何受到提示和所有先前生成步骤的影响。该图被构建以保留两个属性——因果性和行随机性。通过在图中路径上对中间贡献进行边缘化，归因图使得上下文归因的计算成为可能。在多个模型、数据集、指标和方法上，CAGE提升了上下文归因的可信度，平均提升幅度高达40%。</div>
</details>
</div>
<div class="card">
<div class="title">Stepwise Think-Critique: A Unified Framework for Robust and Interpretable LLM Reasoning</div>
<div class="meta-line">Authors: Jiaqi Xu, Cuiling Lan, Xuejin Chen, Yan LU</div>
<div class="meta-line">First: 2025-12-17T18:15:17+00:00 · Latest: 2025-12-17T18:15:17+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.15662v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.15662v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human beings solve complex problems through critical thinking, where reasoning and evaluation are intertwined to converge toward correct solutions. However, most existing large language models (LLMs) decouple reasoning from verification: they either generate reasoning without explicit self-checking or rely on external verifiers to detect errors post hoc. The former lacks immediate feedback, while the latter increases system complexity and hinders synchronized learning. Motivated by human critical thinking, we propose Stepwise Think-Critique (STC), a unified framework that interleaves reasoning and self-critique at each step within a single model. STC is trained with a hybrid reinforcement learning objective combining reasoning rewards and critique-consistency rewards to jointly optimize reasoning quality and self-evaluation. Experiments on mathematical reasoning benchmarks show that STC demonstrates strong critic-thinking capabilities and produces more interpretable reasoning traces, representing a step toward LLMs with built-in critical thinking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分步思考-批判：一种统一的框架，用于实现鲁棒且可解释的LLM推理</div>
<div class="mono" style="margin-top:8px">人类通过批判性思维解决复杂问题，其中推理与评估相互交织以收敛到正确解。然而，大多数现有的大语言模型（LLMs）将推理与验证分离：它们要么在没有显式自我检查的情况下生成推理，要么依赖外部验证器在事后检测错误。前者缺乏即时反馈，而后者增加了系统复杂性并阻碍了同步学习。受人类批判性思维启发，我们提出了分步思考-批判（STC），这是一种统一框架，它在一个模型内通过每一步的推理与自我批判交替进行。STC采用混合强化学习目标进行训练，结合推理奖励和批判一致性奖励，以联合优化推理质量和自我评估。在数学推理基准上的实验表明，STC展现出强大的批判性思维能力，并产生更可解释的推理轨迹，标志着向具有内置批判性思维的LLM迈进了一步。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
