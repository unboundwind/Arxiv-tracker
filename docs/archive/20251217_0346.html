<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-17 03:46</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251217_0346</div>
    <div class="row"><div class="card">
<div class="title">DiffusionBrowser: Interactive Diffusion Previews via Multi-Branch Decoders</div>
<div class="meta-line">Authors: Susung Hong, Chongjian Ge, Zhifei Zhang, Jui-Hsien Wang</div>
<div class="meta-line">First: 2025-12-15T18:59:57+00:00 · Latest: 2025-12-15T18:59:57+00:00</div>
<div class="meta-line">Comments: Project page: https://susunghong.github.io/DiffusionBrowser</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13690v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13690v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://susunghong.github.io/DiffusionBrowser">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video diffusion models have revolutionized generative video synthesis, but they are imprecise, slow, and can be opaque during generation -- keeping users in the dark for a prolonged period. In this work, we propose DiffusionBrowser, a model-agnostic, lightweight decoder framework that allows users to interactively generate previews at any point (timestep or transformer block) during the denoising process. Our model can generate multi-modal preview representations that include RGB and scene intrinsics at more than 4$\times$ real-time speed (less than 1 second for a 4-second video) that convey consistent appearance and motion to the final video. With the trained decoder, we show that it is possible to interactively guide the generation at intermediate noise steps via stochasticity reinjection and modal steering, unlocking a new control capability. Moreover, we systematically probe the model using the learned decoders, revealing how scene, object, and other details are composed and assembled during the otherwise black-box denoising process.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiffusionBrowser：通过多分支解码器实现的交互式扩散预览</div>
<div class="mono" style="margin-top:8px">视频扩散模型革新了生成式视频合成，但在生成过程中存在不精确、缓慢和不透明的问题，使用户长时间处于信息不明确的状态。在本工作中，我们提出了DiffusionBrowser，这是一个模型无关且轻量的解码器框架，允许用户在去噪过程中任意时间点（时间步或Transformer块）进行交互式预览生成。我们的模型能够在超过4倍实时速度下生成包含RGB和场景内禀信息的多模态预览表示，这些表示能够传达与最终视频一致的外观和运动。通过训练的解码器，我们展示了如何通过随机性重新注入和模态引导在中间噪声步骤中进行交互式引导，从而解锁新的控制能力。此外，我们系统地利用学习到的解码器对模型进行探测，揭示了在看似黑箱的去噪过程中场景、物体及其他细节是如何被组合和构建的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind DiffusionBrowser is to address the limitations of video diffusion models, such as imprecision, slowness, and lack of transparency during generation. The proposed method introduces a model-agnostic, lightweight decoder framework that enables interactive preview generation at any stage of the denoising process. Experimental results demonstrate that the framework can produce multi-modal preview representations, including RGB and scene intrinsics, at over 4× real-time speed, maintaining consistent appearance and motion with the final video. Additionally, the decoder allows for interactive guidance through stochasticity reinjection and modal steering, and systematic analysis reveals how scene and object details are formed during the denoising process.</div>
<div class="mono" style="margin-top:8px">DiffusionBrowser的动机是解决视频扩散模型在生成过程中存在的不精确、速度慢和不透明等问题。该方法提出了一种模型无关、轻量级的解码器框架，允许用户在去噪过程的任意阶段进行交互式预览生成。实验结果表明，该框架可以在超过4倍实时速度下生成包括RGB和场景内禀信息的多模态预览表示，提供与最终视频一致的外观和运动信息。此外，解码器通过随机性注入和模态引导实现了对生成过程的交互式控制，提供了新的操控能力。使用学习到的解码器对模型进行系统性探测，还揭示了场景和物体细节在去噪过程中的组合与构建机制。</div>
</details>
</div>
<div class="card">
<div class="title">LitePT: Lighter Yet Stronger Point Transformer</div>
<div class="meta-line">Authors: Yuanwen Yue, Damien Robert, Jianyuan Wang, Sunghwan Hong, Jan Dirk Wegner, Christian Rupprecht, Konrad Schindler</div>
<div class="meta-line">First: 2025-12-15T18:59:57+00:00 · Latest: 2025-12-15T18:59:57+00:00</div>
<div class="meta-line">Comments: Project page: https://litept.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13689v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13689v1">PDF</a> · <a href="https://github.com/prs-eth/LitePT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://litept.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern neural architectures for 3D point cloud processing contain both convolutional layers and attention blocks, but the best way to assemble them remains unclear. We analyse the role of different computational blocks in 3D point cloud networks and find an intuitive behaviour: convolution is adequate to extract low-level geometry at high-resolution in early layers, where attention is expensive without bringing any benefits; attention captures high-level semantics and context in low-resolution, deep layers more efficiently. Guided by this design principle, we propose a new, improved 3D point cloud backbone that employs convolutions in early stages and switches to attention for deeper layers. To avoid the loss of spatial layout information when discarding redundant convolution layers, we introduce a novel, training-free 3D positional encoding, PointROPE. The resulting LitePT model has $3.6\times$ fewer parameters, runs $2\times$ faster, and uses $2\times$ less memory than the state-of-the-art Point Transformer V3, but nonetheless matches or even outperforms it on a range of tasks and datasets. Code and models are available at: https://github.com/prs-eth/LitePT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LitePT：更轻量且更强的点变换模型</div>
<div class="mono" style="margin-top:8px">现代用于3D点云处理的神经网络架构包含卷积层和注意力模块，但如何最佳地组合它们仍不清楚。我们分析了3D点云网络中不同计算模块的作用，并发现一种直观的行为：在早期层中，卷积层足以在高分辨率下提取低级几何信息，而此时注意力模块的开销大但无明显收益；注意力模块在低分辨率、深层网络中更高效地捕捉高级语义和上下文信息。基于这一设计原则，我们提出了一种新的改进型3D点云主干网络，早期阶段使用卷积层，深层阶段切换为注意力模块。为了在丢弃冗余卷积层时不丢失空间布局信息，我们引入了一种新颖的、无需训练的3D位置编码方法PointROPE。最终的LitePT模型参数量比最先进的Point Transformer V3少3.6倍，运行速度提升2倍，内存使用减少2倍，但在多种任务和数据集上表现与之相当甚至更优。代码和模型可在https://github.com/prs-eth/LitePT获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to improve the efficiency of 3D point cloud processing by optimizing the combination of convolutional layers and attention blocks. The authors propose LitePT, a new backbone architecture that uses convolutions in early layers for high-resolution geometry extraction and switches to attention mechanisms in deeper layers for capturing high-level semantics. They introduce PointROPE, a training-free 3D positional encoding method to retain spatial layout information. Experimental results show that LitePT has 3.6 times fewer parameters, runs 2 times faster, and uses 2 times less memory than Point Transformer V3, while achieving comparable or better performance on various tasks and datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过优化卷积层和注意力块的组合来提高3D点云处理的效率。作者提出了LitePT，一种新的主干网络架构，利用卷积在早期层提取低级几何信息，并在深层切换为注意力机制以捕捉高级语义。他们引入了PointROPE，一种无需训练的3D位置编码方法，以保留空间布局信息同时减少冗余卷积层。实验结果表明，LitePT的参数量减少了3.6倍，运行速度是Point Transformer V3的两倍，内存使用量减半，同时在多个任务和数据集上表现与之相当或更优。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Scalable Pre-training of Visual Tokenizers for Generation</div>
<div class="meta-line">Authors: Jingfeng Yao, Yuda Song, Yucong Zhou, Xinggang Wang</div>
<div class="meta-line">First: 2025-12-15T18:59:54+00:00 · Latest: 2025-12-15T18:59:54+00:00</div>
<div class="meta-line">Comments: Our pre-trained models are available at https://github.com/MiniMax-AI/VTP</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13687v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13687v1">PDF</a> · <a href="https://github.com/MiniMax-AI/VTP">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The quality of the latent space in visual tokenizers (e.g., VAEs) is crucial for modern generative models. However, the standard reconstruction-based training paradigm produces a latent space that is biased towards low-level information, leading to a foundation flaw: better pixel-level accuracy does not lead to higher-quality generation. This implies that pouring extensive compute into visual tokenizer pre-training translates poorly to improved performance in generation. We identify this as the ``pre-training scaling problem`` and suggest a necessary shift: to be effective for generation, a latent space must concisely represent high-level semantics. We present VTP, a unified visual tokenizer pre-training framework, pioneering the joint optimization of image-text contrastive, self-supervised, and reconstruction losses. Our large-scale study reveals two principal findings: (1) understanding is a key driver of generation, and (2) much better scaling properties, where generative performance scales effectively with compute, parameters, and data allocated to the pretraining of the visual tokenizer. After large-scale pre-training, our tokenizer delivers a competitive profile (78.2 zero-shot accuracy and 0.36 rFID on ImageNet) and 4.1 times faster convergence on generation compared to advanced distillation methods. More importantly, it scales effectively: without modifying standard DiT training specs, solely investing more FLOPS in pretraining VTP achieves 65.8\% FID improvement in downstream generation, while conventional autoencoder stagnates very early at 1/10 FLOPS. Our pre-trained models are available at https://github.com/MiniMax-AI/VTP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向生成的可扩展视觉分词器预训练</div>
<div class="mono" style="margin-top:8px">视觉分词器（如VAE）的潜在空间质量对现代生成模型至关重要。然而，标准的基于重建的训练范式导致潜在空间偏向于低层次信息，从而产生一个基础性缺陷：更高的像素级准确性并不意味着生成质量的提升。这表明，将大量计算资源投入到视觉分词器的预训练中，无法有效转化为生成性能的提升。我们将这一问题称为 ``预训练扩展问题``，并提出必要的转变：为了有效支持生成，潜在空间必须能够简洁地表示高层次语义。我们提出了VTP，一个统一的视觉分词器预训练框架，首次联合优化了图像-文本对比、自监督和重建损失。我们的大规模研究揭示了两个主要发现：(1) 理解是生成的关键驱动因素；(2) 在视觉分词器预训练中，生成性能能够随着计算资源、参数和数据量的增加而有效扩展。在大规模预训练后，我们的分词器在ImageNet上实现了具有竞争力的性能（78.2零样本准确率和0.36 rFID），并且在生成任务上的收敛速度比先进的蒸馏方法快4.1倍。更重要的是，它具有良好的扩展性：在不修改标准DiT训练规范的情况下，仅通过增加VTP预训练的FLOPS，就能在下游生成任务中实现65.8\%的FID提升，而传统自编码器在仅1/10 FLOPS时就停滞不前。我们的预训练模型可在https://github.com/MiniMax-AI/VTP获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research addresses the limitation of visual tokenizers in generative models, where standard reconstruction-based training leads to a latent space biased towards low-level details, negatively impacting generation quality. The proposed method, VTP, introduces a unified pre-training framework that jointly optimizes image-text contrastive, self-supervised, and reconstruction losses. Experimental results show that VTP significantly improves generative performance, achieving a 65.8% FID improvement with increased pre-training compute, while conventional autoencoders show limited scaling benefits. The pre-trained tokenizer also demonstrates competitive zero-shot accuracy on ImageNet and faster convergence in generation tasks compared to advanced distillation methods.</div>
<div class="mono" style="margin-top:8px">本文指出，传统的基于重建的视觉分词器训练会导致潜在空间偏向低层次图像细节，从而无法提升生成质量。作者提出了VTP，一个统一的预训练框架，联合优化图像-文本对比、自监督和重建损失，以构建能够捕捉高层次语义的潜在空间。大规模实验表明，VTP在增加预训练计算量、参数和数据量时，无需修改下游训练规格即可实现比传统自编码器高65.8%的FID改进，显示出更优的扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Recurrent Video Masked Autoencoders</div>
<div class="meta-line">Authors: Daniel Zoran, Nikhil Parthasarathy, Yi Yang, Drew A Hudson, Joao Carreira, Andrew Zisserman</div>
<div class="meta-line">First: 2025-12-15T18:59:48+00:00 · Latest: 2025-12-15T18:59:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13684v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13684v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Recurrent Video Masked-Autoencoders (RVM): a novel video representation learning approach that uses a transformer-based recurrent neural network to aggregate dense image features over time, effectively capturing the spatio-temporal structure of natural video data. RVM learns via an asymmetric masked prediction task requiring only a standard pixel reconstruction objective. This design yields a highly efficient ``generalist&#x27;&#x27; encoder: RVM achieves competitive performance with state-of-the-art video models (e.g. VideoMAE, V-JEPA) on video-level tasks like action recognition and point/object tracking, while also performing favorably against image models (e.g. DINOv2) on tasks that test geometric and dense spatial understanding. Notably, RVM achieves strong performance in the small-model regime without requiring knowledge distillation, exhibiting up to 30x greater parameter efficiency than competing video masked autoencoders. Moreover, we demonstrate that RVM&#x27;s recurrent nature allows for stable feature propagation over long temporal horizons with linear computational cost, overcoming some of the limitations of standard spatio-temporal attention-based architectures. Finally, we use qualitative visualizations to highlight that RVM learns rich representations of scene semantics, structure, and motion.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>循环视频掩码自编码器</div>
<div class="mono" style="margin-top:8px">我们提出了循环视频掩码自编码器（RVM）：一种新颖的视频表征学习方法，利用基于Transformer的循环神经网络在时间维度上聚合密集的图像特征，从而有效捕捉自然视频数据的时空结构。RVM通过一个不对称的掩码预测任务进行学习，仅需要标准的像素重建目标。这种设计产生了一个高度高效的『通用型』编码器：RVM在视频级任务（如动作识别和点/物体跟踪）上实现了与最先进的视频模型（如VideoMAE、V-JEPA）相当的性能，同时在测试几何和密集空间理解的任务上也表现出色于图像模型（如DINOv2）。值得注意的是，RVM在无需知识蒸馏的情况下，在小型模型范围内实现了强劲的性能，其参数效率比其他视频掩码自编码器高出多达30倍。此外，我们证明RVM的循环特性允许在长时序范围内以线性计算成本实现稳定的特征传播，克服了标准时空注意力架构的一些局限性。最后，我们通过定性可视化展示RVM学习了丰富的场景语义、结构和运动表征。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research is to develop an efficient and effective video representation learning method that captures spatio-temporal structure without requiring complex training procedures. The proposed method, Recurrent Video Masked Autoencoders (RVM), employs a transformer-based recurrent neural network to aggregate dense image features over time. Experimental results show that RVM achieves competitive performance on video-level tasks such as action recognition and object tracking compared to state-of-the-art models like VideoMAE and V-JEPA, and also performs well against image models like DINOv2 on tasks involving geometric and spatial understanding. Additionally, RVM demonstrates superior parameter efficiency, up to 30 times that of other video masked autoencoders, and enables stable feature propagation over long time periods with linear computational cost.</div>
<div class="mono" style="margin-top:8px">本研究的动机是开发一种高效且有效的视频表征学习方法，能够在不采用复杂训练流程的情况下捕捉时空结构。所提出的方法 Recurrent Video Masked Autoencoders (RVM) 使用基于变压器的循环神经网络来聚合密集的图像特征。实验结果表明，RVM 在视频级任务如动作识别和物体跟踪上表现与 VideoMAE 和 V-JEPA 等先进模型相当，并且在几何和空间理解任务上也优于图像模型如 DINOv2。此外，RVM 展现出高达其他视频掩码自编码器 30 倍的参数效率，并且能够在长时序上实现稳定的特征传播，计算成本呈线性增长。</div>
</details>
</div>
<div class="card">
<div class="title">I-Scene: 3D Instance Models are Implicit Generalizable Spatial Learners</div>
<div class="meta-line">Authors: Lu Ling, Yunhao Ge, Yichen Sheng, Aniket Bera</div>
<div class="meta-line">First: 2025-12-15T18:59:13+00:00 · Latest: 2025-12-15T18:59:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13683v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13683v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://luling06.github.io/I-Scene-project/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generalization remains the central challenge for interactive 3D scene generation. Existing learning-based approaches ground spatial understanding in limited scene dataset, restricting generalization to new layouts. We instead reprogram a pre-trained 3D instance generator to act as a scene level learner, replacing dataset-bounded supervision with model-centric spatial supervision. This reprogramming unlocks the generator transferable spatial knowledge, enabling generalization to unseen layouts and novel object compositions. Remarkably, spatial reasoning still emerges even when the training scenes are randomly composed objects. This demonstrates that the generator&#x27;s transferable scene prior provides a rich learning signal for inferring proximity, support, and symmetry from purely geometric cues. Replacing widely used canonical space, we instantiate this insight with a view-centric formulation of the scene space, yielding a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model. Quantitative and qualitative results show that a 3D instance generator is an implicit spatial learner and reasoner, pointing toward foundation models for interactive 3D scene understanding and generation. Project page: https://luling06.github.io/I-Scene-project/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>I-Scene：3D实例模型是隐式可泛化的空间学习者</div>
<div class="mono" style="margin-top:8px">泛化能力仍然是交互式3D场景生成的核心挑战。现有的基于学习的方法将空间理解建立在有限的场景数据集上，限制了其对新布局的泛化能力。我们相反地将一个预训练的3D实例生成器重新编程为场景级学习者，用以模型为中心的空间监督替代数据集受限的监督。这种重新编程释放了生成器可迁移的空间知识，使其能够泛化到未见过的布局和新的物体组合。令人惊讶的是，即使训练场景由随机组合的物体构成，空间推理仍然能够出现。这表明生成器的可迁移场景先验为从纯粹的几何线索中推断邻近性、支撑关系和对称性提供了丰富的学习信号。我们用以视角为中心的场景空间形式化来实现这一见解，从而得到一个完全前馈、可泛化的场景生成器，该生成器能够直接从实例模型中学习空间关系。定量和定性结果表明，3D实例生成器是一种隐式空间学习者和推理者，这为交互式3D场景理解和生成的基础模型提供了方向。项目页面：https://luling06.github.io/I-Scene-project/</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the challenge of generalization in interactive 3D scene generation, where existing methods are limited by dataset-specific spatial understanding. The authors propose reprogramming a pre-trained 3D instance generator to function as a scene-level learner, using model-centric spatial supervision instead of dataset-bounded supervision. This approach enables the generator to transfer spatial knowledge across unseen layouts and novel object compositions. Experimental results show that spatial reasoning emerges even with randomly composed training scenes, indicating that the model&#x27;s transferable scene prior can infer proximity, support, and symmetry from geometric cues alone. The method introduces a view-centric formulation of scene space, leading to a fully feed-forward, generalizable scene generator that learns spatial relations directly from the instance model.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决交互式3D场景生成中的泛化问题，现有方法受限于数据集特定的空间理解。作者提出将预训练的3D实例生成器重新编程为场景级学习者，采用以模型为中心的空间监督替代数据集绑定监督。这种方法使生成器能够跨不同布局和物体组合迁移空间知识。实验表明，即使训练场景由随机组合的物体构成，空间推理仍能出现，说明模型的可迁移场景先验为从几何线索中推断邻近、支撑和对称性提供了强大的学习信号。</div>
</details>
</div>
<div class="card">
<div class="title">LASER: Layer-wise Scale Alignment for Training-Free Streaming 4D Reconstruction</div>
<div class="meta-line">Authors: Tianye Ding, Yiming Xie, Yiqing Liang, Moitreya Chatterjee, Pedro Miraldo, Huaizu Jiang</div>
<div class="meta-line">First: 2025-12-15T18:59:04+00:00 · Latest: 2025-12-15T18:59:04+00:00</div>
<div class="meta-line">Comments: 16 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13680v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13680v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://neu-vi.github.io/LASER/}{\texttt{https://neu-vi.github.io/LASER/}}$">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent feed-forward reconstruction models like VGGT and $π^3$ achieve impressive reconstruction quality but cannot process streaming videos due to quadratic memory complexity, limiting their practical deployment. While existing streaming methods address this through learned memory mechanisms or causal attention, they require extensive retraining and may not fully leverage the strong geometric priors of state-of-the-art offline models. We propose LASER, a training-free framework that converts an offline reconstruction model into a streaming system by aligning predictions across consecutive temporal windows. We observe that simple similarity transformation ($\mathrm{Sim}(3)$) alignment fails due to layer depth misalignment: monocular scale ambiguity causes relative depth scales of different scene layers to vary inconsistently between windows. To address this, we introduce layer-wise scale alignment, which segments depth predictions into discrete layers, computes per-layer scale factors, and propagates them across both adjacent windows and timestamps. Extensive experiments show that LASER achieves state-of-the-art performance on camera pose estimation and point map reconstruction %quality with offline models while operating at 14 FPS with 6 GB peak memory on a RTX A6000 GPU, enabling practical deployment for kilometer-scale streaming videos. Project website: $\href{https://neu-vi.github.io/LASER/}{\texttt{https://neu-vi.github.io/LASER/}}$</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LASER: 分层尺度对齐用于无训练的流式4D重建</div>
<div class="mono" style="margin-top:8px">近期的前馈重建模型如 VGGT 和 $π^3$ 虽然实现了令人印象深刻的重建质量，但由于二次内存复杂度，无法处理流式视频，限制了其实际部署。虽然现有的流式方法通过学习的内存机制或因果注意力来解决这一问题，但它们需要大量的重新训练，并可能无法充分利用当前最先进的离线模型的强几何先验。我们提出 LASER，一个无需训练的框架，通过在连续时间窗口之间对齐预测，将离线重建模型转换为流式系统。我们观察到，简单的相似变换 ($\mathrm{Sim}(3)$) 对齐由于层深度对齐错误而失败：单目尺度模糊性导致不同场景层的相对深度尺度在窗口之间不一致。为了解决这个问题，我们引入了分层尺度对齐，将深度预测分割为离散层，计算每层的尺度因子，并在相邻窗口和时间戳之间进行传播。大量实验表明，LASER 在相机姿态估计和点图重建方面实现了与离线模型相当的性能质量，同时在 RTX A6000 GPU 上以 14 FPS 的速度运行，峰值内存为 6 GB，使得其适用于千米级流式视频的实际部署。项目网站: $\href{https://neu-vi.github.io/LASER/}{\texttt{https://neu-vi.github.io/LASER/}}$</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to enable efficient streaming video reconstruction without the need for retraining, addressing the limitations of existing feed-forward models like VGGT and $π^3$ which suffer from quadratic memory complexity. The proposed method, LASER, introduces a training-free approach that aligns predictions across consecutive temporal windows by segmenting depth outputs into discrete layers and computing per-layer scale factors. This allows for consistent depth scaling across time, overcoming the issue of monocular scale ambiguity. Experimental results demonstrate that LASER achieves state-of-the-art performance in camera pose estimation and point map reconstruction while maintaining a practical frame rate of 14 FPS and a peak memory usage of 6 GB on a RTX A6000 GPU, making it suitable for kilometer-scale streaming videos.</div>
<div class="mono" style="margin-top:8px">本研究的动机是实现无需训练的高效视频流重建，解决现有前馈模型如VGGT和$\pi^3$因二次内存复杂度而无法处理视频流的问题。所提出的方法LASER通过将深度预测分割为离散层并计算每层的尺度因子，引入了一种无需训练的框架，以在连续时间窗口间对齐预测结果。这种方法能够有效传播尺度信息，提升相机姿态估计和点云重建的性能。实验结果表明，LASER在保持与离线模型相当的重建质量的同时，能够在RTX A6000 GPU上实现14 FPS的处理速度和6 GB的峰值内存使用。</div>
</details>
</div>
<div class="card">
<div class="title">Feedforward 3D Editing via Text-Steerable Image-to-3D</div>
<div class="meta-line">Authors: Ziqi Ma, Hongqiao Chen, Yisong Yue, Georgia Gkioxari</div>
<div class="meta-line">First: 2025-12-15T18:58:55+00:00 · Latest: 2025-12-15T18:58:55+00:00</div>
<div class="meta-line">Comments: https://glab-caltech.github.io/steer3d/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13678v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13678v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://glab-caltech.github.io/steer3d/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in image-to-3D has opened up immense possibilities for design, AR/VR, and robotics. However, to use AI-generated 3D assets in real applications, a critical requirement is the capability to edit them easily. We present a feedforward method, Steer3D, to add text steerability to image-to-3D models, which enables editing of generated 3D assets with language. Our approach is inspired by ControlNet, which we adapt to image-to-3D generation to enable text steering directly in a forward pass. We build a scalable data engine for automatic data generation, and develop a two-stage training recipe based on flow-matching training and Direct Preference Optimization (DPO). Compared to competing methods, Steer3D more faithfully follows the language instruction and maintains better consistency with the original 3D asset, while being 2.4x to 28.5x faster. Steer3D demonstrates that it is possible to add a new modality (text) to steer the generation of pretrained image-to-3D generative models with 100k data. Project website: https://glab-caltech.github.io/steer3d/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过文本引导的图像到三维的前馈三维编辑</div>
<div class="mono" style="margin-top:8px">图像到三维生成领域的最新进展为设计、AR/VR和机器人技术打开了巨大的可能性。然而，为了在实际应用中使用AI生成的三维资产，一个关键要求是能够轻松地对其进行编辑。我们提出了一种前馈方法Steer3D，用于为图像到三维模型添加文本引导能力，从而实现通过语言对生成的三维资产进行编辑。我们的方法受到ControlNet的启发，并将其适应到图像到三维生成中，使文本引导能够在一次前向传递中直接实现。我们构建了一个可扩展的数据引擎用于自动数据生成，并基于流匹配训练和直接偏好优化（DPO）开发了两阶段的训练方案。与竞争方法相比，Steer3D更忠实地遵循语言指令，并且在保持与原始三维资产的一致性方面表现更好，同时速度提升了2.4到28.5倍。Steer3D展示了在10万数据量下，可以将新的模态（文本）添加到预训练的图像到三维生成模型中以引导生成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to enhance the usability of AI-generated 3D assets by enabling language-driven editing. The authors introduce Steer3D, a feedforward method that integrates text steerability into image-to-3D models, inspired by the ControlNet framework. They develop a scalable data engine for automatic generation and employ a two-stage training approach combining flow-matching and Direct Preference Optimization (DPO). Experimental results show that Steer3D more accurately follows language instructions and preserves better consistency with the original 3D asset compared to existing methods, while achieving speed improvements of 2.4x to 28.5x.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过文本指令提升AI生成3D资产的可用性，使其更容易进行编辑。提出的方法Steer3D借鉴了ControlNet的思想，将文本控制应用于图像到3D的生成过程中，实现单次前向传递的文本引导。该方法利用可扩展的数据引擎进行自动数据生成，并采用结合流匹配训练和直接偏好优化（DPO）的两阶段训练流程。实验结果表明，Steer3D在遵循语言指令的准确性以及与原始3D资产的一致性方面优于现有方法，同时速度提升了2.4到28.5倍。</div>
</details>
</div>
<div class="card">
<div class="title">Active 6D Pose Estimation for Textureless Objects using Multi-View RGB Frames</div>
<div class="meta-line">Authors: Jun Yang, Wenjie Xue, Sahar Ghavidel, Steven L. Waslander</div>
<div class="meta-line">First: 2025-03-05T18:28:32+00:00 · Latest: 2025-12-15T18:58:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.03726v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.03726v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://trailab.github.io/ActiveODPE">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Estimating the 6D pose of textureless objects from RGB images is an important problem in robotics. Due to appearance ambiguities, rotational symmetries, and severe occlusions, single-view based 6D pose estimators are still unable to handle a wide range of objects, motivating research towards multi-view pose estimation and next-best-view prediction that addresses these limitations. In this work, we propose a comprehensive active perception framework for estimating the 6D poses of textureless objects using only RGB images. Our approach is built upon a key idea: decoupling the 6D pose estimation into a two-step sequential process can greatly improve both accuracy and efficiency. First, we estimate the 3D translation of each object, resolving scale and depth ambiguities inherent to RGB images. These estimates are then used to simplify the subsequent task of determining the 3D orientation, which we achieve through canonical scale template matching. Building on this formulation, we then introduce an active perception strategy that predicts the next best camera viewpoint to capture an RGB image, effectively reducing object pose uncertainty and enhancing pose accuracy. We evaluate our method on the public ROBI and TOD datasets, as well as on our reconstructed transparent object dataset, T-ROBI. Under the same camera viewpoints, our multi-view pose estimation significantly outperforms state-of-the-art approaches. Furthermore, by leveraging our next-best-view strategy, our approach achieves high pose accuracy with fewer viewpoints than heuristic-based policies across all evaluated datasets. The accompanying video and T-ROBI dataset will be released on our project page: https://trailab.github.io/ActiveODPE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用多视角RGB图像进行无纹理物体的主动6D位姿估计</div>
<div class="mono" style="margin-top:8px">从RGB图像中估计无纹理物体的6D位姿是机器人学中的一个重要问题。由于外观歧义、旋转对称性和严重遮挡，基于单视角的6D位姿估计器仍无法处理广泛种类的物体，这推动了针对这些限制的多视角位姿估计和最佳下一视角预测的研究。在本工作中，我们提出了一种全面的主动感知框架，仅使用RGB图像来估计无纹理物体的6D位姿。我们的方法基于一个关键思想：将6D位姿估计解耦为一个两步的顺序过程，可以显著提高准确性和效率。首先，我们估计每个物体的3D平移，解决RGB图像中固有的尺度和深度歧义。随后，利用这些估计结果简化3D方向的确定任务，我们通过规范尺度模板匹配来实现这一目标。在此基础上，我们引入了一种主动感知策略，预测最佳的下一相机视角以捕捉RGB图像，从而有效降低物体位姿的不确定性并提高位姿估计的准确性。我们在公开的ROBI和TOD数据集以及我们重建的透明物体数据集T-ROBI上评估了我们的方法。在相同的相机视角下，我们的多视角位姿估计显著优于当前最先进的方法。此外，通过利用我们的最佳下一视角策略，我们的方法在所有评估数据集上，使用比基于启发式策略更少的视角即可实现高位姿精度。相关视频和T-ROBI数据集将在我们的项目页面上发布：https://trailab.github.io/ActiveODPE.</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the challenges of estimating 6D poses for textureless objects using single-view RGB images, such as appearance ambiguities, rotational symmetries, and occlusions. The proposed method decouples 6D pose estimation into two sequential steps: first estimating 3D translation to resolve scale and depth ambiguities, then determining 3D orientation through canonical scale template matching. The active perception strategy further predicts the next best camera viewpoint to reduce pose uncertainty and improve accuracy. Experimental results on ROBI, TOD, and T-ROBI datasets show that the multi-view approach outperforms state-of-the-art methods under the same viewpoints, and achieves high accuracy with fewer views compared to heuristic-based policies.</div>
<div class="mono" style="margin-top:8px">由于外观模糊性、旋转对称性和严重遮挡，从RGB图像中估计无纹理物体的6D位姿仍然是一个挑战，这限制了单视角方法的有效性。本文提出了一种主动感知框架，将6D位姿估计分为两个顺序步骤：首先估计3D平移以解决尺度和深度的模糊性，然后通过规范尺度模板匹配确定3D方向。该框架还引入了一种预测最佳下一视角的策略以减少位姿不确定性。在ROBI、TOD和T-ROBI数据集上的实验结果表明，多视角方法在位姿估计精度上显著优于现有方法，并且在所有评估数据集中，使用该策略的方案在较少视角下仍能实现高精度的位姿估计。</div>
</details>
</div>
<div class="card">
<div class="title">JoVA: Unified Multimodal Learning for Joint Video-Audio Generation</div>
<div class="meta-line">Authors: Xiaohu Huang, Hao Zhou, Qiangpeng Yang, Shilei Wen, Kai Han</div>
<div class="meta-line">First: 2025-12-15T18:58:18+00:00 · Latest: 2025-12-15T18:58:18+00:00</div>
<div class="meta-line">Comments: Project page: \url{https://visual-ai.github.io/jova}</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13677v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13677v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://visual-ai.github.io/jova">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we present JoVA, a unified framework for joint video-audio generation. Despite recent encouraging advances, existing methods face two critical limitations. First, most existing approaches can only generate ambient sounds and lack the capability to produce human speech synchronized with lip movements. Second, recent attempts at unified human video-audio generation typically rely on explicit fusion or modality-specific alignment modules, which introduce additional architecture design and weaken the model simplicity of the original transformers. To address these issues, JoVA employs joint self-attention across video and audio tokens within each transformer layer, enabling direct and efficient cross-modal interaction without the need for additional alignment modules. Furthermore, to enable high-quality lip-speech synchronization, we introduce a simple yet effective mouth-area loss based on facial keypoint detection, which enhances supervision on the critical mouth region during training without compromising architectural simplicity. Extensive experiments on benchmarks demonstrate that JoVA outperforms or is competitive with both unified and audio-driven state-of-the-art methods in lip-sync accuracy, speech quality, and overall video-audio generation fidelity. Our results establish JoVA as an elegant framework for high-quality multimodal generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>JoVA：面向联合视频-音频生成的统一多模态学习</div>
<div class="mono" style="margin-top:8px">本文提出JoVA，一个用于联合视频-音频生成的统一框架。尽管近期取得了令人鼓舞的进展，现有方法仍面临两个关键限制。首先，大多数现有方法只能生成环境音，缺乏与唇部动作同步生成人类语音的能力。其次，近期尝试统一人类视频-音频生成的方法通常依赖显式的融合或模态特定的对齐模块，这引入了额外的架构设计并削弱了原始Transformer模型的简洁性。为了解决这些问题，JoVA在每个Transformer层中采用跨视频和音频标记的联合自注意力机制，使得模态间可以直接且高效地交互，而无需额外的对齐模块。此外，为了实现高质量的唇语-语音同步，我们引入了一个基于面部关键点检测的简单而有效的唇部区域损失，该损失在训练过程中增强了对关键唇部区域的监督，同时不损害架构的简洁性。在基准数据集上的大量实验表明，JoVA在唇同步准确性、语音质量和整体视频-音频生成保真度方面均优于或与现有的统一和音频驱动的最先进方法具有竞争力。我们的结果确立了JoVA作为高质量多模态生成的优雅框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind JoVA is to overcome the limitations of existing methods in generating synchronized human speech and lip movements in video-audio generation. JoVA introduces a unified framework that uses joint self-attention within each transformer layer to enable direct cross-modal interaction between video and audio tokens, eliminating the need for explicit fusion or alignment modules. Experimental results show that JoVA achieves superior performance in lip-sync accuracy, speech quality, and overall video-audio generation fidelity compared to state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">JoVA的动机是解决现有方法在视频音频生成中同步人类语音和嘴唇运动方面的不足。JoVA提出一个统一框架，通过在每个Transformer层中使用联合自注意力机制，实现视频和音频标记之间的直接跨模态交互，无需额外的融合或对齐模块。实验结果表明，JoVA在唇同步准确性、语音质量和整体视频音频生成保真度方面均优于或与最先进的方法相当，包括统一和音频驱动的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Interactive Intelligence for Digital Humans</div>
<div class="meta-line">Authors: Yiyi Cai, Xuangeng Chu, Xiwei Gao, Sitong Gong, Yifei Huang, Caixin Kang, Kunhang Li, Haiyang Liu, Ruicong Liu, Yun Liu, Dianwen Ng, Zixiong Su, Erwin Wu, Yuhan Wu, Dingkun Yan, Tianyu Yan, Chang Zeng, Bo Zheng, You Zhou</div>
<div class="meta-line">First: 2025-12-15T18:57:35+00:00 · Latest: 2025-12-15T18:57:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13674v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13674v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Interactive Intelligence, a novel paradigm of digital human that is capable of personality-aligned expression, adaptive interaction, and self-evolution. To realize this, we present Mio (Multimodal Interactive Omni-Avatar), an end-to-end framework composed of five specialized modules: Thinker, Talker, Face Animator, Body Animator, and Renderer. This unified architecture integrates cognitive reasoning with real-time multimodal embodiment to enable fluid, consistent interaction. Furthermore, we establish a new benchmark to rigorously evaluate the capabilities of interactive intelligence. Extensive experiments demonstrate that our framework achieves superior performance compared to state-of-the-art methods across all evaluated dimensions. Together, these contributions move digital humans beyond superficial imitation toward intelligent interaction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向数字人类的交互智能</div>
<div class="mono" style="margin-top:8px">我们引入交互智能，这是一种能够实现个性对齐表达、自适应交互和自我演化的数字人类新范式。为实现这一目标，我们提出了Mio（多模态交互全息化身），一个由五个专用模块组成的端到端框架：Thinker、Talker、Face Animator、Body Animator和Renderer。该统一架构将认知推理与实时多模态具身化相结合，从而实现流畅且一致的交互。此外，我们建立了一个新的基准，以严格评估交互智能的能力。大量实验表明，我们的框架在所有评估维度上均优于现有最先进的方法。这些贡献使数字人类从表面模仿迈向智能交互。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to develop digital humans that can exhibit intelligent, personality-aligned interactions. The authors propose Mio, an end-to-end framework with five specialized modules, to enable adaptive and self-evolving digital human behavior. Experimental results show that Mio outperforms existing methods in all evaluated dimensions, demonstrating its effectiveness in achieving fluid and consistent interaction through integrated cognitive reasoning and real-time multimodal embodiment.</div>
<div class="mono" style="margin-top:8px">本研究旨在开发能够展现智能、人格一致互动并具备自我演进能力的数字人类。作者提出了Mio框架，该框架由五个专用模块组成，将认知推理与实时多模态具身化相结合。实验结果表明，Mio在所有评估维度上均优于现有方法，证明了其在实现适应性和一致性互动方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Directional Textual Inversion for Personalized Text-to-Image Generation</div>
<div class="meta-line">Authors: Kunhee Kim, NaHyeon Park, Kibeom Hong, Hyunjung Shim</div>
<div class="meta-line">First: 2025-12-15T18:57:07+00:00 · Latest: 2025-12-15T18:57:07+00:00</div>
<div class="meta-line">Comments: Project page: https://kunheek.github.io/dti</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13672v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13672v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://kunheek.github.io/dti">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Textual Inversion (TI) is an efficient approach to text-to-image personalization but often fails on complex prompts. We trace these failures to embedding norm inflation: learned tokens drift to out-of-distribution magnitudes, degrading prompt conditioning in pre-norm Transformers. Empirically, we show semantics are primarily encoded by direction in CLIP token space, while inflated norms harm contextualization; theoretically, we analyze how large magnitudes attenuate positional information and hinder residual updates in pre-norm blocks. We propose Directional Textual Inversion (DTI), which fixes the embedding magnitude to an in-distribution scale and optimizes only direction on the unit hypersphere via Riemannian SGD. We cast direction learning as MAP with a von Mises-Fisher prior, yielding a constant-direction prior gradient that is simple and efficient to incorporate. Across personalization tasks, DTI improves text fidelity over TI and TI-variants while maintaining subject similarity. Crucially, DTI&#x27;s hyperspherical parameterization enables smooth, semantically coherent interpolation between learned concepts (slerp), a capability that is absent in standard TI. Our findings suggest that direction-only optimization is a robust and scalable path for prompt-faithful personalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于个性化文本到图像生成的方向性文本倒置</div>
<div class="mono" style="margin-top:8px">文本倒置（TI）是一种高效的文本到图像个性化方法，但在处理复杂提示时常常失败。我们将这些失败归因于嵌入范数膨胀：学习到的标记会漂移到分布外的尺度，从而损害了预范数Transformer中的提示条件。实证研究表明，在CLIP标记空间中，语义主要由方向编码，而膨胀的范数会损害上下文建模；理论上，我们分析了大范数如何削弱位置信息并阻碍预范数块中的残差更新。我们提出了方向性文本倒置（DTI），通过黎曼SGD将嵌入尺度固定在分布内，并仅在单位超球面上优化方向。我们将方向学习建模为带有von Mises-Fisher先验的MAP估计，从而得到一个简单且高效的常方向先验梯度。在各种个性化任务中，DTI在保持主题相似性的同时，比TI及其变体提升了文本保真度。关键的是，DTI的超球面参数化使得在学习概念之间进行平滑且语义一致的插值（slerp）成为可能，而这是标准TI所不具备的能力。我们的研究结果表明，仅优化方向的方法是实现提示忠实个性化的一种稳健且可扩展的途径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the limitations of Textual Inversion (TI) in generating personalized images from complex text prompts, attributing these issues to embedding norm inflation that disrupts prompt conditioning in pre-norm Transformers. To resolve this, the authors introduce Directional Textual Inversion (DTI), which constrains embedding magnitudes to an in-distribution scale and optimizes only the direction on the unit hypersphere using Riemannian SGD. This approach, framed as a MAP estimation with a von Mises-Fisher prior, allows for efficient and robust direction learning. Experimental results show that DTI enhances text fidelity compared to TI and its variants while preserving subject similarity, and it supports smooth, semantically coherent interpolation between learned concepts through slerp.</div>
<div class="mono" style="margin-top:8px">该论文针对文本倒置（TI）在处理复杂文本提示生成个性化图像时的局限性，指出嵌入向量范数膨胀会破坏预范数Transformer中的提示条件化。为此，作者提出了方向文本倒置（DTI），通过限制嵌入向量的范数在分布内尺度，并使用黎曼SGD仅优化方向。他们将方向学习建模为带有von Mises-Fisher先验的MAP估计，从而实现高效且稳定的优化。实验结果表明，DTI在提升文本保真度方面优于TI及其变体，同时保持主体相似性，并支持通过slerp实现平滑且语义连贯的概念插值。</div>
</details>
</div>
<div class="card">
<div class="title">AgentIAD: Tool-Augmented Single-Agent for Industrial Anomaly Detection</div>
<div class="meta-line">Authors: Junwen Miao, Penghui Du, Yi Liu, Yu Wang, Yan Wang</div>
<div class="meta-line">First: 2025-12-15T18:57:04+00:00 · Latest: 2025-12-15T18:57:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13671v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13671v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Industrial anomaly detection (IAD) is difficult due to the scarcity of normal reference samples and the subtle, localized nature of many defects. Single-pass vision-language models (VLMs) often overlook small abnormalities and lack explicit mechanisms to compare against canonical normal patterns. We propose AgentIAD, a tool-driven agentic framework that enables multi-stage visual inspection. The agent is equipped with a Perceptive Zoomer (PZ) for localized fine-grained analysis and a Comparative Retriever (CR) for querying normal exemplars when evidence is ambiguous. To teach these inspection behaviors, we construct structured perceptive and comparative trajectories from the MMAD dataset and train the model in two stages: supervised fine-tuning followed by reinforcement learning. A two-part reward design drives this process: a perception reward that supervises classification accuracy, spatial alignment, and type correctness, and a behavior reward that encourages efficient tool use. Together, these components enable the model to refine its judgment through step-wise observation, zooming, and verification. AgentIAD achieves a new state-of-the-art 97.62% classification accuracy on MMAD, surpassing prior MLLM-based approaches while producing transparent and interpretable inspection traces.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AgentIAD：用于工业异常检测的工具增强型单智能体</div>
<div class="mono" style="margin-top:8px">由于正常参考样本稀缺以及许多缺陷具有细微且局部化的特性，工业异常检测（IAD）具有挑战性。单次通过的视觉-语言模型（VLMs）通常会忽略小异常，并缺乏与标准正常模式进行显式比较的机制。我们提出了AgentIAD，一个基于工具驱动的智能体框架，支持多阶段的视觉检测。该智能体配备了感知放大器（Perceptive Zoomer, PZ）以实现局部细粒度分析，并配备比较检索器（Comparative Retriever, CR）以在证据模糊时查询正常示例。为了训练这些检测行为，我们从MMAD数据集中构建了结构化的感知和比较轨迹，并采用两个阶段训练模型：监督微调后接强化学习。一个双部分奖励设计驱动这一过程：感知奖励用于监督分类准确性、空间对齐和类型正确性，行为奖励则鼓励高效使用工具。这些组件共同作用，使模型能够通过逐步观察、放大和验证来完善其判断。AgentIAD在MMAD数据集上实现了新的分类准确率97.62%，超越了以往基于MLLM的方法，同时生成了透明且可解释的检测轨迹。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Industrial anomaly detection is challenging due to the lack of normal reference samples and the subtle nature of defects. AgentIAD addresses these issues by introducing a tool-driven agentic framework that includes a Perceptive Zoomer for localized analysis and a Comparative Retriever for querying normal patterns. The model is trained using structured trajectories from the MMAD dataset through supervised fine-tuning and reinforcement learning, with a dual reward system focusing on classification accuracy and tool efficiency. Experimental results show that AgentIAD achieves a classification accuracy of 97.62%, outperforming previous MLLM-based methods and providing interpretable inspection processes.</div>
<div class="mono" style="margin-top:8px">工业异常检测由于缺乏正常参考样本和缺陷的微妙性而具有挑战性。AgentIAD提出了一种工具驱动的智能体框架，包含用于局部分析的感知变焦器和用于查询正常模式的比较检索器。该模型通过从MMAD数据集构建的结构化轨迹进行监督微调和强化学习训练，采用双奖励机制关注分类准确性和工具效率。实验结果显示，AgentIAD在MMAD数据集上实现了97.62%的分类准确率，超越了以往基于MLLM的方法，并提供了可解释的检测过程。</div>
</details>
</div>
<div class="card">
<div class="title">Template-Guided Reconstruction of Pulmonary Segments with Neural Implicit Functions</div>
<div class="meta-line">Authors: Kangxian Xie, Yufei Zhu, Kaiming Kuang, Li Zhang, Hongwei Bran Li, Mingchen Gao, Jiancheng Yang</div>
<div class="meta-line">First: 2025-05-13T19:31:01+00:00 · Latest: 2025-12-15T18:56:33+00:00</div>
<div class="meta-line">Comments: Manuscript accepted by Medical Image Analysis, 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.08919v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.08919v2">PDF</a> · <a href="https://github.com/HINTLab/ImPulSe">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-quality 3D reconstruction of pulmonary segments plays a crucial role in segmentectomy and surgical planning for the treatment of lung cancer. Due to the resolution requirement of the target reconstruction, conventional deep learning-based methods often suffer from computational resource constraints or limited granularity. Conversely, implicit modeling is favored due to its computational efficiency and continuous representation at any resolution. We propose a neural implicit function-based method to learn a 3D surface to achieve anatomy-aware, precise pulmonary segment reconstruction, represented as a shape by deforming a learnable template. Additionally, we introduce two clinically relevant evaluation metrics to comprehensively assess the quality of the reconstruction. Furthermore, to address the lack of publicly available shape datasets for benchmarking reconstruction algorithms, we developed a shape dataset named Lung3D, which includes the 3D models of 800 labeled pulmonary segments and their corresponding airways, arteries, veins, and intersegmental veins. We demonstrate that the proposed approach outperforms existing methods, providing a new perspective for pulmonary segment reconstruction. Code and data will be available at https://github.com/HINTLab/ImPulSe.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于神经隐式函数的肺段重建方法</div>
<div class="mono" style="margin-top:8px">高质量的肺段三维重建在肺段切除术和肺癌治疗的手术规划中起着至关重要的作用。由于目标重建对分辨率的要求，传统基于深度学习的方法常面临计算资源限制或细节粒度不足的问题。相反，隐式建模因其计算效率高且在任意分辨率下均可提供连续表示而受到青睐。我们提出了一种基于神经隐式函数的方法，通过学习三维表面实现解剖学感知、精确的肺段重建，该重建以变形可学习模板的形式表示形状。此外，我们引入了两个具有临床意义的评估指标，以全面评估重建质量。为了解决缺乏用于评估重建算法的公开形状数据集的问题，我们开发了一个名为Lung3D的形状数据集，包含800个标注肺段的三维模型及其对应的支气管、动脉、静脉和段间静脉。我们证明了所提出的方法优于现有方法，为肺段重建提供了新的视角。代码和数据将在https://github.com/HINTLab/ImPulSe上发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research stems from the need for high-quality 3D pulmonary segment reconstruction in lung cancer treatment and surgical planning. The proposed method employs neural implicit functions to learn a 3D surface that deforms a learnable template, enabling anatomy-aware and precise reconstruction. The study introduces two new clinical evaluation metrics and develops a shape dataset called Lung3D, containing 800 labeled pulmonary segments along with their associated airways and vessels. Experimental results show that the method outperforms existing approaches in terms of reconstruction accuracy and efficiency.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高肺段三维重建的质量，以更好地进行肺癌治疗的手术规划。所提出的方法利用神经隐式函数学习三维表面，通过变形可学习的模板实现解剖学感知和精确的肺段重建。研究引入了两个临床相关的评估指标，并开发了一个名为Lung3D的新数据集，包含800个标注的肺段及其对应的支气管、动脉、静脉和段间静脉的三维模型。实验结果表明，该方法在重建精度和效率方面优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">A Scientific Reasoning Model for Organic Synthesis Procedure Generation</div>
<div class="meta-line">Authors: Guoqing Liu, Junren Li, Zihan Zhao, Eray Inanc, Krzysztof Maziarz, Jose Garrido Torres, Victor Garcia Satorras, Shoko Ueda, Christopher M. Bishop, Marwin Segler</div>
<div class="meta-line">First: 2025-12-15T18:55:39+00:00 · Latest: 2025-12-15T18:55:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13668v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13668v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Solving computer-aided synthesis planning is essential for enabling fully automated, robot-assisted synthesis workflows and improving the efficiency of drug discovery. A key challenge, however, is bridging the gap between computational route design and practical laboratory execution, particularly the accurate prediction of viable experimental procedures for each synthesis step. In this work, we present QFANG, a scientific reasoning language model capable of generating precise, structured experimental procedures directly from reaction equations, with explicit chain-of-thought reasoning. To develop QFANG, we curated a high-quality dataset comprising 905,990 chemical reactions paired with structured action sequences, extracted and processed from patent literature using large language models. We introduce a Chemistry-Guided Reasoning (CGR) framework that produces chain-of-thought data grounded in chemical knowledge at scale. The model subsequently undergoes supervised fine-tuning to elicit complex chemistry reasoning. Finally, we apply Reinforcement Learning from Verifiable Rewards (RLVR) to further enhance procedural accuracy. Experimental results demonstrate that QFANG outperforms advanced general-purpose reasoning models and nearest-neighbor retrieval baselines, measured by traditional NLP similarity metrics and a chemically aware evaluator using an LLM-as-a-judge. Moreover, QFANG generalizes to certain out-of-domain reaction classes and adapts to variations in laboratory conditions and user-specific constraints. We believe that QFANG&#x27;s ability to generate high-quality synthesis procedures represents an important step toward bridging the gap between computational synthesis planning and fully automated laboratory synthesis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于有机合成过程生成的科学推理模型</div>
<div class="mono" style="margin-top:8px">解决计算机辅助合成规划对于实现完全自动化的机器人辅助合成流程和提高药物发现效率至关重要。然而，一个关键挑战在于弥合计算路线设计与实际实验室执行之间的差距，尤其是对每个合成步骤中可行实验程序的准确预测。在本工作中，我们提出了QFANG，这是一种能够从反应方程式直接生成精确、结构化的实验程序的科学推理语言模型，并具有显式的推理过程。为开发QFANG，我们整理了一个高质量的数据集，包含905,990个化学反应及其对应的结构化操作序列，这些数据通过大型语言模型从专利文献中提取和处理而来。我们引入了一种化学引导推理（Chemistry-Guided Reasoning, CGR）框架，该框架大规模地生成基于化学知识的推理数据。随后，模型经过监督微调以激发复杂的化学推理能力。最后，我们应用了可验证奖励的强化学习（Reinforcement Learning from Verifiable Rewards, RLVR）来进一步提升程序的准确性。实验结果表明，QFANG在传统NLP相似性指标和使用LLM作为评判者的化学感知评估器的衡量下，优于先进的通用推理模型和最近邻检索基线。此外，QFANG能够推广到某些领域外的反应类别，并适应实验室条件和用户特定约束的变化。我们认为，QFANG生成高质量合成程序的能力是弥合计算合成规划与完全自动化实验室合成之间差距的重要一步。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research is to improve the efficiency of drug discovery by enabling fully automated synthesis workflows through accurate prediction of experimental procedures. The authors developed QFANG, a scientific reasoning language model that generates structured synthesis procedures from reaction equations using explicit chain-of-thought reasoning. They trained QFANG on a large dataset of 905,990 chemical reactions with corresponding action sequences, using a Chemistry-Guided Reasoning framework and fine-tuning with reinforcement learning from verifiable rewards. Experimental results show that QFANG outperforms general-purpose models and retrieval baselines in generating accurate procedures, as evaluated by both traditional NLP metrics and a chemically aware evaluator. Additionally, QFANG demonstrates the ability to generalize to out-of-domain reactions and adapt to varying lab conditions and user constraints.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过精确的计算机辅助合成规划提高药物发现的效率，从而实现全自动化合成工作流程。提出的方法是QFANG，一个能够从反应方程式生成精确结构化实验步骤的科学推理语言模型，采用显式的思维链推理机制。该模型基于包含905,990个化学反应及其对应操作序列的高质量数据集进行训练，并通过化学引导推理框架、监督微调和基于可验证奖励的强化学习进行优化。实验结果表明，QFANG在生成准确合成步骤方面优于通用模型和检索基线，评估采用传统NLP指标和化学意识评估器。此外，QFANG还展示了对超出训练数据范围的反应类别的泛化能力，并能适应不同的实验室条件和用户特定约束。</div>
</details>
</div>
<div class="card">
<div class="title">SEDULity: A Proof-of-Learning Framework for Distributed and Secure Blockchains with Efficient Useful Work</div>
<div class="meta-line">Authors: Weihang Cao, Mustafa Doger, Sennur Ulukus</div>
<div class="meta-line">First: 2025-12-15T18:55:20+00:00 · Latest: 2025-12-15T18:55:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13666v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13666v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The security and decentralization of Proof-of-Work (PoW) have been well-tested in existing blockchain systems. However, its tremendous energy waste has raised concerns about sustainability. Proof-of-Useful-Work (PoUW) aims to redirect the meaningless computation to meaningful tasks such as solving machine learning (ML) problems, giving rise to the branch of Proof-of-Learning (PoL). While previous studies have proposed various PoLs, they all, to some degree, suffer from security, decentralization, or efficiency issues. In this paper, we propose a PoL framework that trains ML models efficiently while maintaining blockchain security in a fully distributed manner. We name the framework SEDULity, which stands for a Secure, Efficient, Distributed, and Useful Learning-based blockchain system. Specifically, we encode the template block into the training process and design a useful function that is difficult to solve but relatively easy to verify, as a substitute for the PoW puzzle. We show that our framework is distributed, secure, and efficiently trains ML models. We further demonstrate that the proposed PoL framework can be extended to other types of useful work and design an incentive mechanism to incentivize task verification. We show theoretically that a rational miner is incentivized to train fully honestly with well-designed system parameters. Finally, we present simulation results to demonstrate the performance of our framework and validate our analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SEDULity：一种用于高效有用工作的分布式和安全区块链的证明学习框架</div>
<div class="mono" style="margin-top:8px">现有的区块链系统已经对工作量证明（PoW）的安全性和去中心化进行了充分验证。然而，其巨大的能源浪费引发了对可持续性的担忧。有用工作证明（PoUW）旨在将无意义的计算转向有意义的任务，如解决机器学习（ML）问题，从而催生了证明学习（PoL）这一分支。尽管已有研究提出了多种PoL方案，但它们在不同程度上都存在安全、去中心化或效率方面的问题。本文提出了一种PoL框架，能够在完全分布式的方式下保持区块链安全的同时高效训练ML模型。我们将该框架命名为SEDULity，意指一种基于学习的安全、高效、分布式和有用的区块链系统。具体而言，我们将模板区块编码到训练过程中，并设计了一个难以解决但相对容易验证的有用函数，以替代PoW难题。我们证明了该框架具有分布式、安全和高效训练ML模型的特性。进一步地，我们展示了所提出的PoL框架可以扩展到其他类型的有用工作，并设计了一种激励机制以鼓励任务验证。我们从理论上证明，在系统参数设计合理的情况下，理性矿工会被激励以完全诚实的方式进行模型训练。最后，我们展示了仿真结果以验证该框架的性能并支持我们的分析。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the energy inefficiency of Proof-of-Work (PoW) in blockchain systems by proposing a new Proof-of-Learning (PoL) framework called SEDULity. This framework integrates machine learning training into the blockchain consensus process, using a template block encoding method and a useful function that is computationally challenging to solve but easy to verify. Experimental results show that SEDULity maintains blockchain security and decentralization while efficiently training ML models, and it can be extended to support other useful work tasks with an incentive mechanism that encourages honest participation from miners.</div>
<div class="mono" style="margin-top:8px">本文针对区块链系统中Proof-of-Work（PoW）的高能耗问题，提出了一种新的Proof-of-Learning（PoL）框架，称为SEDULity。该框架通过将模板区块编码到机器学习模型的训练过程中，并设计一个难以求解但易于验证的有用函数，实现了区块链的安全性和去中心化与高效模型训练的结合。实验结果表明，SEDULity能够有效训练机器学习模型，同时保持区块链的安全和去中心化特性，并可通过激励机制扩展以支持其他有用工作，鼓励矿工诚实参与。</div>
</details>
</div>
<div class="card">
<div class="title">Grab-3D: Detecting AI-Generated Videos from 3D Geometric Temporal Consistency</div>
<div class="meta-line">Authors: Wenhan Chen, Sezer Karaoglu, Theo Gevers</div>
<div class="meta-line">First: 2025-12-15T18:54:30+00:00 · Latest: 2025-12-15T18:54:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13665v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13665v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in diffusion-based generation techniques enable AI models to produce highly realistic videos, heightening the need for reliable detection mechanisms. However, existing detection methods provide only limited exploration of the 3D geometric patterns present in generated videos. In this paper, we use vanishing points as an explicit representation of 3D geometry patterns, revealing fundamental discrepancies in geometric consistency between real and AI-generated videos. We introduce Grab-3D, a geometry-aware transformer framework for detecting AI-generated videos based on 3D geometric temporal consistency. To enable reliable evaluation, we construct an AI-generated video dataset of static scenes, allowing stable 3D geometric feature extraction. We propose a geometry-aware transformer equipped with geometric positional encoding, temporal-geometric attention, and an EMA-based geometric classifier head to explicitly inject 3D geometric awareness into temporal modeling. Experiments demonstrate that Grab-3D significantly outperforms state-of-the-art detectors, achieving robust cross-domain generalization to unseen generators.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Grab-3D：基于三维几何时间一致性检测AI生成视频</div>
<div class="mono" style="margin-top:8px">基于扩散模型的生成技术的最新进展使得AI模型能够生成高度逼真的视频，从而加剧了对可靠检测机制的需求。然而，现有检测方法仅对生成视频中的三维几何模式进行了有限的探索。本文中，我们利用消失点作为三维几何模式的显式表示，揭示了真实视频与AI生成视频在几何一致性上的根本差异。我们提出了Grab-3D，一种基于三维几何时间一致性的几何感知Transformer框架用于检测AI生成视频。为了实现可靠的评估，我们构建了一个静态场景的AI生成视频数据集，从而实现稳定的三维几何特征提取。我们提出了一种配备几何位置编码、时间-几何注意力机制以及基于EMA的几何分类器头部的几何感知Transformer，以显式地将三维几何感知注入时间建模中。实验表明，Grab-3D显著优于现有最先进的检测器，实现了对未见过的生成器的鲁棒跨域泛化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing realism of AI-generated videos necessitates effective detection methods. Grab-3D leverages 3D geometric temporal consistency, using vanishing points as a representation to identify discrepancies between real and generated videos. The framework incorporates a geometry-aware transformer with geometric positional encoding, temporal-geometric attention, and an EMA-based classifier head. Experimental results show that Grab-3D outperforms existing detectors and exhibits strong cross-domain generalization capabilities.</div>
<div class="mono" style="margin-top:8px">随着AI生成视频的逼真度不断提高，需要有效的检测方法。Grab-3D通过利用消失点作为三维几何结构的表示，揭示了真实视频与生成视频在几何一致性上的根本差异。该框架采用几何感知的Transformer，结合几何位置编码、时序-几何注意力机制以及基于EMA的分类器头，以增强时序建模中的三维几何感知能力。实验结果表明，Grab-3D在检测性能上显著优于现有方法，并展现出强大的跨领域泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">RoboTracer: Mastering Spatial Trace with Reasoning in Vision-Language Models for Robotics</div>
<div class="meta-line">Authors: Enshen Zhou, Cheng Chi, Yibo Li, Jingkun An, Jiayuan Zhang, Shanyu Rong, Yi Han, Yuheng Ji, Mengzhen Liu, Pengwei Wang, Zhongyuan Wang, Lu Sheng, Shanghang Zhang</div>
<div class="meta-line">First: 2025-12-15T18:52:43+00:00 · Latest: 2025-12-15T18:52:43+00:00</div>
<div class="meta-line">Comments: Project page: https://zhoues.github.io/RoboTracer</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13660v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13660v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://zhoues.github.io/RoboTracer">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial tracing, as a fundamental embodied interaction ability for robots, is inherently challenging as it requires multi-step metric-grounded reasoning compounded with complex spatial referring and real-world metric measurement. However, existing methods struggle with this compositional task. To this end, we propose RoboTracer, a 3D-aware VLM that first achieves both 3D spatial referring and measuring via a universal spatial encoder and a regression-supervised decoder to enhance scale awareness during supervised fine-tuning (SFT). Moreover, RoboTracer advances multi-step metric-grounded reasoning via reinforcement fine-tuning (RFT) with metric-sensitive process rewards, supervising key intermediate perceptual cues to accurately generate spatial traces. To support SFT and RFT training, we introduce TraceSpatial, a large-scale dataset of 30M QA pairs, spanning outdoor/indoor/tabletop scenes and supporting complex reasoning processes (up to 9 steps). We further present TraceSpatial-Bench, a challenging benchmark filling the gap to evaluate spatial tracing. Experimental results show that RoboTracer surpasses baselines in spatial understanding, measuring, and referring, with an average success rate of 79.1%, and also achieves SOTA performance on TraceSpatial-Bench by a large margin, exceeding Gemini-2.5-Pro by 36% accuracy. Notably, RoboTracer can be integrated with various control policies to execute long-horizon, dynamic tasks across diverse robots (UR5, G1 humanoid) in cluttered real-world scenes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboTracer：基于视觉语言模型的机器人空间追踪与推理</div>
<div class="mono" style="margin-top:8px">空间追踪作为机器人的一种基本具身交互能力，本质上具有挑战性，因为它需要多步骤的度量基础推理，并结合复杂的空间指称和现实世界度量。然而，现有方法在处理这种组合任务时表现不佳。为此，我们提出了RoboTracer，这是一种3D感知的视觉语言模型（VLM），通过一个通用的空间编码器和一个回归监督解码器，首先实现3D空间指称和测量，以增强监督微调（SFT）过程中的尺度感知。此外，RoboTracer通过带有度量敏感过程奖励的强化微调（RFT）推进多步骤的度量基础推理，监督关键的中间感知线索，从而准确生成空间轨迹。为支持SFT和RFT训练，我们引入了TraceSpatial，一个包含3000万问答对的大规模数据集，涵盖户外、室内和桌面场景，并支持复杂的推理过程（最多9步）。我们进一步提出了TraceSpatial-Bench，一个具有挑战性的基准测试，填补了评估空间追踪能力的空白。实验结果表明，RoboTracer在空间理解、测量和指称方面均优于基线方法，平均成功率为79.1%，并在TraceSpatial-Bench上取得了显著的SOTA性能，准确率超过Gemini-2.5-Pro 36%。值得注意的是，RoboTracer可以与各种控制策略集成，用于在杂乱的真实世界场景中执行长时域、动态任务（如UR5、G1人形机器人）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">RoboTracer was developed to address the challenges of spatial tracing in robotics, which involves multi-step metric-grounded reasoning and complex spatial referencing. The method employs a 3D-aware vision-language model with a universal spatial encoder and a regression-supervised decoder to improve scale awareness during supervised fine-tuning. It further enhances multi-step reasoning through reinforcement fine-tuning with metric-sensitive rewards. The model is trained on TraceSpatial, a large-scale dataset with 30 million QA pairs, and evaluated on TraceSpatial-Bench, a new benchmark for spatial tracing. Experimental results demonstrate that RoboTracer outperforms existing baselines with a 79.1% average success rate and achieves state-of-the-art performance on the benchmark, surpassing Gemini-2.5-Pro by 36% in accuracy.</div>
<div class="mono" style="margin-top:8px">RoboTracer 是为了解决机器人中空间追踪任务的挑战而开发的，该任务需要多步度量推理和复杂的空间引用。该方法结合了3D感知的视觉语言模型，使用通用空间编码器和回归监督解码器来提升监督微调过程中的尺度感知能力，并通过带有度量敏感奖励的强化微调来增强多步推理能力。模型在 TraceSpatial 数据集上进行训练，该数据集包含3000万对问答对，涵盖户外、室内和桌面场景，并支持复杂的推理过程（最多9步）。在 TraceSpatial-Bench 这一新的空间追踪基准上进行了评估，实验结果表明 RoboTracer 在空间理解、测量和引用方面优于现有基线，平均成功率为79.1%，并在该基准上取得了最先进的性能，比 Gemini-2.5-Pro 准确率高出36%。</div>
</details>
</div>
<div class="card">
<div class="title">Embedding-Based Rankings of Educational Resources based on Learning Outcome Alignment: Benchmarking, Expert Validation, and Learner Performance</div>
<div class="meta-line">Authors: Mohammadreza Molavi, Mohammad Moein, Mohammadreza Tavakoli, Abdolali Faraji, Stefan T. Mol, Gábor Kismihók</div>
<div class="meta-line">First: 2025-12-15T18:51:00+00:00 · Latest: 2025-12-15T18:51:00+00:00</div>
<div class="meta-line">Comments: Accepted for publication at the 16th International Conference on Learning Analytics &amp; Knowledge (LAK 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13658v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13658v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As the online learning landscape evolves, the need for personalization is increasingly evident. Although educational resources are burgeoning, educators face challenges selecting materials that both align with intended learning outcomes and address diverse learner needs. Large Language Models (LLMs) are attracting growing interest for their potential to create learning resources that better support personalization, but verifying coverage of intended outcomes still requires human alignment review, which is costly and limits scalability. We propose a framework that supports the cost-effective automation of evaluating alignment between educational resources and intended learning outcomes. Using human-generated materials, we benchmarked LLM-based text-embedding models and found that the most accurate model (Voyage) achieved 79% accuracy in detecting alignment. We then applied the optimal model to LLM-generated resources and, via expert evaluation, confirmed that it reliably assessed correspondence to intended outcomes (83% accuracy). Finally, in a three-group experiment with 360 learners, higher alignment scores were positively related to greater learning performance, chi-squared(2, N = 360) = 15.39, p &lt; 0.001. These findings show that embedding-based alignment scores can facilitate scalable personalization by confirming alignment with learning outcomes, which allows teachers to focus on tailoring content to diverse learner needs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于学习成果对齐的教育资源嵌入式排名：基准测试、专家验证与学习者表现</div>
<div class="mono" style="margin-top:8px">随着在线学习环境的发展，个性化需求日益凸显。尽管教育资源日益丰富，但教育者在选择既符合预期学习成果又能满足多样化学习者需求的材料时仍面临挑战。大型语言模型（LLMs）因其在创建更支持个性化的学习资源方面的潜力而受到越来越多的关注，但验证其是否覆盖预期成果仍需人工对齐审查，这既昂贵又限制了可扩展性。我们提出了一种框架，支持以低成本自动化评估教育资源与预期学习成果之间的对齐情况。我们使用人工生成的材料对基于文本嵌入的LLM模型进行了基准测试，发现最准确的模型（Voyage）在检测对齐情况时达到了79%的准确率。随后，我们将最优模型应用于LLM生成的资源，并通过专家评估确认其能够可靠地评估与预期成果的对应关系（准确率为83%）。最后，在一项包含360名学习者的三组实验中，较高的对齐得分与更好的学习表现呈正相关，卡方检验（2, N = 360）= 15.39，p &lt; 0.001。这些发现表明，基于嵌入的对齐得分可以通过确认与学习成果的一致性，促进可扩展的个性化，从而使教师能够专注于为不同学习者需求定制内容。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing demand for personalized learning in online education necessitates effective methods to align educational resources with intended learning outcomes. This study proposes an embedding-based framework to automate the evaluation of alignment, using text-embedding models to assess the match between resources and outcomes. Benchmarking against human-generated materials showed that the Voyage model achieved 79% accuracy in detecting alignment, and expert validation confirmed its reliability at 83% accuracy for LLM-generated content. A three-group experiment with 360 learners revealed a statistically significant positive correlation between alignment scores and learning performance, chi-squared(2, N = 360) = 15.39, p &lt; 0.001, demonstrating the potential of embedding-based scores to support scalable personalization.</div>
<div class="mono" style="margin-top:8px">随着在线教育的发展，个性化学习的需求日益增加，这要求教育资源能够与预期的学习成果对齐。本研究提出了一种基于嵌入的框架，用于自动化评估教育资源与学习目标的匹配度，特别测试了基于大型语言模型（LLM）的文本嵌入方法。在人类生成材料的基准测试中，最准确的模型Voyage达到了79%的对齐检测准确率，专家评估进一步确认了其在LLM生成资源中的可靠性，准确率为83%。一项涉及360名学习者的三组实验显示，对齐得分与学习表现之间存在显著正相关（卡方检验，χ²(2, N=360)=15.39, p&lt;0.001）。这些结果表明，基于嵌入的对齐得分可以促进可扩展的个性化学习，使教师能够专注于为不同学习者需求定制内容。</div>
</details>
</div>
<div class="card">
<div class="title">Large-Language Memorization During the Classification of United States Supreme Court Cases</div>
<div class="meta-line">Authors: John E. Ortega, Dhruv D. Joshi, Matt P. Borkowski</div>
<div class="meta-line">First: 2025-12-15T18:47:48+00:00 · Latest: 2025-12-15T18:47:48+00:00</div>
<div class="meta-line">Comments: 7 pages, 1 figure, Appendix of Prompts</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13654v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13654v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-language models (LLMs) have been shown to respond in a variety of ways for classification tasks outside of question-answering. LLM responses are sometimes called &quot;hallucinations&quot; since the output is not what is ex pected. Memorization strategies in LLMs are being studied in detail, with the goal of understanding how LLMs respond. We perform a deep dive into a classification task based on United States Supreme Court (SCOTUS) decisions. The SCOTUS corpus is an ideal classification task to study for LLM memory accuracy because it presents significant challenges due to extensive sentence length, complex legal terminology, non-standard structure, and domain-specific vocabulary. Experimentation is performed with the latest LLM fine tuning and retrieval-based approaches, such as parameter-efficient fine-tuning, auto-modeling, and others, on two traditional category-based SCOTUS classification tasks: one with 15 labeled topics and another with 279. We show that prompt-based models with memories, such as DeepSeek, can be more robust than previous BERT-based models on both tasks scoring about 2 points better than previous models not based on prompting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>美国最高法院案件分类中的大语言模型记忆</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在非问答任务的分类任务中表现出多种响应方式。LLM的响应有时被称为“幻觉”，因为输出结果与预期不符。目前对LLM中的记忆策略进行了详细研究，以理解其响应机制。我们深入探讨了基于美国最高法院（SCOTUS）判决的分类任务。SCOTUS语料库是研究LLM记忆准确性的理想分类任务，因为它具有长句、复杂的法律术语、非标准结构和领域特定词汇等显著挑战。我们使用最新的LLM微调和基于检索的方法，如参数高效微调、自动建模等，在两个传统的基于类别的SCOTUS分类任务上进行实验：一个包含15个标记主题，另一个包含279个。我们证明了基于提示的记忆模型（如DeepSeek）在两个任务中都比之前的BERT模型更稳健，得分大约高出2分。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates the memorization capabilities of large-language models (LLMs) in the context of classifying United States Supreme Court decisions. The researchers focus on understanding how LLMs handle complex legal texts with long sentences, specialized terminology, and non-standard structures. They evaluate various LLM fine-tuning and retrieval-based methods, including parameter-efficient fine-tuning and auto-modeling, on two SCOTUS classification tasks with 15 and 279 labeled topics respectively. The results indicate that prompt-based models with memory, such as DeepSeek, outperform previous BERT-based models, achieving a 2-point improvement in accuracy on both tasks.</div>
<div class="mono" style="margin-top:8px">本研究探讨了大型语言模型（LLM）在处理美国最高法院案件分类任务时的记忆能力。研究人员对比了基于提示的模型如DeepSeek与传统的BERT模型，使用包含15个和279个标签主题的两个分类任务进行实验。结果显示，基于提示的模型在这些任务中表现出更强的鲁棒性和准确性，得分比非提示模型高出约2分。</div>
</details>
</div>
<div class="card">
<div class="title">World Models Can Leverage Human Videos for Dexterous Manipulation</div>
<div class="meta-line">Authors: Raktim Gautam Goswami, Amir Bar, David Fan, Tsung-Yen Yang, Gaoyue Zhou, Prashanth Krishnamurthy, Michael Rabbat, Farshad Khorrami, Yann LeCun</div>
<div class="meta-line">First: 2025-12-15T18:37:12+00:00 · Latest: 2025-12-15T18:37:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13644v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13644v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dexterous manipulation is challenging because it requires understanding how subtle hand motion influences the environment through contact with objects. We introduce DexWM, a Dexterous Manipulation World Model that predicts the next latent state of the environment conditioned on past states and dexterous actions. To overcome the scarcity of dexterous manipulation datasets, DexWM is trained on over 900 hours of human and non-dexterous robot videos. To enable fine-grained dexterity, we find that predicting visual features alone is insufficient; therefore, we introduce an auxiliary hand consistency loss that enforces accurate hand configurations. DexWM outperforms prior world models conditioned on text, navigation, and full-body actions, achieving more accurate predictions of future states. DexWM also demonstrates strong zero-shot generalization to unseen manipulation skills when deployed on a Franka Panda arm equipped with an Allegro gripper, outperforming Diffusion Policy by over 50% on average in grasping, placing, and reaching tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>世界模型可以利用人类视频实现灵巧操作</div>
<div class="mono" style="margin-top:8px">灵巧操作具有挑战性，因为它需要理解微妙的手部运动如何通过与物体的接触影响环境。我们引入了DexWM，这是一种基于灵巧操作的世界模型，它根据过去的状态和灵巧动作预测环境的下一个潜在状态。为了解决灵巧操作数据集稀缺的问题，DexWM在超过900小时的人类和非灵巧机器人视频上进行训练。为了实现精细的灵巧性，我们发现仅预测视觉特征是不够的；因此，我们引入了一个辅助的手部一致性损失函数，以确保手部配置的准确性。DexWM在基于文本、导航和全身动作的先前世界模型上表现更优，实现了对未来状态更准确的预测。当部署在配备Allegro夹爪的Franka Panda机械臂上时，DexWM在未见过的操作技能上展现出强大的零样本泛化能力，在抓取、放置和到达任务中平均优于Diffusion Policy超过50%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the difficulty of dexterous manipulation, which requires precise understanding of how hand motions affect the environment. DexWM, a World Model for dexterous manipulation, is trained on over 900 hours of human and non-dexterous robot videos to predict future latent states based on past states and actions. The model incorporates an auxiliary hand consistency loss to ensure accurate hand configurations, which is crucial for fine-grained dexterity. Experimental results show that DexWM outperforms prior world models in predicting future states and achieves strong zero-shot generalization on a Franka Panda arm, surpassing Diffusion Policy by over 50% in grasping, placing, and reaching tasks.</div>
<div class="mono" style="margin-top:8px">由于需要理解细微的手部动作如何影响环境，灵巧操作具有挑战性。DexWM 是一种用于灵巧操作的世界模型，能够根据过去的状态和动作预测未来的潜在状态。为了解决灵巧操作数据集不足的问题，DexWM 在大量人类和非灵巧机器人视频上进行训练。为了提升灵巧性，DexWM 引入了辅助的手部一致性损失，以确保手部配置的准确性。该模型在预测未来状态方面优于先前的世界模型，并在 Franka Panda 臂上展示了强大的零样本泛化能力，其在抓取、放置和移动任务中的表现比 Diffusion Policy 平均高出 50% 以上。</div>
</details>
</div>
<div class="card">
<div class="title">From Code to Field: Evaluating the Robustness of Convolutional Neural Networks for Disease Diagnosis in Mango Leaves</div>
<div class="meta-line">Authors: Gabriel Vitorino de Andrade, Saulo Roberto dos Santos, Itallo Patrick Castro Alves da Silva, Emanuel Adler Medeiros Pereira, Erick de Andrade Barboza</div>
<div class="meta-line">First: 2025-12-15T18:36:48+00:00 · Latest: 2025-12-15T18:36:48+00:00</div>
<div class="meta-line">Comments: This work was presented at the BRACIS 2025 conference in Fortaleza</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13641v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13641v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The validation and verification of artificial intelligence (AI) models through robustness assessment are essential to guarantee the reliable performance of intelligent systems facing real-world challenges, such as image corruptions including noise, blurring, and weather variations. Despite the global importance of mango (Mangifera indica L.), there is a lack of studies on the robustness of models for the diagnosis of disease in its leaves. This paper proposes a methodology to evaluate convolutional neural networks (CNNs) under adverse conditions. We adapted the MangoLeafDB dataset, generating MangoLeafDB-C with 19 types of artificial corruptions at five severity levels. We conducted a benchmark comparing five architectures: ResNet-50, ResNet-101, VGG-16, Xception, and LCNN (the latter being a lightweight architecture designed specifically for mango leaf diagnosis). The metrics include the F1 score, the corruption error (CE) and the relative mean corruption error (relative mCE). The results show that LCNN outperformed complex models in corruptions that can be present in real-world scenarios such as Defocus Blur, Motion Blur, while also achieving the lowest mCE. Modern architectures (e.g., ResNet-101) exhibited significant performance degradation in corrupted scenarios, despite their high accuracy under ideal conditions. These findings suggest that lightweight and specialized models may be more suitable for real-world applications in edge devices, where robustness and efficiency are critical. The study highlights the need to incorporate robustness assessments in the development of intelligent systems for agriculture, particularly in regions with technological limitations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从代码到田间：评估卷积神经网络在芒果叶疾病诊断中的鲁棒性</div>
<div class="mono" style="margin-top:8px">通过鲁棒性评估对人工智能（AI）模型进行验证和确认，对于确保智能系统在现实世界挑战中的可靠性能至关重要，例如图像退化包括噪声、模糊和天气变化。尽管芒果（Mangifera indica L.）在全球范围内具有重要性，但针对其叶片疾病诊断模型鲁棒性的研究仍较为缺乏。本文提出了一种在不利条件下评估卷积神经网络（CNNs）的方法。我们对MangoLeafDB数据集进行了调整，生成包含19种人工退化类型和五个严重程度等级的MangoLeafDB-C数据集。我们比较了五种架构：ResNet-50、ResNet-101、VGG-16、Xception和LCNN（后者是一种专门为芒果叶诊断设计的轻量级架构）。评估指标包括F1分数、退化误差（CE）和相对平均退化误差（relative mCE）。结果表明，LCNN在现实世界中可能出现的退化情况（如失焦模糊、运动模糊）中表现优于复杂模型，同时实现了最低的mCE。尽管现代架构（如ResNet-101）在理想条件下具有高准确率，但在退化场景中表现出显著的性能下降。这些发现表明，轻量级和专用的模型可能更适合在边缘设备上进行现实世界应用，因为鲁棒性和效率至关重要。本研究强调了在农业智能系统开发中纳入鲁棒性评估的必要性，特别是在技术条件有限的地区。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study aims to evaluate the robustness of convolutional neural networks (CNNs) for disease diagnosis in mango leaves, addressing the lack of research on model reliability under real-world image corruptions. The authors adapted the MangoLeafDB dataset to create MangoLeafDB-C, which includes 19 types of artificial corruptions at five severity levels. They benchmarked five CNN architectures—ResNet-50, ResNet-101, VGG-16, Xception, and LCNN—using metrics like F1 score, corruption error (CE), and relative mean corruption error (relative mCE). The results indicate that LCNN, a lightweight architecture tailored for mango leaf diagnosis, outperformed more complex models in handling real-world corruptions such as Defocus Blur and Motion Blur, achieving the lowest relative mCE.</div>
<div class="mono" style="margin-top:8px">本研究旨在评估卷积神经网络（CNN）在芒果叶疾病诊断中的鲁棒性，填补该领域研究的空白。作者对MangoLeafDB数据集进行了调整，生成包含19种人工破坏类型和五个严重程度级别的MangoLeafDB-C数据集。他们使用F1分数、破坏误差（CE）和相对平均破坏误差（relative mCE）等指标，对ResNet-50、ResNet-101、VGG-16、Xception和LCNN这五种CNN架构进行了评估。结果表明，LCNN这种轻量级架构在处理现实场景中的破坏，如失焦模糊和运动模糊时，表现优于更复杂的模型，并实现了最低的相对mCE。</div>
</details>
</div>
<div class="card">
<div class="title">Charge: A Comprehensive Novel View Synthesis Benchmark and Dataset to Bind Them All</div>
<div class="meta-line">Authors: Michal Nazarczuk, Thomas Tanay, Arthur Moreau, Zhensong Zhang, Eduardo Pérez-Pellitero</div>
<div class="meta-line">First: 2025-12-15T18:33:08+00:00 · Latest: 2025-12-15T18:33:08+00:00</div>
<div class="meta-line">Comments: Project page: https://charge-benchmark.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13639v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13639v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://charge-benchmark.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a new dataset for Novel View Synthesis, generated from a high-quality, animated film with stunning realism and intricate detail. Our dataset captures a variety of dynamic scenes, complete with detailed textures, lighting, and motion, making it ideal for training and evaluating cutting-edge 4D scene reconstruction and novel view generation models. In addition to high-fidelity RGB images, we provide multiple complementary modalities, including depth, surface normals, object segmentation and optical flow, enabling a deeper understanding of scene geometry and motion. The dataset is organised into three distinct benchmarking scenarios: a dense multi-view camera setup, a sparse camera arrangement, and monocular video sequences, enabling a wide range of experimentation and comparison across varying levels of data sparsity. With its combination of visual richness, high-quality annotations, and diverse experimental setups, this dataset offers a unique resource for pushing the boundaries of view synthesis and 3D vision.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Charge: 一个全面的新型视图合成基准和数据集，将它们全部绑定</div>
<div class="mono" style="margin-top:8px">本文提出了一种新型视图合成数据集，该数据集来源于一部高质量的动画电影，具有惊人的真实感和精细的细节。我们的数据集捕捉了多种动态场景，包含详细的纹理、光照和运动信息，非常适合训练和评估前沿的4D场景重建和新型视图生成模型。除了高保真度的RGB图像，我们还提供了多种互补的模态，包括深度、表面法线、物体分割和光流，从而有助于更深入地理解场景几何和运动。该数据集分为三种不同的基准测试场景：密集多视角相机设置、稀疏相机布置和单目视频序列，使得在不同数据稀疏程度下能够进行广泛实验和比较。凭借其视觉丰富性、高质量的标注以及多样化的实验设置，该数据集为推动视图合成和3D视觉的边界提供了独特的资源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research is to provide a comprehensive and high-quality dataset for novel view synthesis that captures dynamic scenes with detailed textures, lighting, and motion. The authors generated the dataset from a high-fidelity animated film, incorporating RGB images, depth, surface normals, object segmentation, and optical flow. The dataset is structured into three benchmarking scenarios—dense multi-view camera setup, sparse camera arrangement, and monocular video sequences—to support diverse experiments and comparisons under different data sparsity conditions. The main experimental results demonstrate the dataset&#x27;s effectiveness in training and evaluating advanced 4D scene reconstruction and novel view generation models, highlighting its potential to advance research in 3D vision and view synthesis.</div>
<div class="mono" style="margin-top:8px">本文介绍了Charge数据集，这是一个用于新型视图合成的综合性资源，由高质量动画电影生成，提供逼真且细节丰富的场景。该数据集包含高保真度的RGB图像以及深度、表面法线、物体分割和光流等多种补充模态，支持先进的4D场景重建。数据集分为三个基准测试场景：密集多视角相机设置、稀疏相机布置和单目视频序列，允许在不同数据稀疏程度下进行多样化的实验和比较。丰富的视觉数据和详细的标注使Charge成为改进和评估视图合成与3D视觉技术的宝贵工具。</div>
</details>
</div>
<div class="card">
<div class="title">MindDrive: A Vision-Language-Action Model for Autonomous Driving via Online Reinforcement Learning</div>
<div class="meta-line">Authors: Haoyu Fu, Diankun Zhang, Zongchuang Zhao, Jianfeng Cui, Hongwei Xie, Bing Wang, Guang Chen, Dingkang Liang, Xiang Bai</div>
<div class="meta-line">First: 2025-12-15T18:31:32+00:00 · Latest: 2025-12-15T18:31:32+00:00</div>
<div class="meta-line">Comments: 16 pages, 12 figures, 6 tables; Project Page: https://xiaomi-mlab.github.io/MindDrive/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13636v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13636v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://xiaomi-mlab.github.io/MindDrive/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current Vision-Language-Action (VLA) paradigms in autonomous driving primarily rely on Imitation Learning (IL), which introduces inherent challenges such as distribution shift and causal confusion. Online Reinforcement Learning offers a promising pathway to address these issues through trial-and-error learning. However, applying online reinforcement learning to VLA models in autonomous driving is hindered by inefficient exploration in continuous action spaces. To overcome this limitation, we propose MindDrive, a VLA framework comprising a large language model (LLM) with two distinct sets of LoRA parameters. The one LLM serves as a Decision Expert for scenario reasoning and driving decision-making, while the other acts as an Action Expert that dynamically maps linguistic decisions into feasible trajectories. By feeding trajectory-level rewards back into the reasoning space, MindDrive enables trial-and-error learning over a finite set of discrete linguistic driving decisions, instead of operating directly in a continuous action space. This approach effectively balances optimal decision-making in complex scenarios, human-like driving behavior, and efficient exploration in online reinforcement learning. MindDrive achieves strong closed-loop performance on the challenging Bench2Drive benchmark, with a Driving Score (DS) of 78.04 and a Success Rate (SR) of 55.09%. To the best of our knowledge, this is the first work to demonstrate the effectiveness of online reinforcement learning for the VLA model in autonomous driving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MindDrive：一种通过在线强化学习实现的视觉-语言-动作模型用于自动驾驶</div>
<div class="mono" style="margin-top:8px">当前自动驾驶中的视觉-语言-动作（VLA）范式主要依赖于模仿学习（IL），这带来了诸如分布偏移和因果混淆等固有挑战。在线强化学习通过试错学习提供了一条有前景的解决路径。然而，在自动驾驶的VLA模型中应用在线强化学习受到连续动作空间中探索效率低下的阻碍。为克服这一限制，我们提出了MindDrive，这是一个包含两个不同LoRA参数集的大型语言模型（LLM）的VLA框架。其中一个LLM作为场景推理和驾驶决策的决策专家，另一个则作为动作专家，动态地将语言决策映射为可行的轨迹。通过将轨迹级别的奖励反馈到推理空间，MindDrive能够在有限的离散语言驾驶决策集合上实现试错学习，而不是直接在连续动作空间中操作。这种方法有效平衡了复杂场景中的最优决策、类人驾驶行为以及在线强化学习中的高效探索。MindDrive在具有挑战性的Bench2Drive基准测试中实现了强大的闭环性能，其驾驶得分（DS）为78.04%，成功率为55.09%。据我们所知，这是首个在自动驾驶VLA模型中展示在线强化学习有效性的研究工作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of Imitation Learning in Vision-Language-Action models for autonomous driving, particularly distribution shift and causal confusion. MindDrive introduces a framework that combines two large language models with distinct LoRA parameters, one for scenario reasoning and decision-making, and the other for dynamically generating feasible driving trajectories from linguistic decisions. The model uses trajectory-level rewards to guide learning in a discrete decision space, improving efficiency in online reinforcement learning. Experimental results on the Bench2Drive benchmark show that MindDrive achieves a Driving Score of 78.04 and a Success Rate of 55.09%, demonstrating its effectiveness in complex driving scenarios.</div>
<div class="mono" style="margin-top:8px">MindDrive的动机源于自动驾驶中模仿学习的局限性，如分布偏移和因果混淆。该方法结合了一个带有两组LoRA参数的大语言模型，一组用于场景推理和决策（决策专家），另一组用于将语言决策映射为可行的驾驶动作（动作专家）。实验结果表明，MindDrive在Bench2Drive基准测试中实现了78.04的驾驶得分和55.09%的成功率，展示了通过在线强化学习实现有效闭环性能的能力。</div>
</details>
</div>
<div class="card">
<div class="title">SCR2-ST: Combine Single Cell with Spatial Transcriptomics for Efficient Active Sampling via Reinforcement Learning</div>
<div class="meta-line">Authors: Junchao Zhu, Ruining Deng, Junlin Guo, Tianyuan Yao, Chongyu Qu, Juming Xiong, Siqi Lu, Zhengyi Lu, Yanfan Zhu, Marilyn Lionts, Yuechen Yang, Yalin Zheng, Yu Wang, Shilin Zhao, Haichun Yang, Yuankai Huo</div>
<div class="meta-line">First: 2025-12-15T18:30:40+00:00 · Latest: 2025-12-15T18:30:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13635v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13635v1">PDF</a> · <a href="https://github.com/hrlblab/SCR2ST">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial transcriptomics (ST) is an emerging technology that enables researchers to investigate the molecular relationships underlying tissue morphology. However, acquiring ST data remains prohibitively expensive, and traditional fixed-grid sampling strategies lead to redundant measurements of morphologically similar or biologically uninformative regions, thus resulting in scarce data that constrain current methods. The well-established single-cell sequencing field, however, could provide rich biological data as an effective auxiliary source to mitigate this limitation. To bridge these gaps, we introduce SCR2-ST, a unified framework that leverages single-cell prior knowledge to guide efficient data acquisition and accurate expression prediction. SCR2-ST integrates a single-cell guided reinforcement learning-based (SCRL) active sampling and a hybrid regression-retrieval prediction network SCR2Net. SCRL combines single-cell foundation model embeddings with spatial density information to construct biologically grounded reward signals, enabling selective acquisition of informative tissue regions under constrained sequencing budgets. SCR2Net then leverages the actively sampled data through a hybrid architecture combining regression-based modeling with retrieval-augmented inference, where a majority cell-type filtering mechanism suppresses noisy matches and retrieved expression profiles serve as soft labels for auxiliary supervision. We evaluated SCR2-ST on three public ST datasets, demonstrating SOTA performance in both sampling efficiency and prediction accuracy, particularly under low-budget scenarios. Code is publicly available at: https://github.com/hrlblab/SCR2ST</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SCR2-ST: 通过强化学习结合单细胞与空间转录组学以实现高效主动采样</div>
<div class="mono" style="margin-top:8px">空间转录组学（ST）是一种新兴技术，使研究人员能够研究组织形态背后的分子关系。然而，获取ST数据仍然成本高昂，传统的固定网格采样策略会导致形态相似或生物学信息不足区域的重复测量，从而产生数据稀缺，限制了当前方法的应用。然而，已建立的单细胞测序领域可以作为有效的辅助数据源，提供丰富的生物数据以缓解这一限制。为弥合这些差距，我们引入了SCR2-ST，这是一个统一框架，利用单细胞先验知识来指导高效的数据获取和准确的表达预测。SCR2-ST集成了基于单细胞引导的强化学习（SCRL）的主动采样方法和混合回归-检索预测网络SCR2Net。SCRL将单细胞基础模型嵌入与空间密度信息相结合，构建生物学基础的奖励信号，从而在受限的测序预算下选择性地获取信息丰富的组织区域。SCR2Net则通过结合基于回归的建模与检索增强推理的混合架构利用主动采样的数据，其中多数细胞类型过滤机制抑制了噪声匹配，而检索到的表达谱则作为软标签用于辅助监督。我们在三个公开的ST数据集上评估了SCR2-ST，展示了其在采样效率和预测准确性方面的最先进性能，尤其是在低预算场景下。代码已公开在：https://github.com/hrlblab/SCR2ST</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research is to address the high cost and inefficiency of spatial transcriptomics (ST) data acquisition by reducing redundant measurements. SCR2-ST introduces a framework that combines single-cell sequencing data with spatial transcriptomics using reinforcement learning. The method involves SCRL, which uses single-cell embeddings and spatial density to create reward signals for active sampling, and SCR2Net, a hybrid model that integrates regression and retrieval for expression prediction. Experimental results on three public ST datasets show that SCR2-ST achieves state-of-the-art performance in both sampling efficiency and prediction accuracy, especially when operating under limited sequencing budgets.</div>
<div class="mono" style="margin-top:8px">SCR2-ST的动机是解决空间转录组学（ST）数据获取成本高和效率低的问题，通过减少冗余测量来优化数据采集。该方法结合单细胞测序数据与空间转录组学，采用基于强化学习的主动采样策略（SCRL）和混合回归-检索预测网络（SCR2Net）。SCRL利用单细胞嵌入和空间密度生成奖励信号以指导有针对性的采样，而SCR2Net通过融合回归建模和检索增强推理来提升预测准确性，并采用主要细胞类型过滤机制抑制噪声匹配。实验结果表明，SCR2-ST在三个公开的ST数据集上均实现了采样效率和预测准确性的最先进水平，特别是在有限测序预算的情况下表现尤为突出。</div>
</details>
</div>
<div class="card">
<div class="title">Universality of high-dimensional scaling limits of stochastic gradient descent</div>
<div class="meta-line">Authors: Reza Gheissari, Aukosh Jagannath</div>
<div class="meta-line">First: 2025-12-15T18:30:26+00:00 · Latest: 2025-12-15T18:30:26+00:00</div>
<div class="meta-line">Comments: 30 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13634v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13634v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider statistical tasks in high dimensions whose loss depends on the data only through its projection into a fixed-dimensional subspace spanned by the parameter vectors and certain ground truth vectors. This includes classifying mixture distributions with cross-entropy loss with one and two-layer networks, and learning single and multi-index models with one and two-layer networks. When the data is drawn from an isotropic Gaussian mixture distribution, it is known that the evolution of a finite family of summary statistics under stochastic gradient descent converges to an autonomous ordinary differential equation (ODE), as the dimension and sample size go to $\infty$ and the step size goes to $0$ commensurately. Our main result is that these ODE limits are universal in that this convergence occurs even when the data is drawn from mixtures of product measures provided the first two moments match the corresponding Gaussian distribution and the initialization and ground truth vectors are sufficiently coordinate-delocalized. We complement this by proving two corresponding non-universality results. We provide a simple example where the ODE limits are non-universal if the initialization is coordinate aligned. We also show that the stochastic differential equation limits arising as fluctuations of the summary statistics around their ODE&#x27;s fixed points are not universal.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随机梯度下降高维缩放极限的普适性</div>
<div class="mono" style="margin-top:8px">我们考虑在高维空间中进行的统计任务，其损失函数仅依赖于数据在由参数向量和某些真实向量张成的固定维子空间中的投影。这包括使用单层和双层网络进行交叉熵损失的混合分布分类，以及使用单层和双层网络学习单指数模型和多指数模型。当数据是从各向同性高斯混合分布中抽取时，已知随机梯度下降下有限数量的统计量的演化会收敛到一个自治的常微分方程（ODE），当维度和样本量趋于无穷，且步长趋于零时，两者呈一致缩放。我们的主要结果是这些ODE极限具有普适性，即即使数据是从乘积测度的混合分布中抽取，只要前两阶矩与对应的高斯分布一致，并且初始化和真实向量具有足够的坐标去本地化，这种收敛依然成立。我们通过证明两个相应的非普适性结果来补充这一结论。我们提供了一个简单例子，说明当初始化是坐标对齐时，ODE极限是非普适的。此外，我们还展示了统计量在ODE固定点附近的波动所导出的随机微分方程极限也是非普适的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper investigates the high-dimensional scaling limits of stochastic gradient descent (SGD) in statistical tasks where the loss function depends on data through a fixed-dimensional subspace defined by parameter and ground truth vectors. The authors demonstrate that under certain conditions, such as matching first two moments of data distributions and coordinate-delocalized initialization, the evolution of summary statistics under SGD converges to a universal ordinary differential equation (ODE). They also present non-universality results, showing that ODE limits can fail when initialization is coordinate-aligned or when considering fluctuations around fixed points of the ODE.</div>
<div class="mono" style="margin-top:8px">本研究探讨了在统计任务中，当损失函数仅通过参数向量和真实向量所张成的固定维子空间依赖数据时，随机梯度下降（SGD）的高维缩放极限。研究证明，在数据分布的前两阶矩与高斯分布匹配且初始化向量足够坐标去相关的情况下，SGD下摘要统计量的演化会收敛到一个通用的常微分方程（ODE）。然而，论文也提出了非通用性结果，表明当初始化向量是坐标对齐的，或者考虑摘要统计量在ODE平衡点附近的波动时，ODE极限可能失效。</div>
</details>
</div>
<div class="card">
<div class="title">Developing synthetic microdata through machine learning for firm-level business surveys</div>
<div class="meta-line">Authors: Jorge Cisneros, Timothy Wojan, Matthew Williams, Jennifer Ozawa, Robert Chew, Kimberly Janda, Timothy Navarro, Michael Floyd, Christine Task, Damon Streat</div>
<div class="meta-line">First: 2025-12-05T18:44:30+00:00 · Latest: 2025-12-15T18:28:42+00:00</div>
<div class="meta-line">Comments: 17 pages, 4 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.05948v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.05948v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Public-use microdata samples (PUMS) from the United States (US) Census Bureau on individuals have been available for decades. However, large increases in computing power and the greater availability of Big Data have dramatically increased the probability of re-identifying anonymized data, potentially violating the pledge of confidentiality given to survey respondents. Data science tools can be used to produce synthetic data that preserve critical moments of the empirical data but do not contain the records of any existing individual respondent or business. Developing public-use firm data from surveys presents unique challenges different from demographic data, because there is a lack of anonymity and certain industries can be easily identified in each geographic area. This paper briefly describes a machine learning model used to construct a synthetic PUMS based on the Annual Business Survey (ABS) and discusses various quality metrics. Although the ABS PUMS is currently being refined and results are confidential, we present two synthetic PUMS developed for the 2007 Survey of Business Owners, similar to the ABS business data. Econometric replication of a high impact analysis published in Small Business Economics demonstrates the verisimilitude of the synthetic data to the true data and motivates discussion of possible ABS use cases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用机器学习开发企业层面的合成微观数据用于商业调查</div>
<div class="mono" style="margin-top:8px">美国人口普查局（US Census Bureau）提供的个体公共使用微观数据样本（PUMS）已有数十年的历史。然而，计算能力的大幅提升和大数据的广泛可用性显著增加了重新识别匿名数据的可能性，这可能违反对调查受访者所承诺的保密性。数据科学工具可用于生成合成数据，这些数据保留了实证数据的关键特征，但不包含任何现有个人受访者或企业的记录。从调查中开发公共使用企业数据面临与人口统计数据不同的独特挑战，因为缺乏匿名性，且某些行业在每个地理区域都容易被识别。本文简要描述了用于基于年度商业调查（ABS）构建合成PUMS的机器学习模型，并讨论了多种质量指标。尽管目前ABS PUMS仍在完善中且结果保密，我们展示了两个为2007年商业业主调查开发的合成PUMS，其与ABS企业数据类似。对《小企业经济学》上发表的一项高影响力分析进行计量经济学复制，展示了合成数据与真实数据的相似性，并激发了对ABS可能应用场景的讨论。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research stems from the increasing risk of re-identifying anonymized microdata due to advancements in computing power and the availability of Big Data, which threatens the confidentiality of survey respondents. The study employs a machine learning approach to generate synthetic microdata that mimics the statistical properties of real firm-level data from the Annual Business Survey (ABS) without revealing individual business records. Experimental results show that the synthetic data accurately replicate key econometric findings from a prior study in Small Business Economics, demonstrating their potential utility for producing public-use firm data while maintaining privacy.</div>
<div class="mono" style="margin-top:8px">本文针对由于计算能力提升和大数据普及导致匿名化企业数据面临重新识别风险的问题，提出了利用机器学习生成合成微观数据的方法，以保持数据的统计特性同时保护受访者隐私。基于2007年企业业主调查构建了两个合成数据集，结果表明这些数据能够再现之前发表在《小企业经济学》上的重要分析，证明了其与真实数据的相似性，为未来ABS数据发布提供了潜在的应用价值。</div>
</details>
</div>
<div class="card">
<div class="title">StutterFuse: Mitigating Modality Collapse in Stuttering Detection with Jaccard-Weighted Metric Learning and Gated Fusion</div>
<div class="meta-line">Authors: Guransh Singh, Md Shah Fahad</div>
<div class="meta-line">First: 2025-12-15T18:28:39+00:00 · Latest: 2025-12-15T18:28:39+00:00</div>
<div class="meta-line">Comments: 13 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13632v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.13632v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Stuttering detection breaks down when disfluencies overlap. Existing parametric models struggle to distinguish complex, simultaneous disfluencies (e.g., a &#x27;block&#x27; with a &#x27;prolongation&#x27;) due to the scarcity of these specific combinations in training data. While Retrieval-Augmented Generation (RAG) has revolutionized NLP by grounding models in external knowledge, this paradigm remains unexplored in pathological speech processing. To bridge this gap, we introduce StutterFuse, the first Retrieval-Augmented Classifier (RAC) for multi-label stuttering detection. By conditioning a Conformer encoder on a non-parametric memory bank of clinical examples, we allow the model to classify by reference rather than memorization. We further identify and solve &quot;Modality Collapse&quot;, an &quot;Echo Chamber&quot; effect where naive retrieval boosts recall but degrades precision. We mitigate this using: (1) SetCon, a Jaccard-Weighted Metric Learning objective that optimizes for multi-label set similarity, and (2) a Gated Mixture-of-Experts fusion strategy that dynamically arbitrates between acoustic evidence and retrieved context. On the SEP-28k dataset, StutterFuse achieves a weighted F1-score of 0.65, outperforming strong baselines and demonstrating remarkable zero-shot cross-lingual generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StutterFuse：通过Jaccard加权度量学习和门控融合缓解模态崩溃问题的口吃检测方法</div>
<div class="mono" style="margin-top:8px">当口吃现象重叠时，口吃检测会失效。现有的参数化模型由于训练数据中缺乏这些特定组合，难以区分复杂的、同时发生的口吃现象（例如，一个&#x27;阻塞&#x27;伴随一个&#x27;延长&#x27;）。虽然检索增强生成（RAG）通过将模型与外部知识结合，革新了自然语言处理领域，但这一范式在病理性语音处理中尚未被探索。为弥合这一差距，我们引入了StutterFuse，这是首个用于多标签口吃检测的检索增强分类器（RAC）。通过将Conformer编码器条件化于一个非参数化的临床示例记忆库，我们使模型能够通过参考而非记忆进行分类。我们进一步识别并解决了“模态崩溃”问题，这是一种“回音室”效应，即简单的检索方法虽然能提升召回率，但会降低精确率。我们通过以下方法缓解这一问题：(1) SetCon，一种Jaccard加权度量学习目标，用于优化多标签集合相似性；(2) 一种门控专家混合融合策略，动态地在声学证据和检索上下文之间进行仲裁。在SEP-28k数据集上，StutterFuse实现了0.65的加权F1分数，优于强大的基线模型，并展示了显著的零样本跨语言泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">StutterFuse was developed to address the issue of modality collapse in stuttering detection, particularly when disfluencies overlap, making it difficult for existing models to distinguish complex patterns. The method introduces a Retrieval-Augmented Classifier that conditions a Conformer encoder on a non-parametric memory bank of clinical examples, enabling classification based on reference rather than memorization. Additionally, it employs a Jaccard-Weighted Metric Learning objective (SetCon) and a Gated Mixture-of-Experts fusion strategy to enhance precision. On the SEP-28k dataset, StutterFuse achieves a weighted F1-score of 0.65, surpassing strong baselines and showing effective zero-shot cross-lingual performance.</div>
<div class="mono" style="margin-top:8px">StutterFuse 是为了解决在重叠的口吃现象中进行检测时遇到的挑战而开发的，因为现有模型在处理复杂同时发生的口吃（如 &#x27;block&#x27; 和 &#x27;prolongation&#x27; 的组合）时表现不佳，由于训练数据中此类组合较少。该方法引入了一个基于检索的分类器，通过将 Conformer 编码器条件化于临床示例的非参数记忆库，使模型能够基于参考进行分类而非单纯记忆。为了解决模态崩溃问题——即检索虽提高召回率但降低精确率——StutterFuse 采用了基于 Jaccard 权重的度量学习目标（SetCon）和门控专家混合融合策略。在 SEP-28k 数据集上，StutterFuse 达到了 0.65 的加权 F1 分数，优于其他强基线模型，并展示了显著的零样本跨语言泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">BlurDM: A Blur Diffusion Model for Image Deblurring</div>
<div class="meta-line">Authors: Jin-Ting He, Fu-Jen Tsai, Yan-Tsung Peng, Min-Hung Chen, Chia-Wen Lin, Yen-Yu Lin</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-12-03T17:10:44+00:00 · Latest: 2025-12-15T18:15:43+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025. Project Page: https://jin-ting-he.github.io/Blur-Diffusion-Model/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.03979v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.03979v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://jin-ting-he.github.io/Blur-Diffusion-Model/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models show promise for dynamic scene deblurring; however, existing studies often fail to leverage the intrinsic nature of the blurring process within diffusion models, limiting their full potential. To address it, we present a Blur Diffusion Model (BlurDM), which seamlessly integrates the blur formation process into diffusion for image deblurring. Observing that motion blur stems from continuous exposure, BlurDM implicitly models the blur formation process through a dual-diffusion forward scheme, diffusing both noise and blur onto a sharp image. During the reverse generation process, we derive a dual denoising and deblurring formulation, enabling BlurDM to recover the sharp image by simultaneously denoising and deblurring, given pure Gaussian noise conditioned on the blurred image as input. Additionally, to efficiently integrate BlurDM into deblurring networks, we perform BlurDM in the latent space, forming a flexible prior generation network for deblurring. Extensive experiments demonstrate that BlurDM significantly and consistently enhances existing deblurring methods on four benchmark datasets. The project page is available at https://jin-ting-he.github.io/Blur-Diffusion-Model/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BlurDM：一种用于图像去模糊的模糊扩散模型</div>
<div class="mono" style="margin-top:8px">扩散模型在动态场景去模糊方面展现出潜力；然而，现有研究往往未能充分利用扩散模型中模糊过程的内在特性，限制了其全部潜力。为了解决这一问题，我们提出了一种模糊扩散模型（BlurDM），它将模糊形成过程无缝整合到扩散过程中，用于图像去模糊。我们观察到运动模糊源于连续曝光，因此BlurDM通过双扩散前向方案隐式建模模糊形成过程，同时将噪声和模糊扩散到清晰图像上。在反向生成过程中，我们推导出一种双去噪和去模糊公式，使得BlurDM能够在输入为纯高斯噪声且基于模糊图像的条件下，同时进行去噪和去模糊以恢复清晰图像。此外，为了高效地将BlurDM集成到去模糊网络中，我们在潜在空间中执行BlurDM，形成一个灵活的先验生成网络。大量实验表明，BlurDM在四个基准数据集上显著且一致地提升了现有去模糊方法的性能。项目页面可在 https://jin-ting-he.github.io/Blur-Diffusion-Model/ 查看。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind BlurDM is to improve image deblurring by better leveraging the intrinsic nature of the blurring process within diffusion models. The method introduces a dual-diffusion forward process that models both noise and blur implicitly, and a dual denoising and deblurring formulation during the reverse generation phase. Experimental results show that BlurDM significantly enhances existing deblurring methods across four benchmark datasets, demonstrating its effectiveness in recovering sharp images from blurred inputs.</div>
<div class="mono" style="margin-top:8px">BlurDM的动机是通过更好地利用扩散模型中模糊过程的内在特性来提升图像去模糊效果。该方法引入了一种双扩散前向方案，通过连续曝光隐式建模运动模糊，将噪声和模糊扩散到清晰图像上。在反向生成过程中，BlurDM采用双去噪和去模糊公式，从模糊输入中恢复清晰图像，其条件为纯高斯噪声。实验结果表明，BlurDM在四个基准数据集上显著提升了现有去模糊方法的性能，证明了其有效性。</div>
</details>
</div>
<div class="card">
<div class="title">OUGS: Active View Selection via Object-aware Uncertainty Estimation in 3DGS</div>
<div class="meta-line">Authors: Haiyi Li, Qi Chen, Denis Kalkofen, Hsiang-Ting Chen</div>
<div class="meta-line">First: 2025-11-12T15:08:46+00:00 · Latest: 2025-12-15T18:12:41+00:00</div>
<div class="meta-line">Comments: Conditionally accepted to Eurographics 2026 (five reviewers)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.09397v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.09397v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in 3D Gaussian Splatting (3DGS) have achieved state-of-the-art results for novel view synthesis. However, efficiently capturing high-fidelity reconstructions of specific objects within complex scenes remains a significant challenge. A key limitation of existing active reconstruction methods is their reliance on scene-level uncertainty metrics, which are often biased by irrelevant background clutter and lead to inefficient view selection for object-centric tasks. We present OUGS, a novel framework that addresses this challenge with a more principled, physically-grounded uncertainty formulation for 3DGS. Our core innovation is to derive uncertainty directly from the explicit physical parameters of the 3D Gaussian primitives (e.g., position, scale, rotation). By propagating the covariance of these parameters through the rendering Jacobian, we establish a highly interpretable uncertainty model. This foundation allows us to then seamlessly integrate semantic segmentation masks to produce a targeted, object-aware uncertainty score that effectively disentangles the object from its environment. This allows for a more effective active view selection strategy that prioritizes views critical to improving object fidelity. Experimental evaluations on public datasets demonstrate that our approach significantly improves the efficiency of the 3DGS reconstruction process and achieves higher quality for targeted objects compared to existing state-of-the-art methods, while also serving as a robust uncertainty estimator for the global scene.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OUGS: 通过面向对象的不确定性估计实现3DGS中的主动视图选择</div>
<div class="mono" style="margin-top:8px">近年来，3D高斯泼溅（3DGS）在新视图合成方面取得了最先进的成果。然而，在复杂场景中高效捕捉特定物体的高保真重建仍然是一个重大挑战。现有主动重建方法的一个关键限制是它们依赖于场景级别的不确定性度量，这些度量通常受到无关背景杂乱信息的干扰，导致面向物体任务的视图选择效率低下。我们提出了OUGS，一个新颖的框架，通过更原则化、基于物理的不确定性公式来解决这一挑战。我们的核心创新是直接从3D高斯基础元素的显式物理参数（如位置、尺度、旋转）中推导不确定性。通过将这些参数的协方差传播到渲染雅可比矩阵中，我们建立了一个高度可解释的不确定性模型。这一基础使我们能够无缝整合语义分割掩码，生成一个有针对性的、面向对象的不确定性评分，从而有效分离物体与其环境。这使得我们能够采用更有效的主动视图选择策略，优先考虑对提升物体保真度至关重要的视图。在公开数据集上的实验评估表明，我们的方法显著提高了3DGS重建过程的效率，并在目标物体上实现了比现有最先进的方法更高的质量，同时也为全局场景提供了一个稳健的不确定性估计器。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to enhance the efficiency and fidelity of object-specific 3D Gaussian Splatting (3DGS) reconstructions in complex scenes. The proposed method, OUGS, introduces an object-aware uncertainty estimation framework that derives uncertainty from the physical parameters of 3D Gaussian primitives, such as position, scale, and rotation, rather than relying on scene-level metrics. By propagating the covariance of these parameters through the rendering Jacobian and integrating semantic segmentation masks, OUGS generates a more accurate and interpretable uncertainty score for objects. This leads to an improved active view selection strategy that focuses on views critical for object reconstruction. Experimental results on public datasets show that OUGS achieves higher quality reconstructions for targeted objects and improves the efficiency of the 3DGS process compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高复杂场景中特定物体的3D高斯点云（3DGS）重建效率和保真度。所提出的方法OUGS引入了一种基于物体感知的不确定性估计框架，通过从3D高斯基本体的物理参数（如位置、尺度和旋转）中直接推导不确定性。利用这些参数的协方差通过渲染雅可比矩阵传播，并结合语义分割掩码，OUGS生成针对物体的不确定性评分，从而优化物体中心任务的视角选择。在公开数据集上的实验结果表明，OUGS在提高特定物体重建效率和质量方面优于现有方法，同时为整个场景提供了可靠的不确定性估计。</div>
</details>
</div>
<div class="card">
<div class="title">AI Copilots for Reproducibility in Science: A Case Study</div>
<div class="meta-line">Authors: Adrien Bibal, Steven N. Minton, Deborah Khider, Yolanda Gil</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-06-25T04:56:28+00:00 · Latest: 2025-12-15T18:11:39+00:00</div>
<div class="meta-line">Comments: Reproducible Artificial Intelligence (RAI2026) Workshop, AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.20130v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.20130v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open science initiatives seek to make research outputs more transparent, accessible, and reusable, but ensuring that published findings can be independently reproduced remains a persistent challenge. In this paper we describe an AI-driven &quot;Reproducibility Copilot&quot; that analyzes manuscripts, code, and supplementary materials to generate structured Jupyter Notebooks and recommendations aimed at facilitating computational, or &quot;rote&quot;, reproducibility. Our initial results suggest that the copilot has the potential to substantially reduce reproduction time (in one case from over 30 hours to about 1 hour) while achieving high coverage of figures, tables, and results suitable for computational reproduction. The system systematically detects barriers to reproducibility, including missing values for hyperparameters, undocumented preprocessing steps, and incomplete or inaccessible datasets. Although preliminary, these findings suggest that AI tools can meaningfully reduce the burden of reproducibility efforts and contribute to more transparent and verifiable scientific communication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>科学中的可重复性AI助手：一个案例研究</div>
<div class="mono" style="margin-top:8px">开放科学倡议旨在使研究成果更加透明、可访问和可重用，但确保已发表的研究结果能够被独立复现仍然是一个持续的挑战。本文描述了一种AI驱动的&quot;可重复性助手&quot;，该助手分析论文、代码和补充材料，生成结构化的Jupyter笔记本和建议，以促进计算或&quot;机械式&quot;的可重复性。我们的初步结果表明，该助手有潜力显著减少复现时间（在某一案例中从超过30小时减少到约1小时），同时实现对适合计算复现的图表、表格和结果的高覆盖率。该系统系统性地检测可重复性障碍，包括超参数缺失值、未记录的预处理步骤以及不完整或不可访问的数据集。尽管尚处于初步阶段，但这些发现表明AI工具可以显著减轻可重复性工作的负担，并有助于更加透明和可验证的科学交流。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of ensuring reproducibility in scientific research, a key goal of open science initiatives. It introduces an AI-driven Reproducibility Copilot that analyzes manuscripts, code, and supplementary materials to generate structured Jupyter Notebooks and recommendations for computational reproducibility. The system identifies common barriers such as missing hyperparameters, undocumented preprocessing, and incomplete datasets. Initial experiments show that the copilot can significantly reduce reproduction time, as demonstrated by a case where it cut the time from over 30 hours to about 1 hour, while achieving high coverage of figures, tables, and results suitable for computational verification.</div>
<div class="mono" style="margin-top:8px">本文针对科学研究中确保可重复性的挑战，这是开放科学倡议中的关键问题。文章介绍了一种AI驱动的可重复性助手（Reproducibility Copilot），该工具通过分析论文、代码和补充材料，生成结构化的Jupyter笔记本和计算可重复性的建议。系统能够识别常见的可重复性障碍，如缺失的超参数、未记录的预处理步骤和不完整的数据集，并且初步结果显示，该助手可以显著减少重复所需的时间，例如将一项任务的时间从超过30小时缩短至约1小时，同时实现对图表、表格和结果的高覆盖率。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
