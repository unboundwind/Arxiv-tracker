<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-12 03:55</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260112_0355</div>
    <div class="row"><div class="card">
<div class="title">Mesh4D: 4D Mesh Reconstruction and Tracking from Monocular Video</div>
<div class="meta-line">Authors: Zeren Jiang, Chuanxia Zheng, Iro Laina, Diane Larlus, Andrea Vedaldi</div>
<div class="meta-line">First: 2026-01-08T18:59:56+00:00 · Latest: 2026-01-08T18:59:56+00:00</div>
<div class="meta-line">Comments: 15 pages, 8 figures, project page: https://mesh-4d.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05251v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05251v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://mesh-4d.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose Mesh4D, a feed-forward model for monocular 4D mesh reconstruction. Given a monocular video of a dynamic object, our model reconstructs the object&#x27;s complete 3D shape and motion, represented as a deformation field. Our key contribution is a compact latent space that encodes the entire animation sequence in a single pass. This latent space is learned by an autoencoder that, during training, is guided by the skeletal structure of the training objects, providing strong priors on plausible deformations. Crucially, skeletal information is not required at inference time. The encoder employs spatio-temporal attention, yielding a more stable representation of the object&#x27;s overall deformation. Building on this representation, we train a latent diffusion model that, conditioned on the input video and the mesh reconstructed from the first frame, predicts the full animation in one shot. We evaluate Mesh4D on reconstruction and novel view synthesis benchmarks, outperforming prior methods in recovering accurate 3D shape and deformation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Mesh4D：从单目视频中进行四维网格重建与跟踪</div>
<div class="mono" style="margin-top:8px">我们提出了Mesh4D，这是一个用于单目四维网格重建的前馈模型。给定一个动态物体的单目视频，我们的模型可以重建该物体的完整三维形状和运动，表示为一个形变场。我们的主要贡献是一个紧凑的潜在空间，它可以在单次通过中编码整个动画序列。该潜在空间通过一个自编码器学习，训练过程中由训练对象的骨骼结构引导，为合理的形变提供了强先验。关键的是，在推理过程中不需要骨骼信息。编码器采用时空注意力机制，从而得到更稳定的物体整体形变表示。在此基础上，我们训练了一个潜在扩散模型，该模型在输入视频和从第一帧重建的网格条件下，能够一次性预测完整的动画。我们在重建和新视角合成基准上评估Mesh4D，在恢复准确的三维形状和形变方面优于先前方法。</div>
</details>
</div>
<div class="card">
<div class="title">RL-AWB: Deep Reinforcement Learning for Auto White Balance Correction in Low-Light Night-time Scenes</div>
<div class="meta-line">Authors: Yuan-Kang Lee, Kuan-Lin Chen, Chia-Che Chang, Yu-Lun Liu</div>
<div class="meta-line">First: 2026-01-08T18:59:55+00:00 · Latest: 2026-01-08T18:59:55+00:00</div>
<div class="meta-line">Comments: Project page: https://ntuneillee.github.io/research/rl-awb/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05249v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05249v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ntuneillee.github.io/research/rl-awb/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Nighttime color constancy remains a challenging problem in computational photography due to low-light noise and complex illumination conditions. We present RL-AWB, a novel framework combining statistical methods with deep reinforcement learning for nighttime white balance. Our method begins with a statistical algorithm tailored for nighttime scenes, integrating salient gray pixel detection with novel illumination estimation. Building on this foundation, we develop the first deep reinforcement learning approach for color constancy that leverages the statistical algorithm as its core, mimicking professional AWB tuning experts by dynamically optimizing parameters for each image. To facilitate cross-sensor evaluation, we introduce the first multi-sensor nighttime dataset. Experiment results demonstrate that our method achieves superior generalization capability across low-light and well-illuminated images. Project page: https://ntuneillee.github.io/research/rl-awb/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RL-AWB：用于低光夜间场景自动白平衡校正的深度强化学习</div>
<div class="mono" style="margin-top:8px">由于低光噪声和复杂的光照条件，夜间颜色恒常性仍然是计算摄影中的一个具有挑战性的问题。我们提出了RL-AWB，这是一种结合统计方法与深度强化学习的新框架，用于夜间白平衡校正。我们的方法首先采用一种针对夜间场景的统计算法，将显著灰像素检测与新的光照估计相结合。在此基础上，我们开发了首个利用统计算法作为核心的深度强化学习方法，通过动态优化每张图像的参数，模拟专业自动白平衡调校专家的工作。为便于跨传感器评估，我们引入了首个多传感器夜间数据集。实验结果表明，我们的方法在低光和良好照明图像中均表现出卓越的泛化能力。项目页面：https://ntuneillee.github.io/research/rl-awb/</div>
</details>
</div>
<div class="card">
<div class="title">QNeRF: Neural Radiance Fields on a Simulated Gate-Based Quantum Computer</div>
<div class="meta-line">Authors: Daniele Lizzio Bosco, Shuteng Wang, Giuseppe Serra, Vladislav Golyanik</div>
<div class="meta-line">First: 2026-01-08T18:59:55+00:00 · Latest: 2026-01-08T18:59:55+00:00</div>
<div class="meta-line">Comments: 30 pages, 15 figures, 11 tables; project page: https://4dqv.mpi-inf.mpg.de/QNeRF/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05250v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05250v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, Quantum Visual Fields (QVFs) have shown promising improvements in model compactness and convergence speed for learning the provided 2D or 3D signals. Meanwhile, novel-view synthesis has seen major advances with Neural Radiance Fields (NeRFs), where models learn a compact representation from 2D images to render 3D scenes, albeit at the cost of larger models and intensive training. In this work, we extend the approach of QVFs by introducing QNeRF, the first hybrid quantum-classical model designed for novel-view synthesis from 2D images. QNeRF leverages parameterised quantum circuits to encode spatial and view-dependent information via quantum superposition and entanglement, resulting in more compact models compared to the classical counterpart. We present two architectural variants. Full QNeRF maximally exploits all quantum amplitudes to enhance representational capabilities. In contrast, Dual-Branch QNeRF introduces a task-informed inductive bias by branching spatial and view-dependent quantum state preparations, drastically reducing the complexity of this operation and ensuring scalability and potential hardware compatibility. Our experiments demonstrate that -- when trained on images of moderate resolution -- QNeRF matches or outperforms classical NeRF baselines while using less than half the number of parameters. These results suggest that quantum machine learning can serve as a competitive alternative for continuous signal representation in mid-level tasks in computer vision, such as 3D representation learning from 2D observations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QNeRF：基于模拟门控量子计算机的神经辐射场</div>
<div class="mono" style="margin-top:8px">最近，量子视觉场（QVFs）在学习提供的2D或3D信号时，显示出在模型紧凑性和收敛速度方面的显著改进。同时，新型视图合成领域因神经辐射场（NeRFs）而取得了重大进展，其中模型从2D图像中学习紧凑表示以渲染3D场景，尽管这需要更大的模型和密集的训练。在本工作中，我们通过引入QNeRF，扩展了QVFs的方法，这是首个用于从2D图像进行新型视图合成的混合量子-经典模型。QNeRF利用参数化量子电路，通过量子叠加和纠缠来编码空间和视角依赖信息，从而比经典模型实现更紧凑的表示。我们提出了两种架构变体。全QNeRF最大限度地利用所有量子幅值以增强表示能力。相比之下，双分支QNeRF通过分支处理空间和视角依赖的量子态制备，引入了任务导向的归纳偏置，大幅降低了该操作的复杂度，并确保了可扩展性和潜在的硬件兼容性。我们的实验表明，当在中等分辨率图像上训练时，QNeRF在参数数量不到经典NeRF基线一半的情况下，能够达到或超越其性能。这些结果表明，量子机器学习可以作为计算机视觉中中级任务（如从2D观测中学习3D表示）中连续信号表示的有力替代方案。</div>
</details>
</div>
<div class="card">
<div class="title">Pixel-Perfect Visual Geometry Estimation</div>
<div class="meta-line">Authors: Gangwei Xu, Haotong Lin, Hongcheng Luo, Haiyang Sun, Bing Wang, Guang Chen, Sida Peng, Hangjun Ye, Xin Yang</div>
<div class="meta-line">First: 2026-01-08T18:59:49+00:00 · Latest: 2026-01-08T18:59:49+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/gangweix/pixel-perfect-depth</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05246v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05246v1">PDF</a> · <a href="https://github.com/gangweix/pixel-perfect-depth">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recovering clean and accurate geometry from images is essential for robotics and augmented reality. However, existing geometry foundation models still suffer severely from flying pixels and the loss of fine details. In this paper, we present pixel-perfect visual geometry models that can predict high-quality, flying-pixel-free point clouds by leveraging generative modeling in the pixel space. We first introduce Pixel-Perfect Depth (PPD), a monocular depth foundation model built upon pixel-space diffusion transformers (DiT). To address the high computational complexity associated with pixel-space diffusion, we propose two key designs: 1) Semantics-Prompted DiT, which incorporates semantic representations from vision foundation models to prompt the diffusion process, preserving global semantics while enhancing fine-grained visual details; and 2) Cascade DiT architecture that progressively increases the number of image tokens, improving both efficiency and accuracy. To further extend PPD to video (PPVD), we introduce a new Semantics-Consistent DiT, which extracts temporally consistent semantics from a multi-view geometry foundation model. We then perform reference-guided token propagation within the DiT to maintain temporal coherence with minimal computational and memory overhead. Our models achieve the best performance among all generative monocular and video depth estimation models and produce significantly cleaner point clouds than all other models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>像素完美的视觉几何估计</div>
<div class="mono" style="margin-top:8px">从图像中恢复干净且精确的几何结构对于机器人技术和增强现实至关重要。然而，现有的几何基础模型仍然严重受到飞溅像素和细节丢失的影响。在本文中，我们提出了像素完美的视觉几何模型，通过在像素空间中利用生成建模技术，可以预测高质量且无飞溅像素的点云。我们首先引入了单目深度基础模型Pixel-Perfect Depth (PPD)，其基于像素空间扩散变压器（DiT）构建。为了解决像素空间扩散带来的高计算复杂性问题，我们提出了两个关键设计：1）语义引导的DiT，它结合了视觉基础模型中的语义表示来引导扩散过程，从而在保持全局语义的同时增强细粒度视觉细节；2）级联DiT架构，通过逐步增加图像标记数量，提高模型的效率和准确性。为了进一步将PPD扩展到视频（PPVD），我们引入了一种新的语义一致的DiT，从多视角几何基础模型中提取时间一致的语义。随后，我们在DiT中执行参考引导的标记传播，以在最小的计算和内存开销下保持时间一致性。我们的模型在所有生成式单目和视频深度估计模型中表现最佳，并生成的点云显著更干净。</div>
</details>
</div>
<div class="card">
<div class="title">Optimal Lower Bounds for Online Multicalibration</div>
<div class="meta-line">Authors: Natalie Collina, Jiuyao Lu, Georgy Noarov, Aaron Roth</div>
<div class="meta-line">First: 2026-01-08T18:59:32+00:00 · Latest: 2026-01-08T18:59:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05245v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05245v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We prove tight lower bounds for online multicalibration, establishing an information-theoretic separation from marginal calibration.
  In the general setting where group functions can depend on both context and the learner&#x27;s predictions, we prove an $Ω(T^{2/3})$ lower bound on expected multicalibration error using just three disjoint binary groups. This matches the upper bounds of Noarov et al. (2025) up to logarithmic factors and exceeds the $O(T^{2/3-\varepsilon})$ upper bound for marginal calibration (Dagan et al., 2025), thereby separating the two problems.
  We then turn to lower bounds for the more difficult case of group functions that may depend on context but not on the learner&#x27;s predictions. In this case, we establish an $\widetildeΩ(T^{2/3})$ lower bound for online multicalibration via a $Θ(T)$-sized group family constructed using orthogonal function systems, again matching upper bounds up to logarithmic factors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在线多校准的最优下界</div>
<div class="mono" style="margin-top:8px">我们证明了在线多校准的紧致下界，确立了其与边缘校准的信息论分离。在一般设定中，当组函数可以依赖上下文和学习者的预测时，我们仅使用三个不相交的二元组，就证明了预期多校准误差的 $Ω(T^{2/3})$ 下界。这一结果在对数因子范围内与 Noarov 等人（2025）的上界相匹配，并且超过了边缘校准的 $O(T^{2/3-\varepsilon})$ 上界（Dagan 等人，2025），从而分离了这两个问题。随后，我们转向更困难的情况，即组函数可能依赖上下文但不依赖学习者的预测。在这种情况下，我们通过使用正交函数系统构造的 $Θ(T)$ 大小的组族，证明了在线多校准的 $\widetildeΩ(T^{2/3})$ 下界，同样在对数因子范围内与上界相匹配。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We prove tight lower bounds for online multicalibration, establishing an information-theoretic separation from marginal calibration.</div>
</details>
</div>
<div class="card">
<div class="title">GREx: Generalized Referring Expression Segmentation, Comprehension, and Generation</div>
<div class="meta-line">Authors: Henghui Ding, Chang Liu, Shuting He, Xudong Jiang, Yu-Gang Jiang</div>
<div class="meta-line">First: 2026-01-08T18:59:30+00:00 · Latest: 2026-01-08T18:59:30+00:00</div>
<div class="meta-line">Comments: IJCV, Project Page: https://henghuiding.com/GREx/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05244v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05244v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://henghuiding.github.io/GREx">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Referring Expression Segmentation (RES) and Comprehension (REC) respectively segment and detect the object described by an expression, while Referring Expression Generation (REG) generates an expression for the selected object. Existing datasets and methods commonly support single-target expressions only, i.e., one expression refers to one object, not considering multi-target and no-target expressions. This greatly limits the real applications of REx (RES/REC/REG). This paper introduces three new benchmarks called Generalized Referring Expression Segmentation (GRES), Comprehension (GREC), and Generation (GREG), collectively denoted as GREx, which extend the classic REx to allow expressions to identify an arbitrary number of objects. We construct the first large-scale GREx dataset gRefCOCO that contains multi-target, no-target, and single-target expressions and their corresponding images with labeled targets. GREx and gRefCOCO are designed to be backward-compatible with REx, facilitating extensive experiments to study the performance gap of the existing REx methods on GREx tasks. One of the challenges of GRES/GREC is complex relationship modeling, for which we propose a baseline ReLA that adaptively divides the image into regions with sub-instance clues and explicitly models the region-region and region-language dependencies. The proposed ReLA achieves the state-of-the-art results on the both GRES and GREC tasks. The proposed gRefCOCO dataset and method are available at https://henghuiding.github.io/GREx.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GREx：通用指代表达分割、理解与生成</div>
<div class="mono" style="margin-top:8px">指代表达分割（RES）和理解（REC）分别对表达所描述的对象进行分割和检测，而指代表达生成（REG）则为选定的对象生成表达。现有的数据集和方法通常仅支持单目标表达，即一个表达仅指代一个对象，未考虑多目标和无目标表达。这极大地限制了REx（RES/REC/REG）在实际应用中的能力。本文引入了三个新的基准数据集，称为通用指代表达分割（GRES）、理解（GREC）和生成（GREG），统称为GREx，将经典的REx扩展为允许表达识别任意数量对象的形式。我们构建了第一个大规模的GREx数据集gRefCOCO，其中包含多目标、无目标和单目标表达及其对应的带有标注目标的图像。GREx和gRefCOCO被设计为与REx兼容，从而便于进行广泛实验，研究现有REx方法在GREx任务上的性能差距。GRES/GREC的一个挑战是复杂的关系建模，为此我们提出了一种基线方法ReLA，该方法通过子实例线索将图像自适应地划分为区域，并显式建模区域-区域和区域-语言之间的依赖关系。所提出的ReLA在GRES和GREC任务上均取得了最先进的结果。所提出的gRefCOCO数据集和方法可在https://henghuiding.github.io/GREx获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Referring Expression Segmentation (RES) and Comprehension (REC) respectively segment and detect the object described by an expression, while Referring Expression Generation (REG) generates an expression for the selected object.</div>
</details>
</div>
<div class="card">
<div class="title">Generate, Transfer, Adapt: Learning Functional Dexterous Grasping from a Single Human Demonstration</div>
<div class="meta-line">Authors: Xingyi He, Adhitya Polavaram, Yunhao Cao, Om Deshmukh, Tianrui Wang, Xiaowei Zhou, Kuan Fang</div>
<div class="meta-line">First: 2026-01-08T18:59:30+00:00 · Latest: 2026-01-08T18:59:30+00:00</div>
<div class="meta-line">Comments: Project Page: https://cordex-manipulation.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05243v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05243v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://cordex-manipulation.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Functional grasping with dexterous robotic hands is a key capability for enabling tool use and complex manipulation, yet progress has been constrained by two persistent bottlenecks: the scarcity of large-scale datasets and the absence of integrated semantic and geometric reasoning in learned models. In this work, we present CorDex, a framework that robustly learns dexterous functional grasps of novel objects from synthetic data generated from just a single human demonstration. At the core of our approach is a correspondence-based data engine that generates diverse, high-quality training data in simulation. Based on the human demonstration, our data engine generates diverse object instances of the same category, transfers the expert grasp to the generated objects through correspondence estimation, and adapts the grasp through optimization. Building on the generated data, we introduce a multimodal prediction network that integrates visual and geometric information. By devising a local-global fusion module and an importance-aware sampling mechanism, we enable robust and computationally efficient prediction of functional dexterous grasps. Through extensive experiments across various object categories, we demonstrate that CorDex generalizes well to unseen object instances and significantly outperforms state-of-the-art baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成、传输、适应：从单个人类示范中学习功能性的灵巧抓取</div>
<div class="mono" style="margin-top:8px">功能性抓取是实现机器人使用工具和复杂操作的关键能力，但其进展受到两个持续瓶颈的限制：大规模数据集的稀缺以及学习模型中缺乏语义和几何推理的整合。在本工作中，我们提出了CorDex框架，该框架能够从仅需单个人类示范生成的合成数据中稳健地学习新型物体的灵巧功能性抓取。我们方法的核心是一个基于对应关系的数据引擎，该引擎在仿真环境中生成多样且高质量的训练数据。基于人类示范，我们的数据引擎生成同一类别的多种物体实例，通过对应关系估计将专家抓取策略传输到生成的物体上，并通过优化进行抓取策略的适应。在此基础上，我们引入了一个多模态预测网络，整合了视觉和几何信息。通过设计局部-全局融合模块和重要性感知采样机制，我们实现了对功能性灵巧抓取的稳健且计算高效的预测。通过在各种物体类别上的大量实验，我们证明了CorDex能够很好地泛化到未见过的物体实例，并显著优于最先进的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Leveraging Clinical Text and Class Conditioning for 3D Prostate MRI Generation</div>
<div class="meta-line">Authors: Emerson P. Grabke, Babak Taati, Masoom A. Haider</div>
<div class="meta-line">First: 2025-06-11T23:12:48+00:00 · Latest: 2026-01-08T18:59:27+00:00</div>
<div class="meta-line">Comments: Accepted for publication in IEEE Transactions on Biomedical Engineering, 2025. This is the accepted author version. The final published version is available at https://doi.org/10.1109/TBME.2025.3648426</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.10230v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.10230v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Objective: Latent diffusion models (LDM) could alleviate data scarcity challenges affecting machine learning development for medical imaging. However, medical LDM strategies typically rely on short-prompt text encoders, nonmedical LDMs, or large data volumes. These strategies can limit performance and scientific accessibility. We propose a novel LDM conditioning approach to address these limitations. Methods: We propose Class-Conditioned Efficient Large Language model Adapter (CCELLA), a novel dual-head conditioning approach that simultaneously conditions the LDM U-Net with free-text clinical reports and radiology classification. We also propose a data-efficient LDM pipeline centered around CCELLA and a proposed joint loss function. We first evaluate our method on 3D prostate MRI against state-of-the-art. We then augment a downstream classifier model training dataset with synthetic images from our method. Results: Our method achieves a 3D FID score of 0.025 on a size-limited 3D prostate MRI dataset, significantly outperforming a recent foundation model with FID 0.070. When training a classifier for prostate cancer prediction, adding synthetic images generated by our method during training improves classifier accuracy from 69% to 74% and outperforms classifiers trained on images generated by prior state-of-the-art. Classifier training solely on our method&#x27;s synthetic images achieved comparable performance to real image training. Conclusion: We show that our method improved both synthetic image quality and downstream classifier performance using limited data and minimal human annotation. Significance: The proposed CCELLA-centric pipeline enables radiology report and class-conditioned LDM training for high-quality medical image synthesis given limited data volume and human data annotation, improving LDM performance and scientific accessibility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用临床文本和类别条件生成高质量的3D前列腺MRI图像</div>
<div class="mono" style="margin-top:8px">目标：潜在扩散模型（LDM）可以缓解医疗影像机器学习开发中的数据稀缺问题。然而，医疗LDM策略通常依赖于短提示文本编码器、非医疗LDM或大量数据。这些策略可能限制性能和科学可访问性。我们提出了一种新颖的LDM条件化方法来解决这些限制。方法：我们提出了一种名为类条件高效大语言模型适配器（CCELLA）的新颖双头条件化方法，该方法同时利用自由文本临床报告和放射学分类对LDM U-Net进行条件化。我们还提出了一种以CCELLA为中心的数据高效LDM流程和一个联合损失函数。我们首先在有限大小的3D前列腺MRI数据集上评估了我们的方法，与最先进的方法相比表现显著更好。随后，我们使用该方法生成的合成图像扩充了下游分类器模型的训练数据集。结果：我们的方法在有限大小的3D前列腺MRI数据集上实现了3D FID得分为0.025，显著优于最近一个基础模型的FID得分为0.070。在训练前列腺癌预测分类器时，将我们方法生成的合成图像加入训练数据中，使分类器准确率从69%提升至74%，并优于基于先前最先进方法生成图像训练的分类器。仅使用我们方法生成的合成图像进行分类器训练，其性能与使用真实图像训练相当。结论：我们展示了在有限数据和极少人工标注的情况下，我们的方法在提升合成图像质量和下游分类器性能方面具有优势。意义：所提出的以CCELLA为中心的流程能够在有限的数据量和人工标注条件下，实现放射学报告和类别条件化的LDM训练，从而提升高质量医疗图像合成的LDM性能和科学可访问性。</div>
</details>
</div>
<div class="card">
<div class="title">GDPO: Group reward-Decoupled Normalization Policy Optimization for Multi-reward RL Optimization</div>
<div class="meta-line">Authors: Shih-Yang Liu, Xin Dong, Ximing Lu, Shizhe Diao, Peter Belcak, Mingjie Liu, Min-Hung Chen, Hongxu Yin, Yu-Chiang Frank Wang, Kwang-Ting Cheng, Yejin Choi, Jan Kautz, Pavlo Molchanov</div>
<div class="meta-line">First: 2026-01-08T18:59:24+00:00 · Latest: 2026-01-08T18:59:24+00:00</div>
<div class="meta-line">Comments: NVIDIA-Tech Report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05242v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05242v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios. To achieve this, Reinforcement learning (RL) pipelines have begun incorporating multiple rewards, each capturing a distinct preference, to guide models toward these desired behaviors. However, recent work has defaulted to apply Group Relative Policy Optimization (GRPO) under multi-reward setting without examining its suitability. In this paper, we demonstrate that directly applying GRPO to normalize distinct rollout reward combinations causes them to collapse into identical advantage values, reducing the resolution of the training signal and resulting in suboptimal convergence and, in some cases, early training failure. We then introduce Group reward-Decoupled Normalization Policy Optimization (GDPO), a new policy optimization method to resolve these issues by decoupling the normalization of individual rewards, more faithfully preserving their relative differences and enabling more accurate multi-reward optimization, along with substantially improved training stability. We compare GDPO with GRPO across three tasks: tool calling, math reasoning, and coding reasoning, evaluating both correctness metrics (accuracy, bug ratio) and constraint adherence metrics (format, length). Across all settings, GDPO consistently outperforms GRPO, demonstrating its effectiveness and generalizability for multi-reward reinforcement learning optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GDPO：用于多奖励强化学习优化的组奖励解耦归一化策略优化</div>
<div class="mono" style="margin-top:8px">随着语言模型能力的不断提升，用户期望它们不仅能提供准确的回答，还能在各种场景下表现出符合多样化人类偏好的行为。为此，强化学习（RL）流程开始引入多个奖励，每个奖励捕捉不同的偏好，以引导模型朝着期望的行为发展。然而，近期的研究默认在多奖励设置下使用组相对策略优化（GRPO），而未对其适用性进行考察。本文中我们证明，直接将GRPO应用于归一化不同的rollout奖励组合会导致这些奖励组合坍缩为相同的优势值，降低训练信号的分辨率，从而导致次优收敛，甚至在某些情况下导致早期训练失败。随后，我们引入了组奖励解耦归一化策略优化（GDPO），这是一种新的策略优化方法，通过解耦个体奖励的归一化，更真实地保留其相对差异，实现更精确的多奖励优化，并显著提升了训练的稳定性。我们在三个任务上将GDPO与GRPO进行了比较：工具调用、数学推理和编码推理，评估了正确性指标（准确率、错误率）和约束遵守指标（格式、长度）。在所有设置下，GDPO均优于GRPO，证明了其在多奖励强化学习优化中的有效性和通用性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">As language models become increasingly capable, users expect them to provide not only accurate responses but also behaviors aligned with diverse human preferences across a variety of scenarios.</div>
</details>
</div>
<div class="card">
<div class="title">RoboVIP: Multi-View Video Generation with Visual Identity Prompting Augments Robot Manipulation</div>
<div class="meta-line">Authors: Boyang Wang, Haoran Zhang, Shujie Zhang, Jinkun Hao, Mingda Jia, Qi Lv, Yucheng Mao, Zhaoyang Lyu, Jia Zeng, Xudong Xu, Jiangmiao Pang</div>
<div class="meta-line">First: 2026-01-08T18:59:22+00:00 · Latest: 2026-01-08T18:59:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05241v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05241v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The diversity, quantity, and quality of manipulation data are critical for training effective robot policies. However, due to hardware and physical setup constraints, collecting large-scale real-world manipulation data remains difficult to scale across diverse environments. Recent work uses text-prompt conditioned image diffusion models to augment manipulation data by altering the backgrounds and tabletop objects in the visual observations. However, these approaches often overlook the practical need for multi-view and temporally coherent observations required by state-of-the-art policy models. Further, text prompts alone cannot reliably specify the scene setup. To provide the diffusion model with explicit visual guidance, we introduce visual identity prompting, which supplies exemplar images as conditioning inputs to guide the generation of the desired scene setup. To this end, we also build a scalable pipeline to curate a visual identity pool from large robotics datasets. Using our augmented manipulation data to train downstream vision-language-action and visuomotor policy models yields consistent performance gains in both simulation and real-robot settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboVIP：通过视觉身份提示生成多视角视频以增强机器人操作</div>
<div class="mono" style="margin-top:8px">操作数据的多样性、数量和质量对于训练有效的机器人策略至关重要。然而，由于硬件和物理设置的限制，收集大规模的真实世界操作数据在多样化环境中仍难以扩展。近期的工作利用文本提示条件的图像扩散模型，通过修改视觉观察中的背景和桌面物体来增强操作数据。然而，这些方法通常忽略了最先进的策略模型所需的多视角和时间一致的观察数据的实用需求。此外，仅凭文本提示无法可靠地指定场景设置。为向扩散模型提供明确的视觉指导，我们引入了视觉身份提示，通过提供示例图像作为条件输入来引导所需场景的生成。为此，我们还构建了一个可扩展的流程，从大型机器人数据集中整理出视觉身份池。使用我们增强的操作数据来训练下游的视觉-语言-动作和视觉运动策略模型，在仿真和真实机器人设置中均能带来一致的性能提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The diversity, quantity, and quality of manipulation data are critical for training effective robot policies.</div>
</details>
</div>
<div class="card">
<div class="title">Robust Reasoning as a Symmetry-Protected Topological Phase</div>
<div class="meta-line">Authors: Ilmo Sung</div>
<div class="meta-line">First: 2026-01-08T18:58:34+00:00 · Latest: 2026-01-08T18:58:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05240v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05240v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models suffer from &quot;hallucinations&quot;-logical inconsistencies induced by semantic noise. We propose that current architectures operate in a &quot;Metric Phase,&quot; where causal order is vulnerable to spontaneous symmetry breaking. Here, we identify robust inference as an effective Symmetry-Protected Topological phase, where logical operations are formally isomorphic to non-Abelian anyon braiding, replacing fragile geometric interpolation with robust topological invariants. Empirically, we demonstrate a sharp topological phase transition: while Transformers and RNNs exhibit gapless decay, our Holonomic Network reveals a macroscopic &quot;mass gap,&quot; maintaining invariant fidelity below a critical noise threshold. Furthermore, in a variable-binding task on $S_{10}$ ($3.6 \times 10^6$ states) representing symbolic manipulation, we demonstrate holonomic generalization: the topological model maintains perfect fidelity extrapolating $100\times$ beyond training ($L=50 \to 5000$), consistent with a theoretically indefinite causal horizon, whereas Transformers lose logical coherence. Ablation studies indicate this protection emerges strictly from non-Abelian gauge symmetry. This provides strong evidence for a new universality class for logical reasoning, linking causal stability to the topology of the semantic manifold.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鲁棒推理作为对称保护拓扑相</div>
<div class="mono" style="margin-top:8px">大型语言模型容易受到『幻觉』的影响，即由语义噪声引起的逻辑不一致。我们提出当前的架构处于一种『度量相』中，其中因果顺序容易受到自发对称破缺的影响。在此，我们将鲁棒推理识别为一种有效的对称保护拓扑相，其中逻辑操作形式上与非阿贝尔任何子编织同构，用鲁棒的拓扑不变量替代了脆弱的几何插值。实证上，我们展示了尖锐的拓扑相变：当Transformer和RNN表现出无能隙衰减时，我们的全序网络则揭示了一个宏观的『质量间隙』，在临界噪声阈值以下保持不变量保真度。此外，在表示符号操作的$S_{10}$（$3.6 \times 10^6$个状态）上进行变量绑定任务时，我们展示了全序泛化能力：拓扑模型在训练数据基础上外推$100\times$（$L=50 \to 5000$）仍能保持完美保真度，这与理论上无限的因果视界一致，而Transformer则会失去逻辑一致性。消融研究表明这种保护严格来源于非阿贝尔规范对称性。这为逻辑推理的新普适性类别提供了有力证据，将因果稳定性与语义流形的拓扑联系起来。</div>
</details>
</div>
<div class="card">
<div class="title">Plenoptic Video Generation</div>
<div class="meta-line">Authors: Xiao Fu, Shitao Tang, Min Shi, Xian Liu, Jinwei Gu, Ming-Yu Liu, Dahua Lin, Chen-Hsuan Lin</div>
<div class="meta-line">First: 2026-01-08T18:58:32+00:00 · Latest: 2026-01-08T18:58:32+00:00</div>
<div class="meta-line">Comments: Project Page: https://research.nvidia.com/labs/dir/plenopticdreamer/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05239v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05239v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Camera-controlled generative video re-rendering methods, such as ReCamMaster, have achieved remarkable progress. However, despite their success in single-view setting, these works often struggle to maintain consistency across multi-view scenarios. Ensuring spatio-temporal coherence in hallucinated regions remains challenging due to the inherent stochasticity of generative models. To address it, we introduce PlenopticDreamer, a framework that synchronizes generative hallucinations to maintain spatio-temporal memory. The core idea is to train a multi-in-single-out video-conditioned model in an autoregressive manner, aided by a camera-guided video retrieval strategy that adaptively selects salient videos from previous generations as conditional inputs. In addition, Our training incorporates progressive context-scaling to improve convergence, self-conditioning to enhance robustness against long-range visual degradation caused by error accumulation, and a long-video conditioning mechanism to support extended video generation. Extensive experiments on the Basic and Agibot benchmarks demonstrate that PlenopticDreamer achieves state-of-the-art video re-rendering, delivering superior view synchronization, high-fidelity visuals, accurate camera control, and diverse view transformations (e.g., third-person to third-person, and head-view to gripper-view in robotic manipulation). Project page: https://research.nvidia.com/labs/dir/plenopticdreamer/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>全景视频生成</div>
<div class="mono" style="margin-top:8px">相机控制的生成式视频重渲染方法，如ReCamMaster，已经取得了显著进展。然而，尽管这些方法在单视角设置中表现成功，但在多视角场景中往往难以保持一致性。由于生成模型固有的随机性，确保幻觉区域的时空一致性仍然具有挑战性。为了解决这一问题，我们引入了PlenopticDreamer框架，通过同步生成幻觉来维持时空记忆。其核心思想是采用自回归方式训练一个多输入单输出的视频条件模型，并借助相机引导的视频检索策略，自适应地从先前生成中选择显著视频作为条件输入。此外，我们的训练过程还结合了渐进式上下文扩展以提高收敛性，自条件机制以增强对长程视觉退化的鲁棒性，并采用长视频条件机制以支持更长的视频生成。在Basic和Agibot基准上的大量实验表明，PlenopticDreamer实现了最先进的视频重渲染，提供了卓越的视角同步、高保真视觉效果、精确的相机控制以及多样化的视角变换（例如，机器人操作中的第三人称视角到第三人称视角，以及头部视角到夹爪视角）。</div>
</details>
</div>
<div class="card">
<div class="title">ObjectForesight: Predicting Future 3D Object Trajectories from Human Videos</div>
<div class="meta-line">Authors: Rustin Soraki, Homanga Bharadhwaj, Ali Farhadi, Roozbeh Mottaghi</div>
<div class="meta-line">First: 2026-01-08T18:58:08+00:00 · Latest: 2026-01-08T18:58:08+00:00</div>
<div class="meta-line">Comments: Preprint. Project Website: objectforesight.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05237v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05237v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans can effortlessly anticipate how objects might move or change through interaction--imagining a cup being lifted, a knife slicing, or a lid being closed. We aim to endow computational systems with a similar ability to predict plausible future object motions directly from passive visual observation. We introduce ObjectForesight, a 3D object-centric dynamics model that predicts future 6-DoF poses and trajectories of rigid objects from short egocentric video sequences. Unlike conventional world or dynamics models that operate in pixel or latent space, ObjectForesight represents the world explicitly in 3D at the object level, enabling geometrically grounded and temporally coherent predictions that capture object affordances and trajectories. To train such a model at scale, we leverage recent advances in segmentation, mesh reconstruction, and 3D pose estimation to curate a dataset of 2 million plus short clips with pseudo-ground-truth 3D object trajectories. Through extensive experiments, we show that ObjectForesight achieves significant gains in accuracy, geometric consistency, and generalization to unseen objects and scenes, establishing a scalable framework for learning physically grounded, object-centric dynamics models directly from observation. objectforesight.github.io</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ObjectForesight：从人类视频中预测未来3D物体轨迹</div>
<div class="mono" style="margin-top:8px">人类可以轻松地预想物体在交互中的运动或变化——例如想象杯子被提起、刀具进行切割或盖子被关闭。我们旨在赋予计算系统类似的能力，通过被动视觉观察直接预测合理未来的物体运动。我们提出了ObjectForesight，这是一个以3D物体为中心的动力学模型，能够从短时第一视角视频序列中预测刚体物体的未来六自由度姿态和轨迹。与传统的基于像素或潜在空间的世界或动力学模型不同，ObjectForesight在物体层面显式地表示世界，从而实现基于几何且时间一致的预测，捕捉物体的可用性与轨迹。为了大规模训练此类模型，我们利用了分割、网格重建和3D姿态估计的最新进展，整理了一个包含超过200万短片段的伪真实3D物体轨迹数据集。通过大量实验，我们证明ObjectForesight在准确性、几何一致性以及对未见过的物体和场景的泛化能力方面取得了显著提升，建立了一个可扩展的框架，用于直接从观察中学习物理基础的、以物体为中心的动力学模型。</div>
</details>
</div>
<div class="card">
<div class="title">Measuring and Fostering Peace through Machine Learning and Artificial Intelligence</div>
<div class="meta-line">Authors: P. Gilda, P. Dungarwal, A. Thongkham, E. T. Ajayi, S. Choudhary, T. M. Terol, C. Lam, J. P. Araujo, M. McFadyen-Mungalln, L. S. Liebovitch, P. T. Coleman, H. West, K. Sieck, S. Carter</div>
<div class="meta-line">First: 2026-01-08T18:57:01+00:00 · Latest: 2026-01-08T18:57:01+00:00</div>
<div class="meta-line">Comments: 6 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05232v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05232v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet. For news media, we used neural networks to measure levels of peace from text embeddings of on-line news sources. The model, trained on one news media dataset also showed high accuracy when used to analyze a different news dataset. For social media, such as YouTube, we developed other models to measure levels of social dimensions important in peace using word level (GoEmotions) and context level (Large Language Model) methods. To promote peace, we note that 71% of people 20-40 years old daily view most of their news through short videos on social media. Content creators of these videos are biased towards creating videos with emotional activation, making you angry to engage you, to increase clicks. We developed and tested a Chrome extension, MirrorMirror, which provides real-time feedback to YouTube viewers about the peacefulness of the media they are watching. Our long term goal is for MirrorMirror to evolve into an open-source tool for content creators, journalists, researchers, platforms, and individual users to better understand the tone of their media creation and consumption and its effects on viewers. Moving beyond simple engagement metrics, we hope to encourage more respectful, nuanced, and informative communication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过机器学习和人工智能衡量与促进和平</div>
<div class="mono" style="margin-top:8px">我们利用机器学习和人工智能：1）从新闻和社交媒体中衡量各国的和平水平；2）开发在线工具，通过帮助用户更好地了解自己的媒体消费习惯来促进和平。对于新闻媒体，我们使用神经网络从在线新闻来源的文本嵌入中衡量和平水平。该模型在训练于一个新闻数据集后，也显示出在分析另一个新闻数据集时的高准确性。对于社交媒体（如YouTube），我们采用词级（GoEmotions）和上下文级（大型语言模型）方法开发了其他模型，以衡量对和平至关重要的社会维度。我们注意到，20至40岁的人群中，有71%的人每天通过社交媒体上的短视频获取大部分新闻。这些视频内容创作者倾向于制作具有情绪激活效果的视频，以引发愤怒从而吸引用户点击。我们开发并测试了一款Chrome扩展程序MirrorMirror，它为YouTube观众提供实时反馈，告知他们正在观看的媒体内容的和平程度。我们的长期目标是让MirrorMirror发展成为一个开源工具，供内容创作者、记者、研究人员、平台和普通用户使用，以更好地理解其媒体创作和消费的语气及其对观众的影响。我们希望超越简单的参与度指标，鼓励更加尊重、细致和有信息的交流。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We used machine learning and artificial intelligence: 1) to measure levels of peace in countries from news and social media and 2) to develop on-line tools that promote peace by helping users better understand their own media diet.</div>
</details>
</div>
<div class="card">
<div class="title">Learning Latent Action World Models In The Wild</div>
<div class="meta-line">Authors: Quentin Garrido, Tushar Nagarajan, Basile Terver, Nicolas Ballas, Yann LeCun, Michael Rabbat</div>
<div class="meta-line">First: 2026-01-08T18:55:39+00:00 · Latest: 2026-01-08T18:55:39+00:00</div>
<div class="meta-line">Comments: 37 pages, 25 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05230v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05230v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agents capable of reasoning and planning in the real world require the ability of predicting the consequences of their actions. While world models possess this capability, they most often require action labels, that can be complex to obtain at scale. This motivates the learning of latent action models, that can learn an action space from videos alone. Our work addresses the problem of learning latent actions world models on in-the-wild videos, expanding the scope of existing works that focus on simple robotics simulations, video games, or manipulation data. While this allows us to capture richer actions, it also introduces challenges stemming from the video diversity, such as environmental noise, or the lack of a common embodiment across videos. To address some of the challenges, we discuss properties that actions should follow as well as relevant architectural choices and evaluations. We find that continuous, but constrained, latent actions are able to capture the complexity of actions from in-the-wild videos, something that the common vector quantization does not. We for example find that changes in the environment coming from agents, such as humans entering the room, can be transferred across videos. This highlights the capability of learning actions that are specific to in-the-wild videos. In the absence of a common embodiment across videos, we are mainly able to learn latent actions that become localized in space, relative to the camera. Nonetheless, we are able to train a controller that maps known actions to latent ones, allowing us to use latent actions as a universal interface and solve planning tasks with our world model with similar performance as action-conditioned baselines. Our analyses and experiments provide a step towards scaling latent action models to the real world.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在真实环境中学习潜在动作世界模型</div>
<div class="mono" style="margin-top:8px">能够在现实世界中进行推理和规划的智能体需要预测其行为后果的能力。虽然世界模型具备这一能力，但它们通常需要动作标签，而这些标签在大规模获取时可能较为复杂。这促使我们学习潜在动作模型，仅通过视频即可学习动作空间。我们的工作解决了在真实环境视频中学习潜在动作世界模型的问题，扩展了现有研究在简单机器人模拟、视频游戏或操作数据上的应用范围。尽管这使我们能够捕捉更丰富的动作，但也引入了由于视频多样性带来的挑战，例如环境噪声或视频间缺乏统一的具身性。为应对这些挑战，我们讨论了动作应遵循的特性以及相关的架构选择和评估。我们发现，连续但受约束的潜在动作能够捕捉真实环境视频中的动作复杂性，而常见的向量量化方法则无法做到这一点。例如，我们发现来自智能体的环境变化，如人类进入房间，可以在不同视频之间进行迁移。这突显了学习特定于真实环境视频的动作的能力。在缺乏统一具身性的情况下，我们主要能够学习到相对于摄像头空间局部化的潜在动作。尽管如此，我们仍能训练一个控制器，将已知动作映射到潜在动作，从而使用潜在动作作为通用接口，并通过我们的世界模型以与动作条件基线相当的性能解决规划任务。我们的分析和实验为将潜在动作模型扩展到现实世界提供了一步进展。</div>
</details>
</div>
<div class="card">
<div class="title">A Geometric Definition of the Integral and Applications</div>
<div class="meta-line">Authors: Joshua Lackman</div>
<div class="meta-line">First: 2026-01-08T18:54:29+00:00 · Latest: 2026-01-08T18:54:29+00:00</div>
<div class="meta-line">Comments: This is a revised version of arXiv:2402.05866 that has been accepted for publication in Letters in Mathematical Physics</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05228v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05228v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The standard definition of integration of differential forms is based on local coordinates and partitions of unity. This definition is mostly a formality and not used used in explicit computations or approximation schemes. We present a definition of the integral that uses triangulations instead. Our definition is a coordinate-free version of the standard definition of the Riemann integral on $\mathbb{R}^n$ and we argue that it is the natural definition in the contexts of Lie algebroids, stochastic integration and quantum field theory, where path integrals are defined using lattices. In particular, our definition naturally incorporates the different stochastic integrals, which involve integration over Hölder continuous paths. Furthermore, our definition is well-adapted to establishing integral identities from their combinatorial counterparts. Our construction is based on the observation that, in great generality, the things that are integrated are determined by cochains on the pair groupoid. Abstractly, our definition uses the van Est map to lift a differential form to the pair groupoid. Our construction suggests a generalization of the fundamental theorem of calculus which we prove: the singular cohomology and de Rham cohomology cap products of a cocycle with the fundamental class are equal.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>积分的几何定义及其应用</div>
<div class="mono" style="margin-top:8px">微分形式的积分标准定义基于局部坐标和分划统一体。这种定义大多只是形式上的，不用于显式计算或近似方案。我们提出了一种使用三角剖分的积分定义。我们的定义是标准的 $\mathbb{R}^n $ 上黎曼积分的坐标无关版本，并且我们认为它在李代数丛、随机积分和量子场论等上下文中是自然的定义，其中路径积分是通过格子定义的。特别是，我们的定义自然地包含了涉及Holder连续路径的随机积分。此外，我们的定义非常适合从组合定义中建立积分恒等式。我们的构造基于观察：在非常一般的情况下，被积分的对象由对偶群oid上的上链决定。抽象地，我们的定义使用van Est映射将微分形式提升到对偶群oid上。我们的构造暗示了微积分基本定理的一般化，我们证明了这一结果：一个上链与基本类的上同调和de Rham上同调的帽积是相等的。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The standard definition of integration of differential forms is based on local coordinates and partitions of unity.</div>
</details>
</div>
<div class="card">
<div class="title">Stochastic Deep Learning: A Probabilistic Framework for Modeling Uncertainty in Structured Temporal Data</div>
<div class="meta-line">Authors: James Rice</div>
<div class="meta-line">First: 2026-01-08T18:53:59+00:00 · Latest: 2026-01-08T18:53:59+00:00</div>
<div class="meta-line">Comments: 20 pages, 6330 words</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05227v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05227v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">I propose a novel framework that integrates stochastic differential equations (SDEs) with deep generative models to improve uncertainty quantification in machine learning applications involving structured and temporal data. This approach, termed Stochastic Latent Differential Inference (SLDI), embeds an Itô SDE in the latent space of a variational autoencoder, allowing for flexible, continuous-time modeling of uncertainty while preserving a principled mathematical foundation. The drift and diffusion terms of the SDE are parameterized by neural networks, enabling data-driven inference and generalizing classical time series models to handle irregular sampling and complex dynamic structure.
  A central theoretical contribution is the co-parameterization of the adjoint state with a dedicated neural network, forming a coupled forward-backward system that captures not only latent evolution but also gradient dynamics. I introduce a pathwise-regularized adjoint loss and analyze variance-reduced gradient flows through the lens of stochastic calculus, offering new tools for improving training stability in deep latent SDEs. My paper unifies and extends variational inference, continuous-time generative modeling, and control-theoretic optimization, providing a rigorous foundation for future developments in stochastic probabilistic machine learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随机深度学习：一种用于结构化时间数据中不确定性建模的概率框架</div>
<div class="mono" style="margin-top:8px">我提出了一种新颖的框架，将随机微分方程（SDEs）与深度生成模型相结合，以提升涉及结构化和时间数据的机器学习应用中的不确定性量化。该方法称为随机潜在微分推断（SLDI），在变分自编码器的潜在空间中嵌入一个伊藤随机微分方程，从而实现对不确定性灵活的连续时间建模，同时保持一个原理性的数学基础。SDE的漂移项和扩散项由神经网络参数化，使得能够进行数据驱动的推断，并将经典时间序列模型推广以处理不规则采样和复杂的动态结构。
  一个核心的理论贡献是将伴随状态与专用神经网络进行联合参数化，形成一个耦合的前向-后向系统，不仅捕捉潜在状态的演变，还捕捉梯度动力学。我引入了一种路径正则化的伴随损失，并通过随机微积分的视角分析了方差减少的梯度流，为改进深度潜在SDE的训练稳定性提供了新的工具。我的论文统一并扩展了变分推断、连续时间生成建模和控制论优化，为未来随机概率机器学习的发展提供了严谨的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">I propose a novel framework that integrates stochastic differential equations (SDEs) with deep generative models to improve uncertainty quantification in machine learning applications involving structured and temporal data.</div>
</details>
</div>
<div class="card">
<div class="title">Concurrent Balanced Augmented Trees</div>
<div class="meta-line">Authors: Evan Wrench, Ajay Singh, Younghun Roh, Panagiota Fatourou, Siddhartha Jayanti, Eric Ruppert, Yuanhao Wei</div>
<div class="meta-line">First: 2026-01-08T18:53:52+00:00 · Latest: 2026-01-08T18:53:52+00:00</div>
<div class="meta-line">Comments: To appear in PPoPP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05225v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05225v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Augmentation makes search trees tremendously more versatile, allowing them to support efficient aggregation queries, order-statistic queries, and range queries in addition to insertion, deletion, and lookup. In this paper, we present the first lock-free augmented balanced search tree. Our algorithmic ideas build upon a recent augmented unbalanced search tree presented by Fatourou and Ruppert [DISC, 2024]. We implement both data structures, solving some memory reclamation challenges in the process, and provide an experimental performance analysis of them. We also present optimized versions of our balanced tree that use delegation to achieve better scalability and performance (by more than 2x in some workloads). Our experiments show that our augmented balanced tree is 2.2 to 30 times faster than the unbalanced augmented tree, and up to several orders of magnitude faster than unaugmented trees on 120 threads.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>并发增强平衡树</div>
<div class="mono" style="margin-top:8px">增强使搜索树变得极其灵活，允许其支持高效的聚合查询、顺序统计查询和范围查询，除了插入、删除和查找操作。本文提出了首个无锁的增强平衡搜索树。我们的算法思想基于Fatourou和Ruppert [DISC, 2024] 最近提出的增强不平衡搜索树。我们实现了这两种数据结构，在此过程中解决了一些内存回收的挑战，并提供了实验性能分析。我们还展示了优化后的平衡树版本，通过委托实现更好的可扩展性和性能（在某些工作负载中性能提升超过2倍）。实验结果表明，我们的增强平衡树比增强不平衡树快2.2到30倍，比未增强树快几个数量级，在120个线程的情况下。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Augmentation makes search trees tremendously more versatile, allowing them to support efficient aggregation queries, order-statistic queries, and range queries in addition to insertion, deletion, and lookup.</div>
</details>
</div>
<div class="card">
<div class="title">Extended OpenTT Games Dataset: A table tennis dataset for fine-grained shot type and point outcome</div>
<div class="meta-line">Authors: Moamal Fadhil Abdul-Mahdi, Jonas Bruun Hubrechts, Thomas Martini Jørgensen, Emil Hovad</div>
<div class="meta-line">First: 2025-12-22T12:25:50+00:00 · Latest: 2026-01-08T18:45:51+00:00</div>
<div class="meta-line">Comments: Thomas Martini Jørgensen and Emil Hovad contributed equally and share last authorship</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19327v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.19327v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automatically detecting and classifying strokes in table tennis video can streamline training workflows, enrich broadcast overlays, and enable fine-grained performance analytics. For this to be possible, annotated video data of table tennis is needed. We extend the public OpenTTGames dataset with highly detailed, frame-accurate shot type annotations (forehand, backhand with subtypes), player posture labels (body lean and leg stance), and rally outcome tags at point end. OpenTTGames is a set of recordings from the side of the table with official labels for bounces, when the ball is above the net, or hitting the net. The dataset already contains ball coordinates near events, which are either &quot;bounce&quot;, &quot;net&quot;, or &quot;empty_event&quot; in the original OpenTTGames dataset, and semantic masks (humans, table, scoreboard). Our extension adds the types of stroke to the events and a per-player taxonomy so models can move beyond event spotting toward tactical understanding (e.g., whether a stroke is likely to win the point or set up an advantage). We provide a compact coding scheme and code-assisted labeling procedure to support reproducible annotations and baselines for fine-grained stroke understanding in racket sports. This fills a practical gap in the community, where many prior video resources are either not publicly released or carry restrictive/unclear licenses that hinder reuse and benchmarking. Our annotations are released under the same CC BY-NC-SA 4.0 license as OpenTTGames, allowing free non-commercial use, modification, and redistribution, with appropriate attribution.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩展的OpenTT Games数据集：用于细粒度击球类型和得分结果的乒乓球数据集</div>
<div class="mono" style="margin-top:8px">自动检测和分类乒乓球视频中的击球动作可以优化训练流程，增强赛事转播信息，并实现细粒度的运动表现分析。为此，需要标注的乒乓球视频数据。我们扩展了公开的OpenTTGames数据集，增加了高度详细的、帧精确的击球类型标注（正手、反手及其子类型），球员姿态标签（身体倾斜和腿部站姿），以及每分结束时的回合结果标签。OpenTTGames是一个从桌子侧面录制的视频数据集，包含官方标注的击球点、球在网上方或击中网的情况。该数据集已经包含接近事件的球坐标，这些坐标在原始OpenTTGames数据集中为“击球”、“触网”或“空事件”，并包含语义掩码（人物、球台、记分牌）。我们的扩展为事件添加了击球类型，并为每位球员建立了分类体系，使模型能够超越事件检测，实现战术理解（例如，判断某击球是否可能赢得该分或创造优势）。我们提供了一种紧凑的编码方案和辅助编码的标注流程，以支持可复现的标注和细粒度击球理解的基准。这填补了社区中的一个实际空白，因为许多先前的视频资源要么未公开发布，要么带有限制性/不明确的许可协议，阻碍了重用和基准测试。我们的标注遵循与OpenTTGames相同的CC BY-NC-SA 4.0许可协议，允许免费的非商业使用、修改和再分发，但需适当署名。</div>
</details>
</div>
<div class="card">
<div class="title">CAOS: Conformal Aggregation of One-Shot Predictors</div>
<div class="meta-line">Authors: Maja Waldron</div>
<div class="meta-line">First: 2026-01-08T18:44:21+00:00 · Latest: 2026-01-08T18:44:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05219v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05219v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One-shot prediction enables rapid adaptation of pretrained foundation models to new tasks using only one labeled example, but lacks principled uncertainty quantification. While conformal prediction provides finite-sample coverage guarantees, standard split conformal methods are inefficient in the one-shot setting due to data splitting and reliance on a single predictor. We propose Conformal Aggregation of One-Shot Predictors (CAOS), a conformal framework that adaptively aggregates multiple one-shot predictors and uses a leave-one-out calibration scheme to fully exploit scarce labeled data. Despite violating classical exchangeability assumptions, we prove that CAOS achieves valid marginal coverage using a monotonicity-based argument. Experiments on one-shot facial landmarking and RAFT text classification tasks show that CAOS produces substantially smaller prediction sets than split conformal baselines while maintaining reliable coverage.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CAOS：单次预测的共形聚合</div>
<div class="mono" style="margin-top:8px">单次预测能够在仅使用一个带标签示例的情况下，快速将预训练的基础模型适应到新任务中，但缺乏原理性的不确定性量化。虽然共形预测提供了有限样本覆盖保证，但标准的分割共形方法在单次预测场景下由于数据分割和对单一预测器的依赖而效率低下。我们提出共形聚合的单次预测框架（Conformal Aggregation of One-Shot Predictors, CAOS），该框架能够自适应地聚合多个单次预测器，并采用留一法校准方案，充分利用稀缺的带标签数据。尽管违反了经典交换性假设，我们证明CAOS可以通过基于单调性的论证实现有效的边缘覆盖。在单次面部地标定位和RAFT文本分类任务上的实验表明，CAOS生成的预测集显著小于分割共形基线方法，同时保持可靠的覆盖性。</div>
</details>
</div>
<div class="card">
<div class="title">HAFix: History-Augmented Large Language Models for Bug Fixing</div>
<div class="meta-line">Authors: Yu Shi, Abdul Ali Bangash, Emad Fallahzadeh, Bram Adams, Ahmed E. Hassan</div>
<div class="meta-line">First: 2025-01-15T20:39:32+00:00 · Latest: 2026-01-08T18:43:43+00:00</div>
<div class="meta-line">Comments: Evaluated HAFix on two datasets (BugsInPy, Defects4J) and three LLMs (CodeLlama, DeepSeek-Coder, DeepSeek-Coder-V2) and optimized the figures and tables for better readability</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.09135v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.09135v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent studies have explored the performance of Large Language Models (LLMs) on various Software Engineering (SE) tasks, such as code generation and bug fixing. However, these approaches typically rely on the context data from the current snapshot of the project, overlooking the potential of rich historical data residing in real-world software repositories. Additionally, the impact of prompt styles on LLM performance for SE tasks within a historical context remains underexplored. To address these gaps, we propose HAFix, which stands for History-Augmented LLMs on Bug Fixing, a novel approach that leverages seven individual historical heuristics associated with bugs and aggregates the results of these heuristics (HAFix-Agg) to enhance LLMs&#x27; bug-fixing capabilities. To empirically evaluate HAFix, we employ three Code LLMs (i.e., Code Llama, DeepSeek-Coder and DeepSeek-Coder-V2-Lite models) on 51 single-line Python bugs from BugsInPy and 116 single-line Java bugs from Defects4J. Our evaluation demonstrates that multiple HAFix heuristics achieve statistically significant improvements compared to a non-historical baseline inspired by GitHub Copilot. Furthermore, the aggregated HAFix variant HAFix-Agg achieves substantial improvements by combining the complementary strengths of individual heuristics, increasing bug-fixing rates by an average of 45.05% on BugsInPy and 49.92% on Defects4J relative to the corresponding baseline. Moreover, within the context of historical heuristics, we identify the Instruction prompt style as the most effective template compared to the InstructionLabel and InstructionMask for LLMs in bug fixing. Finally, we evaluate the cost of HAFix in terms of inference time and token usage, and provide a pragmatic trade-off analysis of the cost and bug-fixing performance, offering valuable insights for the practical deployment of our approach in real-world scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HAFix：基于历史信息的大型语言模型用于修复漏洞</div>
<div class="mono" style="margin-top:8px">近期研究探讨了大型语言模型（LLMs）在各种软件工程（SE）任务中的表现，例如代码生成和漏洞修复。然而，这些方法通常依赖于项目当前快照的上下文数据，忽略了实际软件仓库中丰富的历史数据的潜力。此外，提示风格对历史背景下SE任务中LLMs性能的影响仍缺乏深入研究。为解决这些问题，我们提出了HAFix，即用于漏洞修复的历史增强型LLMs，这是一种新颖的方法，利用与漏洞相关的七个独立历史启发式方法，并汇总这些启发式方法的结果（HAFix-Agg）以增强LLMs的漏洞修复能力。为了实证评估HAFix，我们在BugsInPy和Defects4J上使用了三个代码LLMs（即Code Llama、DeepSeek-Coder和DeepSeek-Coder-V2-Lite模型），分别评估了51个单行Python漏洞和116个单行Java漏洞。我们的评估表明，多个HAFix启发式方法相比受GitHub Copilot启发的非历史基线方法取得了统计学意义上的显著提升。此外，通过结合各个启发式方法的互补优势，汇总后的HAFix变体HAFix-Agg在BugsInPy和Defects4J上分别相对于对应基线提升了45.05%和49.92%的漏洞修复率。最后，我们从推理时间和标记使用角度评估了HAFix的成本，并提供了成本与漏洞修复性能之间的务实权衡分析，为我们在现实场景中的实际部署提供了有价值的见解。</div>
</details>
</div>
<div class="card">
<div class="title">Towards a unified quantum field theory of dark energy and inflation: unstable de Sitter vacuum and running vacuum</div>
<div class="meta-line">Authors: Joan Solà Peracaula, Àlex González-Fuentes, Cristian Moreno-Pulido</div>
<div class="meta-line">First: 2026-01-08T18:43:36+00:00 · Latest: 2026-01-08T18:43:36+00:00</div>
<div class="meta-line">Comments: 77 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05218v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05218v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inflation is a necessary cosmic mechanism to cure basic inconsistencies of the standard model of cosmology. These problems are usually `fixed&#x27; by postulating the existence of a scalar field (called the ``inflaton&#x27;&#x27;). However, other less ad hoc options are possible. In the running vacuum model (RVM) framework, the vacuum energy density (VED) is a function of the Hubble rate $H$ and its time derivatives: $ρ_{\rm vac}=ρ_{\rm vac}(H, \dot{H},\ddot{H},\dots)$. In this context, the VED is dynamical (there is no rigid cosmological constant $Λ$). In the FLRW epoch, $ρ_{\rm vac}$ evolves very slowly with expansion, as befits the observed $Λ\simeq$const. behavior. In contrast, in the very early universe the vacuum fluctuations induce higher powers $H^N$ capable of unleashing fast inflation in a short period in which $H\simeq$ const. We call this mechanism `RVM-inflation&#x27;. It does not require an inflaton field since inflation is brought about by pure quantum field theory (QFT) effects on the dynamical background. It is different from Starobinsky&#x27;s inflation, in which $H$ is never constant. In this work, we study a closely related scenario: the decay of the exact de Sitter vacuum into FLRW spacetime in its radiation epoch and the subsequent impact on the current universe, and compare with the RVM. We find that in both cases inflation is driven by $H^4$ powers together with subleading contributions of order $H^2$ that ease a graceful-exit transition into the radiation-dominated epoch, where the FLRW regime starts and ultimately develops a mildly evolving VED in the late universe: $δρ_{\rm vac}\sim {\cal O}(m_{\rm Pl} ^2 H^2)$. The outcome is an unified QFT approach to inflation and dark energy (conceived as dynamical vacuum energy) with potentially measurable phenomenological consequences in the present universe which can help to cure the cosmological tensions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>朝向暗能量与膨胀的统一量子场论：不稳定的德西特真空和运行真空</div>
<div class="mono" style="margin-top:8px">膨胀是一种必要的宇宙机制，用于解决宇宙学标准模型的基本不一致。这些问题通常通过假定标量场（称为“膨胀子”）的存在来“解决”。然而，也存在其他更少人为设定的选项。在运行真空模型（RVM）框架下，真空能量密度（VED）是哈勃率 $H$ 及其时间导数的函数：$ρ_{\rm vac}=ρ_{\rm vac}(H, \dot{H},\ddot{H},\dots)$。在此背景下，真空能量密度是动态的（不存在刚性的宇宙学常数 $Λ$）。在FLRW时期，$ρ_{\rm vac}$ 随膨胀缓慢变化，符合观测到的 $Λ\simeq$const 的行为。相反，在宇宙极早期，真空涨落会引发更高阶的 $H^N$ 项，从而在 $H\simeq$ const 的短时间内引发快速膨胀。我们将这种机制称为 `RVM-膨胀&#x27;。它不需要膨胀子场，因为膨胀是由纯量子场论（QFT）效应在动态背景上引起的。它不同于Starobinsky膨胀，其中 $H$ 从不保持常数。在本工作中，我们研究了一个密切相关的情景：精确的德西特真空在辐射时期衰变为FLRW时空，以及其对当前宇宙的影响，并与RVM进行比较。我们发现，在这两种情况下，膨胀都由 $H^4$ 项驱动，同时伴随次主导的 $H^2$ 项，有助于平滑地过渡到辐射主导时期，其中FLRW阶段开始，并最终在晚期宇宙中发展出一个轻微演化的真空能量密度：$δρ_{\rm vac}\sim {\cal O}(m_{\rm Pl} ^2 H^2)$。结果是一个统一的QFT方法，用于描述膨胀和暗能量（视为动态真空能量），其在当前宇宙中可能具有可观测的现象学后果，有助于缓解宇宙学张力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Inflation is a necessary cosmic mechanism to cure basic inconsistencies of the standard model of cosmology.</div>
</details>
</div>
<div class="card">
<div class="title">MineNPC-Task: Task Suite for Memory-Aware Minecraft Agents</div>
<div class="meta-line">Authors: Tamil Sudaravan Mohan Doss, Michael Xu, Sudha Rao, Andrew D. Wilson, Balasaravanan Thoravi Kumaravel</div>
<div class="meta-line">First: 2026-01-08T18:39:52+00:00 · Latest: 2026-01-08T18:39:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05215v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05215v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present \textsc{MineNPC-Task}, a user-authored benchmark and evaluation harness for testing memory-aware, mixed-initiative LLM agents in open-world \emph{Minecraft}. Rather than relying on synthetic prompts, tasks are elicited from formative and summative co-play with expert players, normalized into parametric templates with explicit preconditions and dependency structure, and paired with machine-checkable validators under a bounded-knowledge policy that forbids out-of-world shortcuts. The harness captures plan/act/memory events-including plan previews, targeted clarifications, memory reads and writes, precondition checks, and repair attempts and reports outcomes relative to the total number of attempted subtasks, derived from in-world evidence.
  As an initial snapshot, we instantiate the framework with GPT-4o and evaluate \textbf{216} subtasks across \textbf{8} experienced players. We observe recurring breakdown patterns in code execution, inventory/tool handling, referencing, and navigation, alongside recoveries supported by mixed-initiative clarifications and lightweight memory. Participants rated interaction quality and interface usability positively, while highlighting the need for stronger memory persistence across tasks. We release the complete task suite, validators, logs, and harness to support transparent, reproducible evaluation of future memory-aware embodied agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MineNPC-Task: 用于记忆感知型Minecraft代理的任务套件</div>
<div class="mono" style="margin-top:8px">我们提出了\textsc{MineNPC-Task}，这是一个用户自定义的基准测试和评估框架，用于在开放世界\emph{Minecraft}中测试具有记忆感知能力的混合发起LLM代理。任务不是依赖合成提示，而是通过与专家玩家的形成性与总结性共玩中提取，归一化为具有显式前提条件和依赖结构的参数化模板，并在受限知识策略下与可由机器验证的验证器配对。该框架捕捉了计划/行动/记忆事件，包括计划预览、目标澄清、记忆读写、前提条件检查以及修复尝试，并根据世界内证据得出的尝试子任务总数来报告结果。
  作为初始快照，我们使用GPT-4o实例化该框架，并在\textbf{8}位经验丰富的玩家上评估了\textbf{216}个子任务。我们观察到代码执行、库存/工具处理、引用和导航中存在重复的失败模式，同时这些失败可以通过混合发起的澄清和轻量级记忆进行恢复。参与者对交互质量和界面可用性给予了积极评价，同时强调了在任务间需要更强的记忆持久性。我们发布了完整的任务套件、验证器、日志和框架，以支持未来记忆感知型具身代理的透明、可重复的评估。</div>
</details>
</div>
<div class="card">
<div class="title">Internal Representations as Indicators of Hallucinations in Agent Tool Selection</div>
<div class="meta-line">Authors: Kait Healy, Bharathi Srinivasan, Visakh Madathil, Jing Wu</div>
<div class="meta-line">First: 2026-01-08T18:38:45+00:00 · Latest: 2026-01-08T18:38:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05214v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05214v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have shown remarkable capabilities in tool calling and tool usage, but suffer from hallucinations where they choose incorrect tools, provide malformed parameters and exhibit &#x27;tool bypass&#x27; behavior by performing simulations and generating outputs instead of invoking specialized tools or external systems. This undermines the reliability of LLM based agents in production systems as it leads to inconsistent results, and bypasses security and audit controls. Such hallucinations in agent tool selection require early detection and error handling. Unlike existing hallucination detection methods that require multiple forward passes or external validation, we present a computationally efficient framework that detects tool-calling hallucinations in real-time by leveraging LLMs&#x27; internal representations during the same forward pass used for generation. We evaluate this approach on reasoning tasks across multiple domains, demonstrating strong detection performance (up to 86.4\% accuracy) while maintaining real-time inference capabilities with minimal computational overhead, particularly excelling at detecting parameter-level hallucinations and inappropriate tool selections, critical for reliable agent deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>智能体工具选择中的幻觉：内部表示作为指示器</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）在工具调用和使用方面展现出显著的能力，但在选择错误工具、提供格式错误参数以及通过模拟和生成输出绕过专用工具或外部系统时会出现幻觉。这会损害基于LLM的智能体在生产系统中的可靠性，导致结果不一致，并绕过安全和审计控制。针对智能体工具选择中的幻觉，需要早期检测和错误处理。与现有需要多次前向传递或外部验证的幻觉检测方法不同，我们提出了一种计算效率高的框架，通过在生成过程中利用LLMs的内部表示，实时检测工具调用幻觉。我们在多个领域的推理任务中评估了该方法，展示了强大的检测性能（最高达到86.4\%的准确率），同时保持了实时推理能力，计算开销极小，尤其擅长检测参数级幻觉和不合适的工具选择，这对可靠部署智能体至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">FlowLet: Conditional 3D Brain MRI Synthesis using Wavelet Flow Matching</div>
<div class="meta-line">Authors: Danilo Danese, Angela Lombardi, Matteo Attimonelli, Giuseppe Fasano, Tommaso Di Noia</div>
<div class="meta-line">First: 2026-01-08T18:36:29+00:00 · Latest: 2026-01-08T18:36:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05212v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05212v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Brain Magnetic Resonance Imaging (MRI) plays a central role in studying neurological development, aging, and diseases. One key application is Brain Age Prediction (BAP), which estimates an individual&#x27;s biological brain age from MRI data. Effective BAP models require large, diverse, and age-balanced datasets, whereas existing 3D MRI datasets are demographically skewed, limiting fairness and generalizability. Acquiring new data is costly and ethically constrained, motivating generative data augmentation. Current generative methods are often based on latent diffusion models, which operate in learned low dimensional latent spaces to address the memory demands of volumetric MRI data. However, these methods are typically slow at inference, may introduce artifacts due to latent compression, and are rarely conditioned on age, thereby affecting the BAP performance. In this work, we propose FlowLet, a conditional generative framework that synthesizes age-conditioned 3D MRIs by leveraging flow matching within an invertible 3D wavelet domain, helping to avoid reconstruction artifacts and reducing computational demands. Experiments show that FlowLet generates high-fidelity volumes with few sampling steps. Training BAP models with data generated by FlowLet improves performance for underrepresented age groups, and region-based analysis confirms preservation of anatomical structures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlowLet：使用小波流匹配的条件三维脑MRI合成</div>
<div class="mono" style="margin-top:8px">脑磁共振成像（MRI）在研究神经发育、衰老和疾病中起着核心作用。一个关键应用是脑龄预测（BAP），它通过MRI数据估计个体的生物脑龄。有效的BAP模型需要大规模、多样化且年龄平衡的数据集，而现有的三维MRI数据集在人口统计学上存在偏差，限制了公平性和泛化能力。获取新数据成本高昂且存在伦理限制，因此生成式数据增强成为一种有吸引力的方法。当前的生成方法通常基于潜在扩散模型，这些模型在学习到的低维潜在空间中运行，以应对体积MRI数据的内存需求。然而，这些方法在推理时通常较慢，由于潜在空间压缩可能引入伪影，并且很少根据年龄进行条件生成，从而影响BAP性能。在本工作中，我们提出了FlowLet，这是一种条件生成框架，通过在可逆的三维小波域中利用流匹配来合成年龄条件的三维MRI，有助于避免重建伪影并降低计算需求。实验表明，FlowLet在少量采样步骤下即可生成高保真度的体积。使用FlowLet生成的数据训练BAP模型可提升少数群体年龄的性能，且基于区域的分析证实了解剖结构的保留。</div>
</details>
</div>
<div class="card">
<div class="title">Belief Is All You Need: Modeling Narrative Archetypes in Conspiratorial Discourse</div>
<div class="meta-line">Authors: Soorya Ram Shimgekar, Abhay Goyal, Roy Ka-Wei Lee, Koustuv Saha, Pi Zonooz, Navin Kumar</div>
<div class="meta-line">First: 2025-12-10T21:51:16+00:00 · Latest: 2026-01-08T18:34:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.10105v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.10105v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conspiratorial discourse is increasingly embedded within digital communication ecosystems, yet its structure and spread remain difficult to study. This work analyzes conspiratorial narratives in Singapore-based Telegram groups, showing that such content is woven into everyday discussions rather than confined to isolated echo chambers. We propose a two-stage computational framework. First, we fine-tune RoBERTa-large to classify messages as conspiratorial or not, achieving an F1-score of 0.866 on 2,000 expert-labeled messages. Second, we build a signed belief graph in which nodes represent messages and edge signs reflect alignment in belief labels, weighted by textual similarity. We introduce a Signed Belief Graph Neural Network (SiBeGNN) that uses a Sign Disentanglement Loss to learn embeddings that separate ideological alignment from stylistic features.
  Using hierarchical clustering on these embeddings, we identify seven narrative archetypes across 553,648 messages: legal topics, medical concerns, media discussions, finance, contradictions in authority, group moderation, and general chat. SiBeGNN yields stronger clustering quality (cDBI = 8.38) than baseline methods (13.60 to 67.27), supported by 88 percent inter-rater agreement in expert evaluations. Our analysis shows that conspiratorial messages appear not only in clusters focused on skepticism or distrust, but also within routine discussions of finance, law, and everyday matters. These findings challenge common assumptions about online radicalization by demonstrating that conspiratorial discourse operates within ordinary social interaction. The proposed framework advances computational methods for belief-driven discourse analysis and offers applications for stance detection, political communication studies, and content moderation policy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>信念即一切：在阴谋论话语中建模叙事原型</div>
<div class="mono" style="margin-top:8px">阴谋论话语正越来越多地嵌入数字通信生态系统，但其结构和传播仍难以研究。本文分析了新加坡Telegram群组中的阴谋论叙事，表明此类内容融入日常讨论，而非局限于孤立的回音室。我们提出一个两阶段的计算框架。首先，我们微调RoBERTa-large以分类消息是否为阴谋论，达到0.866的F1分数（基于2000条专家标注的消息）。其次，我们构建了一个带符号信念图，其中节点代表消息，边的符号反映信念标签的一致性，权重由文本相似性决定。我们引入了带符号信念图神经网络（SiBeGNN），利用符号解缠损失来学习分离意识形态一致性和风格特征的嵌入。通过在这些嵌入上进行层次聚类，我们识别出553,648条消息中的七个叙事原型：法律议题、医疗问题、媒体讨论、金融、权威矛盾、群组管理以及一般聊天。SiBeGNN在聚类质量（cDBI = 8.38）上优于基线方法（13.60至67.27），专家评估中88%的评分者间一致性支持了这一结果。我们的分析表明，阴谋论消息不仅出现在以怀疑或不信任为中心的聚类中，也出现在金融、法律和日常事务的常规讨论中。这些发现挑战了关于在线激进化的常见假设，表明阴谋论话语实际上是在普通社会互动中运作的。所提出的框架推进了以信念驱动的话语分析计算方法，并为立场检测、政治传播研究和内容管理政策提供了应用。</div>
</details>
</div>
<div class="card">
<div class="title">MoE3D: A Mixture-of-Experts Module for 3D Reconstruction</div>
<div class="meta-line">Authors: Zichen Wang, Ang Cao, Liam J. Wang, Jeong Joon Park</div>
<div class="meta-line">First: 2026-01-08T18:33:52+00:00 · Latest: 2026-01-08T18:33:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05208v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05208v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">MoE3D is a mixture-of-experts module designed to sharpen depth boundaries and mitigate flying-point artifacts (highlighted in red) of existing feed-forward 3D reconstruction models (left side). MoE3D predicts multiple candidate depth maps and fuses them via dynamic weighting (visualized by MoE weights on the right side). When integrated with a pre-trained 3D reconstruction backbone such as VGGT, it substantially enhances reconstruction quality with minimal additional computational overhead. Best viewed digitally.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MoE3D：一种用于3D重建的专家混合模块</div>
<div class="mono" style="margin-top:8px">MoE3D是一种专家混合模块，旨在锐化深度边界并减轻现有前馈3D重建模型（左侧）的飞点伪影（红色突出显示）。MoE3D预测多个候选深度图，并通过动态加权（右侧的MoE权重可视化）进行融合。当与预训练的3D重建主干网络（如VGGT）集成时，它能显著提升重建质量，同时仅带来极少的额外计算开销。建议数字查看。</div>
</details>
</div>
<div class="card">
<div class="title">EARL: Energy-Aware Optimization of Liquid State Machines for Pervasive AI</div>
<div class="meta-line">Authors: Zain Iqbal, Lorenzo Valerio</div>
<div class="meta-line">First: 2026-01-08T18:31:11+00:00 · Latest: 2026-01-08T18:31:11+00:00</div>
<div class="meta-line">Comments: 6 pages, 9 figures, 2 Tables, conference [Submitted in PerConAI-2026]</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05205v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05205v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pervasive AI increasingly depends on on-device learning systems that deliver low-latency and energy-efficient computation under strict resource constraints. Liquid State Machines (LSMs) offer a promising approach for low-power temporal processing in pervasive and neuromorphic systems, but their deployment remains challenging due to high hyperparameter sensitivity and the computational cost of traditional optimization methods that ignore energy constraints. This work presents EARL, an energy-aware reinforcement learning framework that integrates Bayesian optimization with an adaptive reinforcement learning based selection policy to jointly optimize accuracy and energy consumption. EARL employs surrogate modeling for global exploration, reinforcement learning for dynamic candidate prioritization, and an early termination mechanism to eliminate redundant evaluations, substantially reducing computational overhead. Experiments on three benchmark datasets demonstrate that EARL achieves 6 to 15 percent higher accuracy, 60 to 80 percent lower energy consumption, and up to an order of magnitude reduction in optimization time compared to leading hyperparameter tuning frameworks. These results highlight the effectiveness of energy-aware adaptive search in improving the efficiency and scalability of LSMs for resource-constrained on-device AI applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EARL：面向普适AI的液态机器能量感知优化</div>
<div class="mono" style="margin-top:8px">普适AI日益依赖于能够在严格资源限制下提供低延迟和高能效计算的本地化学习系统。液态机器（LSMs）为普适和神经形态系统中的低功耗时序处理提供了一种有前景的方法，但由于高超参数敏感性和传统优化方法忽略能量约束所带来的计算成本，其部署仍面临挑战。本文提出EARL，这是一种能量感知的强化学习框架，通过将贝叶斯优化与基于自适应强化学习的选取策略相结合，共同优化准确性和能耗。EARL采用替代模型进行全局探索，利用强化学习实现动态候选优先级排序，并引入提前终止机制以消除冗余评估，从而显著降低计算开销。在三个基准数据集上的实验表明，与领先的超参数调优框架相比，EARL在准确率上提高了6到15%，能耗降低了60到80%，优化时间减少了多达一个数量级。这些结果突显了能量感知自适应搜索在提升资源受限本地AI应用中LSMs效率和可扩展性方面的有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Pervasive AI increasingly depends on on-device learning systems that deliver low-latency and energy-efficient computation under strict resource constraints.</div>
</details>
</div>
<div class="card">
<div class="title">From Policy to Logic for Efficient and Interpretable Coverage Assessment</div>
<div class="meta-line">Authors: Rhitabrat Pokharel, Hamid Reza Hassanzadeh, Ameeta Agrawal</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2026-01-03T19:24:51+00:00 · Latest: 2026-01-08T18:28:40+00:00</div>
<div class="meta-line">Comments: Accepted at AIMedHealth @ AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.01266v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.01266v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have demonstrated strong capabilities in interpreting lengthy, complex legal and policy language. However, their reliability can be undermined by hallucinations and inconsistencies, particularly when analyzing subjective and nuanced documents. These challenges are especially critical in medical coverage policy review, where human experts must be able to rely on accurate information. In this paper, we present an approach designed to support human reviewers by making policy interpretation more efficient and interpretable. We introduce a methodology that pairs a coverage-aware retriever with symbolic rule-based reasoning to surface relevant policy language, organize it into explicit facts and rules, and generate auditable rationales. This hybrid system minimizes the number of LLM inferences required which reduces overall model cost. Notably, our approach achieves a 44% reduction in inference cost alongside a 4.5% improvement in F1 score, demonstrating both efficiency and effectiveness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从政策到逻辑：高效且可解释的覆盖评估方法</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在解释冗长且复杂的法律和政策语言方面表现出强大的能力。然而，它们的可靠性可能因幻觉和不一致性而受到损害，尤其是在分析主观性和细微性的文件时。这些挑战在医疗覆盖政策审查中尤为关键，因为人类专家必须能够依赖准确的信息。本文提出了一种方法，旨在通过提高政策解释的效率和可解释性来支持人类审查者。我们引入了一种方法，将一个覆盖感知的检索器与符号规则推理相结合，以揭示相关政策语言，将其组织为明确的事实和规则，并生成可审计的推理依据。这种混合系统减少了所需的LLM推理次数，从而降低了整体模型成本。值得注意的是，我们的方法在推理成本上实现了44%的降低，同时F1分数提高了4.5%，展示了其在效率和效果上的双重优势。</div>
</details>
</div>
<div class="card">
<div class="title">Surprisal and Metaphor Novelty: Moderate Correlations and Divergent Scaling Effects</div>
<div class="meta-line">Authors: Omar Momen, Emilie Sitter, Berenike Herrmann, Sina Zarrieß</div>
<div class="meta-line">First: 2026-01-05T11:24:33+00:00 · Latest: 2026-01-08T18:27:27+00:00</div>
<div class="meta-line">Comments: to be published at EACL 2026 main conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02015v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02015v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Novel metaphor comprehension involves complex semantic processes and linguistic creativity, making it an interesting task for studying language models (LMs). This study investigates whether surprisal, a probabilistic measure of predictability in LMs, correlates with different metaphor novelty datasets. We analyse surprisal from 16 LM variants on corpus-based and synthetic metaphor novelty datasets. We explore a cloze-style surprisal method that conditions on full-sentence context. Results show that LMs yield significant moderate correlations with scores/labels of metaphor novelty. We further identify divergent scaling patterns: on corpus-based data, correlation strength decreases with model size (inverse scaling effect), whereas on synthetic data it increases (Quality-Power Hypothesis). We conclude that while surprisal can partially account for annotations of metaphor novelty, it remains a limited metric of linguistic creativity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>惊讶度与隐喻新颖性：中等相关性与不同的标度效应</div>
<div class="mono" style="margin-top:8px">新颖隐喻的理解涉及复杂的语义过程和语言创造性，使其成为研究语言模型（LMs）的一个有趣任务。本研究探讨了惊讶度（一种衡量语言模型预测性的概率指标）是否与不同的隐喻新颖性数据集相关。我们分析了16种LM变体在基于语料库和合成数据集上的惊讶度。我们还探索了一种基于填空式任务的惊讶度方法，该方法依赖于完整的句子上下文。结果表明，语言模型在隐喻新颖性评分/标签上表现出显著的中等相关性。我们进一步发现不同的标度模式：在基于语料库的数据上，相关性强度随着模型规模的增大而降低（反向标度效应），而在合成数据上则增强（质量-力量假说）。我们得出结论，尽管惊讶度可以在一定程度上解释隐喻新颖性的标注，但它仍然是衡量语言创造性的一个有限指标。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
