<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-12 04:25</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260212_0425</div>
    <div class="row"><div class="card">
<div class="title">Biases in the Blind Spot: Detecting What LLMs Fail to Mention</div>
<div class="meta-line">Authors: Iván Arcuschin, David Chanin, Adrià Garriga-Alonso, Oana-Maria Camburu</div>
<div class="meta-line">Venue: ICML 2026</div>
<div class="meta-line">First: 2026-02-10T18:59:56+00:00 · Latest: 2026-02-10T18:59:56+00:00</div>
<div class="meta-line">Comments: 10 pages, Under review at ICML 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10117v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10117v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) often provide chain-of-thought (CoT) reasoning traces that appear plausible, but may hide internal biases. We call these *unverbalized biases*. Monitoring models via their stated reasoning is therefore unreliable, and existing bias evaluations typically require predefined categories and hand-crafted datasets. In this work, we introduce a fully automated, black-box pipeline for detecting task-specific unverbalized biases. Given a task dataset, the pipeline uses LLM autoraters to generate candidate bias concepts. It then tests each concept on progressively larger input samples by generating positive and negative variations, and applies statistical techniques for multiple testing and early stopping. A concept is flagged as an unverbalized bias if it yields statistically significant performance differences while not being cited as justification in the model&#x27;s CoTs. We evaluate our pipeline across six LLMs on three decision tasks (hiring, loan approval, and university admissions). Our technique automatically discovers previously unknown biases in these models (e.g., Spanish fluency, English proficiency, writing formality). In the same run, the pipeline also validates biases that were manually identified by prior work (gender, race, religion, ethnicity). More broadly, our proposed approach provides a practical, scalable path to automatic task-specific bias discovery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>盲点中的偏见：检测LLMs未提及的任务特定偏见</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）通常提供看似合理的链式推理（CoT）轨迹，但可能隐藏内部偏见。我们将这些称为*未表达的偏见*。因此，通过模型陈述的推理来监控模型是不可靠的，而现有的偏见评估通常需要预定义的类别和人工构建的数据集。在本工作中，我们引入了一种全自动、黑盒的流程，用于检测任务特定的未表达偏见。给定一个任务数据集，该流程使用LLM自动评分者生成候选偏见概念，然后通过生成正负变体，在逐步扩大的输入样本上测试每个概念，并应用多重检验和早期停止的统计技术。如果一个概念导致统计上显著的性能差异，但未被模型的CoT用作理由，则该概念被标记为未表达的偏见。我们在三个决策任务（招聘、贷款审批和大学录取）上对六个LLMs评估了我们的流程。我们的技术能够自动发现这些模型中之前未知的偏见（例如西班牙语流利度、英语熟练度、写作正式程度）。在同一次运行中，该流程还验证了之前研究中手动识别的偏见（如性别、种族、宗教、民族）。更广泛地说，我们提出的方法为自动任务特定偏见发现提供了一条实用且可扩展的路径。</div>
</details>
</div>
<div class="card">
<div class="title">SAGE: Scalable Agentic 3D Scene Generation for Embodied AI</div>
<div class="meta-line">Authors: Hongchi Xia, Xuan Li, Zhaoshuo Li, Qianli Ma, Jiashu Xu, Ming-Yu Liu, Yin Cui, Tsung-Yi Lin, Wei-Chiu Ma, Shenlong Wang, Shuran Song, Fangyin Wei</div>
<div class="meta-line">First: 2026-02-10T18:59:55+00:00 · Latest: 2026-02-10T18:59:55+00:00</div>
<div class="meta-line">Comments: Project Page: https://nvlabs.github.io/sage</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10116v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10116v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://nvlabs.github.io/sage">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world data collection for embodied agents remains costly and unsafe, calling for scalable, realistic, and simulator-ready 3D environments. However, existing scene-generation systems often rely on rule-based or task-specific pipelines, yielding artifacts and physically invalid scenes. We present SAGE, an agentic framework that, given a user-specified embodied task (e.g., &quot;pick up a bowl and place it on the table&quot;), understands the intent and automatically generates simulation-ready environments at scale. The agent couples multiple generators for layout and object composition with critics that evaluate semantic plausibility, visual realism, and physical stability. Through iterative reasoning and adaptive tool selection, it self-refines the scenes until meeting user intent and physical validity. The resulting environments are realistic, diverse, and directly deployable in modern simulators for policy training. Policies trained purely on this data exhibit clear scaling trends and generalize to unseen objects and layouts, demonstrating the promise of simulation-driven scaling for embodied AI. Code, demos, and the SAGE-10k dataset can be found on the project page here: https://nvlabs.github.io/sage.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAGE：用于具身人工智能的可扩展智能体3D场景生成</div>
<div class="mono" style="margin-top:8px">现实世界中对具身智能体的数据收集仍然成本高昂且不安全，因此需要可扩展、真实且适用于模拟器的3D环境。然而，现有的场景生成系统通常依赖基于规则或任务特定的流水线，导致生成的场景出现伪影和物理无效的问题。我们提出了SAGE，一个智能体框架，它可以根据用户指定的具身任务（例如&quot;拿起碗并放在桌子上&quot;），理解意图并自动大规模生成可模拟的环境。该智能体结合了用于布局和物体组合的多个生成器，以及评估语义合理性、视觉真实性和物理稳定性的批评模块。通过迭代推理和自适应工具选择，它会自我优化场景，直到满足用户意图和物理有效性。生成的环境具有真实性和多样性，并可直接部署在现代模拟器中用于策略训练。仅在这些数据上训练的策略表现出明显的可扩展趋势，并能泛化到未见过的物体和布局，展示了模拟驱动扩展在具身人工智能中的前景。项目页面包含代码、演示和SAGE-10k数据集，链接为：https://nvlabs.github.io/sage。</div>
</details>
</div>
<div class="card">
<div class="title">Quantum Multiple Rotation Averaging</div>
<div class="meta-line">Authors: Shuteng Wang, Natacha Kuete Meli, Michael Möller, Vladislav Golyanik</div>
<div class="meta-line">Venue: International Conference on 3D Vision (3DV) 2026</div>
<div class="meta-line">First: 2026-02-10T18:59:54+00:00 · Latest: 2026-02-10T18:59:54+00:00</div>
<div class="meta-line">Comments: 16 pages, 13 figures, 4 tables; project page: https://4dqv.mpi-inf.mpg.de/QMRA/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10115v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10115v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multiple rotation averaging (MRA) is a fundamental optimization problem in 3D vision and robotics that aims to recover globally consistent absolute rotations from noisy relative measurements. Established classical methods, such as L1-IRLS and Shonan, face limitations including local minima susceptibility and reliance on convex relaxations that fail to preserve the exact manifold geometry, leading to reduced accuracy in high-noise scenarios. We introduce IQARS (Iterative Quantum Annealing for Rotation Synchronization), the first algorithm that reformulates MRA as a sequence of local quadratic non-convex sub-problems executable on quantum annealers after binarization, to leverage inherent hardware advantages. IQARS removes convex relaxation dependence and better preserves non-Euclidean rotation manifold geometry while leveraging quantum tunneling and parallelism for efficient solution space exploration. We evaluate IQARS&#x27;s performance on synthetic and real-world datasets. While current annealers remain in their nascent phase and only support solving problems of limited scale with constrained performance, we observed that IQARS on D-Wave annealers can already achieve ca. 12% higher accuracy than Shonan, i.e., the best-performing classical method evaluated empirically.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>量子多旋转平均</div>
<div class="mono" style="margin-top:8px">多旋转平均（MRA）是三维视觉和机器人领域中的一个基本优化问题，旨在从有噪声的相对测量中恢复全局一致的绝对旋转。现有的经典方法，如L1-IRLS和Shonan，存在诸如易陷入局部极小值和依赖凸松弛方法而无法保持精确流形几何等局限性，导致在高噪声场景下精度降低。我们引入IQARS（用于旋转同步的迭代量子退火算法），这是首个将MRA重新表述为一系列可执行于量子退火设备上的局部二次非凸子问题的算法，通过二值化实现。IQARS消除了对凸松弛的依赖，更好地保持了非欧几里得旋转流形的几何特性，同时利用量子隧穿效应和并行性进行高效解空间探索。我们在合成数据集和真实世界数据集上评估了IQARS的性能。尽管当前退火设备仍处于初级阶段，仅能解决有限规模且受限性能的问题，但我们观察到，在D-Wave退火设备上运行的IQARS已经能够实现比Shonan（即目前经验评估中表现最好的经典方法）高出约12%的精度。</div>
</details>
</div>
<div class="card">
<div class="title">ConsID-Gen: View-Consistent and Identity-Preserving Image-to-Video Generation</div>
<div class="meta-line">Authors: Mingyang Wu, Ashirbad Mishra, Soumik Dey, Shuo Xing, Naveen Ravipati, Hansi Wu, Binbin Li, Zhengzhong Tu</div>
<div class="meta-line">First: 2026-02-10T18:59:51+00:00 · Latest: 2026-02-10T18:59:51+00:00</div>
<div class="meta-line">Comments: Project page: https://myangwu.github.io/ConsID-Gen</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10113v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10113v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://myangwu.github.io/ConsID-Gen">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image-to-Video generation (I2V) animates a static image into a temporally coherent video sequence following textual instructions, yet preserving fine-grained object identity under changing viewpoints remains a persistent challenge. Unlike text-to-video models, existing I2V pipelines often suffer from appearance drift and geometric distortion, artifacts we attribute to the sparsity of single-view 2D observations and weak cross-modal alignment. Here we address this problem from both data and model perspectives. First, we curate ConsIDVid, a large-scale object-centric dataset built with a scalable pipeline for high-quality, temporally aligned videos, and establish ConsIDVid-Bench, where we present a novel benchmarking and evaluation framework for multi-view consistency using metrics sensitive to subtle geometric and appearance deviations. We further propose ConsID-Gen, a view-assisted I2V generation framework that augments the first frame with unposed auxiliary views and fuses semantic and structural cues via a dual-stream visual-geometric encoder as well as a text-visual connector, yielding unified conditioning for a Diffusion Transformer backbone. Experiments across ConsIDVid-Bench demonstrate that ConsID-Gen consistently outperforms in multiple metrics, with the best overall performance surpassing leading video generation models like Wan2.1 and HunyuanVideo, delivering superior identity fidelity and temporal coherence under challenging real-world scenarios. We will release our model and dataset at https://myangwu.github.io/ConsID-Gen.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ConsID-Gen：视图一致且身份保持的图像到视频生成</div>
<div class="mono" style="margin-top:8px">图像到视频生成（I2V）根据文本指令将静态图像转化为时间上连贯的视频序列，但在变化视角下保持细粒度物体身份仍是一个持续存在的挑战。与文本到视频模型不同，现有的I2V流程通常会受到外观漂移和几何失真等问题的影响，这些问题我们归因于单视角2D观测的稀疏性以及跨模态对齐的不足。本文从数据和模型两个角度解决这一问题。首先，我们构建了ConsIDVid，这是一个大规模以物体为中心的数据集，采用可扩展的流程生成高质量、时间对齐的视频，并建立了ConsIDVid-Bench，其中我们提出了一个新颖的多视角一致性基准测试和评估框架，使用对细微几何和外观偏差敏感的指标。我们进一步提出了ConsID-Gen，一个视图辅助的I2V生成框架，通过在第一帧中加入未指定的辅助视角，并利用双流视觉-几何编码器以及文本-视觉连接器融合语义和结构线索，为扩散Transformer主干网络提供统一的条件。在ConsIDVid-Bench上的实验表明，ConsID-Gen在多个指标上持续优于其他方法，其整体表现超越了Wan2.1和HunyuanVideo等领先的视频生成模型，在具有挑战性的现实场景下实现了更优的身份保真度和时间连贯性。</div>
</details>
</div>
<div class="card">
<div class="title">ST4VLA: Spatially Guided Training for Vision-Language-Action Models</div>
<div class="meta-line">Authors: Jinhui Ye, Fangjing Wang, Ning Gao, Junqiu Yu, Yangkun Zhu, Bin Wang, Jinyu Zhang, Weiyang Jin, Yanwei Fu, Feng Zheng, Yilun Chen, Jiangmiao Pang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-10T18:59:17+00:00 · Latest: 2026-02-10T18:59:17+00:00</div>
<div class="meta-line">Comments: Spatially Training for VLA, Accepted by ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10109v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10109v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://internrobotics.github.io/internvla-m1.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (VLMs) excel at multimodal understanding but fall short when extended to embodied tasks, where instructions must be transformed into low-level motor actions. We introduce ST4VLA, a dual-system Vision-Language-Action framework that leverages Spatial Guided Training to align action learning with spatial priors in VLMs. ST4VLA includes two stages: (i) spatial grounding pre-training, which equips the VLM with transferable priors via scalable point, box, and trajectory prediction from both web-scale and robot-specific data, and (ii) spatially guided action post-training, which encourages the model to produce richer spatial priors to guide action generation via spatial prompting. This design preserves spatial grounding during policy learning and promotes consistent optimization across spatial and action objectives. Empirically, ST4VLA achieves substantial improvements over vanilla VLA, with performance increasing from 66.1 -&gt; 84.6 on Google Robot and from 54.7 -&gt; 73.2 on WidowX Robot, establishing new state-of-the-art results on SimplerEnv. It also demonstrates stronger generalization to unseen objects and paraphrased instructions, as well as robustness to long-horizon perturbations in real-world settings. These results highlight scalable spatially guided training as a promising direction for robust, generalizable robot learning. Source code, data and models are released at https://internrobotics.github.io/internvla-m1.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ST4VLA：面向视觉-语言-动作模型的空间引导训练</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（VLMs）在多模态理解方面表现出色，但在扩展到具身任务时表现不足，因为需要将指令转化为低级运动动作。我们提出了ST4VLA，这是一种双系统视觉-语言-动作框架，通过空间引导训练将动作学习与VLM中的空间先验对齐。ST4VLA包含两个阶段：(i) 空间锚定预训练，通过从大规模网络数据和机器人专用数据中进行可扩展的点、框和轨迹预测，为VLM提供可迁移的空间先验；(ii) 空间引导动作后训练，通过空间提示鼓励模型生成更丰富的空间先验以指导动作生成。这种设计在策略学习过程中保留了空间锚定能力，并在空间和动作目标之间实现了一致的优化。实验表明，ST4VLA在Google机器人和WidowX机器人上分别将性能从66.1提升至84.6，从54.7提升至73.2，在SimplerEnv上建立了新的最先进结果。此外，它还展示了对未见过的对象和改写指令更强的泛化能力，以及在现实场景中对长时序扰动的鲁棒性。这些结果突显了可扩展的空间引导训练作为构建稳健、可泛化的机器人学习方法的有前景方向。源代码、数据和模型已发布于https://internrobotics.github.io/internvla-m1.github.io/</div>
</details>
</div>
<div class="card">
<div class="title">Olaf-World: Orienting Latent Actions for Video World Modeling</div>
<div class="meta-line">Authors: Yuxin Jiang, Yuchao Gu, Ivor W. Tsang, Mike Zheng Shou</div>
<div class="meta-line">First: 2026-02-10T18:58:41+00:00 · Latest: 2026-02-10T18:58:41+00:00</div>
<div class="meta-line">Comments: Project page: https://showlab.github.io/Olaf-World/ Code: https://github.com/showlab/Olaf-World</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10104v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10104v1">PDF</a> · <a href="https://github.com/showlab/Olaf-World">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://showlab.github.io/Olaf-World/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling action-controllable world models is limited by the scarcity of action labels. While latent action learning promises to extract control interfaces from unlabeled video, learned latents often fail to transfer across contexts: they entangle scene-specific cues and lack a shared coordinate system. This occurs because standard objectives operate only within each clip, providing no mechanism to align action semantics across contexts. Our key insight is that although actions are unobserved, their semantic effects are observable and can serve as a shared reference. We introduce Seq$Δ$-REPA, a sequence-level control-effect alignment objective that anchors integrated latent action to temporal feature differences from a frozen, self-supervised video encoder. Building on this, we present Olaf-World, a pipeline that pretrains action-conditioned video world models from large-scale passive video. Extensive experiments demonstrate that our method learns a more structured latent action space, leading to stronger zero-shot action transfer and more data-efficient adaptation to new control interfaces than state-of-the-art baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Olaf-World: 为视频世界建模定向潜在动作</div>
<div class="mono" style="margin-top:8px">动作可控的世界模型扩展受到动作标签稀缺的限制。尽管潜在动作学习有望从无标签视频中提取控制接口，但学习到的潜在变量往往无法跨上下文迁移：它们纠缠了场景特定的线索，并缺乏共享的坐标系统。这是因为标准目标仅在每个片段内操作，没有机制对不同上下文中的动作语义进行对齐。我们的关键洞察是，虽然动作本身不可观测，但它们的语义效果是可观测的，并可以作为共享参考。我们引入了Seq$Δ$-REPA，这是一个基于序列的控制效果对齐目标，将整合的潜在动作锚定到一个冻结的自监督视频编码器产生的时序特征差异上。在此基础上，我们提出了Olaf-World，这是一个从大规模被动视频中预训练动作条件视频世界模型的流程。大量实验表明，我们的方法学习到了更结构化的潜在动作空间，从而在零样本动作迁移和对新控制接口的数据高效适应方面优于最先进的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">VideoWorld 2: Learning Transferable Knowledge from Real-world Videos</div>
<div class="meta-line">Authors: Zhongwei Ren, Yunchao Wei, Xiao Yu, Guixun Luo, Yao Zhao, Bingyi Kang, Jiashi Feng, Xiaojie Jin</div>
<div class="meta-line">First: 2026-02-10T18:58:19+00:00 · Latest: 2026-02-10T18:58:19+00:00</div>
<div class="meta-line">Comments: Code and models are released at: https://maverickren.github.io/VideoWorld2.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10102v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10102v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://maverickren.github.io/VideoWorld2.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning transferable knowledge from unlabeled video data and applying it in new environments is a fundamental capability of intelligent agents. This work presents VideoWorld 2, which extends VideoWorld and offers the first investigation into learning transferable knowledge directly from raw real-world videos. At its core, VideoWorld 2 introduces a dynamic-enhanced Latent Dynamics Model (dLDM) that decouples action dynamics from visual appearance: a pretrained video diffusion model handles visual appearance modeling, enabling the dLDM to learn latent codes that focus on compact and meaningful task-related dynamics. These latent codes are then modeled autoregressively to learn task policies and support long-horizon reasoning. We evaluate VideoWorld 2 on challenging real-world handcraft making tasks, where prior video generation and latent-dynamics models struggle to operate reliably. Remarkably, VideoWorld 2 achieves up to 70% improvement in task success rate and produces coherent long execution videos. In robotics, we show that VideoWorld 2 can acquire effective manipulation knowledge from the Open-X dataset, which substantially improves task performance on CALVIN. This study reveals the potential of learning transferable world knowledge directly from raw videos, with all code, data, and models to be open-sourced for further research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoWorld 2: 从真实世界视频中学习可迁移的知识</div>
<div class="mono" style="margin-top:8px">从无标签视频数据中学习可迁移的知识并将其应用于新环境是智能代理的基本能力。本文提出了VideoWorld 2，它扩展了VideoWorld，并首次直接从原始真实世界视频中学习可迁移的知识。VideoWorld 2的核心是一个动态增强的潜在动态模型（dLDM），它将动作动态与视觉外观解耦：一个预训练的视频扩散模型负责视觉外观建模，使dLDM能够学习专注于紧凑且有意义的任务相关动态的潜在编码。这些潜在编码随后被自回归建模以学习任务策略并支持长时推理。我们在具有挑战性的现实世界手工制作任务上评估VideoWorld 2，此前的视频生成和潜在动态模型难以可靠地执行这些任务。令人惊讶的是，VideoWorld 2在任务成功率上实现了高达70%的提升，并生成了连贯的长执行视频。在机器人领域，我们展示了VideoWorld 2可以从Open-X数据集中获取有效的操作知识，这显著提升了CALVIN上的任务表现。本研究揭示了直接从原始视频中学习可迁移世界知识的潜力，并将所有代码、数据和模型开源以供进一步研究。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Explainable Federated Learning: Understanding the Impact of Differential Privacy</div>
<div class="meta-line">Authors: Júlio Oliveira, Rodrigo Ferreira, André Riker, Glaucio H. S. Carvalho, Eirini Eleni Tsilopoulou</div>
<div class="meta-line">First: 2026-02-10T18:58:11+00:00 · Latest: 2026-02-10T18:58:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10100v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10100v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data privacy and eXplainable Artificial Intelligence (XAI) are two important aspects for modern Machine Learning systems. To enhance data privacy, recent machine learning models have been designed as a Federated Learning (FL) system. On top of that, additional privacy layers can be added, via Differential Privacy (DP). On the other hand, to improve explainability, ML must consider more interpretable approaches with reduced number of features and less complex internal architecture. In this context, this paper aims to achieve a machine learning (ML) model that combines enhanced data privacy with explainability. So, we propose a FL solution, called Federated EXplainable Trees with Differential Privacy (FEXT-DP), that: (i) is based on Decision Trees, since they are lightweight and have superior explainability than neural networks-based FL systems; (ii) provides additional layer of data privacy protection applying Differential Privacy (DP) to the Tree-Based model. However, there is a side effect adding DP: it harms the explainability of the system. So, this paper also presents the impact of DP protection on the explainability of the ML model. The carried out performance assessment shows improvements of FEXT-DP in terms of a faster training, i.e., numbers of rounds, Mean Squared Error and explainability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向可解释的联邦学习：理解差分隐私的影响</div>
<div class="mono" style="margin-top:8px">数据隐私和可解释人工智能（XAI）是现代机器学习系统中的两个重要方面。为了增强数据隐私，最近的机器学习模型被设计为联邦学习（FL）系统。在此基础上，还可以通过差分隐私（DP）添加额外的隐私保护层。另一方面，为了提高可解释性，机器学习必须考虑具有更少特征和更简单内部架构的可解释方法。在此背景下，本文旨在实现一种结合增强数据隐私与可解释性的机器学习（ML）模型。因此，我们提出了一种名为联邦差分隐私可解释树（FEXT-DP）的联邦学习解决方案，其特点包括：(i) 基于决策树，因为它们比基于神经网络的联邦学习系统更轻量且具有更强的可解释性；(ii) 在基于树的模型中应用差分隐私（DP）以提供额外的数据隐私保护。然而，添加差分隐私会产生副作用：它会损害系统的可解释性。因此，本文还探讨了差分隐私保护对机器学习模型可解释性的影响。所进行的性能评估表明，FEXT-DP在训练速度（即轮次数量）、均方误差和可解释性方面均有所提升。</div>
</details>
</div>
<div class="card">
<div class="title">Learning on the Manifold: Unlocking Standard Diffusion Transformers with Representation Encoders</div>
<div class="meta-line">Authors: Amandeep Kumar, Vishal M. Patel</div>
<div class="meta-line">First: 2026-02-10T18:58:04+00:00 · Latest: 2026-02-10T18:58:04+00:00</div>
<div class="meta-line">Comments: Technical Report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10099v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10099v1">PDF</a> · <a href="https://github.com/amandpkr/RJF">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Leveraging representation encoders for generative modeling offers a path for efficient, high-fidelity synthesis. However, standard diffusion transformers fail to converge on these representations directly. While recent work attributes this to a capacity bottleneck proposing computationally expensive width scaling of diffusion transformers we demonstrate that the failure is fundamentally geometric. We identify Geometric Interference as the root cause: standard Euclidean flow matching forces probability paths through the low-density interior of the hyperspherical feature space of representation encoders, rather than following the manifold surface. To resolve this, we propose Riemannian Flow Matching with Jacobi Regularization (RJF). By constraining the generative process to the manifold geodesics and correcting for curvature-induced error propagation, RJF enables standard Diffusion Transformer architectures to converge without width scaling. Our method RJF enables the standard DiT-B architecture (131M parameters) to converge effectively, achieving an FID of 3.37 where prior methods fail to converge. Code: https://github.com/amandpkr/RJF</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在流形上学习：通过表示编码器解锁标准扩散变换器</div>
<div class="mono" style="margin-top:8px">利用表示编码器进行生成建模提供了一条高效、高保真的合成路径。然而，标准扩散变换器无法直接收敛到这些表示。尽管近期的研究将这一问题归因于容量瓶颈，并提出计算成本高昂的宽度扩展方案，但我们证明该失败本质上是几何性的。我们识别出几何干扰是根本原因：标准欧几里得流匹配迫使概率路径穿过表示编码器超球面特征空间的低密度内部，而非沿着流形表面进行。为了解决这一问题，我们提出了带雅可比正则化的黎曼流匹配（RJF）。通过将生成过程约束在流形测地线上，并校正曲率引起的误差传播，RJF使标准扩散变换器架构无需宽度扩展即可收敛。我们的方法RJF使标准DiT-B架构（1.31亿参数）能够有效收敛，实现了FID为3.37的成绩，而此前的方法无法收敛。代码：https://github.com/amandpkr/RJF</div>
</details>
</div>
<div class="card">
<div class="title">VLA-JEPA: Enhancing Vision-Language-Action Model with Latent World Model</div>
<div class="meta-line">Authors: Jingwen Sun, Wenyao Zhang, Zekun Qi, Shaojie Ren, Zezhi Liu, Hanxin Zhu, Guangzhong Sun, Xin Jin, Zhibo Chen</div>
<div class="meta-line">First: 2026-02-10T18:58:01+00:00 · Latest: 2026-02-10T18:58:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10098v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10098v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pretraining Vision-Language-Action (VLA) policies on internet-scale video is appealing, yet current latent-action objectives often learn the wrong thing: they remain anchored to pixel variation rather than action-relevant state transitions, making them vulnerable to appearance bias, nuisance motion, and information leakage. We introduce VLA-JEPA, a JEPA-style pretraining framework that sidesteps these pitfalls by design. The key idea is \emph{leakage-free state prediction}: a target encoder produces latent representations from future frames, while the student pathway sees only the current observation -- future information is used solely as supervision targets, never as input. By predicting in latent space rather than pixel space, VLA-JEPA learns dynamics abstractions that are robust to camera motion and irrelevant background changes. This yields a simple two-stage recipe -- JEPA pretraining followed by action-head fine-tuning -- without the multi-stage complexity of prior latent-action pipelines. Experiments on LIBERO, LIBERO-Plus, SimplerEnv and real-world manipulation tasks show that VLA-JEPA achieves consistent gains in generalization and robustness over existing methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VLA-JEPA：通过潜在世界模型增强视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">在互联网级视频上预训练视觉-语言-动作（VLA）策略具有吸引力，但当前的潜在动作目标常常学习错误的内容：它们仍然依赖于像素变化而非与动作相关的状态转移，从而容易受到外观偏差、干扰运动和信息泄露的影响。我们引入了VLA-JEPA，这是一种JEPA风格的预训练框架，通过设计避免了这些陷阱。其核心思想是\emph{无信息泄露的状态预测}：目标编码器从未来帧中生成潜在表示，而学生路径仅能看到当前观察结果——未来信息仅作为监督目标使用，从不作为输入。通过在潜在空间而非像素空间中进行预测，VLA-JEPA学习到对相机运动和无关背景变化具有鲁棒性的动态抽象。这提供了一个简单的两阶段方案——先进行JEPA预训练，再进行动作头微调——而无需像以往潜在动作管道那样复杂的多阶段流程。在LIBERO、LIBERO-Plus、SimplerEnv和现实世界操作任务上的实验表明，VLA-JEPA在泛化能力和鲁棒性方面均优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Step-resolved data attribution for looped transformers</div>
<div class="meta-line">Authors: Georgios Kaissis, David Mildenberger, Juan Felipe Gomez, Martin J. Menten, Eleni Triantafillou</div>
<div class="meta-line">First: 2026-02-10T18:57:53+00:00 · Latest: 2026-02-10T18:57:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10097v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10097v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study how individual training examples shape the internal computation of looped transformers, where a shared block is applied for $τ$ recurrent iterations to enable latent reasoning. Existing training-data influence estimators such as TracIn yield a single scalar score that aggregates over all loop iterations, obscuring when during the recurrent computation a training example matters. We introduce \textit{Step-Decomposed Influence (SDI)}, which decomposes TracIn into a length-$τ$ influence trajectory by unrolling the recurrent computation graph and attributing influence to specific loop iterations. To make SDI practical at transformer scale, we propose a TensorSketch implementation that never materialises per-example gradients. Experiments on looped GPT-style models and algorithmic reasoning tasks show that SDI scales excellently, matches full-gradient baselines with low error and supports a broad range of data attribution and interpretability tasks with per-step insights into the latent reasoning process.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于循环变压器的分步数据归因</div>
<div class="mono" style="margin-top:8px">我们研究个体训练样本如何影响循环变压器的内部计算，其中共享块被应用 $τ$ 次递归迭代以实现潜在推理。现有的训练数据影响估计器（如TracIn）会产生一个单一的标量分数，汇总所有循环迭代，从而掩盖了训练样本在递归计算中的具体作用时间。我们引入了\textit{分步分解影响（SDI）}，通过展开递归计算图，将TracIn分解为长度为$τ$的影响轨迹，并将影响归因于特定的循环迭代。为了使SDI在变压器规模上实用，我们提出了一种TensorSketch实现，该方法从不显式生成每个样本的梯度。在循环GPT风格模型和算法推理任务上的实验表明，SDI具有出色的可扩展性，与全梯度基线匹配误差低，并且能够支持广泛的数据归因和可解释性任务，提供对潜在推理过程的每一步洞察。</div>
</details>
</div>
<div class="card">
<div class="title">Causality in Video Diffusers is Separable from Denoising</div>
<div class="meta-line">Authors: Xingjian Bai, Guande He, Zhengqi Li, Eli Shechtman, Xun Huang, Zongze Wu</div>
<div class="meta-line">First: 2026-02-10T18:57:21+00:00 · Latest: 2026-02-10T18:57:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10095v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10095v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Causality -- referring to temporal, uni-directional cause-effect relationships between components -- underlies many complex generative processes, including videos, language, and robot trajectories. Current causal diffusion models entangle temporal reasoning with iterative denoising, applying causal attention across all layers, at every denoising step, and over the entire context. In this paper, we show that the causal reasoning in these models is separable from the multi-step denoising process. Through systematic probing of autoregressive video diffusers, we uncover two key regularities: (1) early layers produce highly similar features across denoising steps, indicating redundant computation along the diffusion trajectory; and (2) deeper layers exhibit sparse cross-frame attention and primarily perform intra-frame rendering. Motivated by these findings, we introduce Separable Causal Diffusion (SCD), a new architecture that explicitly decouples once-per-frame temporal reasoning, via a causal transformer encoder, from multi-step frame-wise rendering, via a lightweight diffusion decoder. Extensive experiments on both pretraining and post-training tasks across synthetic and real benchmarks show that SCD significantly improves throughput and per-frame latency while matching or surpassing the generation quality of strong causal diffusion baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视频扩散器中的因果性可与去噪过程分离</div>
<div class="mono" style="margin-top:8px">因果性——指组件之间的时序、单向因果关系——是许多复杂生成过程的基础，包括视频、语言和机器人轨迹。当前的因果扩散模型将时序推理与迭代去噪过程交织在一起，在所有层、每个去噪步骤以及整个上下文中应用因果注意力。本文表明，这些模型中的因果推理可以与多步去噪过程分离。通过对自回归视频扩散器的系统性探测，我们发现了两个关键规律：(1) 早期层在去噪步骤中生成高度相似的特征，表明扩散轨迹中存在冗余计算；(2) 更深层则表现出稀疏的跨帧注意力，并主要执行帧内渲染。基于这些发现，我们提出了可分离因果扩散（SCD），这是一种新的架构，通过因果变压器编码器显式地将每帧一次的时序推理与多步帧级渲染（通过轻量级扩散解码器）分离。在合成和真实基准上的预训练和后训练任务的大量实验表明，SCD在保持或超越强大因果扩散基线生成质量的同时，显著提高了吞吐量和每帧延迟。</div>
</details>
</div>
<div class="card">
<div class="title">4RC: 4D Reconstruction via Conditional Querying Anytime and Anywhere</div>
<div class="meta-line">Authors: Yihang Luo, Shangchen Zhou, Yushi Lan, Xingang Pan, Chen Change Loy</div>
<div class="meta-line">First: 2026-02-10T18:57:04+00:00 · Latest: 2026-02-10T18:57:04+00:00</div>
<div class="meta-line">Comments: Project page: https://yihangluo.com/projects/4RC/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10094v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10094v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://yihangluo.com/projects/4RC/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present 4RC, a unified feed-forward framework for 4D reconstruction from monocular videos. Unlike existing approaches that typically decouple motion from geometry or produce limited 4D attributes such as sparse trajectories or two-view scene flow, 4RC learns a holistic 4D representation that jointly captures dense scene geometry and motion dynamics. At its core, 4RC introduces a novel encode-once, query-anywhere and anytime paradigm: a transformer backbone encodes the entire video into a compact spatio-temporal latent space, from which a conditional decoder can efficiently query 3D geometry and motion for any query frame at any target timestamp. To facilitate learning, we represent per-view 4D attributes in a minimally factorized form by decomposing them into base geometry and time-dependent relative motion. Extensive experiments demonstrate that 4RC outperforms prior and concurrent methods across a wide range of 4D reconstruction tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>4RC：通过条件查询实现任意时间和地点的4D重建</div>
<div class="mono" style="margin-top:8px">我们提出了4RC，这是一个统一的前馈框架，用于从单目视频中进行4D重建。与现有方法通常将运动与几何解耦或仅生成有限的4D属性（如稀疏轨迹或双视角场景流）不同，4RC学习一种整体的4D表示，能够联合捕捉密集场景几何和运动动态。其核心在于引入了一种新颖的编码一次、查询任意时间和地点的范式：一个Transformer主干网络将整个视频编码为一个紧凑的时空潜在空间，从而可以高效地在任意目标时间戳下，通过条件解码器查询任意查询帧的3D几何和运动信息。为了促进学习，我们通过将4D属性分解为基本几何和时间相关的相对运动，以最小化因子化形式表示每视角的4D属性。大量实验表明，4RC在广泛的4D重建任务中优于现有和同时期的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present 4RC, a unified feed-forward framework for 4D reconstruction from monocular videos.</div>
</details>
</div>
<div class="card">
<div class="title">Noisy-Pair Robust Representation Alignment for Positive-Unlabeled Learning</div>
<div class="meta-line">Authors: Hengwei Zhao, Zhengzhong Tu, Zhuo Zheng, Wei Wang, Junjue Wang, Rusty Feagin, Wenzhe Jiao</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-09-30T18:22:30+00:00 · Latest: 2026-02-10T18:56:09+00:00</div>
<div class="meta-line">Comments: Published at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.01278v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.01278v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Positive-Unlabeled (PU) learning aims to train a binary classifier (positive vs. negative) where only limited positive data and abundant unlabeled data are available. While widely applicable, state-of-the-art PU learning methods substantially underperform their supervised counterparts on complex datasets, especially without auxiliary negatives or pre-estimated parameters (e.g., a 14.26% gap on CIFAR-100 dataset). We identify the primary bottleneck as the challenge of learning discriminative representations under unreliable supervision. To tackle this challenge, we propose NcPU, a non-contrastive PU learning framework that requires no auxiliary information. NcPU combines a noisy-pair robust supervised non-contrastive loss (NoiSNCL), which aligns intra-class representations despite unreliable supervision, with a phantom label disambiguation (PLD) scheme that supplies conservative negative supervision via regret-based label updates. Theoretically, NoiSNCL and PLD can iteratively benefit each other from the perspective of the Expectation-Maximization framework. Empirically, extensive experiments demonstrate that: (1) NoiSNCL enables simple PU methods to achieve competitive performance; and (2) NcPU achieves substantial improvements over state-of-the-art PU methods across diverse datasets, including challenging datasets on post-disaster building damage mapping, highlighting its promise for real-world applications. Code: Code will be open-sourced after review.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于正无标签学习的噪声对鲁棒表示对齐</div>
<div class="mono" style="margin-top:8px">正无标签（PU）学习旨在训练一个二分类器（正类 vs. 负类），其中仅有有限的正样本数据和大量的无标签数据。尽管应用广泛，但最先进的PU学习方法在复杂数据集上显著不如其监督学习对应方法，尤其是在没有辅助负样本或预估参数的情况下（例如，在CIFAR-100数据集上存在14.26%的性能差距）。我们识别出主要瓶颈是学习在不可靠监督下的判别性表示的挑战。为了解决这一挑战，我们提出了NcPU，一个无需辅助信息的非对比PU学习框架。NcPU结合了一个噪声对鲁棒的监督非对比损失（NoiSNCL），该损失能够在不可靠监督下对同类表示进行对齐，并结合一个幻象标签消歧（PLD）方案，通过基于遗憾的标签更新提供保守的负样本监督。理论上，NoiSNCL和PLD可以从期望最大化框架的角度相互迭代地提升。实验上，大量实验表明：(1) NoiSNCL使简单的PU方法能够达到具有竞争力的性能；(2) NcPU在多种数据集上显著优于最先进的PU方法，包括灾难后建筑物损毁映射等具有挑战性的数据集，突显了其在现实应用中的潜力。代码：代码将在审核后开源。</div>
</details>
</div>
<div class="card">
<div class="title">Agent World Model: Infinity Synthetic Environments for Agentic Reinforcement Learning</div>
<div class="meta-line">Authors: Zhaoyang Wang, Canwen Xu, Boyi Liu, Yite Wang, Siwei Han, Zhewei Yao, Huaxiu Yao, Yuxiong He</div>
<div class="meta-line">First: 2026-02-10T18:55:41+00:00 · Latest: 2026-02-10T18:55:41+00:00</div>
<div class="meta-line">Comments: 41 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10090v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10090v1">PDF</a> · <a href="https://github.com/Snowflake-Labs/agent-world-model">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language model (LLM) have empowered autonomous agents to perform complex tasks that require multi-turn interactions with tools and environments. However, scaling such agent training is limited by the lack of diverse and reliable environments. In this paper, we propose Agent World Model (AWM), a fully synthetic environment generation pipeline. Using this pipeline, we scale to 1,000 environments covering everyday scenarios, in which agents can interact with rich toolsets (35 tools per environment on average) and obtain high-quality observations. Notably, these environments are code-driven and backed by databases, providing more reliable and consistent state transitions than environments simulated by LLMs. Moreover, they enable more efficient agent interaction compared with collecting trajectories from realistic environments. To demonstrate the effectiveness of this resource, we perform large-scale reinforcement learning for multi-turn tool-use agents. Thanks to the fully executable environments and accessible database states, we can also design reliable reward functions. Experiments on three benchmarks show that training exclusively in synthetic environments, rather than benchmark-specific ones, yields strong out-of-distribution generalization. The code is available at https://github.com/Snowflake-Labs/agent-world-model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>智能体世界模型：用于智能体强化学习的无限合成环境</div>
<div class="mono" style="margin-top:8px">近年来，大语言模型（LLM）的进步使自主智能体能够执行需要与工具和环境进行多轮交互的复杂任务。然而，这种智能体训练的扩展受到缺乏多样性和可靠性的环境的限制。在本文中，我们提出了一种完全合成环境生成管道，称为Agent World Model（AWM）。利用该管道，我们扩展到涵盖日常场景的1,000个环境，其中智能体可以与丰富的工具集（平均每环境35个工具）进行交互，并获得高质量的观测结果。值得注意的是，这些环境是代码驱动的，并由数据库支持，相较于由LLM模拟的环境，提供了更可靠和一致的状态转移。此外，它们相比从现实环境收集轨迹，能够实现更高效的智能体交互。为了展示该资源的有效性，我们对多轮工具使用智能体进行了大规模强化学习。得益于完全可执行的环境和可访问的数据库状态，我们还可以设计可靠的奖励函数。在三个基准测试上的实验表明，仅在合成环境中进行训练，而非特定于基准的环境，能够实现强大的分布外泛化能力。代码可在https://github.com/Snowflake-Labs/agent-world-model获取。</div>
</details>
</div>
<div class="card">
<div class="title">Story-Iter: A Training-free Iterative Paradigm for Long Story Visualization</div>
<div class="meta-line">Authors: Jiawei Mao, Xiaoke Huang, Yunfei Xie, Yuanqi Chang, Mude Hui, Bingjie Xu, Zeyu Zheng, Zirui Wang, Cihang Xie, Yuyin Zhou</div>
<div class="meta-line">First: 2024-10-08T17:59:30+00:00 · Latest: 2026-02-10T18:53:34+00:00</div>
<div class="meta-line">Comments: 31 pages, 33 figures, The project page and associated code can be accessed via https://jwmao1.github.io/storyiter/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.06244v2">Abs</a> · <a href="https://arxiv.org/pdf/2410.06244v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://jwmao1.github.io/storyiter/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces Story-Iter, a new training-free iterative paradigm to enhance long-story generation. Unlike existing methods that rely on fixed reference images to construct a complete story, our approach features a novel external iterative paradigm, extending beyond the internal iterative denoising steps of diffusion models, to continuously refine each generated image by incorporating all reference images from the previous round. To achieve this, we propose a plug-and-play, training-free global reference cross-attention (GRCA) module, modeling all reference frames with global embeddings, ensuring semantic consistency in long sequences. By progressively incorporating holistic visual context and text constraints, our iterative paradigm enables precise generation with fine-grained interactions, optimizing the story visualization step-by-step. Extensive experiments in the official story visualization dataset and our long story benchmark demonstrate that Story-Iter&#x27;s state-of-the-art performance in long-story visualization (up to 100 frames) excels in both semantic consistency and fine-grained interactions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Story-Iter: 一种无需训练的长故事可视化迭代范式</div>
<div class="mono" style="margin-top:8px">本文介绍了Story-Iter，一种新的无需训练的迭代范式，用于增强长故事生成。与依赖固定参考图像构建完整故事的现有方法不同，我们的方法采用了一种新颖的外部迭代范式，超越扩散模型内部的迭代去噪步骤，通过整合上一轮所有参考图像，持续优化每张生成的图像。为实现这一目标，我们提出了一种即插即用、无需训练的全局参考交叉注意力（GRCA）模块，利用全局嵌入建模所有参考帧，确保长序列中的语义一致性。通过逐步整合整体视觉上下文和文本约束，我们的迭代范式实现了精确生成与细粒度交互，逐步优化故事可视化过程。在官方故事可视化数据集和我们构建的长故事基准数据集上的大量实验表明，Story-Iter在长故事可视化（最多100帧）方面表现出最先进的性能，同时在语义一致性和细粒度交互方面均表现出色。</div>
</details>
</div>
<div class="card">
<div class="title">CODE-SHARP: Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs</div>
<div class="meta-line">Authors: Richard Bornemann, Pierluigi Vito Amadori, Antoine Cully</div>
<div class="meta-line">First: 2026-02-10T18:51:39+00:00 · Latest: 2026-02-10T18:51:39+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10085v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10085v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/code-sharp/homepage}{here}$">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence. While reinforcement learning offers a powerful framework for training agents to master complex skills, it typically relies on hand-designed reward functions. This is infeasible for open-ended skill discovery, where the set of meaningful skills is not known a priori. While recent methods have shown promising results towards automating reward function design, they remain limited to refining rewards for pre-defined tasks. To address this limitation, we introduce Continuous Open-ended Discovery and Evolution of Skills as Hierarchical Reward Programs (CODE-SHARP), a novel framework leveraging Foundation Models (FM) to open-endedly expand and refine a hierarchical skill archive, structured as a directed graph of executable reward functions in code. We show that a goal-conditioned agent trained exclusively on the rewards generated by the discovered SHARP skills learns to solve increasingly long-horizon goals in the Craftax environment. When composed by a high-level FM-based planner, the discovered skills enable a single goal-conditioned agent to solve complex, long-horizon tasks, outperforming both pretrained agents and task-specific expert policies by over $134$% on average. We will open-source our code and provide additional videos $\href{https://sites.google.com/view/code-sharp/homepage}{here}$.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CODE-SHARP：作为分层奖励程序的持续开放式技能发现与演化</div>
<div class="mono" style="margin-top:8px">开发能够开放性地发现和学习新技能的智能体是人工智能领域的一个重大挑战。虽然强化学习为训练智能体掌握复杂技能提供了一个强大的框架，但它通常依赖于人工设计的奖励函数。这在开放性技能发现中是不可行的，因为有意义的技能集合事先并不已知。尽管近期的方法在自动化奖励函数设计方面取得了有希望的成果，但它们仍然局限于对预定义任务的奖励进行优化。为了解决这一限制，我们引入了持续开放性技能发现与演化作为分层奖励程序（CODE-SHARP），这是一种利用基础模型（FM）的新框架，用于开放性地扩展和优化分层技能库，该技能库以可执行奖励函数的代码有向图形式构建。我们展示了，一个仅在发现的SHARP技能生成的奖励上进行训练的目标条件智能体，能够在Craftax环境中解决越来越长的时域目标。当由一个高层基础模型规划器组合时，发现的技能使单个目标条件智能体能够解决复杂且长时域的任务，其表现平均超过预训练智能体和任务特定专家策略 $134$% 以上。我们将开源我们的代码，并提供额外的视频 $\href{https://sites.google.com/view/code-sharp/homepage}{此处}$。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Developing agents capable of open-endedly discovering and learning novel skills is a grand challenge in Artificial Intelligence.</div>
</details>
</div>
<div class="card">
<div class="title">CyberExplorer: Benchmarking LLM Offensive Security Capabilities in a Real-World Attacking Simulation Environment</div>
<div class="meta-line">Authors: Nanda Rani, Kimberly Milner, Minghao Shao, Meet Udeshi, Haoran Xi, Venkata Sai Charan Putrevu, Saksham Aggarwal, Sandeep K. Shukla, Prashanth Krishnamurthy, Farshad Khorrami, Muhammad Shafique, Ramesh Karri</div>
<div class="meta-line">First: 2026-02-08T15:56:22+00:00 · Latest: 2026-02-10T18:48:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08023v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08023v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-world offensive security operations are inherently open-ended: attackers explore unknown attack surfaces, revise hypotheses under uncertainty, and operate without guaranteed success. Existing LLM-based offensive agent evaluations rely on closed-world settings with predefined goals and binary success criteria. To address this gap, we introduce CyberExplorer, an evaluation suite with two core components: (1) an open-environment benchmark built on a virtual machine hosting 40 vulnerable web services derived from real-world CTF challenges, where agents autonomously perform reconnaissance, target selection, and exploitation without prior knowledge of vulnerability locations; and (2) a reactive multi-agent framework supporting dynamic exploration without predefined plans. CyberExplorer enables fine-grained evaluation beyond flag recovery, capturing interaction dynamics, coordination behavior, failure modes, and vulnerability discovery signals-bridging the gap between benchmarks and realistic multi-target attack scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CyberExplorer：在真实攻击模拟环境中对LLM进攻性安全能力的基准测试</div>
<div class="mono" style="margin-top:8px">现实中的进攻性安全操作本质上是开放式的：攻击者探索未知的攻击面，面对不确定性时修正假设，并且在没有保证成功的情况下进行操作。现有的基于LLM的进攻性代理评估依赖于封闭世界环境，具有预定义目标和二元成功标准。为了解决这一问题，我们引入了CyberExplorer，一个包含两个核心组件的评估套件：(1) 一个基于虚拟机的开放环境基准，该虚拟机托管了40个源自现实CTF挑战的易受攻击的Web服务，代理在没有预先知道漏洞位置的情况下自主进行侦察、目标选择和利用；(2) 一个反应式多代理框架，支持动态探索而无需预定义计划。CyberExplorer能够超越旗帜回收的细粒度评估，捕捉交互动态、协调行为、失败模式和漏洞发现信号，弥合基准测试与现实多目标攻击场景之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">Anagent For Enhancing Scientific Table &amp; Figure Analysis</div>
<div class="meta-line">Authors: Xuehang Guo, Zhiyong Lu, Tom Hope, Qingyun Wang</div>
<div class="meta-line">First: 2026-02-10T18:46:28+00:00 · Latest: 2026-02-10T18:46:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10081v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10081v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://xhguo7.github.io/Anagent/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In scientific research, analysis requires accurately interpreting complex multimodal knowledge, integrating evidence from different sources, and drawing inferences grounded in domain-specific knowledge. However, current artificial intelligence (AI) systems struggle to consistently demonstrate such capabilities. The complexity and variability of scientific tables and figures, combined with heterogeneous structures and long-context requirements, pose fundamental obstacles to scientific table \&amp; figure analysis. To quantify these challenges, we introduce AnaBench, a large-scale benchmark featuring $63,178$ instances from nine scientific domains, systematically categorized along seven complexity dimensions. To tackle these challenges, we propose Anagent, a multi-agent framework for enhanced scientific table \&amp; figure analysis through four specialized agents: Planner decomposes tasks into actionable subtasks, Expert retrieves task-specific information through targeted tool execution, Solver synthesizes information to generate coherent analysis, and Critic performs iterative refinement through five-dimensional quality assessment. We further develop modular training strategies that leverage supervised finetuning and specialized reinforcement learning to optimize individual capabilities while maintaining effective collaboration. Comprehensive evaluation across 170 subdomains demonstrates that Anagent achieves substantial improvements, up to $\uparrow 13.43\%$ in training-free settings and $\uparrow 42.12\%$ with finetuning, while revealing that task-oriented reasoning and context-aware problem-solving are essential for high-quality scientific table \&amp; figure analysis. Our project page: https://xhguo7.github.io/Anagent/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于增强科学表格与图表分析的代理</div>
<div class="mono" style="margin-top:8px">在科学研究中，分析需要准确解释复杂的多模态知识，整合不同来源的证据，并基于领域特定知识进行推理。然而，当前的人工智能（AI）系统难以一致地展现这些能力。科学表格和图表的复杂性和变异性，结合异构结构和长上下文需求，构成了科学表格与图表分析的基本障碍。为量化这些挑战，我们引入了AnaBench，这是一个包含来自九个科学领域的63,178个实例的大规模基准，系统地沿七个复杂度维度进行分类。为应对这些挑战，我们提出了Anagent，一个通过四个专用代理增强科学表格与图表分析的多代理框架：Planner将任务分解为可执行的子任务，Expert通过有针对性的工具执行来检索任务特定信息，Solver综合信息以生成连贯的分析，Critic则通过五维质量评估进行迭代优化。我们进一步开发了模块化训练策略，利用监督微调和专用强化学习来优化个体能力，同时保持有效的协作。在170个子领域上的全面评估表明，Anagent在无训练设置下实现了高达13.43%的提升，在微调设置下则达到42.12%的提升，同时揭示了任务导向推理和上下文感知问题解决对于高质量科学表格与图表分析的重要性。我们的项目页面：https://xhguo7.github.io/Anagent/</div>
</details>
</div>
<div class="card">
<div class="title">Beyond a Single Queue: Multi-Level-Multi-Queue as an Effective Design for SSSP problems on GPUs</div>
<div class="meta-line">Authors: Zhengding Hu, Jingwen Sun, Le Jiang, Yuhao Wang, Junqing Lin, Yi Zong, Guangzhong Sun</div>
<div class="meta-line">First: 2026-02-10T18:46:16+00:00 · Latest: 2026-02-10T18:46:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10080v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10080v1">PDF</a> · <a href="https://github.com/Leo9660/MLMQ.git">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As one of the most fundamental problems in graph processing, the Single-Source Shortest Path (SSSP) problem plays a critical role in numerous application scenarios. However, existing GPU-based solutions remain inefficient, as they typically rely on a single, fixed queue design that incurs severe synchronization overhead, high memory latency, and poor adaptivity to diverse inputs. To address these inefficiencies, we propose MultiLevelMultiQueue (MLMQ), a novel data structure that distributes multiple queues across the GPU&#x27;s multi-level parallelism and memory hierarchy. To realize MLMQ, we introduce a cache-like collaboration mechanism for efficient inter-queue coordination, and develop a modular queue design based on unified Read and Write primitives. Within this framework, we expand the optimization space by designing a set of GPU-friendly queues, composing them across multiple levels, and further providing an input-adaptive MLMQ configuration scheme. Our MLMQ design achieves average speedups of 1.87x to 17.13x over state-of-the-art implementations. Our code is open-sourced at https://github.com/Leo9660/MLMQ.git.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越单一队列：一种适用于GPU的多级多队列设计用于SSSP问题</div>
<div class="mono" style="margin-top:8px">作为图处理中最基础的问题之一，单源最短路径（SSSP）问题在众多应用场景中起着关键作用。然而，现有的基于GPU的解决方案效率较低，因为它们通常依赖于单一固定队列设计，导致严重的同步开销、高内存延迟以及对多样输入的适应性差。为了解决这些问题，我们提出了MultiLevelMultiQueue（MLMQ），一种新颖的数据结构，将多个队列分布在GPU的多级并行性和内存层次结构上。为了实现MLMQ，我们引入了一种类似缓存的协作机制，以实现高效的队列间协调，并基于统一的读写原语开发了一种模块化的队列设计。在此框架下，我们通过设计一组适用于GPU的队列、在多个层级上组合这些队列，并进一步提供一种输入自适应的MLMQ配置方案，扩展了优化空间。我们的MLMQ设计在最先进的实现上实现了平均1.87到17.13倍的速度提升。我们的代码已开源，地址为https://github.com/Leo9660/MLMQ.git。</div>
</details>
</div>
<div class="card">
<div class="title">Can Image Splicing and Copy-Move Forgery Be Detected by the Same Model? Forensim: An Attention-Based State-Space Approach</div>
<div class="meta-line">Authors: Soumyaroop Nandi, Prem Natarajan</div>
<div class="meta-line">First: 2026-02-10T18:46:04+00:00 · Latest: 2026-02-10T18:46:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10079v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10079v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Forensim, an attention-based state-space framework for image forgery detection that jointly localizes both manipulated (target) and source regions. Unlike traditional approaches that rely solely on artifact cues to detect spliced or forged areas, Forensim is designed to capture duplication patterns crucial for understanding context. In scenarios such as protest imagery, detecting only the forged region, for example a duplicated act of violence inserted into a peaceful crowd, can mislead interpretation, highlighting the need for joint source-target localization. Forensim outputs three-class masks (pristine, source, target) and supports detection of both splicing and copy-move forgeries within a unified architecture. We propose a visual state-space model that leverages normalized attention maps to identify internal similarities, paired with a region-based block attention module to distinguish manipulated regions. This design enables end-to-end training and precise localization. Forensim achieves state-of-the-art performance on standard benchmarks. We also release CMFD-Anything, a new dataset addressing limitations of existing copy-move forgery datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>图像拼接和复制移动伪造能否被同一模型检测？Forensim：一种基于注意力的状态空间方法</div>
<div class="mono" style="margin-top:8px">我们引入了Forensim，这是一种基于注意力的状态空间框架，用于图像篡改检测，能够联合定位被篡改（目标）区域和源区域。与仅依赖于伪影线索的传统方法不同，Forensim旨在捕捉对理解上下文至关重要的重复模式。例如，在抗议图像中，仅检测伪造区域（如插入到和平人群中的重复暴力行为）可能会误导解释，突显了联合源-目标定位的重要性。Forensim输出三类掩码（原始、源、目标），并在统一架构中支持拼接和复制移动伪造的检测。我们提出了一种视觉状态空间模型，利用归一化注意力图来识别内部相似性，并结合基于区域的块注意力模块来区分被篡改区域。这种设计实现了端到端训练和精确定位。Forensim在标准基准上实现了最先进的性能。我们还发布了CMFD-Anything，这是一个新数据集，旨在解决现有复制移动伪造数据集的局限性。</div>
</details>
</div>
<div class="card">
<div class="title">Designing Multi-Robot Ground Video Sensemaking with Public Safety Professionals</div>
<div class="meta-line">Authors: Puqi Zhou, Ali Asgarov, Aafiya Hussain, Wonjoon Park, Amit Paudyal, Sameep Shrestha, Chia-wei Tang, Michael F. Lighthiser, Michael R. Hieb, Xuesu Xiao, Chris Thomas, Sungsoo Ray Hong</div>
<div class="meta-line">First: 2026-02-09T16:43:37+00:00 · Latest: 2026-02-10T18:41:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08882v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08882v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Videos from fleets of ground robots can advance public safety by providing scalable situational awareness and reducing professionals&#x27; burden. Yet little is known about how to design and integrate multi-robot videos into public safety workflows. Collaborating with six police agencies, we examined how such videos could be made practical. In Study 1, we presented the first testbed for multi-robot ground video sensemaking. The testbed includes 38 events-of-interest (EoI) relevant to public safety, a dataset of 20 robot patrol videos (10 day/night pairs) covering EoI types, and 6 design requirements aimed at improving current video sensemaking practices. In Study 2, we built MRVS, a tool that augments multi-robot patrol video streams with a prompt-engineered video understanding model. Participants reported reduced manual workload and greater confidence with LLM-based explanations, while noting concerns about false alarms and privacy. We conclude with implications for designing future multi-robot video sensemaking tools.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>与公共安全专业人士合作设计多机器人地面视频态势感知</div>
<div class="mono" style="margin-top:8px">地面机器人车队的视频可以提升公共安全水平，提供可扩展的情境感知并减轻专业人员的负担。然而，关于如何设计和整合多机器人视频到公共安全工作流程中仍知之甚少。我们与六个警察部门合作，探讨了如何使此类视频更具实用性。在第一项研究中，我们构建了首个多机器人地面视频态势感知测试平台，该平台包含38个与公共安全相关的事件类型（EoI），一个涵盖这些事件类型的20个机器人巡逻视频数据集（包含10组昼夜视频），以及6项旨在改进当前视频态势感知实践的设计需求。在第二项研究中，我们开发了MRVS工具，该工具通过提示工程视频理解模型增强了多机器人巡逻视频流。参与者报告称，使用基于大语言模型（LLM）的解释减少了手动工作量，并提高了信心，但也提到了对误报和隐私问题的担忧。我们最后总结了对未来多机器人视频态势感知工具设计的启示。</div>
</details>
</div>
<div class="card">
<div class="title">CAPID: Context-Aware PII Detection for Question-Answering Systems</div>
<div class="meta-line">Authors: Mariia Ponomarenko, Sepideh Abedini, Masoumeh Shafieinejad, D. B. Emerson, Shubhankar Mohapatra, Xi He</div>
<div class="meta-line">First: 2026-02-10T18:41:31+00:00 · Latest: 2026-02-10T18:41:31+00:00</div>
<div class="meta-line">Comments: Accepted to the Student Research Workshop at EACL 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10074v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10074v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Detecting personally identifiable information (PII) in user queries is critical for ensuring privacy in question-answering systems. Current approaches mainly redact all PII, disregarding the fact that some of them may be contextually relevant to the user&#x27;s question, resulting in a degradation of response quality. Large language models (LLMs) might be able to help determine which PII are relevant, but due to their closed source nature and lack of privacy guarantees, they are unsuitable for sensitive data processing. To achieve privacy-preserving PII detection, we propose CAPID, a practical approach that fine-tunes a locally owned small language model (SLM) that filters sensitive information before it is passed to LLMs for QA. However, existing datasets do not capture the context-dependent relevance of PII needed to train such a model effectively. To fill this gap, we propose a synthetic data generation pipeline that leverages LLMs to produce a diverse, domain-rich dataset spanning multiple PII types and relevance levels. Using this dataset, we fine-tune an SLM to detect PII spans, classify their types, and estimate contextual relevance. Our experiments show that relevance-aware PII detection with a fine-tuned SLM substantially outperforms existing baselines in span, relevance and type accuracy while preserving significantly higher downstream utility under anonymization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CAPID: 面向问答系统的上下文感知个人身份信息检测</div>
<div class="mono" style="margin-top:8px">在用户查询中检测个人身份信息（PII）对于确保问答系统中的隐私至关重要。当前方法主要对所有PII进行遮蔽，忽略了其中一些信息可能与用户问题上下文相关，从而导致响应质量下降。大型语言模型（LLMs）可能有助于判断哪些PII是相关的，但由于其闭源性质和缺乏隐私保障，不适合用于敏感数据处理。为实现隐私保护的PII检测，我们提出了CAPID，一种实用方法，通过本地拥有的小型语言模型（SLM）在传递给LLMs进行问答前过滤敏感信息。然而，现有数据集无法有效捕捉训练此类模型所需的上下文相关性。为填补这一空白，我们提出了一种合成数据生成流程，利用LLMs创建涵盖多种PII类型和相关性级别的多样化、领域丰富的数据集。使用该数据集，我们对SLM进行了微调，以检测PII片段、分类其类型并估计上下文相关性。实验表明，使用微调SLM进行的上下文感知PII检测在片段、相关性和类型准确性方面显著优于现有基线，同时在匿名化后保持更高的下游实用性。</div>
</details>
</div>
<div class="card">
<div class="title">Data-efficient and Interpretable Inverse Materials Design using a Disentangled Variational Autoencoder</div>
<div class="meta-line">Authors: Cheng Zeng, Zulqarnain Khan, Nathan L. Post</div>
<div class="meta-line">Venue: AI Mater. 2025(1):0002</div>
<div class="meta-line">First: 2024-09-10T02:21:13+00:00 · Latest: 2026-02-10T18:34:17+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/cengc13/d_vae_hea</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2409.06740v3">Abs</a> · <a href="https://arxiv.org/pdf/2409.06740v3">PDF</a> · <a href="https://github.com/cengc13/d_vae_hea">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inverse materials design has proven successful in accelerating novel material discovery. Many inverse materials design methods use unsupervised learning where a latent space is learned to offer a compact description of materials representations. A latent space learned this way is likely to be entangled, in terms of the target property and other properties of the materials. This makes the inverse design process ambiguous. Here, we present a semi-supervised learning approach based on a disentangled variational autoencoder to learn a probabilistic relationship between features, latent variables and target properties. This approach is data efficient because it combines all labelled and unlabelled data in a coherent manner, and it uses expert-informed prior distributions to improve model robustness even with limited labelled data. It is in essence interpretable, as the learnable target property is disentangled out of the other properties of the materials, and an extra layer of interpretability can be provided by a post-hoc analysis of the classification head of the model. We demonstrate this new approach on an experimental high-entropy alloy dataset with chemical compositions as input and single-phase formation as the single target property. High-entropy alloys were chosen as example materials because of the vast chemical space of their possible combinations of compositions and atomic configurations. While single property is used in this work, the disentangled model can be extended to customize for inverse design of materials with multiple target properties.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于解耦变分自编码器的数据高效且可解释的逆材料设计</div>
<div class="mono" style="margin-top:8px">逆材料设计已被证明在加速新材料发现方面非常成功。许多逆材料设计方法采用无监督学习，通过学习潜在空间来提供材料表示的紧凑描述。通过这种方式学习的潜在空间可能在目标属性和其他材料属性之间存在纠缠，这使得逆设计过程变得模糊。本文提出了一种基于解耦变分自编码器的半监督学习方法，用于学习特征、潜在变量和目标属性之间的概率关系。该方法具有数据高效性，因为它以一种连贯的方式结合了所有标记和未标记数据，并利用专家提供的先验分布来提高模型的鲁棒性，即使在标记数据有限的情况下。本质上，该方法是可解释的，因为可学习的目标属性被从其他材料属性中解耦出来，而通过模型分类头的后置分析还可以提供额外的可解释性。我们在一个实验性的高熵合金数据集上展示了这种方法，该数据集以化学成分作为输入，以单相形成作为单一目标属性。选择高熵合金作为示例材料，是因为其可能的成分组合和原子配置的化学空间非常广阔。虽然本工作中仅使用了单一属性，但解耦模型可以扩展用于具有多个目标属性的材料逆设计。</div>
</details>
</div>
<div class="card">
<div class="title">Features as Rewards: Scalable Supervision for Open-Ended Tasks via Interpretability</div>
<div class="meta-line">Authors: Aaditya Vikram Prasad, Connor Watts, Jack Merullo, Dhruvil Gala, Owen Lewis, Thomas McGrath, Ekdeep Singh Lubana</div>
<div class="meta-line">First: 2026-02-10T18:33:45+00:00 · Latest: 2026-02-10T18:33:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10067v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10067v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language models trained on large-scale datasets have been shown to learn features that encode abstract concepts such as factuality or intent. Such features are traditionally used for test-time monitoring or steering. We present an alternative affordance: features as scalable supervision for open-ended tasks. We consider the case of hallucination-reduction as a desirable, yet open-ended behavior and design a reinforcement learning (RL) pipeline, titled RLFR (Reinforcement Learning from Feature Rewards), that uses features as reward functions. Grounded in a novel probing framework that identifies candidate hallucinated claims, our pipeline teaches a model to intervene and correct its completions when it is uncertain of their factuality. Furthermore, the pipeline enables scalable test-time compute, guided once more by our reward features. This end-to-end process operationalized on Gemma-3-12B-IT results in a policy that is 58% less likely to hallucinate compared to the original model, while preserving performance on standard benchmarks. Taken together, by grounding supervision in the language of features, this paper introduces a novel paradigm in the use of interpretability for learning open-ended tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>特征作为奖励：通过可解释性实现开放任务的可扩展监督</div>
<div class="mono" style="margin-top:8px">在大规模数据集上训练的语言模型已被证明能够学习编码抽象概念（如事实性或意图）的特征。这些特征传统上用于测试时的监控或引导。我们提出了一种替代方法：将特征作为开放任务的可扩展监督。我们将减少幻觉视为一种期望但开放的行为，并设计了一个名为RLFR（基于特征奖励的强化学习）的强化学习（RL）流程，该流程利用特征作为奖励函数。我们的流程基于一种新颖的探针框架，该框架可以识别候选的幻觉声明，从而教导模型在对其事实性不确定时进行干预和纠正。此外，该流程还通过我们的奖励特征实现了可扩展的测试时计算。在Gemma-3-12B-IT上实现的端到端过程产生了一种比原始模型少58%幻觉的策略，同时保持了在标准基准上的性能。总体而言，通过将监督建立在特征的语言上，本文引入了在使用可解释性学习开放任务方面的一种新范式。</div>
</details>
</div>
<div class="card">
<div class="title">Variational Sparse Paired Autoencoders (vsPAIR) for Inverse Problems and Uncertainty Quantification</div>
<div class="meta-line">Authors: Jack Michael Solomon, Rishi Leburu, Matthias Chung</div>
<div class="meta-line">First: 2026-02-03T00:46:29+00:00 · Latest: 2026-02-10T18:33:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02948v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02948v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inverse problems are fundamental to many scientific and engineering disciplines; they arise when one seeks to reconstruct hidden, underlying quantities from noisy measurements. Many applications demand not just point estimates but interpretable uncertainty. Providing fast inference alongside uncertainty estimates remains challenging yet desirable in numerous applications.
  We propose the Variational Sparse Paired Autoencoder (vsPAIR) to address this challenge. The architecture pairs a standard VAE encoding observations with a sparse VAE encoding quantities of interest, connected through a learned latent mapping. The variational structure enables uncertainty estimation, the paired architecture encourages interpretability by anchoring QoI representations to clean data, and sparse encodings provide structure by concentrating information into identifiable factors rather than diffusing across all dimensions. To validate the effectiveness of our proposed architecture, we conduct experiments on blind inpainting and computed tomography, demonstrating that vsPAIR is a capable inverse problem solver that can provide interpretable and structured uncertainty estimates.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于反问题和不确定性量化变分稀疏配对自编码器（vsPAIR）</div>
<div class="mono" style="margin-top:8px">反问题在许多科学和工程领域中是基础性的；它们出现在试图从有噪声的测量中重建隐藏的、基础的量时。许多应用不仅需要点估计，还需要可解释的不确定性。在众多应用中，提供快速推理和不确定性估计仍然是一个具有挑战性但极具吸引力的问题。
我们提出变分稀疏配对自编码器（vsPAIR）来解决这一挑战。该架构将标准VAE编码观测值与稀疏VAE编码感兴趣量配对，并通过学习的潜在映射连接。变分结构使得不确定性估计成为可能，配对架构通过将QoI表示锚定到干净数据来促进可解释性，而稀疏编码则通过将信息集中到可识别的因素中，而不是扩散到所有维度，从而提供结构。为了验证我们提出架构的有效性，我们在盲图像修复和计算机断层扫描上进行了实验，证明vsPAIR是一种有能力的反问题求解器，能够提供可解释和结构化的不确定性估计。</div>
</details>
</div>
<div class="card">
<div class="title">Dark Energy Survey Year 6 Results: Cosmological Constraints from Cosmic Shear</div>
<div class="meta-line">Authors: DES Collaboration, T. M. C. Abbott, M. Aguena, A. Alarcon, O. Alves, A. Amon, D. Anbajagane, F. Andrade-Oliveira, W. d&#x27;Assignies, S. Avila, D. Bacon, J. Beas-Gonzalez, K. Bechtol, M. R. Becker, G. M. Bernstein, J. Blazek, S. Bocquet, D. Brooks, H. Camacho, G. Camacho-Ciurana, R. Camilleri, G. Campailla, A. Campos, A. Carnero Rosell, M. Carrasco Kind, J. Carretero, F. J. Castander, R. Cawthon, C. Chang, A. Choi, J. M. Coloma-Nadal, C. Conselice, L. N. da Costa, M. Costanzi, M. Crocce, T. M. Davis, J. De Vicente, D. L. DePoy, J. DeRose, S. Desai, H. T. Diehl, P. Doel, C. Doux, A. Drlica-Wagner, T. F. Eifler, S. Everett, A. E. Evrard, A. Ferté, B. Flaugher, P. Fosalba, O. Friedrich, J. Frieman, J. García-Bellido, M. Gatti, G. Giannini, P. Giles, K. Glazebrook, D. Gruen, R. A. Gruendl, G. Gutierrez, I. Harrison, W. G. Hartley, K. Herner, S. R. Hinton, D. L. Hollowood, K. Honscheid, D. Huterer, B. Jain, D. J. James, M. Jarvis, N. Jeffrey, T. Jeltema, T. Kacprzak, S. Kent, E. Krause, O. Lahav, S. Lee, E. Legnani, H. Lin, J. L. Marshall, S. Mau, J. Mena-Fernández, F. Menanteau, R. Miquel, J. J. Mohr, J. Muir, J. Myles, R. C. Nichol, R. L. C. Ogando, A. Palmese, M. Paterno, W. J. Percival, D. Petravick, A. A. Plazas Malagón, A. Porredon, J. Prat, C. Preston, M. Raveri, M. Rodriguez-Monroy, A. K. Romer, A. Roodman, E. S. Rykoff, S. Samuroff, C. Sánchez, E. Sanchez, D. Sanchez Cid, T. Schutt, I. Sevilla-Noarbe, E. Sheldon, T. Shin, M. E. da Silva Pereira, M. Smith, M. Soares-Santos, E. Suchyta, M. E. C. Swanson, M. Tabbutt, G. Tarle, D. Thomas, C. To, M. A. Troxel, V. Vikram, M. Vincenzi, N. Weaverdyck, J. Weller, P. Wiseman, M. Yamamoto, B. Yanny, B. Yin, J. Zuntz</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2026-02-10T18:33:06+00:00 · Latest: 2026-02-10T18:33:06+00:00</div>
<div class="meta-line">Comments: See this https://www.darkenergysurvey.org/des-y6-cosmology-results-papers/ for the full DES Y6 3x2pt cosmology release</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10065v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10065v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present legacy cosmic shear measurements and cosmological constraints using six years of Dark Energy Survey imaging data. From these data, we study ~140 million galaxies (8.29 galaxies/arcmin$^2$) that are 50% complete at i=24.0 and extend beyond z=1.2. We divide the galaxies into four redshift bins, and obtain cosmic shear measurement with a signal-to-noise of 83, a factor of 2 higher than the Year 3 analysis. We model the uncertainties due to shear and redshift calibrations, and discard measurements on small angular scales to mitigate baryon feedback and other small-scale uncertainties. We consider two fiducial models to account for the intrinsic alignment (IA) of the galaxies. We conduct a blind analysis in the context of the $Λ$CDM model and find $S_8 \equiv σ_8(Ω_m/0.3)^{0.5}=0.798^{+0.014}_{-0.015}$ (marginalized mean with 68% CL) when using the non-linear alignment model (NLA) and $S_{8} = 0.783^{+0.019}_{-0.015}$ with the tidal alignment and tidal torque model (TATT), providing 1.8% and 2.5% uncertainty on $S_8$. Compared to constraints from the cosmic microwave background from Planck 2018, ACT DR6 and SPT-3G DR1, we find consistency in the full parameter space at 1.1$σ$ (1.7$σ$) and in $S_8$ at 2.0$σ$ (2.3$σ$) for NLA (TATT). The result using the NLA model is preferred according to the Bayesian evidence. We find that the model choice for IA and baryon feedback can impact the value of our $S_8$ constraint up to $1σ$. For our fiducial model choices, the resultant uncertainties in $S_8$ are primarily degraded by the removal of scales, as well as the marginalization over the IA parameters. We demonstrate that our result is internally consistent and robust to different choices in calibrating the data, owing to methodological improvements in shear and redshift measurement, laying the foundation for next-generation cosmic shear programs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>暗能量巡天第六年结果：宇宙视差的宇宙学约束</div>
<div class="mono" style="margin-top:8px">我们利用暗能量巡天六年来的成像数据，展示了遗留的宇宙视差测量结果及宇宙学约束。这些数据研究了约1.4亿个星系（8.29个星系/角分²），在i=24.0处完成度为50%，并延伸至z=1.2以上。我们将星系分为四个红移区间，并获得了信噪比为83的宇宙视差测量结果，比第三年分析提高了两倍。我们建模了由于视差和红移校准引起的不确定性，并剔除小角度尺度的测量以减轻星系反馈和其他小尺度不确定性的影响。我们考虑了两种基准模型以处理星系的本征对齐（IA）问题。在ΛCDM模型框架下进行盲分析，使用非线性对齐模型（NLA）得到S₈ ≡ σ₈(Ωₘ/0.3)^{0.5} = 0.798^{+0.014}_{-0.015}（68%置信度的边际化平均值），使用潮汐对齐和潮汐力矩模型（TATT）得到S₈ = 0.783^{+0.019}_{-0.015}，对S₈的不确定性分别为1.8%和2.5%。与Planck 2018、ACT DR6和SPT-3G DR1的宇宙微波背景约束相比，我们在全参数空间内的一致性达到1.1σ（1.7σ），在S₈上的一致性达到2.0σ（2.3σ）（NLA和TATT）。根据贝叶斯证据，使用NLA模型的结果更优。我们发现，IA模型和星系反馈的选择会影响我们S₈约束值高达1σ。对于我们的基准模型选择，S₈的不确定性主要受到尺度剔除以及对IA参数的边际化影响。由于在视差和红移测量方法上的改进，我们证明了结果在内部上是一致的，并且对数据校准的不同选择具有鲁棒性，这为下一代宇宙视差项目奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present legacy cosmic shear measurements and cosmological constraints using six years of Dark Energy Survey imaging data.</div>
</details>
</div>
<div class="card">
<div class="title">From Spatial to Actions: Grounding Vision-Language-Action Model in Spatial Foundation Priors</div>
<div class="meta-line">Authors: Zhengshen Zhang, Hao Li, Yalun Dai, Zhengbang Zhu, Lei Zhou, Chenchen Liu, Dong Wang, Francis E. H. Tay, Sijin Chen, Ziwei Liu, Yuxiao Liu, Xinghang Li, Pan Zhou</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-20T11:26:45+00:00 · Latest: 2026-02-10T18:32:44+00:00</div>
<div class="meta-line">Comments: ICLR 2026, Project page: https://falcon-vla.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.17439v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.17439v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://falcon-vla.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing vision-language-action (VLA) models act in 3D real-world but are typically built on 2D encoders, leaving a spatial reasoning gap that limits generalization and adaptability. Recent 3D integration techniques for VLAs either require specialized sensors and transfer poorly across modalities, or inject weak cues that lack geometry and degrade vision-language alignment. In this work, we introduce FALCON (From Spatial to Action), a novel paradigm that injects rich 3D spatial tokens into the action head. FALCON leverages spatial foundation models to deliver strong geometric priors from RGB alone, and includes an Embodied Spatial Model that can optionally fuse depth, or pose for higher fidelity when available, without retraining or architectural changes. To preserve language reasoning, spatial tokens are consumed by a Spatial-Enhanced Action Head rather than being concatenated into the vision-language backbone. These designs enable FALCON to address limitations in spatial representation, modality transferability, and alignment. In comprehensive evaluations across three simulation benchmarks and eleven real-world tasks, our proposed FALCON achieves state-of-the-art performance, consistently surpasses competitive baselines, and remains robust under clutter, spatial-prompt conditioning, and variations in object scale and height.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从空间到动作：在空间基础先验中构建视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">现有的视觉-语言-动作（VLA）模型在三维真实世界中进行操作，但通常基于二维编码器，这导致了空间推理的缺失，限制了其泛化能力和适应性。近期的VLA三维集成技术要么需要专用传感器且跨模态迁移效果差，要么注入的线索较弱，缺乏几何信息并损害了视觉-语言对齐。在本工作中，我们引入了FALCON（从空间到动作），一种新颖范式，它将丰富的三维空间标记注入到动作头中。FALCON利用空间基础模型，仅通过RGB图像即可提供强大的几何先验，并包含一个可选的具身空间模型，可在有深度或姿态信息时融合这些数据以提高保真度，而无需重新训练或架构修改。为了保持语言推理能力，空间标记被输入到空间增强的动作头中，而不是简单地连接到视觉-语言主干网络中。这些设计使FALCON能够解决空间表示、模态迁移性和对齐方面的局限性。在三个模拟基准和十一项真实世界任务的全面评估中，我们提出的FALCON取得了最先进的性能，持续超越竞争基线，并在杂乱环境、空间提示条件和物体尺度与高度变化下保持鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Chain of Mindset: Reasoning with Adaptive Cognitive Modes</div>
<div class="meta-line">Authors: Tianyi Jiang, Arctanx An, Hengyi Feng, Naixin Zhai, Haodong Li, Xiaomin Yu, Jiahui Liu, Hanwen Du, Shuo Zhang, Zhi Yang, Jie Huang, Yuhua Li, Yongxin Ni, Huacan Wang, Ronghao Chen</div>
<div class="meta-line">First: 2026-02-10T18:31:47+00:00 · Latest: 2026-02-10T18:31:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10063v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10063v1">PDF</a> · <a href="https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset">Code1</a> · <a href="https://github.com/QuantaAlpha/chain-of-mindset">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human problem-solving is never the repetition of a single mindset, by which we mean a distinct mode of cognitive processing. When tackling a specific task, we do not rely on a single mindset; instead, we integrate multiple mindsets within the single solution process. However, existing LLM reasoning methods fall into a common trap: they apply the same fixed mindset across all steps, overlooking that different stages of solving the same problem require fundamentally different mindsets. This single-minded assumption prevents models from reaching the next level of intelligence. To address this limitation, we propose Chain of Mindset (CoM), a training-free agentic framework that enables step-level adaptive mindset orchestration. CoM decomposes reasoning into four functionally heterogeneous mindsets: Spatial, Convergent, Divergent, and Algorithmic. A Meta-Agent dynamically selects the optimal mindset based on the evolving reasoning state, while a bidirectional Context Gate filters cross-module information flow to maintain effectiveness and efficiency. Experiments across six challenging benchmarks spanning mathematics, code generation, scientific QA, and spatial reasoning demonstrate that CoM achieves state-of-the-art performance, outperforming the strongest baseline by 4.96\% and 4.72\% in overall accuracy on Qwen3-VL-32B-Instruct and Gemini-2.0-Flash, while balancing reasoning efficiency. Our code is publicly available at \href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>思维链：基于自适应认知模式的推理</div>
<div class="mono" style="margin-top:8px">人类解决问题的过程从不是单一思维模式的重复，这里的思维模式指的是不同的认知处理方式。在解决特定任务时，我们并非依赖单一思维模式，而是将多种思维模式整合到同一个解决方案中。然而，现有的LLM推理方法陷入了一个普遍的误区：它们在所有步骤中都使用相同的固定思维模式，忽视了同一问题的不同阶段需要根本不同的思维模式。这种单一假设阻碍了模型向更高层次智能的发展。为了解决这一局限性，我们提出了Chain of Mindset（CoM），一个无需训练的代理框架，能够实现步骤级的自适应思维模式协调。CoM将推理分解为四种功能上异质的思维模式：空间思维、收敛思维、发散思维和算法思维。一个元代理根据推理状态的演变动态选择最优的思维模式，而双向上下文门控机制则过滤跨模块的信息流，以保持推理的有效性和效率。我们在涵盖数学、代码生成、科学问答和空间推理的六个具有挑战性的基准测试中进行了实验，结果表明CoM在整体准确率上分别比最强基线模型高出4.96\%和4.72\%，同时保持推理效率。我们的代码已公开在\href{https://github.com/QuantaAlpha/chain-of-mindset}{https://github.com/QuantaAlpha/chain-of-mindset}。</div>
</details>
</div>
<div class="card">
<div class="title">Vendi Novelty Scores for Out-of-Distribution Detection</div>
<div class="meta-line">Authors: Amey P. Pasarkar, Adji Bousso Dieng</div>
<div class="meta-line">First: 2026-02-10T18:30:29+00:00 · Latest: 2026-02-10T18:30:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10062v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.10062v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Out-of-distribution (OOD) detection is critical for the safe deployment of machine learning systems. Existing post-hoc detectors typically rely on model confidence scores or likelihood estimates in feature space, often under restrictive distributional assumptions. In this work, we introduce a third paradigm and formulate OOD detection from a diversity perspective. We propose the Vendi Novelty Score (VNS), an OOD detector based on the Vendi Scores (VS), a family of similarity-based diversity metrics. VNS quantifies how much a test sample increases the VS of the in-distribution feature set, providing a principled notion of novelty that does not require density modeling. VNS is linear-time, non-parametric, and naturally combines class-conditional (local) and dataset-level (global) novelty signals. Across multiple image classification benchmarks and network architectures, VNS achieves state-of-the-art OOD detection performance. Remarkably, VNS retains this performance when computed using only 1% of the training data, enabling deployment in memory- or access-constrained settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于Vendi新颖度得分的分布外检测</div>
<div class="mono" style="margin-top:8px">分布外（OOD）检测对于机器学习系统的安全部署至关重要。现有的后验检测器通常依赖于特征空间中的模型置信度得分或似然估计，往往需要严格的分布假设。在本文中，我们引入了第三种范式，从多样性角度对OOD检测进行建模。我们提出了基于Vendi得分（VS）的Vendi新颖度得分（VNS），这是一种分布外检测器。VNS衡量测试样本如何增加分布内特征集的VS，提供了一种无需密度建模的新颖度概念。VNS具有线性时间复杂度，是非参数方法，并自然地结合了类别条件（局部）和数据集级（全局）的新颖度信号。在多个图像分类基准和网络架构上，VNS实现了最先进的OOD检测性能。值得注意的是，即使仅使用1%的训练数据计算VNS，其性能依然保持不变，从而使得其能够在内存或访问受限的环境中部署。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260210_0435.html">20260210_0435</a>
<a href="archive/20260209_0404.html">20260209_0404</a>
<a href="archive/20260208_0353.html">20260208_0353</a>
<a href="archive/20260207_0410.html">20260207_0410</a>
<a href="archive/20260206_0412.html">20260206_0412</a>
<a href="archive/20260205_0417.html">20260205_0417</a>
<a href="archive/20260204_0421.html">20260204_0421</a>
<a href="archive/20260202_0402.html">20260202_0402</a>
<a href="archive/20260201_0354.html">20260201_0354</a>
<a href="archive/20260131_0408.html">20260131_0408</a>
<a href="archive/20260130_0406.html">20260130_0406</a>
<a href="archive/20260129_0407.html">20260129_0407</a>
<a href="archive/20260128_0407.html">20260128_0407</a>
<a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
