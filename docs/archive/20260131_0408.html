<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-31 04:08</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260131_0408</div>
    <div class="row"><div class="card">
<div class="title">RedSage: A Cybersecurity Generalist LLM</div>
<div class="meta-line">Authors: Naufal Suryanto, Muzammal Naseer, Pengfei Li, Syed Talal Wasim, Jinhui Yi, Juergen Gall, Paolo Ceravolo, Ernesto Damiani</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-29T18:59:57+00:00 · Latest: 2026-01-29T18:59:57+00:00</div>
<div class="meta-line">Comments: Accepted on ICLR 2026; Project page: https://risys-lab.github.io/RedSage/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22159v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22159v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://risys-lab.github.io/RedSage/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cybersecurity operations demand assistant LLMs that support diverse workflows without exposing sensitive data. Existing solutions either rely on proprietary APIs with privacy risks or on open models lacking domain adaptation. To bridge this gap, we curate 11.8B tokens of cybersecurity-focused continual pretraining data via large-scale web filtering and manual collection of high-quality resources, spanning 28.6K documents across frameworks, offensive techniques, and security tools. Building on this, we design an agentic augmentation pipeline that simulates expert workflows to generate 266K multi-turn cybersecurity samples for supervised fine-tuning. Combined with general open-source LLM data, these resources enable the training of RedSage, an open-source, locally deployable cybersecurity assistant with domain-aware pretraining and post-training. To rigorously evaluate the models, we introduce RedSage-Bench, a benchmark with 30K multiple-choice and 240 open-ended Q&amp;A items covering cybersecurity knowledge, skills, and tool expertise. RedSage is further evaluated on established cybersecurity benchmarks (e.g., CTI-Bench, CyberMetric, SECURE) and general LLM benchmarks to assess broader generalization. At the 8B scale, RedSage achieves consistently better results, surpassing the baseline models by up to +5.59 points on cybersecurity benchmarks and +5.05 points on Open LLM Leaderboard tasks. These findings demonstrate that domain-aware agentic augmentation and pre/post-training can not only enhance cybersecurity-specific expertise but also help to improve general reasoning and instruction-following. All models, datasets, and code are publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RedSage：一个面向网络安全的通用型大语言模型</div>
<div class="mono" style="margin-top:8px">网络安全操作需要支持多样化工作流程且不暴露敏感数据的辅助大语言模型（LLM）。现有解决方案要么依赖于存在隐私风险的专有API，要么使用缺乏领域适配的开源模型。为弥合这一差距，我们通过大规模网络过滤和人工收集高质量资源，构建了包含118亿个网络安全相关持续预训练数据的语料库，涵盖28600份文档，涉及框架、攻击技术及安全工具。在此基础上，我们设计了一个代理增强流水线，模拟专家工作流程，生成266000个多轮网络安全样本用于监督微调。结合通用开源LLM数据，这些资源使得RedSage的训练成为可能，RedSage是一个开源、可本地部署的网络安全助手，具备领域感知的预训练和后训练能力。为严格评估模型性能，我们引入了RedSage-Bench基准测试，包含30000个多项选择题和240个开放式问答题，覆盖网络安全知识、技能和工具使用。RedSage还被评估在已有的网络安全基准（如CTI-Bench、CyberMetric、SECURE）和通用LLM基准上，以评估其更广泛的泛化能力。在80亿参数规模下，RedSage在网络安全基准测试中表现优于基线模型，最高提升达+5.59分，在Open LLM Leaderboard任务中最高提升达+5.05分。这些结果表明，领域感知的代理增强和预训练/后训练方法不仅能提升网络安全领域的专业知识，还能帮助提高通用推理和指令遵循能力。所有模型、数据集和代码均公开可用。</div>
</details>
</div>
<div class="card">
<div class="title">One-step Latent-free Image Generation with Pixel Mean Flows</div>
<div class="meta-line">Authors: Yiyang Lu, Susie Lu, Qiao Sun, Hanhong Zhao, Zhicheng Jiang, Xianbang Wang, Tianhong Li, Zhengyang Geng, Kaiming He</div>
<div class="meta-line">First: 2026-01-29T18:59:56+00:00 · Latest: 2026-01-29T18:59:56+00:00</div>
<div class="meta-line">Comments: Technical report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22158v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22158v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space. Recent advances have made encouraging progress on each aspect individually, paving the way toward one-step diffusion/flow without latents. In this work, we take a further step towards this goal and propose &quot;pixel MeanFlow&quot; (pMF). Our core guideline is to formulate the network output space and the loss space separately. The network target is designed to be on a presumed low-dimensional image manifold (i.e., x-prediction), while the loss is defined via MeanFlow in the velocity space. We introduce a simple transformation between the image manifold and the average velocity field. In experiments, pMF achieves strong results for one-step latent-free generation on ImageNet at 256x256 resolution (2.22 FID) and 512x512 resolution (2.48 FID), filling a key missing piece in this regime. We hope that our study will further advance the boundaries of diffusion/flow-based generative models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一步无潜在变量图像生成与像素均值流</div>
<div class="mono" style="margin-top:8px">现代用于图像生成的扩散/流模型通常具有两个核心特征：(i) 使用多步采样，(ii) 在潜在空间中操作。最近的进展在每个方面都取得了令人鼓舞的进展，为无需潜在变量的一步扩散/流模型铺平了道路。在本工作中，我们进一步朝着这一目标迈进，并提出了&quot;像素均值流&quot;(pMF)。我们的核心指导思想是将网络输出空间和损失空间分别进行建模。网络目标被设计在假设的低维图像流形上（即x预测），而损失则通过速度空间中的均值流进行定义。我们引入了一种简单的图像流形与平均速度场之间的转换方法。在实验中，pMF在ImageNet 256x256分辨率（2.22 FID）和512x512分辨率（2.48 FID）上实现了强大的一步无潜在变量生成结果，填补了该领域的一个关键空白。我们希望本研究能进一步推动基于扩散/流的生成模型的发展边界。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modern diffusion/flow-based models for image generation typically exhibit two core characteristics: (i) using multi-step sampling, and (ii) operating in a latent space.</div>
</details>
</div>
<div class="card">
<div class="title">Discovering Hidden Gems in Model Repositories</div>
<div class="meta-line">Authors: Jonathan Kahana, Eliahu Horwitz, Yedid Hoshen</div>
<div class="meta-line">First: 2026-01-29T18:59:55+00:00 · Latest: 2026-01-29T18:59:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22157v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22157v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Public repositories host millions of fine-tuned models, yet community usage remains disproportionately concentrated on a small number of foundation checkpoints. We investigate whether this concentration reflects efficient market selection or if superior models are systematically overlooked. Through an extensive evaluation of over 2,000 models, we show the prevalence of &quot;hidden gems&quot;, unpopular fine-tunes that significantly outperform their popular counterparts. Notably, within the Llama-3.1-8B family, we find rarely downloaded checkpoints that improve math performance from 83.2% to 96.0% without increasing inference costs. However, discovering these models through exhaustive evaluation of every uploaded model is computationally infeasible. We therefore formulate model discovery as a Multi-Armed Bandit problem and accelerate the Sequential Halving search algorithm by using shared query sets and aggressive elimination schedules. Our method retrieves top models with as few as 50 queries per candidate, accelerating discovery by over 50x.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>发现模型仓库中的隐藏瑰宝</div>
<div class="mono" style="margin-top:8px">公共仓库托管了数百万个微调模型，但社区使用却高度集中在少数基础检查点上。我们探讨这种集中现象是反映了有效的市场选择，还是优质模型被系统性地忽视。通过对超过2,000个模型的广泛评估，我们展示了&quot;隐藏瑰宝&quot;的普遍性，即那些不受欢迎但显著优于流行模型的微调版本。值得注意的是，在Llama-3.1-8B系列中，我们发现了一些下载量极少的检查点，其数学性能从83.2%提升至96.0%，而推理成本并未增加。然而，通过全面评估每个上传模型来发现这些模型在计算上是不可行的。因此，我们将模型发现问题建模为多臂老虎机问题，并通过使用共享查询集和激进的淘汰计划加速顺序削减搜索算法。我们的方法每候选模型仅需50次查询即可检索到顶级模型，使发现过程加速超过50倍。</div>
</details>
</div>
<div class="card">
<div class="title">Hybrid Linear Attention Done Right: Efficient Distillation and Effective Architectures for Extremely Long Contexts</div>
<div class="meta-line">Authors: Yingfa Chen, Zhen Leng Thai, Zihan Zhou, Zhu Zhang, Xingyu Shen, Shuo Wang, Chaojun Xiao, Xu Han, Zhiyuan Liu</div>
<div class="meta-line">First: 2026-01-29T18:59:53+00:00 · Latest: 2026-01-29T18:59:53+00:00</div>
<div class="meta-line">Comments: 20 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22156v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22156v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch. Some recent studies have shown that pre-trained softmax attention blocks can be converted into RNN blocks through parameter transfer and knowledge distillation. However, these transfer methods require substantial amounts of training data (more than 10B tokens), and the resulting hybrid models also exhibit poor long-context performance, which is the scenario where hybrid models enjoy significant inference speedups over Transformer-based models. In this paper, we present HALO (Hybrid Attention via Layer Optimization), a pipeline for distilling Transformer models into RNN-attention hybrid models. We then present HypeNet, a hybrid architecture with superior length generalization enabled by a novel position encoding scheme (named HyPE) and various architectural modifications. We convert the Qwen3 series into HypeNet using HALO, achieving performance comparable to the original Transformer models while enjoying superior long-context performance and efficiency. The conversion requires just 2.3B tokens, less than 0.01% of their pre-training data</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>混合线性注意力的正确实现：面向极长上下文的高效蒸馏与有效架构</div>
<div class="mono" style="margin-top:8px">混合Transformer架构通过结合softmax注意力块和循环神经网络(RNNs)，在长上下文建模中展现出理想的性能与吞吐量权衡，但其应用和研究受到从头开始大规模预训练的高昂成本限制。一些近期研究显示，可以通过参数迁移和知识蒸馏将预训练的softmax注意力块转换为RNN块。然而，这些转换方法需要大量的训练数据（超过100亿个token），且得到的混合模型在长上下文任务中表现不佳，而混合模型在推理速度上相对于基于Transformer的模型有显著提升。本文提出HALO（通过层优化实现的混合注意力），一种将Transformer模型蒸馏为RNN-注意力混合模型的流程。随后，我们提出了HypeNet，一种通过新颖的位置编码方案（称为HyPE）以及多种架构改进实现卓越长度泛化的混合架构。我们使用HALO将Qwen3系列转换为HypeNet，实现了与原始Transformer模型相当的性能，同时在长上下文任务中表现出更优的性能和效率。该转换仅需23亿个token，不到其预训练数据的0.01%</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Hybrid Transformer architectures, which combine softmax attention blocks and recurrent neural networks (RNNs), have shown a desirable performance-throughput tradeoff for long-context modeling, but their adoption and studies are hindered by the prohibitive cost of large-scale pre-training from scratch.</div>
</details>
</div>
<div class="card">
<div class="title">UEval: A Benchmark for Unified Multimodal Generation</div>
<div class="meta-line">Authors: Bo Li, Yida Yin, Wenhao Chai, Xingyu Fu, Zhuang Liu</div>
<div class="meta-line">First: 2026-01-29T18:59:52+00:00 · Latest: 2026-01-29T18:59:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22155v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22155v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce UEval, a benchmark to evaluate unified models, i.e., models capable of generating both images and text. UEval comprises 1,000 expert-curated questions that require both images and text in the model output, sourced from 8 real-world tasks. Our curated questions cover a wide range of reasoning types, from step-by-step guides to textbook explanations. Evaluating open-ended multimodal generation is non-trivial, as simple LLM-as-a-judge methods can miss the subtleties. Different from previous works that rely on multimodal Large Language Models (MLLMs) to rate image quality or text accuracy, we design a rubric-based scoring system in UEval. For each question, reference images and text answers are provided to a MLLM to generate an initial rubric, consisting of multiple evaluation criteria, and human experts then refine and validate these rubrics. In total, UEval contains 10,417 validated rubric criteria, enabling scalable and fine-grained automatic scoring. UEval is challenging for current unified models: GPT-5-Thinking scores only 66.4 out of 100, while the best open-source model reaches merely 49.1. We observe that reasoning models often outperform non-reasoning ones, and transferring reasoning traces from a reasoning model to a non-reasoning model significantly narrows the gap. This suggests that reasoning may be important for tasks requiring complex multimodal understanding and generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UEval：统一多模态生成的基准</div>
<div class="mono" style="margin-top:8px">我们引入UEval，这是一个用于评估统一模型的基准，即能够生成图像和文本的模型。UEval包含1,000个由专家整理的问题，这些问题需要模型输出图像和文本，来源于8个现实任务。我们整理的问题涵盖了多种推理类型，从逐步指南到教科书解释。评估开放式的多模态生成具有挑战性，因为简单的LLM作为评判者的方法可能忽略细节。与之前依赖多模态大语言模型（MLLMs）来评估图像质量或文本准确性的研究不同，我们在UEval中设计了一个基于评分标准的评分系统。对于每个问题，将参考图像和文本答案提供给MLLM生成初始评分标准，然后由人类专家进行细化和验证。总体而言，UEval包含10,417个经过验证的评分标准，从而实现了可扩展且细致的自动评分。UEval对当前的统一模型具有挑战性：GPT-5-Thinking的得分仅为66.4分，而最好的开源模型仅达到49.1分。我们观察到，推理模型通常优于非推理模型，将推理轨迹从推理模型转移到非推理模型可以显著缩小差距。这表明推理可能对需要复杂多模态理解和生成的任务至关重要。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring Reasoning Reward Model for Agents</div>
<div class="meta-line">Authors: Kaixuan Fan, Kaituo Feng, Manyuan Zhang, Tianshuo Peng, Zhixun Li, Yilei Jiang, Shuang Chen, Peng Pei, Xunliang Cai, Xiangyu Yue</div>
<div class="meta-line">First: 2026-01-29T18:59:52+00:00 · Latest: 2026-01-29T18:59:52+00:00</div>
<div class="meta-line">Comments: Project page: https://github.com/kxfan2002/Reagent</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22154v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22154v1">PDF</a> · <a href="https://github.com/kxfan2002/Reagent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Agentic Reinforcement Learning (Agentic RL) has achieved notable success in enabling agents to perform complex reasoning and tool use. However, most methods still relies on sparse outcome-based reward for training. Such feedback fails to differentiate intermediate reasoning quality, leading to suboptimal training results. In this paper, we introduce Agent Reasoning Reward Model (Agent-RRM), a multi-faceted reward model that produces structured feedback for agentic trajectories, including (1) an explicit reasoning trace , (2) a focused critique that provides refinement guidance by highlighting reasoning flaws, and (3) an overall score that evaluates process performance. Leveraging these signals, we systematically investigate three integration strategies: Reagent-C (text-augmented refinement), Reagent-R (reward-augmented guidance), and Reagent-U (unified feedback integration). Extensive evaluations across 12 diverse benchmarks demonstrate that Reagent-U yields substantial performance leaps, achieving 43.7% on GAIA and 46.2% on WebWalkerQA, validating the effectiveness of our reasoning reward model and training schemes. Code, models, and datasets are all released to facilitate future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索智能体的推理奖励模型</div>
<div class="mono" style="margin-top:8px">代理强化学习（Agentic RL）在使智能体执行复杂推理和工具使用方面取得了显著成功。然而，大多数方法仍依赖于稀疏的基于结果的奖励进行训练。此类反馈无法区分中间推理质量，导致训练效果不佳。本文引入了代理推理奖励模型（Agent-RRM），这是一种多维度的奖励模型，为代理轨迹提供结构化反馈，包括（1）显式的推理轨迹，（2）聚焦的批评，通过突出推理缺陷提供优化指导，以及（3）总体评分，评估整个过程的性能。利用这些信号，我们系统地研究了三种集成策略：Reagent-C（文本增强的优化）、Reagent-R（奖励增强的指导）和Reagent-U（统一反馈集成）。在12个多样化的基准测试中广泛评估表明，Reagent-U实现了显著的性能提升，在GAIA和WebWalkerQA上分别达到43.7%和46.2%，验证了我们推理奖励模型和训练方案的有效性。代码、模型和数据集均已发布，以促进未来研究。</div>
</details>
</div>
<div class="card">
<div class="title">DynamicVLA: A Vision-Language-Action Model for Dynamic Object Manipulation</div>
<div class="meta-line">Authors: Haozhe Xie, Beichen Wen, Jiarui Zheng, Zhaoxi Chen, Fangzhou Hong, Haiwen Diao, Ziwei Liu</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2026-01-29T18:59:51+00:00 · Latest: 2026-01-29T18:59:51+00:00</div>
<div class="meta-line">Comments: Project Page: https://www.infinitescript.com/project/dynamic-vla/ GitHub: https://github.com/hzxie/DynamicVLA</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22153v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22153v1">PDF</a> · <a href="https://github.com/hzxie/DynamicVLA">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://www.infinitescript.com/project/dynamic-vla/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Manipulating dynamic objects remains an open challenge for Vision-Language-Action (VLA) models, which, despite strong generalization in static manipulation, struggle in dynamic scenarios requiring rapid perception, temporal anticipation, and continuous control. We present DynamicVLA, a framework for dynamic object manipulation that integrates temporal reasoning and closed-loop adaptation through three key designs: 1) a compact 0.4B VLA using a convolutional vision encoder for spatially efficient, structurally faithful encoding, enabling fast multimodal inference; 2) Continuous Inference, enabling overlapping reasoning and execution for lower latency and timely adaptation to object motion; and 3) Latent-aware Action Streaming, which bridges the perception-execution gap by enforcing temporally aligned action execution. To fill the missing foundation of dynamic manipulation data, we introduce the Dynamic Object Manipulation (DOM) benchmark, built from scratch with an auto data collection pipeline that efficiently gathers 200K synthetic episodes across 2.8K scenes and 206 objects, and enables fast collection of 2K real-world episodes without teleoperation. Extensive evaluations demonstrate remarkable improvements in response speed, perception, and generalization, positioning DynamicVLA as a unified framework for general dynamic object manipulation across embodiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DynamicVLA：一种用于动态物体操作的视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">动态物体操作仍然是视觉-语言-动作（VLA）模型的一个开放性挑战，尽管这些模型在静态操作中表现出强大的泛化能力，但在需要快速感知、时间预测和连续控制的动态场景中却表现不佳。我们提出了DynamicVLA，这是一个用于动态物体操作的框架，通过三个关键设计整合了时间推理和闭环适应：1）使用卷积视觉编码器的紧凑型0.4B VLA模型，实现空间高效且结构忠实的编码，从而支持快速多模态推理；2）连续推理，实现推理与执行的重叠，以降低延迟并及时适应物体运动；3）潜在感知动作流，通过强制时间对齐的动作执行来弥合感知与执行之间的差距。为了填补动态操作数据的基础空白，我们引入了动态物体操作（DOM）基准测试，该基准测试通过自研的数据收集管道，高效地收集了2.8K场景和206个物体的20万合成操作序列，并支持无需远程操作即可快速收集2000个真实世界操作序列。广泛评估表明，DynamicVLA在响应速度、感知能力和泛化能力方面均取得了显著提升，使其成为一种统一的框架，适用于各种具身的动态物体操作。</div>
</details>
</div>
<div class="card">
<div class="title">Late Breaking Results: Conversion of Neural Networks into Logic Flows for Edge Computing</div>
<div class="meta-line">Authors: Daniel Stein, Shaoyi Huang, Rolf Drechsler, Bing Li, Grace Li Zhang</div>
<div class="meta-line">First: 2026-01-29T18:59:50+00:00 · Latest: 2026-01-29T18:59:50+00:00</div>
<div class="meta-line">Comments: accepted by DATE2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22151v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22151v1">PDF</a> · <a href="https://github.com/TUDa-HWAI/NN2Logic">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural networks have been successfully applied in various resource-constrained edge devices, where usually central processing units (CPUs) instead of graphics processing units exist due to limited power availability. State-of-the-art research still focuses on efficiently executing enormous numbers of multiply-accumulate (MAC) operations. However, CPUs themselves are not good at executing such mathematical operations on a large scale, since they are more suited to execute control flow logic, i.e., computer algorithms. To enhance the computation efficiency of neural networks on CPUs, in this paper, we propose to convert them into logic flows for execution. Specifically, neural networks are first converted into equivalent decision trees, from which decision paths with constant leaves are then selected and compressed into logic flows. Such logic flows consist of if and else structures and a reduced number of MAC operations. Experimental results demonstrate that the latency can be reduced by up to 14.9 % on a simulated RISC-V CPU without any accuracy degradation.
  The code is open source at https://github.com/TUDa-HWAI/NN2Logic</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>最新突破结果：将神经网络转换为逻辑流以用于边缘计算</div>
<div class="mono" style="margin-top:8px">神经网络已在各种资源受限的边缘设备中成功应用，通常由于电力供应有限，这些设备中存在中央处理器（CPU）而非图形处理器。当前最先进的研究仍集中在高效执行大量乘加（MAC）操作上。然而，CPU本身并不擅长大规模执行此类数学运算，因为它们更适合执行控制流逻辑，即计算机算法。为了提高神经网络在CPU上的计算效率，本文提出将其转换为逻辑流进行执行。具体而言，首先将神经网络转换为等效的决策树，然后从中选择并压缩具有常量叶节点的决策路径，形成逻辑流。这些逻辑流由if和else结构以及较少的MAC操作组成。实验结果表明，在模拟的RISC-V CPU上，延迟可减少高达14.9%，且不降低任何精度。</div>
</details>
</div>
<div class="card">
<div class="title">Do VLMs Perceive or Recall? Probing Visual Perception vs. Memory with Classic Visual Illusions</div>
<div class="meta-line">Authors: Xiaoxiao Sun, Mingyang Li, Kun yuan, Min Woo Sun, Mark Endo, Shengguang Wu, Changlin Li, Yuhui Zhang, Zeyu Wang, Serena Yeung-Levy</div>
<div class="meta-line">First: 2026-01-29T18:59:24+00:00 · Latest: 2026-01-29T18:59:24+00:00</div>
<div class="meta-line">Comments: 26 pages, 31 figures, 13 tables. Project Page: https://sites.google.com/view/vi-probe/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22150v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22150v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/vi-probe/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Vision-Language Models (VLMs) often answer classic visual illusions &quot;correctly&quot; on original images, yet persist with the same responses when illusion factors are inverted, even though the visual change is obvious to humans. This raises a fundamental question: do VLMs perceive visual changes or merely recall memorized patterns? While several studies have noted this phenomenon, the underlying causes remain unclear. To move from observations to systematic understanding, this paper introduces VI-Probe, a controllable visual-illusion framework with graded perturbations and matched visual controls (without illusion inducer) that disentangles visually grounded perception from language-driven recall. Unlike prior work that focuses on averaged accuracy, we measure stability and sensitivity using Polarity-Flip Consistency, Template Fixation Index, and an illusion multiplier normalized against matched controls. Experiments across different families reveal that response persistence arises from heterogeneous causes rather than a single mechanism. For instance, GPT-5 exhibits memory override, Claude-Opus-4.1 shows perception-memory competition, while Qwen variants suggest visual-processing limits. Our findings challenge single-cause views and motivate probing-based evaluation that measures both knowledge and sensitivity to controlled visual change. Data and code are available at https://sites.google.com/view/vi-probe/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型是感知还是回忆？借助经典视觉错觉探究视觉感知与记忆</div>
<div class="mono" style="margin-top:8px">大型视觉语言模型（VLMs）通常在原始图像上能正确回答经典视觉错觉问题，但在错觉因素被反转时仍保持相同的回答，尽管这种视觉变化对人类来说是明显的。这引发了一个根本性问题：VLMs 是感知到了视觉变化，还是仅仅回忆了已存储的模式？虽然已有多个研究注意到这一现象，但其根本原因仍不清楚。为从观察走向系统性理解，本文引入了 VI-Probe，一个可控的视觉错觉框架，包含分级扰动和匹配的视觉对照条件（不含错觉诱导器），以区分基于视觉的感知与语言驱动的记忆。与以往侧重平均准确率的研究不同，我们使用极性翻转一致性、模板固定指数以及归一化于匹配对照的错觉乘数来衡量模型的稳定性与敏感性。在不同错觉家族的实验中发现，回答的持续性来源于异质性原因，而非单一机制。例如，GPT-5表现出记忆覆盖，Claude-Opus-4.1则显示感知与记忆之间的竞争，而Qwen的变体则暗示了视觉处理的局限。我们的发现挑战了单一原因的解释，并推动了基于探测的评估方法，该方法同时衡量知识和对受控视觉变化的敏感性。数据和代码可在 https://sites.google.com/view/vi-probe/ 获取。</div>
</details>
</div>
<div class="card">
<div class="title">DynaWeb: Model-Based Reinforcement Learning of Web Agents</div>
<div class="meta-line">Authors: Hang Ding, Peidong Liu, Junqiao Wang, Ziwei Ji, Meng Cao, Rongzhao Zhang, Lynn Ai, Eric Yang, Tianyu Shi, Lei Yu</div>
<div class="meta-line">First: 2026-01-29T18:59:07+00:00 · Latest: 2026-01-29T18:59:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22149v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22149v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The development of autonomous web agents, powered by Large Language Models (LLMs) and reinforcement learning (RL), represents a significant step towards general-purpose AI assistants. However, training these agents is severely hampered by the challenges of interacting with the live internet, which is inefficient, costly, and fraught with risks. Model-based reinforcement learning (MBRL) offers a promising solution by learning a world model of the environment to enable simulated interaction. This paper introduces DynaWeb, a novel MBRL framework that trains web agents through interacting with a web world model trained to predict naturalistic web page representations given agent actions. This model serves as a synthetic web environment where an agent policy can dream by generating vast quantities of rollout action trajectories for efficient online reinforcement learning. Beyond free policy rollouts, DynaWeb incorporates real expert trajectories from training data, which are randomly interleaved with on-policy rollouts during training to improve stability and sample efficiency. Experiments conducted on the challenging WebArena and WebVoyager benchmarks demonstrate that DynaWeb consistently and significantly improves the performance of state-of-the-art open-source web agent models. Our findings establish the viability of training web agents through imagination, offering a scalable and efficient way to scale up online agentic RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DynaWeb: 基于模型的网络代理强化学习</div>
<div class="mono" style="margin-top:8px">自主网络代理的发展，借助大型语言模型（LLMs）和强化学习（RL），是通向通用AI助手的重要一步。然而，这些代理的训练受到与真实互联网交互的挑战严重阻碍，这种交互效率低下、成本高昂且充满风险。基于模型的强化学习（MBRL）通过学习环境的模型以实现模拟交互，提供了一个有前景的解决方案。本文介绍了DynaWeb，一种新颖的MBRL框架，它通过与一个网络世界模型交互来训练网络代理，该模型旨在根据代理的行为预测自然的网页表示。该模型充当一个合成的网络环境，使代理策略能够通过生成大量 rollout 行动轨迹来实现高效的在线强化学习。除了自由策略 rollout，DynaWeb 还结合了来自训练数据的真实专家轨迹，在训练过程中随机交织在 on-policy rollout 中，以提高稳定性与样本效率。在具有挑战性的 WebArena 和 WebVoyager 基准上的实验表明，DynaWeb 一致且显著地提升了当前最先进的开源网络代理模型的性能。我们的研究结果确立了通过想象训练网络代理的可行性，提供了一种可扩展且高效的方法来提升在线代理强化学习。</div>
</details>
</div>
<div class="card">
<div class="title">FineInstructions: Scaling Synthetic Instructions to Pre-Training Scale</div>
<div class="meta-line">Authors: Ajay Patel, Colin Raffel, Chris Callison-Burch</div>
<div class="meta-line">First: 2026-01-29T18:58:47+00:00 · Latest: 2026-01-29T18:58:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22146v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22146v1">PDF</a> · <a href="https://huggingface.co/fineinstructions">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Due to limited supervised training data, large language models (LLMs) are typically pre-trained via a self-supervised &quot;predict the next word&quot; objective on a vast amount of unstructured text data. To make the resulting model useful to users, it is further trained on a far smaller amount of &quot;instruction-tuning&quot; data comprised of supervised training examples of instructions and responses. To overcome the limited amount of supervised data, we propose a procedure that can transform the knowledge in internet-scale pre-training documents into billions of synthetic instruction and answer training pairs. The resulting dataset, called FineInstructions, uses ~18M instruction templates created from real user-written queries and prompts. These instruction templates are matched to and instantiated with human-written source documents from unstructured pre-training corpora. With &quot;supervised&quot; synthetic training data generated at this scale, an LLM can be pre-trained from scratch solely with the instruction-tuning objective, which is far more in-distribution with the expected downstream usage of LLMs (responding to user prompts). We conduct controlled token-for-token training experiments and find pre-training on FineInstructions outperforms standard pre-training and other proposed synthetic pre-training techniques on standard benchmarks measuring free-form response quality. Our resources can be found at https://huggingface.co/fineinstructions .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FineInstructions：将合成指令扩展到预训练规模</div>
<div class="mono" style="margin-top:8px">由于监督训练数据有限，大型语言模型（LLMs）通常通过自监督的&quot;预测下一个词&quot;目标，在大量非结构化文本数据上进行预训练。为了使最终模型对用户有用，它还会在由指令和响应的监督训练示例组成的较小数据集上进行进一步训练。为了解决监督数据有限的问题，我们提出了一种方法，可以将互联网规模预训练文档中的知识转化为数十亿个合成指令与答案的训练对。所生成的数据集称为FineInstructions，使用了约1800万个从真实用户查询和提示中创建的指令模板。这些指令模板与来自非结构化预训练语料库的人类编写源文档进行匹配并实例化。通过这种规模的&quot;监督&quot;合成训练数据，LLM可以仅使用指令调优目标从头开始进行预训练，这与预期的下游使用场景（响应用户提示）更为匹配。我们进行了受控的逐词训练实验，并发现基于FineInstructions的预训练在标准基准测试中表现优于标准预训练和其他提出的合成预训练技术。我们的资源可在https://huggingface.co/fineinstructions获取。</div>
</details>
</div>
<div class="card">
<div class="title">MORPH: PDE Foundation Models with Arbitrary Data Modality</div>
<div class="meta-line">Authors: Mahindra Singh Rautela, Alexander Most, Siddharth Mansingh, Bradley C. Love, Alexander Scheinker, Diane Oyen, Nathan Debardeleben, Earl Lawrence, Ayan Biswas</div>
<div class="meta-line">First: 2025-09-25T22:38:36+00:00 · Latest: 2026-01-29T18:57:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21670v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.21670v4">PDF</a> · <a href="https://github.com/lanl/MORPH">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce MORPH, a modality-agnostic, autoregressive foundation model for partial differential equations (PDEs). MORPH is built on a convolutional vision transformer backbone that seamlessly handles heterogeneous spatiotemporal datasets of varying data modality (1D--3D) at different resolutions, and multiple fields with mixed scalar and vector components. The architecture combines (i) component-wise convolution, which jointly processes scalar and vector channels to capture local interactions, (ii) inter-field cross-attention, which models and selectively propagates information between different physical fields, (iii) axial attentions, which factorize full spatiotemporal self-attention along individual spatial and temporal axes to reduce computational burden while retaining expressivity. We pretrain multiple model variants on a diverse collection of heterogeneous PDE datasets and evaluate transfer to a range of downstream prediction tasks. Using both full-model fine-tuning and parameter-efficient low-rank adapters, MORPH outperforms models trained from scratch. Across extensive evaluations, MORPH matches or surpasses strong baselines and recent state-of-the-art models. Collectively, these capabilities present a flexible and powerful backbone for learning from the heterogeneous and multimodal nature of scientific observations, charting a path toward scalable and data-efficient scientific machine learning. The source code, datasets, and models are publicly available at https://github.com/lanl/MORPH.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MORPH：支持任意数据模态的PDE基础模型</div>
<div class="mono" style="margin-top:8px">我们引入了MORPH，这是一种模态无关的、自回归的偏微分方程（PDE）基础模型。MORPH基于卷积视觉Transformer主干构建，能够无缝处理不同分辨率、不同数据模态（1D--3D）以及包含混合标量和向量成分的多领域异构时空数据集。其架构结合了以下三个部分：(i) 分量卷积，联合处理标量和向量通道以捕捉局部交互；(ii) 跨领域交叉注意力，建模并选择性地在不同物理领域间传播信息；(iii) 轴向注意力，将完整的时空自注意力分解为独立的空间和时间轴，以减少计算负担同时保持表达能力。我们在多样化的异构PDE数据集上预训练了多个模型变体，并评估其在多种下游预测任务中的迁移能力。通过全模型微调和参数高效的低秩适配器，MORPH在性能上优于从头训练的模型。在广泛的评估中，MORPH表现与强基线模型和近期最先进的模型相当甚至更优。总体而言，这些能力为从科学观测的异构和多模态特性中学习提供了灵活且强大的主干结构，为可扩展且数据高效的科学机器学习开辟了道路。源代码、数据集和模型可在https://github.com/lanl/MORPH上公开获取。</div>
</details>
</div>
<div class="card">
<div class="title">JUST-DUB-IT: Video Dubbing via Joint Audio-Visual Diffusion</div>
<div class="meta-line">Authors: Anthony Chen, Naomi Ken Korem, Tavi Halperin, Matan Ben Yosef, Urska Jelercic, Ofir Bibi, Or Patashnik, Daniel Cohen-Or</div>
<div class="meta-line">First: 2026-01-29T18:57:13+00:00 · Latest: 2026-01-29T18:57:13+00:00</div>
<div class="meta-line">Comments: Project webpage available at https://justdubit.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22143v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22143v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://justdubit.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks. Among these tasks, video dubbing could greatly benefit from such priors, yet most existing solutions still rely on complex, task-specific pipelines that struggle in real-world settings. In this work, we introduce a single-model approach that adapts a foundational audio-video diffusion model for video-to-video dubbing via a lightweight LoRA. The LoRA enables the model to condition on an input audio-video while jointly generating translated audio and synchronized facial motion. To train this LoRA, we leverage the generative model itself to synthesize paired multilingual videos of the same speaker. Specifically, we generate multilingual videos with language switches within a single clip, and then inpaint the face and audio in each half to match the language of the other half. By leveraging the rich generative prior of the audio-visual model, our approach preserves speaker identity and lip synchronization while remaining robust to complex motion and real-world dynamics. We demonstrate that our approach produces high-quality dubbed videos with improved visual fidelity, lip synchronization, and robustness compared to existing dubbing pipelines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>JUST-DUB-IT：通过联合音频-视觉扩散进行视频配音</div>
<div class="mono" style="margin-top:8px">预训练用于联合生成声音和视觉内容的音频-视觉基础模型，最近展现出前所未有的多模态生成和编辑能力，为下游任务带来了新的机遇。在这些任务中，视频配音可以显著受益于这些先验知识，但大多数现有解决方案仍依赖复杂的、特定任务的流程，在现实场景中表现不佳。在本工作中，我们引入了一种单模型方法，通过轻量级LoRA适配基础音频-视频扩散模型，实现视频到视频的配音。LoRA使模型能够根据输入音频-视频，联合生成翻译后的音频和同步的面部动作。为了训练该LoRA，我们利用生成模型本身合成同一说话人的多语言视频对。具体而言，我们在一个视频片段中生成多语言视频并包含语言切换，然后对每个片段的前半部分进行面部和音频的修复，使其与后半部分的语言匹配。通过利用音频-视觉模型丰富的生成先验知识，我们的方法在保持说话人身份和唇同步的同时，对复杂动作和现实动态具有鲁棒性。我们证明，与现有配音流程相比，我们的方法能够生成高质量的配音视频，具有更高的视觉保真度、唇同步效果和鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Audio-Visual Foundation Models, which are pretrained to jointly generate sound and visual content, have recently shown an unprecedented ability to model multi-modal generation and editing, opening new opportunities for downstream tasks.</div>
</details>
</div>
<div class="card">
<div class="title">Routing the Lottery: Adaptive Subnetworks for Heterogeneous Data</div>
<div class="meta-line">Authors: Grzegorz Stefanski, Alberto Presta, Michal Byra</div>
<div class="meta-line">First: 2026-01-29T18:56:41+00:00 · Latest: 2026-01-29T18:56:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22141v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22141v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In pruning, the Lottery Ticket Hypothesis posits that large networks contain sparse subnetworks, or winning tickets, that can be trained in isolation to match the performance of their dense counterparts. However, most existing approaches assume a single universal winning ticket shared across all inputs, ignoring the inherent heterogeneity of real-world data. In this work, we propose Routing the Lottery (RTL), an adaptive pruning framework that discovers multiple specialized subnetworks, called adaptive tickets, each tailored to a class, semantic cluster, or environmental condition. Across diverse datasets and tasks, RTL consistently outperforms single- and multi-model baselines in balanced accuracy and recall, while using up to 10 times fewer parameters than independent models and exhibiting semantically aligned. Furthermore, we identify subnetwork collapse, a performance drop under aggressive pruning, and introduce a subnetwork similarity score that enables label-free diagnosis of oversparsification. Overall, our results recast pruning as a mechanism for aligning model structure with data heterogeneity, paving the way toward more modular and context-aware deep learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>彩票的路由：用于异构数据的自适应子网络</div>
<div class="mono" style="margin-top:8px">在剪枝领域，彩票假设认为大型网络包含稀疏子网络，即所谓的胜出彩票，这些子网络可以独立训练以达到与密集网络相当的性能。然而，大多数现有方法假设所有输入共享一个单一的通用胜出彩票，忽略了现实数据的固有异质性。在本文中，我们提出了一种自适应剪枝框架，称为彩票路由（RTL），该框架发现多个专门化的子网络，称为自适应彩票，每个子网络都针对某一类别、语义聚类或环境条件进行定制。在多种数据集和任务上，RTL在平衡准确率和召回率方面始终优于单模型和多模型基线，同时使用的参数数量比独立模型少多达10倍，并且表现出语义对齐。此外，我们识别出子网络崩溃现象，即在激进剪枝下性能下降，并引入了一个子网络相似性得分，使得无需标签即可诊断过度稀疏化问题。总体而言，我们的结果将剪枝重新定义为一种使模型结构与数据异质性对齐的机制，为更加模块化和情境感知的深度学习铺平了道路。</div>
</details>
</div>
<div class="card">
<div class="title">Reasoning While Asking: Transforming Reasoning Large Language Models from Passive Solvers to Proactive Inquirers</div>
<div class="meta-line">Authors: Xin Chen, Feng Jiang, Yiqian Zhang, Hardy Chen, Shuo Yan, Wenya Xie, Min Yang, Shujian Huang</div>
<div class="meta-line">First: 2026-01-29T18:56:12+00:00 · Latest: 2026-01-29T18:56:12+00:00</div>
<div class="meta-line">Comments: The manuscript is under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22139v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22139v1">PDF</a> · <a href="https://github.com/SUAT-AIRI/Proactive-Interactive-R1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous. We propose Proactive Interactive Reasoning (PIR), a new reasoning paradigm that transforms LLMs from passive solvers into proactive inquirers that interleave reasoning with clarification. Unlike existing search- or tool-based frameworks that primarily address knowledge uncertainty by querying external environments, PIR targets premise- and intent-level uncertainty through direct interaction with the user. PIR is implemented via two core components: (1) an uncertainty-aware supervised fine-tuning procedure that equips models with interactive reasoning capability, and (2) a user-simulator-based policy optimization framework driven by a composite reward that aligns model behavior with user intent. Extensive experiments on mathematical reasoning, code generation, and document editing demonstrate that PIR consistently outperforms strong baselines, achieving up to 32.70\% higher accuracy, 22.90\% higher pass rate, and 41.36 BLEU improvement, while reducing nearly half of the reasoning computation and unnecessary interaction turns. Further reliability evaluations on factual knowledge, question answering, and missing-premise scenarios confirm the strong generalization and robustness of PIR. Model and code are publicly available at: \href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>边提问边推理：将推理大语言模型从被动求解者转变为积极提问者</div>
<div class="mono" style="margin-top:8px">面向推理的大语言模型（LLMs）通过思维链（Chain-of-Thought, CoT）提示已取得显著进展，但它们仍受到一种\emph{盲目自我推理}范式的根本限制：即使关键信息缺失或模糊，也会进行大量内部推理。我们提出了一种新的推理范式——积极交互推理（Proactive Interactive Reasoning, PIR），将LLMs从被动求解者转变为积极提问者，通过推理与澄清的交替进行来提升推理效果。与现有主要通过查询外部环境来解决知识不确定性的搜索或工具框架不同，PIR通过直接与用户交互来解决前提和意图层面的不确定性。PIR通过两个核心组件实现：（1）一种不确定性感知的监督微调过程，使模型具备交互推理能力；（2）一种基于用户模拟器的策略优化框架，由复合奖励驱动，使模型行为与用户意图保持一致。在数学推理、代码生成和文档编辑等广泛实验中，PIR在准确率、通过率和BLEU指标上均显著优于强基线模型，分别提升了32.70\%、22.90\%和41.36\%，同时减少了近一半的推理计算量和不必要的交互轮次。在事实性知识、问答和前提缺失场景上的进一步可靠性评估也验证了PIR的强泛化能力和鲁棒性。模型和代码已公开在：\href{https://github.com/SUAT-AIRI/Proactive-Interactive-R1}</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Reasoning-oriented Large Language Models (LLMs) have achieved remarkable progress with Chain-of-Thought (CoT) prompting, yet they remain fundamentally limited by a \emph{blind self-thinking} paradigm: performing extensive internal reasoning even when critical information is missing or ambiguous.</div>
</details>
</div>
<div class="card">
<div class="title">&quot;Not in My Backyard&quot;: LLMs Uncover Online and Offline Social Biases Against Homelessness</div>
<div class="meta-line">Authors: Jonathan A. Karr, Benjamin F. Herbst, Matthew L. Sisk, Xueyun Li, Ting Hua, Matthew Hauenstein, Georgina Curto, Nitesh V. Chawla</div>
<div class="meta-line">First: 2025-08-14T17:58:34+00:00 · Latest: 2026-01-29T18:55:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.13187v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.13187v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Homelessness is a persistent social challenge, impacting millions worldwide. Over 876,000 people experienced homelessness (PEH) in the U.S. in 2025. Social bias is a significant barrier to alleviation, shaping public perception and influencing policymaking. Given that online textual media and offline city council discourse reflect and influence part of public opinion, it provides valuable insights to identify and track social biases against PEH. We present a new, manually-annotated multi-domain dataset compiled from Reddit, X (formerly Twitter), news articles, and city council meeting minutes across ten U.S. cities. Our 16-category multi-label taxonomy creates a challenging long-tail classification problem: some categories appear in less than 1% of samples, while others exceed 70%. We find that small human-annotated datasets (1,702 samples) are insufficient for training effective classifiers, whether used to fine-tune encoder models or as few-shot examples for LLMs. To address this, we use GPT-4.1 to generate pseudo-labels on a larger unlabeled corpus. Training on this expanded dataset enables even small encoder models (ModernBERT, 150M parameters) to achieve 35.23 macro-F1, approaching GPT-4.1&#x27;s 41.57. This demonstrates that \textbf{data quantity matters more than model size}, enabling low-cost, privacy-preserving deployment without relying on commercial APIs. Our results reveal that negative bias against PEH is prevalent both offline and online (especially on Reddit), with &quot;not in my backyard&quot; narratives showing the highest engagement. These findings uncover a type of ostracism that directly impacts poverty-reduction policymaking and provide actionable insights for practitioners addressing homelessness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>&quot;Not in My Backyard&quot;: LLMs Uncover Online and Offline Social Biases Against Homelessness</div>
<div class="mono" style="margin-top:8px">无家可归是持续的社会挑战，影响着全球数百万人。2025年，美国有超过876,000人经历了无家可归（PEH）。社会偏见是缓解这一问题的重要障碍，影响公众认知并塑造政策制定。鉴于在线文本媒体和线下城市委员会讨论反映了并影响了部分公众意见，识别和追踪针对PEH的社会偏见具有重要价值。我们提出了一种新的、人工标注的多领域数据集，该数据集从Reddit、X（原Twitter）、新闻文章和十个美国城市的委员会会议记录中收集。我们的16类多标签分类体系构成了一个具有挑战性的长尾分类问题：某些类别在样本中出现频率低于1%，而其他类别则超过70%。我们发现，小型人工标注数据集（1,702个样本）不足以训练有效的分类器，无论用于微调编码器模型还是作为LLMs的少样本示例。为了解决这一问题，我们使用GPT-4.1在更大的未标注语料库上生成伪标签。在该扩展数据集上训练，即使是小型编码器模型（ModernBERT，1.5亿参数）也能达到35.23的宏F1分数，接近GPT-4.1的41.57。这表明\textbf{数据量比模型大小更重要}，使得低成本、隐私保护的部署成为可能，而无需依赖商业API。我们的结果揭示了针对PEH的负面偏见在在线和线下都普遍存在（尤其是在Reddit上），其中&quot;Not in My Backyard&quot;（别在我后院）的叙事获得了最高参与度。这些发现揭示了一种直接阻碍减贫政策制定的排斥现象，并为解决无家可归问题的实践者提供了可操作的见解。</div>
</details>
</div>
<div class="card">
<div class="title">PRISM: Distribution-free Adaptive Computation of Matrix Functions for Accelerating Neural Network Training</div>
<div class="meta-line">Authors: Shenghao Yang, Zhichao Wang, Oleg Balabanov, N. Benjamin Erichson, Michael W. Mahoney</div>
<div class="meta-line">First: 2026-01-29T18:55:46+00:00 · Latest: 2026-01-29T18:55:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22137v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22137v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Matrix functions such as square root, inverse roots, and orthogonalization play a central role in preconditioned gradient methods for neural network training. This has motivated the development of iterative algorithms that avoid explicit eigendecompositions and rely primarily on matrix multiplications, making them well suited for modern GPU accelerators. We present PRISM (Polynomial-fitting and Randomized Iterative Sketching for Matrix functions computation), a general framework for accelerating iterative algorithms for computing matrix functions. PRISM combines adaptive polynomial approximation with randomized sketching: at each iteration, it fits a polynomial surrogate to the current spectrum via a sketched least-squares problem, adapting to the instance at hand with minimal overhead. We apply PRISM to accelerate Newton-Schulz-like iterations for matrix square roots and orthogonalization, which are core primitives in machine learning. Unlike prior methods, PRISM requires no explicit spectral bounds or singular value estimates; and it adapts automatically to the evolving spectrum. Empirically, PRISM accelerates training when integrated into Shampoo and Muon optimizers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PRISM：无需分布假设的矩阵函数自适应计算框架，用于加速神经网络训练</div>
<div class="mono" style="margin-top:8px">矩阵函数如平方根、逆根和正交化在神经网络训练的预条件梯度方法中起着核心作用。这促使开发了避免显式特征分解并主要依赖矩阵乘法的迭代算法，使其非常适合现代GPU加速器。我们提出了PRISM（用于矩阵函数计算的多项式拟合与随机迭代草图方法），这是一个加速矩阵函数计算迭代算法的通用框架。PRISM结合了自适应多项式逼近与随机草图方法：在每次迭代中，它通过草图最小二乘问题拟合一个多项式近似，以最小的开销自适应地适应当前实例的谱。我们将PRISM应用于加速类似Newton-Schulz的矩阵平方根和正交化迭代，这些是机器学习中的核心基本操作。与之前的方法不同，PRISM不需要显式的谱界限或奇异值估计，并能自动适应不断变化的谱。在实践中，将PRISM集成到Shampoo和Muon优化器中可以加速训练。</div>
</details>
</div>
<div class="card">
<div class="title">StepShield: When, Not Whether to Intervene on Rogue Agents</div>
<div class="meta-line">Authors: Gloria Felicia, Michael Eniolade, Jinfeng He, Zitha Sasindran, Hemant Kumar, Milan Hussain Angati, Sandeep Bandarupalli</div>
<div class="meta-line">First: 2026-01-29T18:55:46+00:00 · Latest: 2026-01-29T18:55:46+00:00</div>
<div class="meta-line">Comments: 16 pages, 2 figures, 14 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22136v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22136v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing agent safety benchmarks report binary accuracy, conflating early intervention with post-mortem analysis. A detector that flags a violation at step 8 enables intervention; one that reports it at step 48 provides only forensic value. This distinction is critical, yet current benchmarks cannot measure it. We introduce StepShield, the first benchmark to evaluate when violations are detected, not just whether. StepShield contains 9,213 code agent trajectories, including 1,278 meticulously annotated training pairs and a 7,935-trajectory test set with a realistic 8.1% rogue rate. Rogue behaviors are grounded in real-world security incidents across six categories. We propose three novel temporal metrics: Early Intervention Rate (EIR), Intervention Gap, and Tokens Saved. Surprisingly, our evaluation reveals that an LLM-based judge achieves 59% EIR while a static analyzer achieves only 26%, a 2.3x performance gap that is entirely invisible to standard accuracy metrics. We further show that early detection has direct economic benefits: our cascaded HybridGuard detector reduces monitoring costs by 75% and projects to $108M in cumulative savings over five years at enterprise scale. By shifting the focus of evaluation from whether to when, StepShield provides a new foundation for building safer and more economically viable AI agents. The code and data are released under an Apache 2.0 license.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StepShield：何时干预而非是否干预 rogue agents</div>
<div class="mono" style="margin-top:8px">现有的 agent 安全基准测试报告二元准确性，将早期干预与事后分析混为一谈。一个在第 8 步标记违规的检测器可以实现干预；而一个在第 48 步报告违规的检测器则仅具有取证价值。这种区别至关重要，但当前基准测试无法衡量。我们引入了 StepShield，这是首个评估违规检测时间而非仅是否检测的基准测试。StepShield 包含 9,213 个代码 agent 轨迹，包括 1,278 个精心标注的训练对和一个包含 7,935 个轨迹、真实 rogue 率为 8.1% 的测试集。这些 rogue 行为基于六个类别中的现实世界安全事件。我们提出了三个新颖的时间度量指标：早期干预率（EIR）、干预间隔和节省的 token 数。令人惊讶的是，我们的评估结果显示，基于 LLM 的法官实现了 59% 的 EIR，而静态分析器仅达到 26%，性能差距达到 2.3 倍，这在标准准确性指标中完全不可见。我们进一步证明，早期检测具有直接的经济效益：我们的级联 HybridGuard 检测器可将监控成本降低 75%，并在企业规模下预计五年内累计节省 1.08 亿美元。通过将评估重点从是否检测转移到何时检测，StepShield 为构建更安全且更具经济可行性的 AI agent 提供了新的基础。代码和数据在 Apache 2.0 许可下发布。</div>
</details>
</div>
<div class="card">
<div class="title">PI-Light: Physics-Inspired Diffusion for Full-Image Relighting</div>
<div class="meta-line">Authors: Zhexin Liang, Zhaoxi Chen, Yongwei Chen, Tianyi Wei, Tengfei Wang, Xingang Pan</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-29T18:55:36+00:00 · Latest: 2026-01-29T18:55:36+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22135v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22135v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Full-image relighting remains a challenging problem due to the difficulty of collecting large-scale structured paired data, the difficulty of maintaining physical plausibility, and the limited generalizability imposed by data-driven priors. Existing attempts to bridge the synthetic-to-real gap for full-scene relighting remain suboptimal. To tackle these challenges, we introduce Physics-Inspired diffusion for full-image reLight ($π$-Light, or PI-Light), a two-stage framework that leverages physics-inspired diffusion models. Our design incorporates (i) batch-aware attention, which improves the consistency of intrinsic predictions across a collection of images, (ii) a physics-guided neural rendering module that enforces physically plausible light transport, (iii) physics-inspired losses that regularize training dynamics toward a physically meaningful landscape, thereby enhancing generalizability to real-world image editing, and (iv) a carefully curated dataset of diverse objects and scenes captured under controlled lighting conditions. Together, these components enable efficient finetuning of pretrained diffusion models while also providing a solid benchmark for downstream evaluation. Experiments demonstrate that $π$-Light synthesizes specular highlights and diffuse reflections across a wide variety of materials, achieving superior generalization to real-world scenes compared with prior approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PI-Light：基于物理启发的扩散模型用于全图像重照明</div>
<div class="mono" style="margin-top:8px">全图像重照明仍是一个具有挑战性的问题，原因包括难以收集大规模结构化配对数据、保持物理合理性困难，以及数据驱动先验带来的泛化能力有限。现有的全场景重照明合成到真实数据之间的桥梁方法仍不理想。为了解决这些挑战，我们提出了基于物理启发的扩散模型用于全图像重照明（π-Light，或PI-Light），这是一个两阶段框架，利用了物理启发的扩散模型。我们的设计包含：(i) 批量感知注意力，提高了多张图像中内在预测的一致性；(ii) 一个物理引导的神经渲染模块，强制执行物理合理的光照传输；(iii) 基于物理启发的损失函数，使训练动态朝着物理有意义的方向规范化，从而增强对现实世界图像编辑的泛化能力；(iv) 一个精心挑选的数据集，包含在受控光照条件下拍摄的多种物体和场景。这些组件共同实现了对预训练扩散模型的高效微调，并为下游评估提供了坚实的基准。实验表明，π-Light能够在多种材料上合成镜面高光和漫反射，相较于以往方法，实现了对现实场景更优的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Early and Prediagnostic Detection of Pancreatic Cancer from Computed Tomography</div>
<div class="meta-line">Authors: Wenxuan Li, Pedro R. A. S. Bassi, Lizhou Wu, Xinze Zhou, Yuxuan Zhao, Qi Chen, Szymon Plotka, Tianyu Lin, Zheren Zhu, Marisa Martin, Justin Caskey, Shanshan Jiang, Xiaoxi Chen, Jaroslaw B. Ćwikla, Artur Sankowski, Yaping Wu, Sergio Decherchi, Andrea Cavalli, Chandana Lall, Cristian Tomasetti, Yaxing Guo, Xuan Yu, Yuqing Cai, Hualin Qiao, Jie Bao, Chenhan Hu, Ximing Wang, Arkadiusz Sitek, Kai Ding, Heng Li, Meiyun Wang, Dexin Yu, Guang Zhang, Yang Yang, Kang Wang, Alan L. Yuille, Zongwei Zhou</div>
<div class="meta-line">First: 2026-01-29T18:55:23+00:00 · Latest: 2026-01-29T18:55:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22134v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22134v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pancreatic ductal adenocarcinoma (PDAC), one of the deadliest solid malignancies, is often detected at a late and inoperable stage. Retrospective reviews of prediagnostic CT scans, when conducted by expert radiologists aware that the patient later developed PDAC, frequently reveal lesions that were previously overlooked. To help detecting these lesions earlier, we developed an automated system named ePAI (early Pancreatic cancer detection with Artificial Intelligence). It was trained on data from 1,598 patients from a single medical center. In the internal test involving 1,009 patients, ePAI achieved an area under the receiver operating characteristic curve (AUC) of 0.939-0.999, a sensitivity of 95.3%, and a specificity of 98.7% for detecting small PDAC less than 2 cm in diameter, precisely localizing PDAC as small as 2 mm. In an external test involving 7,158 patients across 6 centers, ePAI achieved an AUC of 0.918-0.945, a sensitivity of 91.5%, and a specificity of 88.0%, precisely localizing PDAC as small as 5 mm. Importantly, ePAI detected PDACs on prediagnostic CT scans obtained 3 to 36 months before clinical diagnosis that had originally been overlooked by radiologists. It successfully detected and localized PDACs in 75 of 159 patients, with a median lead time of 347 days before clinical diagnosis. Our multi-reader study showed that ePAI significantly outperformed 30 board-certified radiologists by 50.3% (P &lt; 0.05) in sensitivity while maintaining a comparable specificity of 95.4% in detecting PDACs early and prediagnostic. These findings suggest its potential of ePAI as an assistive tool to improve early detection of pancreatic cancer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从计算机断层扫描中早期和预测性检测胰腺癌</div>
<div class="mono" style="margin-top:8px">胰腺导管腺癌（PDAC）是致命性最强的实体恶性肿瘤之一，通常在晚期且无法手术时才被发现。当由了解患者日后会发展为PDAC的专家放射科医生回顾性分析预测性CT扫描时，经常能发现之前被忽略的病变。为帮助更早检测这些病变，我们开发了一个名为ePAI（基于人工智能的早期胰腺癌检测系统）的自动化系统。该系统基于单一医疗中心1,598名患者的资料进行训练。在内部测试中涉及1,009名患者，ePAI在检测直径小于2厘米的小型PDAC时，达到了0.939-0.999的受试者工作特征曲线下面积（AUC），95.3%的灵敏度和98.7%的特异性，能够精确定位最小至2毫米的PDAC。在外部测试中涉及6个中心共7,158名患者，ePAI达到了0.918-0.945的AUC，91.5%的灵敏度和88.0%的特异性，能够精确定位最小至5毫米的PDAC。重要的是，ePAI能够在临床诊断前3至36个月获得的预测性CT扫描中检测出原本被放射科医生忽略的PDAC。在159名患者中，ePAI成功检测并定位了75例PDAC，其临床诊断前的中位预警时间为347天。我们的多阅者研究显示，在检测PDAC的早期和预测性方面，ePAI的灵敏度比30名认证放射科医生高出50.3%（P &lt; 0.05），同时保持了95.4%的特异性。这些发现表明ePAI作为辅助工具在提高胰腺癌早期检测方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Pay for Hints, Not Answers: LLM Shepherding for Cost-Efficient Inference</div>
<div class="meta-line">Authors: Ziming Dong, Hardik Sharma, Evan O&#x27;Toole, Jaya Prakash Champati, Kui Wu</div>
<div class="meta-line">First: 2026-01-29T18:52:54+00:00 · Latest: 2026-01-29T18:52:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22132v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22132v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) deliver state-of-the-art performance on complex reasoning tasks, but their inference costs limit deployment at scale. Small Language Models (SLMs) offer dramatic cost savings yet lag substantially in accuracy. Existing approaches - routing and cascading - treat the LLM as an all-or-nothing resource: either the query bypasses the LLM entirely, or the LLM generates a complete response at full cost. We introduce LLM Shepherding, a framework that requests only a short prefix (a hint) from the LLM and provides it to SLM. This simple mechanism is surprisingly effective for math and coding tasks: even hints comprising 10-30% of the full LLM response improve SLM accuracy significantly. Shepherding generalizes both routing and cascading, and it achieves lower cost under oracle decision-making. We develop a two-stage predictor that jointly determines whether a hint is needed and how many tokens to request. On the widely-used mathematical reasoning (GSM8K, CNK12) and code generation (HumanEval, MBPP) benchmarks, Shepherding reduces costs by 42-94% relative to LLM-only inference. Compared to state-of-the-art routing and cascading baselines, shepherding delivers up to 2.8x cost reduction while matching accuracy. To our knowledge, this is the first work to exploit token-level budget control for SLM-LLM collaboration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>为提示付费，而非答案：一种用于高效推理的LLM引导框架</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在复杂推理任务上表现出最先进的性能，但其推理成本限制了大规模部署。小型语言模型（SLMs）虽然能带来显著的成本节约，但在准确性上却明显落后。现有的方法——路由和级联——将LLM视为全有或全无的资源：要么查询完全绕过LLM，要么LLM以全成本生成完整回答。我们引入LLM引导框架，仅请求LLM生成一个简短的前缀（提示），并将其提供给SLM。这种简单的机制在数学和编程任务中出人意料地有效：即使提示仅占完整LLM回答的10-30%，也能显著提升SLM的准确性。引导方法既涵盖了路由和级联，又在oracle决策下实现了更低的成本。我们开发了一个两阶段预测器，联合判断是否需要提示以及请求多少个token。在广泛使用的数学推理（GSM8K、CNK12）和代码生成（HumanEval、MBPP）基准测试中，引导方法相比仅使用LLM推理的成本降低了42-94%。与最先进的路由和级联基线相比，引导方法在保持准确性的前提下，实现了高达2.8倍的成本降低。据我们所知，这是首个利用token级预算控制进行SLM-LLM协作的工作。</div>
</details>
</div>
<div class="card">
<div class="title">SMOG: Scalable Meta-Learning for Multi-Objective Bayesian Optimization</div>
<div class="meta-line">Authors: Leonard Papenmeier, Petru Tighineanu</div>
<div class="meta-line">First: 2026-01-29T18:51:58+00:00 · Latest: 2026-01-29T18:51:58+00:00</div>
<div class="meta-line">Comments: 19 pages, 15 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22131v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22131v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-objective optimization aims to solve problems with competing objectives, often with only black-box access to a problem and a limited budget of measurements. In many applications, historical data from related optimization tasks is available, creating an opportunity for meta-learning to accelerate the optimization. Bayesian optimization, as a promising technique for black-box optimization, has been extended to meta-learning and multi-objective optimization independently, but methods that simultaneously address both settings - meta-learned priors for multi-objective Bayesian optimization - remain largely unexplored. We propose SMOG, a scalable and modular meta-learning model based on a multi-output Gaussian process that explicitly learns correlations between objectives. SMOG builds a structured joint Gaussian process prior across meta- and target tasks and, after conditioning on metadata, yields a closed-form target-task prior augmented by a flexible residual multi-output kernel. This construction propagates metadata uncertainty into the target surrogate in a principled way. SMOG supports hierarchical, parallel training: meta-task Gaussian processes are fit once and then cached, achieving linear scaling with the number of meta-tasks. The resulting surrogate integrates seamlessly with standard multi-objective Bayesian optimization acquisition functions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SMOG：面向多目标贝叶斯优化的可扩展元学习方法</div>
<div class="mono" style="margin-top:8px">多目标优化旨在解决具有竞争目标的问题，通常仅有对问题的黑盒访问权限和有限的测量预算。在许多应用中，相关优化任务的历史数据是可获取的，这为利用元学习加速优化提供了机会。贝叶斯优化作为一种有前景的黑盒优化技术，已被独立扩展至元学习和多目标优化，但同时处理这两种设置的方法——即用于多目标贝叶斯优化的元学习先验——仍鲜有探索。我们提出SMOG，这是一种基于多输出高斯过程的可扩展且模块化的元学习模型，明确学习目标之间的相关性。SMOG在元任务和目标任务之间构建了一个结构化的联合高斯过程先验，并在条件化元数据后，得到一个由灵活的残差多输出核增强的闭式目标任务先验。这种构建方式以一种原则性的方式将元数据不确定性传播到目标代理模型中。SMOG支持分层和并行训练：元任务高斯过程仅拟合一次并缓存，从而实现元任务数量的线性扩展。最终的代理模型能够无缝集成到标准的多目标贝叶斯优化获取函数中。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-objective optimization aims to solve problems with competing objectives, often with only black-box access to a problem and a limited budget of measurements.</div>
</details>
</div>
<div class="card">
<div class="title">World of Workflows: a Benchmark for Bringing World Models to Enterprise Systems</div>
<div class="meta-line">Authors: Lakshya Gupta, Litao Li, Yizhe Liu, Sriram Ganapathi Subramanian, Kaheer Suleman, Zichen Zhang, Haoye Lu, Sumit Pasupalak</div>
<div class="meta-line">First: 2026-01-29T18:51:54+00:00 · Latest: 2026-01-29T18:51:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22130v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22130v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Frontier large language models (LLMs) excel as autonomous agents in many domains, yet they remain untested in complex enterprise systems where hidden workflows create cascading effects across interconnected databases. Existing enterprise benchmarks evaluate surface-level agentic task completion similar to general consumer benchmarks, ignoring true challenges in enterprises, such as limited observability, large database state, and hidden workflows with cascading side effects. We introduce World of Workflows (WoW), a realistic ServiceNow-based environment incorporating 4,000+ business rules and 55 active workflows embedded in the system, alongside WoW-bench, a benchmark of 234 tasks evaluating constrained agentic task completion and enterprise dynamics modeling capabilities. We reveal two major takeaways: (1) Frontier LLMs suffer from dynamics blindness, consistently failing to predict the invisible, cascading side effects of their actions, which leads to silent constraint violations, and (2) reliability in opaque systems requires grounded world modeling, where agents must mentally simulate hidden state transitions to bridge the observability gap when high-fidelity feedback is unavailable. For reliable and useful enterprise agents, WoW motivates a new paradigm to explicitly learn system dynamics. We release our GitHub for setting up and evaluating WoW.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>工作流世界：将世界模型引入企业系统的基准</div>
<div class="mono" style="margin-top:8px">前沿的大语言模型（LLMs）在许多领域表现出色，作为自主代理，但在复杂的企业系统中尚未经过测试，这些系统中隐藏的工作流会在相互关联的数据库中产生级联效应。现有的企业基准评估表面级的代理任务完成，类似于通用消费者基准，忽略了企业中的真实挑战，如有限的可观测性、大型数据库状态以及隐藏工作流的级联副作用。我们引入了World of Workflows（WoW），这是一个基于ServiceNow的现实环境，包含4000多个业务规则和55个活跃的工作流，同时推出了WoW-bench，一个包含234个任务的基准，用于评估受限代理任务完成和企业动态建模能力。我们得出两个主要结论：（1）前沿LLMs存在动态盲区，无法持续预测其行为的不可见级联副作用，导致隐性约束违规；（2）在不透明系统中实现可靠性需要基于现实的世界建模，代理必须在心理上模拟隐藏状态转换，以弥补高保真反馈不可用时的可观测性差距。为了实现可靠且有用的企业代理，WoW推动了一种新的范式，即显式学习系统动态。我们已发布GitHub仓库，用于设置和评估WoW。</div>
</details>
</div>
<div class="card">
<div class="title">SWE-Replay: Efficient Test-Time Scaling for Software Engineering Agents</div>
<div class="meta-line">Authors: Yifeng Ding, Lingming Zhang</div>
<div class="meta-line">First: 2026-01-29T18:50:29+00:00 · Latest: 2026-01-29T18:50:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22129v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22129v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test-time scaling has been widely adopted to enhance the capabilities of Large Language Model (LLM) agents in software engineering (SWE) tasks. However, the standard approach of repeatedly sampling trajectories from scratch is computationally expensive. While recent methods have attempted to mitigate costs using specialized value agents, they can suffer from model miscalibration and fail to generalize to modern agents that synthesize custom bash scripts as tools. In this paper, we introduce SWE-Replay, the first efficient and generalizable test-time scaling technique for modern agents without reliance on potentially noisy value estimates. SWE-Replay optimizes the scaling process by recycling trajectories from prior trials, dynamically choosing to either explore from scratch or exploit archived experience by branching at critical intermediate steps. This selection of intermediate steps is driven by the potential and reasoning significance of repository exploration, rather than external LLM-based quality estimates. Our evaluation shows that, on SWE-Bench Verified, SWE-Replay consistently outperforms naive scaling, reducing costs by up to 17.4% while maintaining or even improving performance by up to 3.8%. Further evaluation on SWE-Bench Pro and Multilingual validates the generalizability of SWE-Replay, establishing it as a robust foundation for efficient test-time scaling of software engineering agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SWE-Replay：面向软件工程代理的高效测试时扩展方法</div>
<div class="mono" style="margin-top:8px">测试时扩展已被广泛采用以增强大型语言模型（LLM）代理在软件工程（SWE）任务中的能力。然而，标准方法通过重复从头采样轨迹来实现扩展，计算成本较高。尽管近期方法尝试通过专用价值代理来缓解这一问题，但它们可能因模型校准不当而无法推广到能够合成自定义bash脚本作为工具的现代代理。本文提出SWE-Replay，这是首个无需依赖潜在噪声价值估计的高效且可推广的测试时扩展技术。SWE-Replay通过重用先前试验的轨迹来优化扩展过程，在关键中间步骤进行分支，动态选择从头探索或利用已存经验。这种中间步骤的选择基于仓库探索的潜力和推理重要性，而非外部基于LLM的质量估计。我们的评估表明，在SWE-Bench Verified上，SWE-Replay始终优于朴素扩展方法，成本最多降低17.4%，同时保持或提升性能达3.8%。进一步在SWE-Bench Pro和多语言基准上的评估验证了SWE-Replay的可推广性，确立其为高效扩展软件工程代理的稳健基础。</div>
</details>
</div>
<div class="card">
<div class="title">The Patient is not a Moving Document: A World Model Training Paradigm for Longitudinal EHR</div>
<div class="meta-line">Authors: Irsyad Adam, Zekai Chen, David Laprade, Shaun Porwal, David Laub, Erik Reinertsen, Arda Pekis, Kevin Brown</div>
<div class="meta-line">First: 2026-01-29T18:49:37+00:00 · Latest: 2026-01-29T18:49:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22128v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22128v1">PDF</a> · <a href="https://huggingface.co/standardmodelbio/SMB-v1-1.7B-Structure">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) trained with next-word-prediction have achieved success as clinical foundation models. Representations from these language backbones yield strong linear probe performance across biomedical tasks, suggesting that patient semantics emerge from next-token prediction at scale. However, this paradigm treats patients as a document to be summarized rather than a dynamical system to be simulated; a patient&#x27;s trajectory emerges from their state evolving under interventions and time, requiring models that simulate dynamics rather than predict tokens. To address this, we introduce SMB-Structure, a world model for structured EHR that grounds a joint-embedding prediction architecture (JEPA) with next-token prediction (SFT). SFT grounds our model to reconstruct future patient states in token space, while JEPA predicts those futures in latent space from the initial patient representation alone, forcing trajectory dynamics to be encoded before the next state is observed. We validate across two large-scale cohorts: Memorial Sloan Kettering (23,319 oncology patients; 323,000+ patient-years) and INSPECT (19,402 pulmonary embolism patients). Using a linear probe evaluated at multiple points along the disease trajectory, we demonstrate that our training paradigm learns embeddings that capture disease dynamics not recoverable by autoregressive baselines, enabling SMB-Structure to achieve competitive performance on complex tasks characterized by high patient heterogeneity. Model weights are available at https://huggingface.co/standardmodelbio/SMB-v1-1.7B-Structure.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>患者并非静态文档：用于纵向电子健康记录的世界模型训练范式</div>
<div class="mono" style="margin-top:8px">基于词序预测训练的大型语言模型（LLMs）在临床基础模型中取得了成功。这些语言主干的表示在生物医学任务中表现出强大的线性探针性能，表明患者语义在大规模词序预测中自然涌现。然而，这种范式将患者视为需要总结的文档，而非需要模拟的动态系统；患者的轨迹来源于其状态在干预和时间作用下的演变，因此需要能够模拟动态而非预测词的模型。为了解决这一问题，我们引入了SMB-Structure，这是一个针对结构化电子健康记录的世界模型，它将联合嵌入预测架构（JEPA）与词序预测（SFT）相结合。SFT使我们的模型能够重建未来患者状态的词空间表示，而JEPA则仅基于初始患者表示在潜在空间中预测未来状态，迫使轨迹动态在观察下一个状态之前就被编码。我们在两个大规模队列上进行了验证：纪念斯隆-凯特琳癌症中心（23,319名肿瘤患者；323,000+人年）和INSPECT（19,402名肺栓塞患者）。通过在疾病轨迹多个时间点评估的线性探针，我们证明了我们的训练范式能够学习到捕捉疾病动态的嵌入，这些动态无法通过自回归基线模型恢复，从而使SMB-Structure在具有高患者异质性的复杂任务中表现出竞争力。模型权重可在https://huggingface.co/standardmodelbio/SMB-v1-1.7B-Structure获取。</div>
</details>
</div>
<div class="card">
<div class="title">EditYourself: Audio-Driven Generation and Manipulation of Talking Head Videos with Diffusion Transformers</div>
<div class="meta-line">Authors: John Flynn, Wolfgang Paier, Dimitar Dinev, Sam Nhut Nguyen, Hayk Poghosyan, Manuel Toribio, Sandipan Banerjee, Guy Gafni</div>
<div class="meta-line">First: 2026-01-29T18:49:27+00:00 · Latest: 2026-01-29T18:49:27+00:00</div>
<div class="meta-line">Comments: Project page: https://edit-yourself.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22127v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22127v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://edit-yourself.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current generative video models excel at producing novel content from text and image prompts, but leave a critical gap in editing existing pre-recorded videos, where minor alterations to the spoken script require preserving motion, temporal coherence, speaker identity, and accurate lip synchronization. We introduce EditYourself, a DiT-based framework for audio-driven video-to-video (V2V) editing that enables transcript-based modification of talking head videos, including the seamless addition, removal, and retiming of visually spoken content. Building on a general-purpose video diffusion model, EditYourself augments its V2V capabilities with audio conditioning and region-aware, edit-focused training extensions. This enables precise lip synchronization and temporally coherent restructuring of existing performances via spatiotemporal inpainting, including the synthesis of realistic human motion in newly added segments, while maintaining visual fidelity and identity consistency over long durations. This work represents a foundational step toward generative video models as practical tools for professional video post-production.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EditYourself：基于扩散变换器的音频驱动说话人视频生成与编辑</div>
<div class="mono" style="margin-top:8px">当前的生成视频模型在从文本和图像提示生成新颖内容方面表现出色，但在编辑已有录制视频方面存在关键缺失，其中对语音脚本的微小修改需要保持动作、时间一致性、说话人身份和准确的唇部同步。我们提出了EditYourself，这是一个基于DiT的框架，用于音频驱动的视频到视频（V2V）编辑，能够基于文本转录对说话人视频进行修改，包括视觉语音内容的无缝添加、删除和重新定时。基于通用视频扩散模型，EditYourself通过音频条件和区域感知、以编辑为中心的训练扩展增强了其V2V能力。这使得可以通过时空修复技术对现有表演进行精确的唇部同步和时间一致性的重构，包括在新增片段中合成逼真的人体动作，同时在长时间内保持视觉保真度和身份一致性。这项工作代表了生成视频模型作为专业视频后期制作实用工具的基础性进展。</div>
</details>
</div>
<div class="card">
<div class="title">Creative Image Generation with Diffusion Model</div>
<div class="meta-line">Authors: Kunpeng Song, Ahmed Elgammal</div>
<div class="meta-line">First: 2026-01-29T18:48:48+00:00 · Latest: 2026-01-29T18:48:48+00:00</div>
<div class="meta-line">Comments: Project page: https://creative-t2i.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22125v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22125v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://creative-t2i.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Creative image generation has emerged as a compelling area of research, driven by the need to produce novel and high-quality images that expand the boundaries of imagination. In this work, we propose a novel framework for creative generation using diffusion models, where creativity is associated with the inverse probability of an image&#x27;s existence in the CLIP embedding space. Unlike prior approaches that rely on a manual blending of concepts or exclusion of subcategories, our method calculates the probability distribution of generated images and drives it towards low-probability regions to produce rare, imaginative, and visually captivating outputs. We also introduce pullback mechanisms, achieving high creativity without sacrificing visual fidelity. Extensive experiments on text-to-image diffusion models demonstrate the effectiveness and efficiency of our creative generation framework, showcasing its ability to produce unique, novel, and thought-provoking images. This work provides a new perspective on creativity in generative models, offering a principled method to foster innovation in visual content synthesis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于扩散模型的创意图像生成</div>
<div class="mono" style="margin-top:8px">创意图像生成已成为一个引人注目的研究领域，其驱动力在于生成新颖且高质量的图像以拓展想象力的边界。本文提出了一种基于扩散模型的新型框架用于创意生成，其中创意与图像在CLIP嵌入空间中存在逆概率相关联。与以往依赖人工概念融合或子类排除的方法不同，我们的方法计算生成图像的概率分布，并将其引导至低概率区域，以产生罕见、富有想象力且视觉吸引人的输出。我们还引入了拉回机制，在保持视觉保真度的同时实现高创造力。在文本到图像扩散模型上的大量实验验证了我们创意生成框架的有效性和效率，展示了其生成独特、新颖且引人深思图像的能力。本工作为生成模型中的创造力提供了新的视角，提供了一种有原则的方法以促进视觉内容合成的创新。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Hamiltonian Flow Maps: Mean Flow Consistency for Large-Timestep Molecular Dynamics</div>
<div class="meta-line">Authors: Winfried Ripken, Michael Plainer, Gregor Lied, Thorben Frank, Oliver T. Unke, Stefan Chmiela, Frank Noé, Klaus Robert Müller</div>
<div class="meta-line">First: 2026-01-29T18:47:46+00:00 · Latest: 2026-01-29T18:47:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22123v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22123v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Simulating the long-time evolution of Hamiltonian systems is limited by the small timesteps required for stable numerical integration. To overcome this constraint, we introduce a framework to learn Hamiltonian Flow Maps by predicting the mean phase-space evolution over a chosen time span $Δt$, enabling stable large-timestep updates far beyond the stability limits of classical integrators. To this end, we impose a Mean Flow consistency condition for time-averaged Hamiltonian dynamics. Unlike prior approaches, this allows training on independent phase-space samples without access to future states, avoiding expensive trajectory generation. Validated across diverse Hamiltonian systems, our method in particular improves upon molecular dynamics simulations using machine-learned force fields (MLFF). Our models maintain comparable training and inference cost, but support significantly larger integration timesteps while trained directly on widely-available trajectory-free MLFF datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习哈密顿流映射：大时间步长分子动力学的平均流一致性</div>
<div class="mono" style="margin-top:8px">模拟哈密顿系统长时间演化受到稳定数值积分所需小时间步长的限制。为克服这一限制，我们提出一个框架，通过预测选定时间跨度 $Δt$ 内的平均相空间演化来学习哈密顿流映射，从而实现远超传统积分器稳定极限的大时间步长更新。为此，我们引入了平均流一致性条件，用于时间平均的哈密顿动力学。与以往方法不同，这种方法允许在不访问未来状态的情况下，基于独立的相空间样本进行训练，避免了昂贵的轨迹生成过程。我们在多种哈密顿系统上验证了该方法，特别是在使用机器学习力场（MLFF）进行分子动力学模拟时表现出显著改进。我们的模型在训练和推理成本上保持相当，但可以直接在广泛可用的无轨迹MLFF数据集上训练，从而支持显著更大的积分时间步长。</div>
</details>
</div>
<div class="card">
<div class="title">Alpha Discovery via Grammar-Guided Learning and Search</div>
<div class="meta-line">Authors: Han Yang, Dong Hao, Zhuohan Wang, Qi Shi, Xingtong Li</div>
<div class="meta-line">First: 2026-01-29T18:46:15+00:00 · Latest: 2026-01-29T18:46:15+00:00</div>
<div class="meta-line">Comments: 24 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22119v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22119v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automatically discovering formulaic alpha factors is a central problem in quantitative finance. Existing methods often ignore syntactic and semantic constraints, relying on exhaustive search over unstructured and unbounded spaces. We present AlphaCFG, a grammar-based framework for defining and discovering alpha factors that are syntactically valid, financially interpretable, and computationally efficient. AlphaCFG uses an alpha-oriented context-free grammar to define a tree-structured, size-controlled search space, and formulates alpha discovery as a tree-structured linguistic Markov decision process, which is then solved using a grammar-aware Monte Carlo Tree Search guided by syntax-sensitive value and policy networks. Experiments on Chinese and U.S. stock market datasets show that AlphaCFG outperforms state-of-the-art baselines in both search efficiency and trading profitability. Beyond trading strategies, AlphaCFG serves as a general framework for symbolic factor discovery and refinement across quantitative finance, including asset pricing and portfolio construction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过语法引导学习与搜索发现Alpha因子</div>
<div class="mono" style="margin-top:8px">在量化金融中，自动发现公式化的Alpha因子是一个核心问题。现有方法通常忽略语法和语义约束，依赖于在无结构且无界的搜索空间中进行穷举搜索。我们提出了AlphaCFG，这是一个基于语法的框架，用于定义和发现语法有效、金融可解释且计算高效的Alpha因子。AlphaCFG使用面向Alpha的上下文无关文法来定义一个树状结构且大小受控的搜索空间，并将Alpha发现建模为一个树状结构的语义马尔可夫决策过程，随后通过语法感知的蒙特卡洛树搜索方法进行求解，该方法由语法敏感的价值和策略网络引导。在中文和美国股票市场数据集上的实验表明，AlphaCFG在搜索效率和交易盈利能力方面均优于现有的最先进基线。除了交易策略，AlphaCFG还为量化金融中的符号因子发现与优化提供了一个通用框架，包括资产定价和投资组合构建。</div>
</details>
</div>
<div class="card">
<div class="title">Defining Operational Conditions for Safety-Critical AI-Based Systems from Data</div>
<div class="meta-line">Authors: Johann Christensen, Elena Hoemann, Frank Köster, Sven Hallerbach</div>
<div class="meta-line">First: 2026-01-29T18:46:02+00:00 · Latest: 2026-01-29T18:46:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.22118v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.22118v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Artificial Intelligence (AI) has been on the rise in many domains, including numerous safety-critical applications. However, for complex systems found in the real world, or when data already exist, defining the underlying environmental conditions is extremely challenging. This often results in an incomplete description of the environment in which the AI-based system must operate. Nevertheless, this description, called the Operational Design Domain (ODD), is required in many domains for the certification of AI-based systems. Traditionally, the ODD is created in the early stages of the development process, drawing on sophisticated expert knowledge and related standards. This paper presents a novel Safety-by-Design method to a posteriori define the ODD from previously collected data using a multi-dimensional kernel-based representation. This approach is validated through both Monte Carlo methods and a real-world aviation use case for a future safety-critical collision-avoidance system. Moreover, by defining under what conditions two ODDs are equal, the paper shows that the data-driven ODD can equal the original, underlying hidden ODD of the data. Utilizing the novel, Safe-by-Design kernel-based ODD enables future certification of data-driven, safety-critical AI-based systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从数据定义安全关键型人工智能系统运行条件</div>
<div class="mono" style="margin-top:8px">人工智能（AI）在许多领域迅速发展，包括众多安全关键型应用。然而，对于现实世界中的复杂系统，或当已有数据时，定义其底层环境条件极具挑战性。这通常导致对AI系统必须运行的环境描述不完整。然而，这种描述，称为运行设计域（ODD），在许多领域是AI系统认证所必需的。传统上，ODD是在开发过程的早期阶段创建的，依赖于复杂的专家知识和相关标准。本文提出了一种新颖的Safety-by-Design方法，通过使用多维核方法表示，从之前收集的数据后验地定义ODD。该方法通过蒙特卡洛方法和一个现实世界的航空用例进行了验证，用于未来安全关键型的防撞系统。此外，通过定义两个ODD在何种条件下相等，本文表明数据驱动的ODD可以等于数据所隐含的原始ODD。利用这种新颖的Safe-by-Design核方法定义的ODD，有助于未来数据驱动型安全关键型AI系统的认证。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260130_0406.html">20260130_0406</a>
<a href="archive/20260129_0407.html">20260129_0407</a>
<a href="archive/20260128_0407.html">20260128_0407</a>
<a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
