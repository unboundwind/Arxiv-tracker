<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-02 03:54</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260102_0354</div>
    <div class="row"><div class="card">
<div class="title">SpaceTimePilot: Generative Rendering of Dynamic Scenes Across Space and Time</div>
<div class="meta-line">Authors: Zhening Huang, Hyeonho Jeong, Xuelin Chen, Yulia Gryaditskaya, Tuanfeng Y. Wang, Joan Lasenby, Chun-Hao Huang</div>
<div class="meta-line">First: 2025-12-31T18:59:57+00:00 · Latest: 2025-12-31T18:59:57+00:00</div>
<div class="meta-line">Comments: Project page: https://zheninghuang.github.io/Space-Time-Pilot/ Code: https://github.com/ZheningHuang/spacetimepilot</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25075v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25075v1">PDF</a> · <a href="https://github.com/ZheningHuang/spacetimepilot">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://zheninghuang.github.io/Space-Time-Pilot/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present SpaceTimePilot, a video diffusion model that disentangles space and time for controllable generative rendering. Given a monocular video, SpaceTimePilot can independently alter the camera viewpoint and the motion sequence within the generative process, re-rendering the scene for continuous and arbitrary exploration across space and time. To achieve this, we introduce an effective animation time-embedding mechanism in the diffusion process, allowing explicit control of the output video&#x27;s motion sequence with respect to that of the source video. As no datasets provide paired videos of the same dynamic scene with continuous temporal variations, we propose a simple yet effective temporal-warping training scheme that repurposes existing multi-view datasets to mimic temporal differences. This strategy effectively supervises the model to learn temporal control and achieve robust space-time disentanglement. To further enhance the precision of dual control, we introduce two additional components: an improved camera-conditioning mechanism that allows altering the camera from the first frame, and CamxTime, the first synthetic space-and-time full-coverage rendering dataset that provides fully free space-time video trajectories within a scene. Joint training on the temporal-warping scheme and the CamxTime dataset yields more precise temporal control. We evaluate SpaceTimePilot on both real-world and synthetic data, demonstrating clear space-time disentanglement and strong results compared to prior work. Project page: https://zheninghuang.github.io/Space-Time-Pilot/ Code: https://github.com/ZheningHuang/spacetimepilot</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpaceTimePilot: 跨空间与时间的动态场景生成渲染</div>
<div class="mono" style="margin-top:8px">我们提出了SpaceTimePilot，这是一个视频扩散模型，能够分离空间和时间，实现可控的生成渲染。给定一个单目视频，SpaceTimePilot可以在生成过程中独立地改变摄像机视角和运动序列，从而在空间和时间上进行连续且任意的探索。为实现这一目标，我们在扩散过程中引入了一种有效的动画时间嵌入机制，允许对输出视频的运动序列进行显式控制，以匹配源视频的运动。由于没有提供同一动态场景连续时间变化的配对视频数据集，我们提出了一种简单而有效的时序变形训练方案，利用现有的多视角数据集来模拟时间差异。该策略有效地监督模型学习时序控制并实现稳健的空间-时间分离。为了进一步提高双控的精度，我们引入了两个额外的组件：一种改进的摄像机条件机制，允许从第一帧开始改变摄像机视角；以及CamxTime，这是首个合成空间-时间全覆盖渲染数据集，为场景中提供了完全自由的空间-时间视频轨迹。在时序变形方案和CamxTime数据集上的联合训练可以实现更精确的时序控制。我们在真实世界和合成数据上评估了SpaceTimePilot，展示了清晰的空间-时间分离效果，并在与先前工作的比较中取得了显著的成果。</div>
</details>
</div>
<div class="card">
<div class="title">GaMO: Geometry-aware Multi-view Diffusion Outpainting for Sparse-View 3D Reconstruction</div>
<div class="meta-line">Authors: Yi-Chuan Huang, Hao-Jen Chien, Chin-Yang Lin, Ying-Huan Chen, Yu-Lun Liu</div>
<div class="meta-line">First: 2025-12-31T18:59:55+00:00 · Latest: 2025-12-31T18:59:55+00:00</div>
<div class="meta-line">Comments: Project page: https://yichuanh.github.io/GaMO/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25073v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25073v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://yichuanh.github.io/GaMO/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in 3D reconstruction have achieved remarkable progress in high-quality scene capture from dense multi-view imagery, yet struggle when input views are limited. Various approaches, including regularization techniques, semantic priors, and geometric constraints, have been implemented to address this challenge. Latest diffusion-based methods have demonstrated substantial improvements by generating novel views from new camera poses to augment training data, surpassing earlier regularization and prior-based techniques. Despite this progress, we identify three critical limitations in these state-of-the-art approaches: inadequate coverage beyond known view peripheries, geometric inconsistencies across generated views, and computationally expensive pipelines. We introduce GaMO (Geometry-aware Multi-view Outpainter), a framework that reformulates sparse-view reconstruction through multi-view outpainting. Instead of generating new viewpoints, GaMO expands the field of view from existing camera poses, which inherently preserves geometric consistency while providing broader scene coverage. Our approach employs multi-view conditioning and geometry-aware denoising strategies in a zero-shot manner without training. Extensive experiments on Replica and ScanNet++ demonstrate state-of-the-art reconstruction quality across 3, 6, and 9 input views, outperforming prior methods in PSNR and LPIPS, while achieving a $25\times$ speedup over SOTA diffusion-based methods with processing time under 10 minutes. Project page: https://yichuanh.github.io/GaMO/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GaMO：面向稀疏视角三维重建的几何感知多视角扩散外补</div>
<div class="mono" style="margin-top:8px">近年来，三维重建技术在从密集多视角图像中高质量捕捉场景方面取得了显著进展，但在输入视角有限时仍面临挑战。已有多种方法，包括正则化技术、语义先验和几何约束，被用于解决这一问题。最新的扩散模型方法通过从新相机姿态生成新视角来增强训练数据，从而在生成视角方面实现了显著提升，超越了早期的正则化和基于先验的方法。尽管取得了这些进展，我们发现当前最先进的方法存在三个关键限制：超出已知视角边缘的覆盖不足、生成视角间的几何不一致，以及计算成本高昂的流程。我们提出GaMO（几何感知多视角外补框架），通过多视角外补重新表述稀疏视角重建问题。与生成新视角不同，GaMO从现有相机姿态扩展视野，从而在保持几何一致性的同时提供更广泛的场景覆盖。我们的方法在无需训练的情况下，采用多视角条件和几何感知去噪策略进行零样本学习。在Replica和ScanNet++上的大量实验表明，GaMO在3、6和9个输入视角下均实现了最先进的重建质量，在PSNR和LPIPS指标上优于现有方法，同时相比最先进的扩散模型方法实现了25倍的速度提升，处理时间低于10分钟。</div>
</details>
</div>
<div class="card">
<div class="title">Edit3r: Instant 3D Scene Editing from Sparse Unposed Images</div>
<div class="meta-line">Authors: Jiageng Liu, Weijie Lyu, Xueting Li, Yejie Guo, Ming-Hsuan Yang</div>
<div class="meta-line">First: 2025-12-31T18:59:53+00:00 · Latest: 2025-12-31T18:59:53+00:00</div>
<div class="meta-line">Comments: Project page: https://edit3r.github.io/edit3r/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25071v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25071v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://edit3r.github.io/edit3r/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Edit3r, a feed-forward framework that reconstructs and edits 3D scenes in a single pass from unposed, view-inconsistent, instruction-edited images. Unlike prior methods requiring per-scene optimization, Edit3r directly predicts instruction-aligned 3D edits, enabling fast and photorealistic rendering without optimization or pose estimation. A key challenge in training such a model lies in the absence of multi-view consistent edited images for supervision. We address this with (i) a SAM2-based recoloring strategy that generates reliable, cross-view-consistent supervision, and (ii) an asymmetric input strategy that pairs a recolored reference view with raw auxiliary views, encouraging the network to fuse and align disparate observations. At inference, our model effectively handles images edited by 2D methods such as InstructPix2Pix, despite not being exposed to such edits during training. For large-scale quantitative evaluation, we introduce DL3DV-Edit-Bench, a benchmark built on the DL3DV test split, featuring 20 diverse scenes, 4 edit types and 100 edits in total. Comprehensive quantitative and qualitative results show that Edit3r achieves superior semantic alignment and enhanced 3D consistency compared to recent baselines, while operating at significantly higher inference speed, making it promising for real-time 3D editing applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Edit3r：从稀疏非定位图像中进行即时3D场景编辑</div>
<div class="mono" style="margin-top:8px">我们提出了Edit3r，这是一个前馈框架，能够通过单次传递从非定位、视角不一致且经过指令编辑的图像中重建和编辑3D场景。与需要逐场景优化的先前方法不同，Edit3r直接预测与指令对齐的3D编辑，从而实现无需优化或姿态估计即可快速且逼真的渲染。训练此类模型的关键挑战在于缺乏多视角一致的编辑图像用于监督。我们通过（i）基于SAM2的重着色策略生成可靠的跨视角一致的监督信息，以及（ii）一种非对称输入策略，将重着色的参考视角与原始辅助视角配对，鼓励网络融合和对齐不同的观测结果。在推理阶段，我们的模型能够有效处理由2D方法（如InstructPix2Pix）编辑的图像，即使在训练过程中未接触此类编辑。为了进行大规模定量评估，我们引入了DL3DV-Edit-Bench基准，该基准基于DL3DV测试集分割，包含20个多样化的场景、4种编辑类型和总共100个编辑。全面的定量和定性结果表明，Edit3r在语义对齐和3D一致性方面优于近期基线方法，同时推理速度显著提高，使其在实时3D编辑应用中具有很大潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present Edit3r, a feed-forward framework that reconstructs and edits 3D scenes in a single pass from unposed, view-inconsistent, instruction-edited images.</div>
</details>
</div>
<div class="card">
<div class="title">Coordinated Humanoid Manipulation with Choice Policies</div>
<div class="meta-line">Authors: Haozhi Qi, Yen-Jen Wang, Toru Lin, Brent Yi, Yi Ma, Koushil Sreenath, Jitendra Malik</div>
<div class="meta-line">First: 2025-12-31T18:59:53+00:00 · Latest: 2025-12-31T18:59:53+00:00</div>
<div class="meta-line">Comments: Code and Website: https://choice-policy.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25072v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25072v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://choice-policy.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humanoid robots hold great promise for operating in human-centric environments, yet achieving robust whole-body coordination across the head, hands, and legs remains a major challenge. We present a system that combines a modular teleoperation interface with a scalable learning framework to address this problem. Our teleoperation design decomposes humanoid control into intuitive submodules, which include hand-eye coordination, grasp primitives, arm end-effector tracking, and locomotion. This modularity allows us to collect high-quality demonstrations efficiently. Building on this, we introduce Choice Policy, an imitation learning approach that generates multiple candidate actions and learns to score them. This architecture enables both fast inference and effective modeling of multimodal behaviors. We validate our approach on two real-world tasks: dishwasher loading and whole-body loco-manipulation for whiteboard wiping. Experiments show that Choice Policy significantly outperforms diffusion policies and standard behavior cloning. Furthermore, our results indicate that hand-eye coordination is critical for success in long-horizon tasks. Our work demonstrates a practical path toward scalable data collection and learning for coordinated humanoid manipulation in unstructured environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>协调人形机器人操作的策略选择</div>
<div class="mono" style="margin-top:8px">人形机器人在人类中心环境中具有广阔的应用前景，但实现头部、手部和腿部的鲁棒全身协调仍然是一个重大挑战。我们提出一个系统，结合模块化的远程操作界面和可扩展的学习框架来解决这一问题。我们的远程操作设计将人形机器人控制分解为直观的子模块，包括手眼协调、抓取基元、手臂末端执行器跟踪和移动。这种模块化设计使我们能够高效地收集高质量的示范数据。在此基础上，我们引入了Choice Policy，这是一种模仿学习方法，能够生成多个候选动作并学习对这些动作进行评分。该架构支持快速推理和对多模态行为的有效建模。我们在两个现实任务上验证了我们的方法：洗碗机装盘和白板擦拭的全身操作。实验表明，Choice Policy在性能上显著优于扩散策略和标准行为克隆方法。此外，我们的结果表明，手眼协调对于长时域任务的成功至关重要。我们的工作展示了一条在非结构化环境中实现协调人形机器人操作的可扩展数据收集和学习的实用路径。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Open-Ended Reasoning to Predict the Future</div>
<div class="meta-line">Authors: Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping</div>
<div class="meta-line">First: 2025-12-31T18:59:51+00:00 · Latest: 2025-12-31T18:59:51+00:00</div>
<div class="meta-line">Comments: 45 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25070v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25070v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将开放式推理扩展到预测未来</div>
<div class="mono" style="margin-top:8px">高风险决策涉及对未来不确定性的推理。在本工作中，我们训练语言模型对开放式预测问题进行预测。为了扩展训练数据，我们通过每日新闻中报道的全球事件，使用全自动且细致的筛选方法合成新的预测问题。我们在我们的数据集OpenForesight上训练Qwen3思考模型。为了防止训练和评估过程中未来信息的泄露，我们在预测系统的数据生成和检索中均使用离线新闻语料库。在小型验证集的指导下，我们展示了检索的优势以及用于强化学习（RL）的改进奖励函数。在获得最终预测系统后，我们在2025年5月至8月期间进行保留测试。我们的专用模型OpenForecaster 8B在性能上与许多更大的专有模型相当，我们的训练提高了预测的准确性、校准度和一致性。我们发现，预测训练带来的校准改进可以推广到流行的基准测试中。我们开源了所有模型、代码和数据，以使语言模型预测研究更加广泛可及。</div>
</details>
</div>
<div class="card">
<div class="title">No-cost Bell Nonlocality Certification from Quantum Tomography and Its Applications in Quantum Magic Witnessing</div>
<div class="meta-line">Authors: Pawel Cieslinski, Lukas Knips, Harald Weinfurter, Wieslaw Laskowski</div>
<div class="meta-line">First: 2025-12-31T18:59:24+00:00 · Latest: 2025-12-31T18:59:24+00:00</div>
<div class="meta-line">Comments: 11 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25068v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25068v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tomographic measurements are the standard tool for characterizing quantum states, yet they are usually regarded only as means for state reconstruction or fidelity measurement. Here, we show that the same Pauli-basis measurements (X, Y, Z) can be directly employed for the certification of nonlocality at no additional experimental cost. Our framework allows any tomographic data - including archival datasets -- to be reinterpreted in terms of fundamental nonlocality tests. We introduce a generic, constructive method to generate tailored Bell inequalities and showcase their applicability to certify the non-locality of states in realistic experimental scenarios. Recognizing the stabilizer nature of the considered operators, we analyze our inequalities in the context of witnessing quantum magic - a crucial resource for quantum computing. Our approach requires Pauli measurements only and tests the quantum magic solely through the resources present in the state. Our results establish a universal standard that unifies state tomography with nonlocality certification and its application to quantum magic witnessing, thereby streamlining both fundamental studies and practical applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无需额外实验成本的Bell非局域性认证：基于量子层析及其在量子魔法见证中的应用</div>
<div class="mono" style="margin-top:8px">量子层析测量是表征量子态的标准工具，但通常仅被视为状态重构或保真度测量的手段。本文展示，相同的泡利基测量（X、Y、Z）可以直接用于非局域性认证，而无需额外的实验成本。我们的框架允许任何层析数据——包括历史数据集——被重新解释为基本非局域性测试。我们引入了一种通用且构造性的方法来生成定制化的Bell不等式，并展示了其在现实实验场景中用于认证态非局域性的适用性。通过识别所考虑算符的稳定子性质，我们在量子魔法见证的背景下分析了这些不等式——这是量子计算中的关键资源。我们的方法仅需泡利测量，并且通过态中已有的资源来测试量子魔法。我们的结果建立了一个统一的标准，将态层析与非局域性认证及其在量子魔法见证中的应用结合起来，从而简化了基础研究和实际应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Tomographic measurements are the standard tool for characterizing quantum states, yet they are usually regarded only as means for state reconstruction or fidelity measurement.</div>
</details>
</div>
<div class="card">
<div class="title">FineTec: Fine-Grained Action Recognition Under Temporal Corruption via Skeleton Decomposition and Sequence Completion</div>
<div class="meta-line">Authors: Dian Shao, Mingfei Shi, Like Liu</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-12-31T18:59:12+00:00 · Latest: 2025-12-31T18:59:12+00:00</div>
<div class="meta-line">Comments: Accepted by AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25067v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25067v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://smartdianlab.github.io/projects-FineTec/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recognizing fine-grained actions from temporally corrupted skeleton sequences remains a significant challenge, particularly in real-world scenarios where online pose estimation often yields substantial missing data. Existing methods often struggle to accurately recover temporal dynamics and fine-grained spatial structures, resulting in the loss of subtle motion cues crucial for distinguishing similar actions. To address this, we propose FineTec, a unified framework for Fine-grained action recognition under Temporal Corruption. FineTec first restores a base skeleton sequence from corrupted input using context-aware completion with diverse temporal masking. Next, a skeleton-based spatial decomposition module partitions the skeleton into five semantic regions, further divides them into dynamic and static subgroups based on motion variance, and generates two augmented skeleton sequences via targeted perturbation. These, along with the base sequence, are then processed by a physics-driven estimation module, which utilizes Lagrangian dynamics to estimate joint accelerations. Finally, both the fused skeleton position sequence and the fused acceleration sequence are jointly fed into a GCN-based action recognition head. Extensive experiments on both coarse-grained (NTU-60, NTU-120) and fine-grained (Gym99, Gym288) benchmarks show that FineTec significantly outperforms previous methods under various levels of temporal corruption. Specifically, FineTec achieves top-1 accuracies of 89.1% and 78.1% on the challenging Gym99-severe and Gym288-severe settings, respectively, demonstrating its robustness and generalizability. Code and datasets could be found at https://smartdianlab.github.io/projects-FineTec/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FineTec：通过骨架分解与序列补全实现时间扰动下的细粒度动作识别</div>
<div class="mono" style="margin-top:8px">从时间扰动的骨架序列中识别细粒度动作仍然是一个重大挑战，尤其是在现实场景中，实时姿态估计常常导致大量缺失数据。现有方法通常难以准确恢复时间动态和细粒度空间结构，从而丢失了区分相似动作的关键细微运动线索。为了解决这一问题，我们提出了FineTec，这是一个统一的框架，用于在时间扰动下进行细粒度动作识别。FineTec首先利用具有多样化时间掩码的上下文感知补全方法，从扰动输入中恢复基础骨架序列。接着，一个基于骨架的空间分解模块将骨架划分为五个语义区域，根据运动方差进一步将其分为动态和静态子组，并通过有针对性的扰动生成两个增强的骨架序列。这些序列与基础序列一起，随后被送入一个物理驱动的估计模块，该模块利用拉格朗日动力学来估计关节加速度。最后，融合后的骨架位置序列和融合后的加速度序列被联合输入到基于图卷积网络（GCN）的动作识别头部。在粗粒度（NTU-60、NTU-120）和细粒度（Gym99、Gym288）基准上的大量实验表明，FineTec在不同水平的时间扰动下显著优于先前方法。具体而言，FineTec在具有挑战性的Gym99-severe和Gym288-severe设置中分别实现了89.1%和78.1%的top-1准确率，展示了其鲁棒性和泛化能力。代码和数据集可在https://smartdianlab.github.io/projects-FineTec/上找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recognizing fine-grained actions from temporally corrupted skeleton sequences remains a significant challenge, particularly in real-world scenarios where online pose estimation often yields substantial missing data.</div>
</details>
</div>
<div class="card">
<div class="title">From Inpainting to Editing: A Self-Bootstrapping Framework for Context-Rich Visual Dubbing</div>
<div class="meta-line">Authors: Xu He, Haoxian Zhang, Hejia Chen, Changyuan Zheng, Liyang Chen, Songlin Tang, Jiehui Huang, Xiaoqiang Liu, Pengfei Wan, Zhiyong Wu</div>
<div class="meta-line">First: 2025-12-31T18:58:30+00:00 · Latest: 2025-12-31T18:58:30+00:00</div>
<div class="meta-line">Comments: Project Page https://hjrphoebus.github.io/X-Dub</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25066v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25066v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hjrphoebus.github.io/X-Dub">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Audio-driven visual dubbing aims to synchronize a video&#x27;s lip movements with new speech, but is fundamentally challenged by the lack of ideal training data: paired videos where only a subject&#x27;s lip movements differ while all other visual conditions are identical. Existing methods circumvent this with a mask-based inpainting paradigm, where an incomplete visual conditioning forces models to simultaneously hallucinate missing content and sync lips, leading to visual artifacts, identity drift, and poor synchronization. In this work, we propose a novel self-bootstrapping framework that reframes visual dubbing from an ill-posed inpainting task into a well-conditioned video-to-video editing problem. Our approach employs a Diffusion Transformer, first as a data generator, to synthesize ideal training data: a lip-altered companion video for each real sample, forming visually aligned video pairs. A DiT-based audio-driven editor is then trained on these pairs end-to-end, leveraging the complete and aligned input video frames to focus solely on precise, audio-driven lip modifications. This complete, frame-aligned input conditioning forms a rich visual context for the editor, providing it with complete identity cues, scene interactions, and continuous spatiotemporal dynamics. Leveraging this rich context fundamentally enables our method to achieve highly accurate lip sync, faithful identity preservation, and exceptional robustness against challenging in-the-wild scenarios. We further introduce a timestep-adaptive multi-phase learning strategy as a necessary component to disentangle conflicting editing objectives across diffusion timesteps, thereby facilitating stable training and yielding enhanced lip synchronization and visual fidelity. Additionally, we propose ContextDubBench, a comprehensive benchmark dataset for robust evaluation in diverse and challenging practical application scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从图像修复到编辑：一种用于上下文丰富的视觉配音的自引导框架</div>
<div class="mono" style="margin-top:8px">音频驱动的视觉配音旨在将视频的唇部动作与新语音同步，但其根本挑战在于缺乏理想的训练数据：仅唇部动作不同而其他视觉条件完全相同的配对视频。现有方法通过基于掩码的图像修复范式来规避这一问题，其中不完整的视觉条件迫使模型同时生成缺失内容并同步唇部动作，导致视觉伪影、身份漂移和同步效果差。在本工作中，我们提出了一种新颖的自引导框架，将视觉配音任务重新定义为一个条件良好的视频到视频编辑问题。我们的方法首先使用扩散Transformer作为数据生成器，合成理想训练数据：为每个真实样本生成一个唇部修改的配对视频，形成视觉对齐的视频对。随后，基于DiT的音频驱动编辑器在这些配对视频上进行端到端训练，利用完整且对齐的输入视频帧，专注于精确的音频驱动唇部修改。这种完整的帧对齐输入条件为编辑器提供了丰富的视觉上下文，包括完整身份提示、场景交互以及连续的时空动态。利用这种丰富的上下文，我们的方法能够实现高度准确的唇部同步、忠实的身份保持以及在复杂真实场景中表现出色的鲁棒性。此外，我们还引入了一种时间步自适应的多阶段学习策略，作为必要组件以在扩散时间步中分离冲突的编辑目标，从而实现稳定训练并提升唇部同步和视觉保真度。我们进一步提出了ContextDubBench，这是一个全面的基准数据集，用于在多样且具有挑战性的实际应用场景中进行鲁棒评估。</div>
</details>
</div>
<div class="card">
<div class="title">Vulcan: Instance-Optimal Systems Heuristics Through LLM-Driven Search</div>
<div class="meta-line">Authors: Rohit Dwivedula, Divyanshu Saxena, Sujay Yadalam, Daehyeok Kim, Aditya Akella</div>
<div class="meta-line">First: 2025-12-31T18:58:19+00:00 · Latest: 2025-12-31T18:58:19+00:00</div>
<div class="meta-line">Comments: 27 pages, 11 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25065v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25065v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Resource-management tasks in modern operating and distributed systems continue to rely primarily on hand-designed heuristics for tasks such as scheduling, caching, or active queue management. Designing performant heuristics is an expensive, time-consuming process that we are forced to continuously go through due to the constant flux of hardware, workloads and environments.
  We propose a new alternative: synthesizing instance-optimal heuristics -- specialized for the exact workloads and hardware where they will be deployed -- using code-generating large language models (LLMs). To make this synthesis tractable, Vulcan separates policy and mechanism through LLM-friendly, task-agnostic interfaces. With these interfaces, users specify the inputs and objectives of their desired policy, while Vulcan searches for performant policies via evolutionary search over LLM-generated code. This interface is expressive enough to capture a wide range of system policies, yet sufficiently constrained to allow even small, inexpensive LLMs to generate correct and executable code.
  We use Vulcan to synthesize performant heuristics for cache eviction and memory tiering, and find that these heuristics outperform all human-designed state-of-the-art algorithms by upto 69% and 7.9% in performance for each of these tasks respectively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Vulcan：通过LLM驱动的搜索实现实例最优的系统启发式方法</div>
<div class="mono" style="margin-top:8px">现代操作系统和分布式系统中的资源管理任务仍然主要依赖人工设计的启发式方法，例如调度、缓存和活动队列管理。设计高性能的启发式方法是一个昂贵且耗时的过程，由于硬件、工作负载和环境的不断变化，我们不得不持续进行这一过程。
我们提出了一种新的替代方案：使用代码生成的大语言模型（LLMs）合成针对特定工作负载和硬件的实例最优启发式方法。为了使这种合成过程可行，Vulcan通过LLM友好的、任务无关的接口将策略与机制分离。通过这些接口，用户可以指定其所需策略的输入和目标，而Vulcan则通过在LLM生成的代码上进行进化搜索来寻找高性能策略。这种接口足够表达以涵盖广泛的系统策略，同时又足够受限，使得即使是小型、低成本的LLMs也能生成正确且可执行的代码。
我们使用Vulcan来合成缓存驱逐和内存分层的高性能启发式方法，并发现这些启发式方法在各自任务中分别比所有人工设计的最先进算法性能高出最多69%和7.9%。</div>
</details>
</div>
<div class="card">
<div class="title">Deep sequence models tend to memorize geometrically; it is unclear why</div>
<div class="meta-line">Authors: Shahriar Noroozizadeh, Vaishnavh Nagarajan, Elan Rosenfeld, Sanjiv Kumar</div>
<div class="meta-line">First: 2025-10-30T17:40:22+00:00 · Latest: 2025-12-31T18:57:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.26745v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.26745v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep sequence models are said to store atomic facts predominantly in the form of associative memory: a brute-force lookup of co-occurring entities. We identify a dramatically different form of storage of atomic facts that we term as geometric memory. Here, the model has synthesized embeddings encoding novel global relationships between all entities, including ones that do not co-occur in training. Such storage is powerful: for instance, we show how it transforms a hard reasoning task involving an $\ell$-fold composition into an easy-to-learn $1$-step navigation task.
  From this phenomenon, we extract fundamental aspects of neural embedding geometries that are hard to explain. We argue that the rise of such a geometry, as against a lookup of local associations, cannot be straightforwardly attributed to typical supervisory, architectural, or optimizational pressures. Counterintuitively, a geometry is learned even when it is more complex than the brute-force lookup.
  Then, by analyzing a connection to Node2Vec, we demonstrate how the geometry stems from a spectral bias that -- in contrast to prevailing theories -- indeed arises naturally despite the lack of various pressures. This analysis also points out to practitioners a visible headroom to make Transformer memory more strongly geometric. We hope the geometric view of parametric memory encourages revisiting the default intuitions that guide researchers in areas like knowledge acquisition, capacity, discovery, and unlearning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度序列模型倾向于记忆几何结构；其原因尚不明确</div>
<div class="mono" style="margin-top:8px">深度序列模型被认为主要以关联记忆的形式存储原子事实：一种暴力查找共现实体的方式。我们识别出一种截然不同的原子事实存储形式，称之为几何记忆。在此形式中，模型综合了嵌入向量，编码了所有实体之间新颖的全局关系，包括那些在训练中并未共现的关系。这种存储方式非常强大：例如，我们展示了它如何将涉及 $\ell$-重组合的困难推理任务转化为易于学习的一步导航任务。
  从这一现象中，我们提取出神经嵌入几何结构的一些基本特性，这些特性难以用现有理论解释。我们认为，这种几何结构的兴起，与局部关联的查找相比，不能简单地归因于典型的监督、架构或优化压力。反直觉的是，即使几何结构比暴力查找更为复杂，它仍然可以被学习到。
  然后，通过分析其与Node2Vec的联系，我们展示了这种几何结构源于一种谱偏倚，尽管缺乏各种压力，这种偏倚却自然产生，与主流理论相反。这一分析也向从业者指出了一个明显的改进空间，即如何使Transformer的记忆更加几何化。我们希望参数化记忆的几何视角能够鼓励研究人员重新审视指导知识获取、容量、发现和遗忘等领域的默认直觉。</div>
</details>
</div>
<div class="card">
<div class="title">Many Minds from One Model: Bayesian Transformers for Population Intelligence</div>
<div class="meta-line">Authors: Diji Yang, Yi Zhang</div>
<div class="meta-line">First: 2025-12-31T18:56:02+00:00 · Latest: 2025-12-31T18:56:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25063v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25063v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite their scale and success, modern transformers are almost universally trained as single-minded systems: optimization produces one deterministic set of parameters, representing a single functional hypothesis about the data. Motivated by the idea that intelligence emerge from many minds, we propose Population Bayesian Transformers (B-Trans), which transform a standard Large Language Model into a Bayesian Transformer model to supports sampling diverse yet coherent model instances from a single set of pre-trained weights.
  B-Trans introduces a Bayesian-motivated posterior proxy by treating the bias-like offsets in normalization layers as stochastic variables with a Gaussian variational approximation, inducing a distribution over model behavior without the cost of training full Bayesian neural networks. Sampling from this proxy yields a set of model instances with diverse behaviors while maintaining general competence. To preserve coherence within each generation, we freeze the sampled noise at the sequence level, enforcing temporal consistency across tokens. B-Trans allows for population-level decision-making, where aggregating predictions across sampled individuals significantly enhances exploration. Experiments across zero-shot generation, Reinforcement Learning with Verifiable Rewards (RLVR), and RL without explicit labels demonstrate that B-Trans effectively leverage the wisdom of crowds, yielding superior semantic diversity while achieving better task performance compared to deterministic baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一个模型多个思维：用于群体智能的贝叶斯变换器</div>
<div class="mono" style="margin-top:8px">尽管现代变换器规模庞大且取得了成功，但它们几乎普遍被训练为单一思维系统：优化过程产生一组确定性的参数，代表对数据的一个单一功能假设。受“智能源于多个思维”的理念启发，我们提出了群体贝叶斯变换器（B-Trans），它将标准的大规模语言模型转换为贝叶斯变换器模型，从而能够在一组预训练权重的基础上，采样出多样化但一致的模型实例。
B-Trans通过将归一化层中的类似偏置的偏移量视为具有高斯变分近似的随机变量，引入了一个贝叶斯驱动的后验代理，从而在不训练完整贝叶斯神经网络的情况下，诱导出模型行为的分布。从该代理中进行采样可以得到一组行为多样化但保持一般能力的模型实例。为了在每次生成中保持一致性，我们在序列级别冻结采样的噪声，强制在标记之间保持时间一致性。B-Trans支持群体层面的决策，通过在采样个体之间聚合预测显著增强探索能力。在零样本生成、可验证奖励强化学习（RLVR）以及无显式标签的强化学习实验中，B-Trans有效利用了群体的智慧，在实现优于确定性基线的任务表现的同时，也获得了更优的语义多样性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Despite their scale and success, modern transformers are almost universally trained as single-minded systems: optimization produces one deterministic set of parameters, representing a single functional hypothesis about the data.</div>
</details>
</div>
<div class="card">
<div class="title">On the geometry and topology of representations: the manifolds of modular addition</div>
<div class="meta-line">Authors: Gabriela Moisescu-Pareja, Gavin McCracken, Harley Wiltzer, Vincent Létourneau, Colin Daniels, Doina Precup, Jonathan Love</div>
<div class="meta-line">First: 2025-12-31T18:53:19+00:00 · Latest: 2025-12-31T18:53:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25060v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25060v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Clock and Pizza interpretations, associated with architectures differing in either uniform or learnable attention, were introduced to argue that different architectural designs can yield distinct circuits for modular addition. In this work, we show that this is not the case, and that both uniform attention and trainable attention architectures implement the same algorithm via topologically and geometrically equivalent representations. Our methodology goes beyond the interpretation of individual neurons and weights. Instead, we identify all of the neurons corresponding to each learned representation and then study the collective group of neurons as one entity. This method reveals that each learned representation is a manifold that we can study utilizing tools from topology. Based on this insight, we can statistically analyze the learned representations across hundreds of circuits to demonstrate the similarity between learned modular addition circuits that arise naturally from common deep learning paradigms.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于表示的几何与拓扑：模加法的流形</div>
<div class="mono" style="margin-top:8px">Clock和Pizza解释分别与在注意力机制上具有均匀或可学习差异的架构相关，被引入以论证不同的架构设计可以产生不同的模加法电路。在本工作中，我们证明了并非如此，均匀注意力和可训练注意力架构通过在拓扑和几何上等价的表示实现了相同的算法。我们的方法超越了对单个神经元和权重的解释。相反，我们识别出每个学习表示对应的全部神经元，然后将这些神经元视为一个整体进行研究。这种方法揭示了每个学习表示实际上是一个流形，我们可以利用拓扑学工具对其进行研究。基于这一见解，我们可以在数百个电路中对学习表示进行统计分析，以展示由常见深度学习范式自然产生的模加法电路之间的相似性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The Clock and Pizza interpretations, associated with architectures differing in either uniform or learnable attention, were introduced to argue that different architectural designs can yield distinct circuits for modular addition.</div>
</details>
</div>
<div class="card">
<div class="title">Reliable and Resilient Collective Communication Library for LLM Training and Serving</div>
<div class="meta-line">Authors: Wei Wang, Nengneng Yu, Sixian Xiong, Zaoxing Liu</div>
<div class="meta-line">First: 2025-12-31T18:53:11+00:00 · Latest: 2025-12-31T18:53:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25059v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25059v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern ML training and inference now span tens to tens of thousands of GPUs, where network faults can waste 10--15\% of GPU hours due to slow recovery. Common network errors and link fluctuations trigger timeouts that often terminate entire jobs, forcing expensive checkpoint rollback during training and request reprocessing during inference. We present R$^2$CCL, a fault-tolerant communication library that provides lossless, low-overhead failover by exploiting multi-NIC hardware. R$^2$CCL performs rapid connection migration, bandwidth-aware load redistribution, and resilient collective algorithms to maintain progress under failures. We evaluate R$^2$CCL on two 8-GPU H100 InfiniBand servers and via large-scale ML simulators modeling hundreds of GPUs with diverse failure patterns. Experiments show that R$^2$CCL is highly robust to NIC failures, incurring less than 1\% training and less than 3\% inference overheads. R$^2$CCL outperforms baselines AdapCC and DejaVu by 12.18$\times$ and 47$\times$, respectively.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于大语言模型训练和服务的可靠且具有弹性的集体通信库</div>
<div class="mono" style="margin-top:8px">现代机器学习训练和推理现在涉及数十到数千个GPU，其中网络故障会导致因缓慢恢复而浪费10--15\%的GPU小时。常见的网络错误和链路波动会触发超时，通常会终止整个任务，迫使训练过程中进行昂贵的检查点回滚和推理过程中重新处理请求。我们提出了R$^2$CCL，这是一个容错通信库，通过利用多NIC硬件提供无损、低开销的故障转移。R$^2$CCL通过快速连接迁移、带宽感知的负载重分配和具有弹性的集体算法，在故障情况下保持任务进展。我们在两台配备8个H100 InfiniBand的服务器以及模拟数百个GPU并具有多种故障模式的大规模机器学习模拟器上评估了R$^2$CCL。实验表明，R$^2$CCL对NIC故障具有高度鲁棒性，训练和推理的开销分别低于1\%和3\%。与基线AdapCC和DejaVu相比，R$^2$CCL分别提升了12.18$\times$和47$\times$。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modern ML training and inference now span tens to tens of thousands of GPUs, where network faults can waste 10--15\% of GPU hours due to slow recovery.</div>
</details>
</div>
<div class="card">
<div class="title">Context-aware LLM-based AI Agents for Human-centered Energy Management Systems in Smart Buildings</div>
<div class="meta-line">Authors: Tianzhi He, Farrokh Jazizadeh</div>
<div class="meta-line">First: 2025-12-31T18:51:19+00:00 · Latest: 2025-12-31T18:51:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25055v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25055v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study presents a conceptual framework and a prototype assessment for Large Language Model (LLM)-based Building Energy Management System (BEMS) AI agents to facilitate context-aware energy management in smart buildings through natural language interaction. The proposed framework comprises three modules: perception (sensing), central control (brain), and action (actuation and user interaction), forming a closed feedback loop that captures, analyzes, and interprets energy data to respond intelligently to user queries and manage connected appliances. By leveraging the autonomous data analytics capabilities of LLMs, the BEMS AI agent seeks to offer context-aware insights into energy consumption, cost prediction, and device scheduling, thereby addressing limitations in existing energy management systems. The prototype&#x27;s performance was evaluated using 120 user queries across four distinct real-world residential energy datasets and different evaluation metrics, including latency, functionality, capability, accuracy, and cost-effectiveness. The generalizability of the framework was demonstrated using ANOVA tests. The results revealed promising performance, measured by response accuracy in device control (86%), memory-related tasks (97%), scheduling and automation (74%), and energy analysis (77%), while more complex cost estimation tasks highlighted areas for improvement with an accuracy of 49%. This benchmarking study moves toward formalizing the assessment of LLM-based BEMS AI agents and identifying future research directions, emphasizing the trade-off between response accuracy and computational efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向以人为本的智能建筑能源管理系统的情境感知LLM型AI代理</div>
<div class="mono" style="margin-top:8px">本研究提出了一种基于大型语言模型（LLM）的建筑能源管理系统（BEMS）AI代理的概念框架和原型评估，旨在通过自然语言交互促进智能建筑中的情境感知能源管理。所提出的框架包含三个模块：感知（传感）、中央控制（大脑）和行动（执行与用户交互），形成一个闭环反馈机制，用于捕捉、分析和解释能源数据，以智能响应用户查询并管理连接设备。通过利用LLM的自主数据分析能力，BEMS AI代理旨在提供情境感知的能源消耗洞察、成本预测和设备调度，从而解决现有能源管理系统中的局限性。原型性能评估使用了四个不同的真实住宅能源数据集和120个用户查询，采用包括延迟、功能、能力、准确性和成本效益在内的多种评估指标。通过方差分析（ANOVA）测试展示了该框架的可推广性。结果表明，原型在设备控制（86%）、与记忆相关的任务（97%）、调度与自动化（74%）以及能源分析（77%）方面的响应准确率表现良好，而在更复杂的成本估算任务中则显示出需要改进的领域，准确率为49%。这项基准研究旨在正式化对基于LLM的BEMS AI代理的评估，并确定未来研究方向，强调响应准确率与计算效率之间的权衡。</div>
</details>
</div>
<div class="card">
<div class="title">AdaGReS:Adaptive Greedy Context Selection via Redundancy-Aware Scoring for Token-Budgeted RAG</div>
<div class="meta-line">Authors: Chao Peng, Bin Wang, Zhilei Long, Jinfang Sheng</div>
<div class="meta-line">First: 2025-12-31T18:48:07+00:00 · Latest: 2025-12-31T18:48:07+00:00</div>
<div class="meta-line">Comments: Preprint. Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25052v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25052v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval-augmented generation (RAG) is highly sensitive to the quality of selected context, yet standard top-k retrieval often returns redundant or near-duplicate chunks that waste token budget and degrade downstream generation. We present AdaGReS, a redundancy-aware context selection framework for token-budgeted RAG that optimizes a set-level objective combining query-chunk relevance and intra-set redundancy penalties. AdaGReS performs greedy selection under a token-budget constraint using marginal gains derived from the objective, and introduces a closed-form, instance-adaptive calibration of the relevance-redundancy trade-off parameter to eliminate manual tuning and adapt to candidate-pool statistics and budget limits. We further provide a theoretical analysis showing that the proposed objective exhibits epsilon-approximate submodularity under practical embedding similarity conditions, yielding near-optimality guarantees for greedy selection. Experiments on open-domain question answering (Natural Questions) and a high-redundancy biomedical (drug) corpus demonstrate consistent improvements in redundancy control and context quality, translating to better end-to-end answer quality and robustness across settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AdaGReS:一种基于冗余感知评分的自适应贪心上下文选择方法用于带token预算的RAG</div>
<div class="mono" style="margin-top:8px">检索增强生成（RAG）高度依赖于所选上下文的质量，但标准的top-k检索常常返回冗余或近似重复的片段，浪费token预算并降低下游生成效果。我们提出了AdaGReS，这是一个针对带token预算的RAG的冗余感知上下文选择框架，通过结合查询-片段相关性和集合内冗余惩罚的集合级目标进行优化。AdaGReS在token预算约束下，利用目标函数推导出的边际增益进行贪心选择，并引入一种闭式、实例自适应的校准方法，以消除手动调参并适应候选池统计信息和预算限制。我们进一步提供了理论分析，表明在实际嵌入相似性条件下，所提出的优化目标具有epsilon近似子模性，从而为贪心选择提供近似最优性保证。在开放域问答（Natural Questions）和高冗余生物医学（药物）语料库上的实验表明，AdaGReS在冗余控制和上下文质量方面表现出一致的改进，从而提升了端到端答案的质量和鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">End-to-End Test-Time Training for Long Context</div>
<div class="meta-line">Authors: Arnuv Tandon, Karan Dalal, Xinhao Li, Daniel Koceja, Marcel Rød, Sam Buchanan, Xiaolong Wang, Jure Leskovec, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin, Jed McCaleb, Yejin Choi, Yu Sun</div>
<div class="meta-line">First: 2025-12-29T18:30:14+00:00 · Latest: 2025-12-31T18:41:09+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/test-time-training/e2e</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23675v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.23675v2">PDF</a> · <a href="https://github.com/test-time-training/e2e">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model&#x27;s initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向长上下文的端到端测试时训练</div>
<div class="mono" style="margin-top:8px">我们将长上下文语言建模问题视为持续学习问题，而非架构设计问题。在此框架下，我们仅使用标准架构——带滑动窗口注意力的Transformer。然而，我们的模型通过在给定上下文中进行下一个token预测，在测试时持续学习，将读取的上下文压缩到模型权重中。此外，我们通过训练时的元学习改进模型的初始化，以提升测试时学习效果。总体而言，我们的方法是一种测试时训练（Test-Time Training, TTT）形式，既在测试时（通过下一个token预测）又在训练时（通过元学习）实现端到端（End-to-End, E2E），与以往形式不同。我们进行了大量实验，重点关注扩展性。特别是，对于使用1640亿token训练的30亿参数模型，我们的方法（TTT-E2E）在上下文长度上的扩展性与全注意力Transformer相同，而其他方法如Mamba 2和Gated DeltaNet则不具备此特性。然而，与RNN类似，TTT-E2E的推理延迟与上下文长度无关，使其在128K上下文长度下比全注意力Transformer快2.7倍。我们的代码已公开。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Classifiers Avoid Shortcut Solutions</div>
<div class="meta-line">Authors: Alexander C. Li, Ananya Kumar, Deepak Pathak</div>
<div class="meta-line">Venue: ICLR 2025</div>
<div class="meta-line">First: 2025-12-31T18:31:46+00:00 · Latest: 2025-12-31T18:31:46+00:00</div>
<div class="meta-line">Comments: ICLR 2025. Code: https://github.com/alexlioralexli/generative-classifiers</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25034v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25034v1">PDF</a> · <a href="https://github.com/alexlioralexli/generative-classifiers">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Discriminative approaches to classification often learn shortcuts that hold in-distribution but fail even under minor distribution shift. This failure mode stems from an overreliance on features that are spuriously correlated with the label. We show that generative classifiers, which use class-conditional generative models, can avoid this issue by modeling all features, both core and spurious, instead of mainly spurious ones. These generative classifiers are simple to train, avoiding the need for specialized augmentations, strong regularization, extra hyperparameters, or knowledge of the specific spurious correlations to avoid. We find that diffusion-based and autoregressive generative classifiers achieve state-of-the-art performance on five standard image and text distribution shift benchmarks and reduce the impact of spurious correlations in realistic applications, such as medical or satellite datasets. Finally, we carefully analyze a Gaussian toy setting to understand the inductive biases of generative classifiers, as well as the data properties that determine when generative classifiers outperform discriminative ones.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成分类器避免捷径解法</div>
<div class="mono" style="margin-top:8px">判别式分类方法通常会学习到在分布内有效但在分布微小偏移时失效的捷径。这种失效模式源于对与标签存在虚假相关性的特征的过度依赖。我们证明，使用类条件生成模型的生成分类器可以通过建模所有特征（包括核心特征和虚假特征），而非主要依赖虚假特征，从而避免这一问题。这些生成分类器易于训练，无需专门的增强、强正则化、额外超参数或对特定虚假相关性的了解。我们发现，基于扩散模型和自回归模型的生成分类器在五个标准的图像和文本分布偏移基准测试中达到了最先进的性能，并在现实应用（如医学或卫星数据集）中减少了虚假相关性的影响。最后，我们仔细分析了一个高斯玩具设置，以理解生成分类器的归纳偏差，以及决定生成分类器优于判别分类器的数据特性。</div>
</details>
</div>
<div class="card">
<div class="title">Plan Verification for LLM-Based Embodied Task Completion Agents</div>
<div class="meta-line">Authors: Ananth Hariharan, Vardhan Dongre, Dilek Hakkani-Tür, Gokhan Tur</div>
<div class="meta-line">First: 2025-09-02T19:06:56+00:00 · Latest: 2025-12-31T18:31:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.02761v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.02761v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM) based task plans and corresponding human demonstrations for embodied AI may be noisy, with unnecessary actions, redundant navigation, and logical errors that reduce policy quality. We propose an iterative verification framework in which a Judge LLM critiques action sequences and a Planner LLM applies the revisions, yielding progressively cleaner and more spatially coherent trajectories. Unlike rule-based approaches, our method relies on natural language prompting, enabling broad generalization across error types including irrelevant actions, contradictions, and missing steps. On a set of manually annotated actions from the TEACh embodied AI dataset, our framework achieves up to 90% recall and 100% precision across four state-of-the-art LLMs (GPT o4-mini, DeepSeek-R1, Gemini 2.5, LLaMA 4 Scout). The refinement loop converges quickly, with 96.5% of sequences requiring at most three iterations, while improving both temporal efficiency and spatial action organization. Crucially, the method preserves human error-recovery patterns rather than collapsing them, supporting future work on robust corrective behavior. By establishing plan verification as a reliable LLM capability for spatial planning and action refinement, we provide a scalable path to higher-quality training data for imitation learning in embodied AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的具身任务完成代理的计划验证</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的任务计划和对应的具身AI人类演示可能包含噪声，如不必要的动作、冗余的导航和逻辑错误，从而降低策略质量。我们提出了一种迭代验证框架，其中Judge LLM对动作序列进行批评，Planner LLM应用修改，从而生成越来越干净且空间上连贯的轨迹。与基于规则的方法不同，我们的方法依赖自然语言提示，能够广泛泛化到各种错误类型，包括无关动作、矛盾和缺失步骤。在TEACh具身AI数据集的手动注释动作集上，我们的框架在四个最先进的LLM（GPT o4-mini、DeepSeek-R1、Gemini 2.5、LLaMA 4 Scout）上实现了高达90%的召回率和100%的精确率。精炼循环收敛迅速，96.5%的序列最多需要三次迭代，同时提高了时间效率和空间动作组织。关键的是，该方法保留了人类的错误恢复模式，而非将其压缩，支持未来关于鲁棒性校正行为的研究。通过将计划验证确立为LLM在空间规划和动作精炼中的可靠能力，我们为具身AI的模仿学习提供了高质量训练数据的可扩展路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language model (LLM) based task plans and corresponding human demonstrations for embodied AI may be noisy, with unnecessary actions, redundant navigation, and logical errors that reduce policy quality.</div>
</details>
</div>
<div class="card">
<div class="title">Fractal conduction pathways governing ionic transport in a glass</div>
<div class="meta-line">Authors: J. L. Iguain, F. O. Sanchez-Varreti, M. A. Frechero</div>
<div class="meta-line">First: 2025-12-31T18:29:50+00:00 · Latest: 2025-12-31T18:29:50+00:00</div>
<div class="meta-line">Comments: 6 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25031v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25031v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a systematic characterization of the fractal conduction pathways governing ionic transport in a non-crystalline solid below the glass-transition temperature. Using classical molecular dynamics simulations of lithium metasilicate, we combine mobility-resolved dynamical analysis with a real-space description of the regions explored by lithium ions. Ensemble-averaged velocity autocorrelation functions rapidly decorrelate and do not resolve the pronounced dynamic heterogeneity of the system, whereas single-ion analysis reveals short-lived episodes of nearly collinear motion. By mapping active-site clusters over increasing time windows, we show that ion-conducting pathways are quasi one-dimensional at short times and evolve into larger, branched structures characterized by a robust fractal dimension $d_f\simeq1.7$. This geometry persists while the silicate backbone remains structurally arrested, whereas near the glass-transition temperature the loss of structural memory leads to the reappearance of small clusters. These results provide a real-space structural interpretation of ionic transport in non-crystalline solids and support fractal pathway models of high-frequency ionic response.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>玻璃中离子传输的分形传导路径</div>
<div class="mono" style="margin-top:8px">我们系统地表征了非晶固体在玻璃转变温度以下控制离子传输的分形传导路径。通过锂甲硅酸盐的经典分子动力学模拟，我们结合了基于迁移率的动力学分析与锂离子探索区域的实空间描述。集合平均的速度自相关函数迅速去相关，无法揭示系统中显著的动力学异质性，而单离子分析则显示出短暂的近直线运动事件。通过在时间窗口逐渐增加的情况下映射活性位点簇，我们表明在短时间尺度下，离子传导路径是准一维的，并逐渐演变为具有稳健分形维数 $d_f\simeq1.7$ 的更大、分支结构。这种几何结构在硅酸盐主链保持结构冻结时持续存在，而在接近玻璃转变温度时，结构记忆的丧失导致小簇的重新出现。这些结果为非晶固体中的离子传输提供了实空间结构解释，并支持高频率离子响应的分形路径模型。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Generalisable Foundation Models for Brain MRI</div>
<div class="meta-line">Authors: Moona Mazher, Geoff J. M. Parker, Daniel C. Alexander</div>
<div class="meta-line">First: 2025-10-27T15:19:46+00:00 · Latest: 2025-12-31T18:26:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.23415v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.23415v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Foundation models in artificial intelligence (AI) are transforming medical imaging by enabling general-purpose feature learning from large-scale, unlabeled datasets. In this work, we introduce BrainFound, a self-supervised foundation model for brain MRI, built by extending DINO-v2, a vision transformer originally designed for 2D natural images. BrainFound adapts DINO-v2 to model full 3D brain anatomy by incorporating volumetric information from sequential MRI slices, moving beyond conventional single-slice paradigms. It supports both single- and multimodal inputs, enabling a broad range of downstream tasks, including disease detection and image segmentation, while generalising across varied imaging protocols and clinical scenarios. We show that BrainFound consistently outperforms existing self-supervised pretraining strategies and supervised baselines, particularly in label-scarce and multi-contrast settings. By integrating information from diverse 3D MRI modalities (e.g., T1, T2, FLAIR), it enhances diagnostic accuracy and reduces dependency on extensive expert annotations. This flexibility makes BrainFound a scalable and practical solution for 3D neuroimaging pipelines, with significant potential for clinical deployment and research innovation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向脑部MRI的通用基础模型</div>
<div class="mono" style="margin-top:8px">人工智能（AI）中的基础模型正在通过从大规模、未标记数据集中进行通用特征学习，改变医学影像领域。在本工作中，我们引入了BrainFound，这是一个用于脑部MRI的自监督基础模型，其基于DINO-v2（最初为2D自然图像设计的视觉Transformer）进行扩展。BrainFound通过整合顺序MRI切片的体积信息，将DINO-v2适应于建模完整的3D脑解剖结构，超越了传统的单层处理范式。它支持单模态和多模态输入，能够广泛应用于下游任务，如疾病检测和图像分割，同时在不同成像协议和临床场景中具有良好的泛化能力。我们证明BrainFound在标签稀缺和多对比度设置中，持续优于现有的自监督预训练策略和监督基线。通过整合来自多种3D MRI模态（如T1、T2、FLAIR）的信息，它提高了诊断准确性并减少了对大量专家标注的依赖。这种灵活性使BrainFound成为3D神经影像处理流程的可扩展且实用的解决方案，在临床部署和研究创新方面具有巨大潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Modeling Language as a Sequence of Thoughts</div>
<div class="meta-line">Authors: Nasim Borazjanizadeh, James McClelland</div>
<div class="meta-line">First: 2025-12-31T18:24:57+00:00 · Latest: 2025-12-31T18:24:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25026v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25026v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformer language models can generate strikingly natural text by modeling language as a sequence of tokens. Yet, by relying primarily on surface-level co-occurrence statistics, they fail to form globally consistent latent representations of entities and events, lack of which contributes to brittleness in relational direction (e.g., reversal curse), contextualization errors, and data inefficiency. On the other hand, cognitive science shows that human comprehension involves converting the input linguistic stream into compact, event-like representations that persist in memory while verbatim form is short-lived. Motivated by this view, we introduce Thought Gestalt (TG) model, a recurrent Transformer that models language at two levels of abstraction - tokens and sentence-level &quot;thought&quot; states. TG generates the tokens of one sentence at a time while cross-attending to a memory of prior sentence representations. In TG, token and sentence representations are generated using the same set of model parameters and trained with a single objective, the next-token cross-entropy: by retaining the computation graph of sentence representations written to memory, gradients from future token losses flow backward through cross-attention to optimize the parameters generating earlier sentence vectors. In scaling experiments, TG consistently improves efficiency over matched GPT-2 runs, among other baselines, with scaling fits indicating GPT-2 requires ~5-8% more data and ~33-42% more parameters to match TG&#x27;s loss. TG also reduces errors on relational direction generalization on a father-son reversal curse probe.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将语言建模为思想序列</div>
<div class="mono" style="margin-top:8px">Transformer语言模型通过将语言建模为标记序列，可以生成非常自然的文本。然而，它们主要依赖于表层共现统计信息，无法形成全局一致的实体和事件的潜在表示，这种不足导致了关系方向上的脆弱性（如反转诅咒）、语境化错误和数据效率低下。另一方面，认知科学表明，人类的理解过程涉及将输入的语流转换为紧凑的、事件式的表示，这些表示在记忆中持久存在，而逐字形式则短暂易逝。受此观点启发，我们引入了Thought Gestalt（TG）模型，这是一种递归Transformer，它在两个抽象层次上建模语言：标记和句子级的“思想”状态。TG逐句生成标记，同时通过交叉注意力机制参考先前句子表示的记忆。在TG中，标记和句子表示使用相同的模型参数集，并以单一目标（下一个标记交叉熵）进行训练：通过保留写入记忆中的句子表示的计算图，未来标记损失的梯度可以反向流经交叉注意力机制，以优化生成早期句子向量的参数。在扩展实验中，TG在与其他基线模型（如匹配的GPT-2运行）相比时，持续提高了效率，其扩展拟合表明GPT-2需要约5-8%更多的数据和33-42%更多的参数才能达到TG的损失水平。此外，TG还在父亲-儿子反转诅咒测试中减少了关系方向泛化错误。</div>
</details>
</div>
<div class="card">
<div class="title">ResponseRank: Data-Efficient Reward Modeling through Preference Strength Learning</div>
<div class="meta-line">Authors: Timo Kaufmann, Yannick Metz, Daniel Keim, Eyke Hüllermeier</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-12-31T18:21:52+00:00 · Latest: 2025-12-31T18:21:52+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25023v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25023v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Binary choices, as often used for reinforcement learning from human feedback (RLHF), convey only the direction of a preference. A person may choose apples over oranges and bananas over grapes, but which preference is stronger? Strength is crucial for decision-making under uncertainty and generalization of preference models, but hard to measure reliably. Metadata such as response times and inter-annotator agreement can serve as proxies for strength, but are often noisy and confounded. We propose ResponseRank to address the challenge of learning from noisy strength signals. Our method uses relative differences in proxy signals to rank responses to pairwise comparisons by their inferred preference strength. To control for systemic variation, we compare signals only locally within carefully constructed strata. This enables robust learning of utility differences consistent with strength-derived rankings while making minimal assumptions about the strength signal. Our contributions are threefold: (1) ResponseRank, a novel method that robustly learns preference strength by leveraging locally valid relative strength signals; (2) empirical evidence of improved sample efficiency and robustness across diverse tasks: synthetic preference learning (with simulated response times), language modeling (with annotator agreement), and RL control tasks (with simulated episode returns); and (3) the Pearson Distance Correlation (PDC), a novel metric that isolates cardinal utility learning from ordinal accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ResponseRank: 通过偏好强度学习实现数据高效的奖励建模</div>
<div class="mono" style="margin-top:8px">二元选择，如常用于人类反馈强化学习（RLHF）中，仅能传达偏好的方向。一个人可能选择苹果胜过橘子，香蕉胜过葡萄，但哪一种偏好更强？强度对于在不确定性下的决策和偏好模型的泛化至关重要，但难以可靠地测量。诸如响应时间与标注者间一致性等元数据可以作为强度的代理指标，但往往存在噪声和混淆。我们提出ResponseRank来应对从噪声强度信号中学习的挑战。我们的方法利用代理信号中的相对差异对成对比较的响应进行排序，以推断其偏好强度。为了控制系统性变化，我们仅在精心构建的层（strata）内进行信号的局部比较。这使得在对强度信号做出最少假设的情况下，能够稳健地学习出与强度推导排序一致的效用差异。我们的贡献有三个方面：(1) ResponseRank，一种新颖的方法，通过利用局部有效的相对强度信号来稳健地学习偏好强度；(2) 在多种任务中实证展示了样本效率和鲁棒性的提升：合成偏好学习（使用模拟响应时间）、语言建模（使用标注者一致性）以及强化学习控制任务（使用模拟回合回报）；(3) 皮尔逊距离相关性（PDC），一种新颖的指标，能够将基数效用学习与序数准确性分离。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Binary choices, as often used for reinforcement learning from human feedback (RLHF), convey only the direction of a preference.</div>
</details>
</div>
<div class="card">
<div class="title">Statistical Taylor Expansion: A New and Path-Independent Method for Uncertainty Analysis</div>
<div class="meta-line">Authors: Chengpu Wang</div>
<div class="meta-line">First: 2024-10-02T04:02:21+00:00 · Latest: 2025-12-31T18:21:44+00:00</div>
<div class="meta-line">Comments: 83 pages, 66 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.01223v14">Abs</a> · <a href="https://arxiv.org/pdf/2410.01223v14">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As a rigorous statistical approach, statistical Taylor expansion extends the conventional Taylor expansion by replacing precise input variables with random variables of known distributions, to compute means and standard deviations of the results. Statistical Taylor expansion traces the dependency of the input uncertainties in the intermediate steps, so that the variables in the intermediate analytic expressions can no longer be regarded as independent of each other, and the result of the analytic expression is path independent. Thus, it differs fundamentally from the conventional common approaches in applied mathematics which optimize execution path for each calculation. In fact, statistical Taylor expansion may standardize numerical calculations for analytic expressions. Its statistical nature allows religious testing of its result when the sample size is large enough. This paper also introduces an implementation of statistical Taylor expansion called variance arithmetic and presents corresponding test results in a very wide range of mathematical applications.
  Another important conclusion of this paper is that the numerical errors in the library function can have significant effects on the result. For example, the periodic numerical errors in the trigonometric library functions can resonate with periodic signals, producing large numerical errors in the results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>统计泰勒展开：一种用于不确定性分析的新方法，且与路径无关</div>
<div class="mono" style="margin-top:8px">作为一种严谨的统计方法，统计泰勒展开通过将精确输入变量替换为已知分布的随机变量，来计算结果的均值和标准差。统计泰勒展开追踪输入不确定性在中间步骤中的依赖关系，使得中间解析表达式中的变量不能再被视为彼此独立，且解析表达式的结果与路径无关。因此，它与应用数学中传统方法在每一步计算中优化执行路径有本质区别。事实上，统计泰勒展开可能标准化解析表达式的数值计算。其统计特性使得当样本量足够大时，可以对结果进行严格的检验。本文还介绍了统计泰勒展开的一种实现方式，称为方差算术，并在非常广泛的应用数学场景中展示了相应的测试结果。另一个重要结论是，库函数中的数值误差会对结果产生显著影响。例如，三角函数库函数中的周期性数值误差可能与周期性信号产生共振，从而在结果中产生较大的数值误差。</div>
</details>
</div>
<div class="card">
<div class="title">Convergence of the generalization error for deep gradient flow methods for PDEs</div>
<div class="meta-line">Authors: Chenguang Liu, Antonis Papapantoleon, Jasper Rou</div>
<div class="meta-line">First: 2025-12-31T18:11:51+00:00 · Latest: 2025-12-31T18:11:51+00:00</div>
<div class="meta-line">Comments: 28 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25017v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25017v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The aim of this article is to provide a firm mathematical foundation for the application of deep gradient flow methods (DGFMs) for the solution of (high-dimensional) partial differential equations (PDEs). We decompose the generalization error of DGFMs into an approximation and a training error. We first show that the solution of PDEs that satisfy reasonable and verifiable assumptions can be approximated by neural networks, thus the approximation error tends to zero as the number of neurons tends to infinity. Then, we derive the gradient flow that the training process follows in the ``wide network limit&#x27;&#x27; and analyze the limit of this flow as the training time tends to infinity. These results combined show that the generalization error of DGFMs tends to zero as the number of neurons and the training time tend to infinity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度梯度流方法求解偏微分方程的泛化误差收敛性</div>
<div class="mono" style="margin-top:8px">本文旨在为深度梯度流方法（DGFMs）在求解（高维）偏微分方程（PDEs）中的应用提供坚实的数学基础。我们将DGFMs的泛化误差分解为逼近误差和训练误差。首先，我们证明满足合理且可验证假设的PDE解可以被神经网络逼近，因此当神经元数量趋于无穷时，逼近误差趋于零。接着，我们在“宽网络极限”下推导训练过程所遵循的梯度流，并分析当训练时间趋于无穷时该流的极限。这些结果结合起来表明，当神经元数量和训练时间趋于无穷时，DGFMs的泛化误差趋于零。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The aim of this article is to provide a firm mathematical foundation for the application of deep gradient flow methods (DGFMs) for the solution of (high-dimensional) partial differential equations (PDEs).</div>
</details>
</div>
<div class="card">
<div class="title">Spiking Manifesto</div>
<div class="meta-line">Authors: Eugene Izhikevich</div>
<div class="meta-line">First: 2025-12-03T23:44:02+00:00 · Latest: 2025-12-31T18:03:46+00:00</div>
<div class="meta-line">Comments: This is a declaration of principles and a roadmap for spiking networks, intended as a manifesto rather than a conventional research article</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11843v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.11843v2">PDF</a> · <a href="https://github.com/izhikevich/SNN">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Practically everything computers do is better, faster, and more power-efficient than the brain. For example, a calculator performs numerical computations more energy-efficiently than any human. Yet modern AI models are a thousand times less efficient than the brain. These models rely on larger and larger artificial neural networks (ANNs) to boost their encoding capacity, requiring GPUs to perform large-scale matrix multiplications. In contrast, the brain&#x27;s spiking neural networks (SNNs) exhibit factorially explosive encoding capacity and compute through the polychronization of spikes rather than explicit matrix-vector products, resulting in lower energy requirements. This manifesto proposes a paradigm for framing popular AI models in terms of spiking networks and polychronization, and for interpreting spiking activity as nature&#x27;s way of implementing look-up tables. This suggests a path toward converting AI models into a novel class of architectures with much smaller size yet combinatorially large encoding capacity, offering the promise of a thousandfold improvement in performance. Code is available at https://github.com/izhikevich/SNN</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>脉冲宣言</div>
<div class="mono" style="margin-top:8px">实际上，计算机所做的几乎所有事情都比大脑更高效、更快且更省电。例如，计算器在执行数值计算时比任何人类都更节能。然而，现代人工智能模型的效率却比大脑低一千倍。这些模型依赖越来越大的人工神经网络（ANNs）来提升其编码能力，需要GPU来进行大规模矩阵乘法运算。相比之下，大脑的脉冲神经网络（SNNs）表现出阶乘级爆炸性的编码能力，并通过脉冲的多时钟同步进行计算，而不是显式的矩阵-向量乘积，从而实现更低的能耗。本宣言提出了一种将主流人工智能模型框架化为脉冲网络和多时钟同步的方法，并将脉冲活动解释为自然界实现查找表的方式。这为将人工智能模型转换为一种新型架构提供了路径，这种架构体积更小，但编码能力呈组合式增长，有望实现性能千倍的提升。代码可在 https://github.com/izhikevich/SNN 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Practically everything computers do is better, faster, and more power-efficient than the brain.</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Language Models are Provably Optimal Parallel Samplers</div>
<div class="meta-line">Authors: Haozhe Jiang, Nika Haghtalab, Lijie Chen</div>
<div class="meta-line">First: 2025-12-31T18:03:05+00:00 · Latest: 2025-12-31T18:03:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25014v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25014v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion language models (DLMs) have emerged as a promising alternative to autoregressive models for faster inference via parallel token generation. We provide a rigorous foundation for this advantage by formalizing a model of parallel sampling and showing that DLMs augmented with polynomial-length chain-of-thought (CoT) can simulate any parallel sampling algorithm using an optimal number of sequential steps. Consequently, whenever a target distribution can be generated using a small number of sequential steps, a DLM can be used to generate the distribution using the same number of optimal sequential steps. However, without the ability to modify previously revealed tokens, DLMs with CoT can still incur large intermediate footprints. We prove that enabling remasking (converting unmasked tokens to masks) or revision (converting unmasked tokens to other unmasked tokens) together with CoT further allows DLMs to simulate any parallel sampling algorithm with optimal space complexity. We further justify the advantage of revision by establishing a strict expressivity gap: DLMs with revision or remasking are strictly more expressive than those without. Our results not only provide a theoretical justification for the promise of DLMs as the most efficient parallel sampler, but also advocate for enabling revision in DLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散语言模型是可证明最优的并行采样器</div>
<div class="mono" style="margin-top:8px">扩散语言模型（DLMs）作为一种替代自回归模型的并行生成方法，已被证明在推理速度上有很大潜力。我们通过形式化并行采样的模型，并展示DLMs结合多项式长度的思维链（CoT）可以使用最优数量的顺序步骤模拟任何并行采样算法。因此，只要目标分布可以通过少量顺序步骤生成，DLMs就可以使用相同数量的最优顺序步骤生成该分布。然而，如果没有修改之前生成的token的能力，DLMs仍可能产生较大的中间足迹。我们证明，当允许重新掩码（将未掩码token转换为掩码）或修订（将未掩码token转换为其他未掩码token）与CoT结合时，DLMs可以以最优的空间复杂度模拟任何并行采样算法。我们进一步通过建立一个严格的表达性差距来证明修订的优势：具有修订或重新掩码的DLMs比没有这些功能的DLMs具有更强的表达能力。我们的结果不仅为DLMs作为最有效并行采样器的潜力提供了理论依据，还倡导在DLMs中启用修订功能。</div>
</details>
</div>
<div class="card">
<div class="title">FoundationSLAM: Unleashing the Power of Depth Foundation Models for End-to-End Dense Visual SLAM</div>
<div class="meta-line">Authors: Yuchen Wu, Jiahe Li, Fabio Tosi, Matteo Poggi, Jin Zheng, Xiao Bai</div>
<div class="meta-line">First: 2025-12-31T17:57:45+00:00 · Latest: 2025-12-31T17:57:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25008v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25008v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present FoundationSLAM, a learning-based monocular dense SLAM system that addresses the absence of geometric consistency in previous flow-based approaches for accurate and robust tracking and mapping. Our core idea is to bridge flow estimation with geometric reasoning by leveraging the guidance from foundation depth models. To this end, we first develop a Hybrid Flow Network that produces geometry-aware correspondences, enabling consistent depth and pose inference across diverse keyframes. To enforce global consistency, we propose a Bi-Consistent Bundle Adjustment Layer that jointly optimizes keyframe pose and depth under multi-view constraints. Furthermore, we introduce a Reliability-Aware Refinement mechanism that dynamically adapts the flow update process by distinguishing between reliable and uncertain regions, forming a closed feedback loop between matching and optimization. Extensive experiments demonstrate that FoundationSLAM achieves superior trajectory accuracy and dense reconstruction quality across multiple challenging datasets, while running in real-time at 18 FPS, demonstrating strong generalization to various scenarios and practical applicability of our method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FoundationSLAM：利用深度基础模型释放端到端密集视觉SLAM的潜力</div>
<div class="mono" style="margin-top:8px">我们提出了FoundationSLAM，这是一个基于学习的单目密集SLAM系统，解决了先前基于光流的方法在几何一致性方面的缺失，从而实现准确且鲁棒的跟踪和建图。我们的核心思想是通过利用基础深度模型的指导，将光流估计与几何推理相结合。为此，我们首先开发了一个混合光流网络，生成具有几何感知的对应关系，从而在多样化的关键帧之间实现一致的深度和姿态推断。为了强制全局一致性，我们提出了一种双一致性捆绑调整层，通过多视角约束联合优化关键帧姿态和深度。此外，我们引入了一种可靠性感知的细化机制，通过区分可靠区域和不确定区域，动态调整光流更新过程，形成匹配与优化之间的闭环反馈。大量实验表明，FoundationSLAM在多个具有挑战性的数据集上实现了优越的轨迹精度和密集重建质量，同时在18 FPS下实时运行，展示了其对各种场景的强大泛化能力和方法的实际应用价值。</div>
</details>
</div>
<div class="card">
<div class="title">Hybrid Learning: A Novel Combination of Self-Supervised and Supervised Learning for Joint MRI Reconstruction and Denoising in Low-Field MRI</div>
<div class="meta-line">Authors: Haoyang Pei, Nikola Janjuvsevic, Renqing Luo, Ding Xia, Xiang Xu, William Moore, Yao Wang, Hersh Chandarana, Li Feng</div>
<div class="meta-line">First: 2025-05-09T00:35:14+00:00 · Latest: 2025-12-31T17:51:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.05703v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.05703v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep learning has demonstrated strong potential for MRI reconstruction. However, conventional supervised learning requires high-quality, high-SNR references for network training, which are often difficult or impossible to obtain in different scenarios, particularly in low-field MRI. Self-supervised learning provides an alternative by removing the need for training references, but its reconstruction performance can degrade when the baseline SNR is low. To address these limitations, we propose hybrid learning, a two-stage training framework that integrates self-supervised and supervised learning for joint MRI reconstruction and denoising when only low-SNR training references are available. Hybrid learning is implemented in two sequential stages. In the first stage, self-supervised learning is applied to fully sampled low-SNR data to generate higher-quality pseudo-references. In the second stage, these pseudo-references are used as targets for supervised learning to reconstruct and denoise undersampled noisy data. The proposed technique was evaluated in multiple experiments involving simulated and real low-field MRI in the lung and brain at different field strengths. Hybrid learning consistently improved image quality over both standard self-supervised learning and supervised learning with noisy training references at different acceleration rates, noise levels, and field strengths, achieving higher SSIM and lower NMSE. The hybrid learning approach is effective for both Cartesian and non-Cartesian acquisitions. Hybrid learning provides an effective solution for training deep MRI reconstruction models in the absence of high-SNR references. By improving image quality in low-SNR settings, particularly for low-field MRI, it holds promise for broader clinical adoption of deep learning-based reconstruction methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>混合学习：一种自监督与监督学习的新型结合方法，用于低场强MRI中的联合图像重建与降噪</div>
<div class="mono" style="margin-top:8px">深度学习在MRI图像重建中展现出强大的潜力。然而，传统监督学习需要高质量、高信噪比（SNR）的参考图像进行网络训练，这在不同场景下，尤其是低场强MRI中，往往难以获得。自监督学习通过去除对训练参考图像的依赖提供了一种替代方案，但在基线SNR较低时，其重建性能可能会下降。为了解决这些限制，我们提出了一种混合学习方法，这是一种两阶段的训练框架，用于在仅有低SNR训练参考图像的情况下进行MRI图像的联合重建与降噪。混合学习分为两个连续的阶段：第一阶段使用自监督学习对完全采样的低SNR数据进行处理，以生成高质量的伪参考图像；第二阶段则利用这些伪参考图像作为监督学习的目标，对欠采样的噪声数据进行重建和降噪。该方法在涉及不同场强的肺部和脑部低场强MRI的模拟和真实数据实验中进行了评估。在不同的加速率、噪声水平和场强条件下，混合学习在图像质量上均优于标准的自监督学习和使用噪声训练参考的监督学习，实现了更高的SSIM值和更低的NMSE值。混合学习方法适用于Cartesian和非Cartesian采集方式。该方法为在缺乏高SNR参考图像的情况下训练深度MRI重建模型提供了一种有效的解决方案。通过在低SNR环境下提升图像质量，特别是在低场强MRI中，它为基于深度学习的重建方法在临床中的广泛应用提供了前景。</div>
</details>
</div>
<div class="card">
<div class="title">Bi-C2R: Bidirectional Continual Compatible Representation for Re-indexing Free Lifelong Person Re-identification</div>
<div class="meta-line">Authors: Zhenyu Cui, Jiahuan Zhou, Yuxin Peng</div>
<div class="meta-line">First: 2025-12-31T17:50:05+00:00 · Latest: 2025-12-31T17:50:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25000v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.25000v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Lifelong person Re-IDentification (L-ReID) exploits sequentially collected data to continuously train and update a ReID model, focusing on the overall performance of all data. Its main challenge is to avoid the catastrophic forgetting problem of old knowledge while training on new data. Existing L-ReID methods typically re-extract new features for all historical gallery images for inference after each update, known as &quot;re-indexing&quot;. However, historical gallery data typically suffers from direct saving due to the data privacy issue and the high re-indexing costs for large-scale gallery images. As a result, it inevitably leads to incompatible retrieval between query features extracted by the updated model and gallery features extracted by those before the update, greatly impairing the re-identification performance. To tackle the above issue, this paper focuses on a new task called Re-index Free Lifelong person Re-IDentification (RFL-ReID), which requires performing lifelong person re-identification without re-indexing historical gallery images. Therefore, RFL-ReID is more challenging than L-ReID, requiring continuous learning and balancing new and old knowledge in diverse streaming data, and making the features output by the new and old models compatible with each other. To this end, we propose a Bidirectional Continuous Compatible Representation (Bi-C2R) framework to continuously update the gallery features extracted by the old model to perform efficient L-ReID in a compatible manner. We verify our proposed Bi-C2R method through theoretical analysis and extensive experiments on multiple benchmarks, which demonstrate that the proposed method can achieve leading performance on both the introduced RFL-ReID task and the traditional L-ReID task.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Bi-C2R：面向无重新索引终身人物重识别的双向连续兼容表示</div>
<div class="mono" style="margin-top:8px">终身人物重识别（L-ReID）利用按顺序收集的数据持续训练和更新重识别模型，关注所有数据的整体性能。其主要挑战是在训练新数据时避免旧知识的灾难性遗忘。现有L-ReID方法通常在每次更新后，为所有历史图库图像重新提取新特征以进行推理，这一过程称为“重新索引”。然而，由于数据隐私问题和大规模图库图像重新索引成本高，历史图库数据通常无法直接保存。因此，这不可避免地导致查询特征（由更新后的模型提取）与图库特征（由更新前的模型提取）之间的检索不兼容，严重影响重识别性能。为解决上述问题，本文聚焦于一个新的任务——无重新索引终身人物重识别（RFL-ReID），该任务要求在不重新索引历史图库图像的情况下进行终身人物重识别。因此，RFL-ReID比L-ReID更具挑战性，需要在多样化的数据流中实现持续学习，并在新旧知识之间进行平衡，使新旧模型输出的特征相互兼容。为此，我们提出了一种双向连续兼容表示（Bi-C2R）框架，用于持续更新旧模型提取的图库特征，以实现兼容的高效L-ReID。我们通过在多个基准数据集上的理论分析和大量实验验证了所提出的Bi-C2R方法，实验结果表明该方法在引入的RFL-ReID任务和传统L-ReID任务中均能取得领先的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Basic Inequalities for First-Order Optimization with Applications to Statistical Risk Analysis</div>
<div class="meta-line">Authors: Seunghoon Paik, Kangjie Zhou, Matus Telgarsky, Ryan J. Tibshirani</div>
<div class="meta-line">First: 2025-12-31T17:49:37+00:00 · Latest: 2025-12-31T17:49:37+00:00</div>
<div class="meta-line">Comments: 47 pages, 3 figures (7 subfigures)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24999v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.24999v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce \textit{basic inequalities} for first-order iterative optimization algorithms, forming a simple and versatile framework that connects implicit and explicit regularization. While related inequalities appear in the literature, we isolate and highlight a specific form and develop it as a well-rounded tool for statistical analysis. Let $f$ denote the objective function to be optimized. Given a first-order iterative algorithm initialized at $θ_0$ with current iterate $θ_T$, the basic inequality upper bounds $f(θ_T)-f(z)$ for any reference point $z$ in terms of the accumulated step sizes and the distances between $θ_0$, $θ_T$, and $z$. The bound translates the number of iterations into an effective regularization coefficient in the loss function. We demonstrate this framework through analyses of training dynamics and prediction risk bounds. In addition to revisiting and refining known results on gradient descent, we provide new results for mirror descent with Bregman divergence projection, for generalized linear models trained by gradient descent and exponentiated gradient descent, and for randomized predictors. We illustrate and supplement these theoretical findings with experiments on generalized linear models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一阶优化算法的基本不等式及其在统计风险分析中的应用</div>
<div class="mono" style="margin-top:8px">我们引入了\textit{基本不等式}，用于一阶迭代优化算法，构建了一个简单且通用的框架，将隐式和显式正则化联系起来。虽然相关不等式在文献中已有出现，但我们隔离并强调了一种特定形式，并将其发展为统计分析的有力工具。设$f$为需要优化的目标函数。给定一个从初始点$θ_0$开始的一阶迭代算法，当前迭代点为$θ_T$，基本不等式以累积步长大小和$θ_0$、$θ_T$与参考点$z$之间的距离为依据，对$f(θ_T)-f(z)$进行上界估计。该界将迭代次数转化为损失函数中的有效正则化系数。我们通过训练动态分析和预测风险界来展示这一框架。除了回顾和改进已知的梯度下降结果外，我们还为使用Bregman散度投影的镜像下降法、通过梯度下降和指数梯度下降训练的广义线性模型以及随机预测器提供了新的结果。我们通过在广义线性模型上的实验来说明并补充这些理论发现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We introduce \textit{basic inequalities} for first-order iterative optimization algorithms, forming a simple and versatile framework that connects implicit and explicit regularization.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
