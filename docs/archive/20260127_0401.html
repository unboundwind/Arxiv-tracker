<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-27 04:01</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260127_0401</div>
    <div class="row"><div class="card">
<div class="title">AnyView: Synthesizing Any Novel View in Dynamic Scenes</div>
<div class="meta-line">Authors: Basile Van Hoorick, Dian Chen, Shun Iwase, Pavel Tokmakov, Muhammad Zubair Irshad, Igor Vasiljevic, Swati Gupta, Fangzhou Cheng, Sergey Zakharov, Vitor Campagnolo Guizilini</div>
<div class="meta-line">First: 2026-01-23T18:59:58+00:00 · Latest: 2026-01-23T18:59:58+00:00</div>
<div class="meta-line">Comments: Project webpage: https://tri-ml.github.io/AnyView/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16982v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16982v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://tri-ml.github.io/AnyView/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern generative video models excel at producing convincing, high-quality outputs, but struggle to maintain multi-view and spatiotemporal consistency in highly dynamic real-world environments. In this work, we introduce \textbf{AnyView}, a diffusion-based video generation framework for \emph{dynamic view synthesis} with minimal inductive biases or geometric assumptions. We leverage multiple data sources with various levels of supervision, including monocular (2D), multi-view static (3D) and multi-view dynamic (4D) datasets, to train a generalist spatiotemporal implicit representation capable of producing zero-shot novel videos from arbitrary camera locations and trajectories. We evaluate AnyView on standard benchmarks, showing competitive results with the current state of the art, and propose \textbf{AnyViewBench}, a challenging new benchmark tailored towards \emph{extreme} dynamic view synthesis in diverse real-world scenarios. In this more dramatic setting, we find that most baselines drastically degrade in performance, as they require significant overlap between viewpoints, while AnyView maintains the ability to produce realistic, plausible, and spatiotemporally consistent videos when prompted from \emph{any} viewpoint. Results, data, code, and models can be viewed at: https://tri-ml.github.io/AnyView/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnyView: 动态场景中任意新视角的合成</div>
<div class="mono" style="margin-top:8px">现代生成式视频模型在生成具有说服力的高质量输出方面表现出色，但在高度动态的现实环境中难以保持多视角和时空一致性。在本工作中，我们引入了AnyView，这是一个基于扩散的视频生成框架，用于动态视角合成，且具有最小的归纳偏置或几何假设。我们利用多种数据源，包括单目（2D）、多视角静态（3D）和多视角动态（4D）数据集，训练出一种通用的时空隐式表示，能够从任意摄像机位置和轨迹生成零样本的新视频。我们在标准基准上评估AnyView，展示了与当前最先进方法相当的结果，并提出了AnyViewBench，一个针对极端动态视角合成的挑战性新基准。在这一更具戏剧性的场景下，我们发现大多数基线方法性能显著下降，因为它们需要视角之间有大量重叠，而AnyView在从任意视角提示时仍能生成逼真、合理且时空一致的视频。</div>
</details>
</div>
<div class="card">
<div class="title">SyncLight: Controllable and Consistent Multi-View Relighting</div>
<div class="meta-line">Authors: David Serrano-Lozano, Anand Bhattad, Luis Herranz, Jean-François Lalonde, Javier Vazquez-Corral</div>
<div class="meta-line">First: 2026-01-23T18:59:57+00:00 · Latest: 2026-01-23T18:59:57+00:00</div>
<div class="meta-line">Comments: Project page: http://sync-light.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16981v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16981v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="http://sync-light.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present SyncLight, the first method to enable consistent, parametric relighting across multiple uncalibrated views of a static scene. While single-view relighting has advanced significantly, existing generative approaches struggle to maintain the rigorous lighting consistency essential for multi-camera broadcasts, stereoscopic cinema, and virtual production. SyncLight addresses this by enabling precise control over light intensity and color across a multi-view capture of a scene, conditioned on a single reference edit. Our method leverages a multi-view diffusion transformer trained using a latent bridge matching formulation, achieving high-fidelity relighting of the entire image set in a single inference step. To facilitate training, we introduce a large-scale hybrid dataset comprising diverse synthetic environments -- curated from existing sources and newly designed scenes -- alongside high-fidelity, real-world multi-view captures under calibrated illumination. Surprisingly, though trained only on image pairs, SyncLight generalizes zero-shot to an arbitrary number of viewpoints, effectively propagating lighting changes across all views, without requiring camera pose information. SyncLight enables practical relighting workflows for multi-view capture systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SyncLight：可控且一致的多视角重照明</div>
<div class="mono" style="margin-top:8px">我们提出了SyncLight，这是首个能够在多个未校准视角下实现静态场景参数化一致重照明的方法。尽管单视角重照明技术已有显著进展，但现有的生成方法难以维持多摄像机广播、立体电影和虚拟制作所需的严格照明一致性。SyncLight通过在单个参考编辑的条件下，对场景的多视角捕捉进行精确的光照强度和颜色控制来解决这一问题。我们的方法利用一种基于潜在桥匹配形式训练的多视角扩散变压器，能够在单次推理步骤中实现整个图像集的高保真重照明。为了促进训练，我们引入了一个大规模的混合数据集，包含多样化的合成环境（从现有资源中整理和新设计的场景）以及在校准光照条件下获取的高保真真实多视角捕捉数据。令人惊讶的是，尽管仅在图像对上进行训练，SyncLight仍能零样本泛化到任意数量的视角，有效地在所有视角中传播光照变化，而无需相机姿态信息。SyncLight为多视角捕捉系统提供了实用的重照明工作流程。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present SyncLight, the first method to enable consistent, parametric relighting across multiple uncalibrated views of a static scene.</div>
</details>
</div>
<div class="card">
<div class="title">A Scalable Measure of Loss Landscape Curvature for Analyzing the Training Dynamics of LLMs</div>
<div class="meta-line">Authors: Dayal Singh Kalra, Jean-Christophe Gagnon-Audet, Andrey Gromov, Ishita Mediratta, Kelvin Niu, Alexander H Miller, Michael Shvartsman</div>
<div class="meta-line">First: 2026-01-23T18:59:40+00:00 · Latest: 2026-01-23T18:59:40+00:00</div>
<div class="meta-line">Comments: 9 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16979v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16979v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding the curvature evolution of the loss landscape is fundamental to analyzing the training dynamics of neural networks. The most commonly studied measure, Hessian sharpness ($λ_{\max}^H$) -- the largest eigenvalue of the loss Hessian -- determines local training stability and interacts with the learning rate throughout training. Despite its significance in analyzing training dynamics, direct measurement of Hessian sharpness remains prohibitive for Large Language Models (LLMs) due to high computational cost. We analyze $\textit{critical sharpness}$ ($λ_c$), a computationally efficient measure requiring fewer than $10$ forward passes given the update direction $Δ\mathbfθ$. Critically, this measure captures well-documented Hessian sharpness phenomena, including progressive sharpening and Edge of Stability. Using this measure, we provide the first demonstration of these sharpness phenomena at scale, up to $7$B parameters, spanning both pre-training and mid-training of OLMo-2 models. We further introduce $\textit{relative critical sharpness}$ ($λ_c^{1\to 2}$), which quantifies the curvature of one loss landscape while optimizing another, to analyze the transition from pre-training to fine-tuning and guide data mixing strategies. Critical sharpness provides practitioners with a practical tool for diagnosing curvature dynamics and informing data composition choices at scale. More broadly, our work shows that scalable curvature measures can provide actionable insights for large-scale training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种用于分析大语言模型训练动态的可扩展损失景观曲率度量</div>
<div class="mono" style="margin-top:8px">理解损失景观的曲率演化对于分析神经网络的训练动态至关重要。最常研究的度量是Hessian sharpness（λ_H^max）——损失Hessian矩阵的最大特征值，它决定了局部训练稳定性，并在整个训练过程中与学习率相互作用。尽管其在分析训练动态中具有重要意义，但由于计算成本高昂，直接测量Hessian sharpness对于大语言模型（LLMs）来说仍然不可行。我们分析了critical sharpness（λ_c），这是一种计算高效的度量，仅需少于10次前向传播即可在给定更新方向Δθ的情况下进行计算。关键的是，该度量能够捕捉到已广泛记录的Hessian sharpness现象，包括渐进锐化和稳定性边缘。利用这一度量，我们首次在大规模上（高达70亿参数）展示了这些sharpness现象，涵盖OLMo-2模型的预训练和中期训练阶段。我们进一步引入了relative critical sharpness（λ_c^{1→2}），它量化了在优化另一个损失景观时另一个景观的曲率，用于分析从预训练到微调的转变，并指导数据混合策略。Critical sharpness为从业者提供了一种实用工具，用于诊断曲率动态并指导大规模的数据组成选择。更广泛地说，我们的工作表明，可扩展的曲率度量可以为大规模训练提供可操作的见解。</div>
</details>
</div>
<div class="card">
<div class="title">MapAnything: Universal Feed-Forward Metric 3D Reconstruction</div>
<div class="meta-line">Authors: Nikhil Keetha, Norman Müller, Johannes Schönberger, Lorenzo Porzi, Yuchen Zhang, Tobias Fischer, Arno Knapitsch, Duncan Zauss, Ethan Weber, Nelson Antunes, Jonathon Luiten, Manuel Lopez-Antequera, Samuel Rota Bulò, Christian Richardt, Deva Ramanan, Sebastian Scherer, Peter Kontschieder</div>
<div class="meta-line">First: 2025-09-16T18:00:14+00:00 · Latest: 2026-01-23T18:59:33+00:00</div>
<div class="meta-line">Comments: 3DV 2026. Project Page: https://map-anything.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.13414v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.13414v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://map-anything.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce MapAnything, a unified transformer-based feed-forward model that ingests one or more images along with optional geometric inputs such as camera intrinsics, poses, depth, or partial reconstructions, and then directly regresses the metric 3D scene geometry and cameras. MapAnything leverages a factored representation of multi-view scene geometry, i.e., a collection of depth maps, local ray maps, camera poses, and a metric scale factor that effectively upgrades local reconstructions into a globally consistent metric frame. Standardizing the supervision and training across diverse datasets, along with flexible input augmentation, enables MapAnything to address a broad range of 3D vision tasks in a single feed-forward pass, including uncalibrated structure-from-motion, calibrated multi-view stereo, monocular depth estimation, camera localization, depth completion, and more. We provide extensive experimental analyses and model ablations demonstrating that MapAnything outperforms or matches specialist feed-forward models while offering more efficient joint training behavior, thus paving the way toward a universal 3D reconstruction backbone.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MapAnything: 通用前馈度量3D重建</div>
<div class="mono" style="margin-top:8px">我们引入了MapAnything，这是一个统一的基于Transformer的前馈模型，可以输入一张或多张图像以及可选的几何信息，如相机内参、姿态、深度或部分重建结果，然后直接回归度量3D场景几何和相机参数。MapAnything利用了多视角场景几何的分解表示，即一组深度图、局部光线图、相机姿态和一个度量尺度因子，有效地将局部重建升级为全局一致的度量框架。通过在多样化数据集上标准化监督和训练，以及灵活的输入增强，MapAnything能够在单次前馈过程中处理广泛的3D视觉任务，包括非校准的运动恢复结构、校准的多视角立体重建、单目深度估计、相机定位、深度补全等。我们提供了广泛的实验分析和模型消融实验，证明MapAnything在性能上优于或匹配专用前馈模型，同时提供了更高效的联合训练行为，从而为通用3D重建主干网络铺平了道路。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Reasoning for PDE Foundation Models: A Reward-Model-Driven Inference-Time-Scaling Algorithm</div>
<div class="meta-line">Authors: Siddharth Mansingh, James Amarel, Ragib Arnab, Arvind Mohan, Kamaljeet Singh, Gerd J. Kunde, Nicolas Hengartner, Benjamin Migliori, Emily Casleton, Nathan A. Debardeleben, Ayan Biswas, Diane Oyen, Earl Lawrence</div>
<div class="meta-line">First: 2025-09-02T21:31:32+00:00 · Latest: 2026-01-23T18:55:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.02846v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.02846v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Partial Differential Equations (PDEs) are the bedrock for modern computational sciences and engineering, and inherently computationally expensive. While PDE foundation models have shown much promise for simulating such complex spatio-temporal phenomena, existing models remain constrained by the pretraining datasets and struggle with auto-regressive rollout performance, especially in out-of-distribution (OOD) cases. Furthermore, they have significant compute and training data requirements which hamper their use in many critical applications. Inspired by recent advances in ``thinking&quot; strategies used in large language models (LLMs), we introduce the first test-time computing (TTC) strategy for PDEs that utilizes computational resources during inference to achieve more accurate predictions with fewer training samples and smaller models. We accomplish this with two types of reward models that evaluate predictions of a stochastic based model for spatio-temporal consistency. We demonstrate this method on compressible Euler-equation simulations from the PDEGym benchmark and show that TTC captures improved predictions relative to standard non-adaptive auto-regressive inference. This TTC framework marks a foundational step towards more advanced reasoning algorithms or PDE modeling, inluding building reinforcement-learning-based approaches, potentially transforming computational workflows in physics and engineering.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向偏微分方程基础模型的推理：一种基于奖励模型的推理时间扩展算法</div>
<div class="mono" style="margin-top:8px">偏微分方程（PDEs）是现代计算科学和工程的基础，但本质上计算成本很高。尽管PDE基础模型在模拟复杂时空现象方面展现出巨大潜力，但现有模型仍受限于预训练数据集，并且在自回归推演性能上表现不佳，尤其是在分布外（OOD）情况下。此外，它们对计算资源和训练数据的需求也限制了其在许多关键应用中的使用。受大型语言模型（LLMs）中“思考”策略的启发，我们引入了首个推理时间计算（TTC）策略，该策略在推理过程中利用计算资源，以更少的训练样本和更小的模型实现更准确的预测。我们通过两种类型的奖励模型来评估基于随机的模型在时空一致性方面的预测效果。我们在PDEGym基准中的可压缩欧拉方程模拟上展示了该方法，并表明TTC相较于标准的非自适应自回归推理能够获得更优的预测结果。这一TTC框架标志着向更先进的推理算法或PDE建模迈出基础性一步，包括构建基于强化学习的方法，可能彻底改变物理学和工程学中的计算流程。</div>
</details>
</div>
<div class="card">
<div class="title">Latent Diffusion for Internet of Things Attack Data Generation in Intrusion Detection</div>
<div class="meta-line">Authors: Estela Sánchez-Carballo, Francisco M. Melgarejo-Meseguer, José Luis Rojo-Álvarez</div>
<div class="meta-line">First: 2026-01-23T18:55:07+00:00 · Latest: 2026-01-23T18:55:07+00:00</div>
<div class="meta-line">Comments: Submitted to IEEE. 15 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16976v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16976v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Intrusion Detection Systems (IDSs) are a key component for protecting Internet of Things (IoT) environments. However, in Machine Learning-based (ML-based) IDSs, performance is often degraded by the strong class imbalance between benign and attack traffic. Although data augmentation has been widely explored to mitigate this issue, existing approaches typically rely on simple oversampling techniques or generative models that struggle to simultaneously achieve high sample fidelity, diversity, and computational efficiency. To address these limitations, we propose the use of a Latent Diffusion Model (LDM) for attack data augmentation in IoT intrusion detection and provide a comprehensive comparison against state-of-the-art baselines. Experiments were conducted on three representative IoT attack types, specifically Distributed Denial-of-Service (DDoS), Mirai, and Man-in-the-Middle, evaluating both downstream IDS performance and intrinsic generative quality using distributional, dependency-based, and diversity metrics. Results show that balancing the training data with LDM-generated samples substantially improves IDS performance, achieving F1-scores of up to 0.99 for DDoS and Mirai attacks and consistently outperforming competing methods. Additionally, quantitative and qualitative analyses demonstrate that LDMs effectively preserve feature dependencies while generating diverse samples and reduce sampling time by approximately 25\% compared to diffusion models operating directly in data space. These findings highlight latent diffusion as an effective and scalable solution for synthetic IoT attack data generation, substantially mitigating the impact of class imbalance in ML-based IDSs for IoT scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于物联网入侵检测的潜在扩散攻击数据生成</div>
<div class="mono" style="margin-top:8px">入侵检测系统（IDS）是保护物联网（IoT）环境的关键组件。然而，在基于机器学习（ML-based）的IDS中，良性流量与攻击流量之间的强烈类别不平衡常常导致性能下降。尽管数据增强已被广泛研究以缓解这一问题，但现有方法通常依赖于简单的过采样技术或生成模型，难以同时实现高样本保真度、多样性和计算效率。为了解决这些限制，我们提出使用潜在扩散模型（LDM）进行物联网入侵检测中的攻击数据增强，并与最先进的基线方法进行了全面比较。实验在三种具有代表性的物联网攻击类型上进行，具体包括分布式拒绝服务（DDoS）、Mirai和中间人（Man-in-the-Middle）攻击，评估了下游IDS性能和内在生成质量，使用分布、依赖性和多样性指标。结果表明，使用LDM生成的样本平衡训练数据显著提升了IDS性能，在DDoS和Mirai攻击中实现了高达0.99的F1分数，并且始终优于竞争方法。此外，定量和定性分析表明，LDM在生成多样化样本的同时有效保留了特征依赖性，并且与直接在数据空间中运行的扩散模型相比，采样时间减少了约25%。这些发现突显了潜在扩散模型作为生成合成物联网攻击数据的有效且可扩展解决方案，显著缓解了基于机器学习的IDS在物联网场景中的类别不平衡问题。</div>
</details>
</div>
<div class="card">
<div class="title">Scribble-Supervised Medical Image Segmentation with Dynamic Teacher Switching and Hierarchical Consistency</div>
<div class="meta-line">Authors: Thanh-Huy Nguyen, Hoang-Loc Cao, Dat T. Chung, Mai-Anh Vu, Thanh-Minh Nguyen, Minh Le, Phat K. Huynh, Ulas Bagci</div>
<div class="meta-line">First: 2026-01-21T01:01:01+00:00 · Latest: 2026-01-23T18:54:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14563v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.14563v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scribble-supervised methods have emerged to mitigate the prohibitive annotation burden in medical image segmentation. However, the inherent sparsity of these annotations introduces significant ambiguity, which results in noisy pseudo-label propagation and hinders the learning of robust anatomical boundaries. To address this challenge, we propose SDT-Net, a novel dual-teacher, single-student framework designed to maximize supervision quality from these weak signals. Our method features a Dynamic Teacher Switching (DTS) module to adaptively select the most reliable teacher. This selected teacher then guides the student via two synergistic mechanisms: high-confidence pseudo-labels, refined by a Pick Reliable Pixels (PRP) mechanism, and multi-level feature alignment, enforced by a Hierarchical Consistency (HiCo) module. Extensive experiments on the ACDC and MSCMRseg datasets demonstrate that SDT-Net achieves state-of-the-art performance, producing more accurate and anatomically plausible segmentation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于涂鸦监督的医学图像分割：动态教师切换与层次一致性</div>
<div class="mono" style="margin-top:8px">涂鸦监督方法已被提出以缓解医学图像分割中的标注负担。然而，这些标注的固有稀疏性引入了显著的歧义，导致伪标签传播噪声，阻碍了鲁棒解剖边界的学习。为解决这一挑战，我们提出了SDT-Net，一种新颖的双教师、单学生框架，旨在从这些弱信号中最大化监督质量。我们的方法包含一个动态教师切换（DTS）模块，用于自适应选择最可靠的教师。该选定教师通过两种协同机制指导学生：由Pick Reliable Pixels（PRP）机制优化的高置信度伪标签，以及由层次一致性（HiCo）模块强制执行的多级特征对齐。在ACDC和MSCMRseg数据集上的大量实验表明，SDT-Net实现了最先进的性能，生成了更准确且符合解剖学的分割结果。</div>
</details>
</div>
<div class="card">
<div class="title">LLM Reasoning for Cold-Start Item Recommendation</div>
<div class="meta-line">Authors: Shijun Li, Yu Wang, Jin Wang, Ying Li, Joydeep Ghosh, Anne Cocos</div>
<div class="meta-line">Venue: WWW 2026</div>
<div class="meta-line">First: 2025-11-23T03:22:53+00:00 · Latest: 2026-01-23T18:51:39+00:00</div>
<div class="meta-line">Comments: Published on Proceedings of the ACM on Web Conference 2026 (WWW 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.18261v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.18261v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have shown significant potential for improving recommendation systems through their inherent reasoning capabilities and extensive knowledge base. Yet, existing studies predominantly address warm-start scenarios with abundant user-item interaction data, leaving the more challenging cold-start scenarios, where sparse interactions hinder traditional collaborative filtering methods, underexplored. To address this limitation, we propose novel reasoning strategies designed for cold-start item recommendations within the Netflix domain. Our method utilizes the advanced reasoning capabilities of LLMs to effectively infer user preferences, particularly for newly introduced or rarely interacted items. We systematically evaluate supervised fine-tuning, reinforcement learning-based fine-tuning, and hybrid approaches that combine both methods to optimize recommendation performance. Extensive experiments on real-world data demonstrate significant improvements in both methodological efficacy and practical performance in cold-start recommendation contexts. Remarkably, our reasoning-based fine-tuned models outperform Netflix&#x27;s production ranking model by up to 8% in certain cases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM推理用于冷启动物品推荐</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）因其固有的推理能力和广泛的知识库，在提升推荐系统方面展现出巨大潜力。然而，现有研究主要关注数据丰富的热启动场景，而冷启动场景由于用户-物品交互数据稀疏，传统协同过滤方法难以有效应用，仍缺乏深入探索。为解决这一问题，我们在Netflix领域提出了针对冷启动物品推荐的新推理策略。我们的方法利用LLMs的先进推理能力，有效推断用户偏好，特别是针对新引入或较少交互的物品。我们系统地评估了监督微调、基于强化学习的微调以及结合两种方法的混合策略，以优化推荐性能。在真实数据上的大量实验表明，在冷启动推荐场景中，我们的方法在方法有效性和实际表现上均有显著提升。值得注意的是，在某些情况下，我们的基于推理的微调模型在性能上超过了Netflix的生产级排序模型，最高可达8%。</div>
</details>
</div>
<div class="card">
<div class="title">VisGym: Diverse, Customizable, Scalable Environments for Multimodal Agents</div>
<div class="meta-line">Authors: Zirui Wang, Junyi Zhang, Jiaxin Ge, Long Lian, Letian Fu, Lisa Dunlap, Ken Goldberg, XuDong Wang, Ion Stoica, David M. Chan, Sewon Min, Joseph E. Gonzalez</div>
<div class="meta-line">First: 2026-01-23T18:43:34+00:00 · Latest: 2026-01-23T18:43:34+00:00</div>
<div class="meta-line">Comments: Project page: https://visgym.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16973v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16973v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://visgym.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons. We introduce VisGym, a gymnasium of 17 environments for evaluating and training VLMs. The suite spans symbolic puzzles, real-image understanding, navigation, and manipulation, and provides flexible controls over difficulty, input representation, planning horizon, and feedback. We also provide multi-step solvers that generate structured demonstrations, enabling supervised finetuning. Our evaluations show that all frontier models struggle in interactive settings, achieving low success rates in both the easy (46.6%) and hard (26.0%) configurations. Our experiments reveal notable limitations: models struggle to effectively leverage long context, performing worse with an unbounded history than with truncated windows. Furthermore, we find that several text-based symbolic tasks become substantially harder once rendered visually. However, explicit goal observations, textual feedback, and exploratory demonstrations in partially observable or unknown-dynamics settings for supervised finetuning yield consistent gains, highlighting concrete failure modes and pathways for improving multi-step visual decision-making. Code, data, and models can be found at: https://visgym.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VisGym：用于多模态代理的多样化、可定制、可扩展环境</div>
<div class="mono" style="margin-top:8px">现代视觉-语言模型（VLMs）在多步视觉交互中，特别是在如何在长时间范围内整合感知、记忆和行动方面，仍缺乏充分表征。我们引入了VisGym，这是一个包含17个环境的平台，用于评估和训练VLMs。该套件涵盖符号谜题、真实图像理解、导航和操作任务，并提供了对难度、输入表示、规划时间范围和反馈的灵活控制。我们还提供了多步求解器，生成结构化演示，从而支持监督微调。我们的评估表明，所有前沿模型在交互式环境中都表现不佳，在简单（46.6%）和困难（26.0%）配置下均取得较低的成功率。我们的实验揭示了显著的局限性：模型难以有效利用长上下文，在无限制历史记录下的表现不如截断窗口。此外，我们发现，一旦将基于文本的符号任务可视化，这些任务会变得显著更难。然而，在部分可观测或未知动态设置中，使用显式目标观察、文本反馈和探索性演示进行监督微调，能够带来一致的性能提升，突显了多步视觉决策的具体失败模式和改进路径。代码、数据和模型可在：https://visgym.github.io/ 找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modern Vision-Language Models (VLMs) remain poorly characterized in multi-step visual interactions, particularly in how they integrate perception, memory, and action over long horizons.</div>
</details>
</div>
<div class="card">
<div class="title">Auto-Regressive Masked Diffusion Models</div>
<div class="meta-line">Authors: Mahdi Karami, Ali Ghodsi</div>
<div class="meta-line">Venue: 29th International Conference on Artificial Intelligence and Statistics (AISTATS) 2026</div>
<div class="meta-line">First: 2026-01-23T18:42:30+00:00 · Latest: 2026-01-23T18:42:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16971v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16971v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Masked diffusion models (MDMs) have emerged as a promising approach for language modeling, yet they face a performance gap compared to autoregressive models (ARMs) and require more training iterations. In this work, we present the Auto-Regressive Masked Diffusion (ARMD) model, an architecture designed to close this gap by unifying the training efficiency of autoregressive models with the parallel generation capabilities of diffusion-based models. Our key insight is to reframe the masked diffusion process as a block-wise causal model. This perspective allows us to design a strictly causal, permutation-equivariant architecture that computes all conditional probabilities across multiple denoising steps in a single, parallel forward pass. The resulting architecture supports efficient, autoregressive-style decoding and a progressive permutation training scheme, allowing the model to learn both canonical left-to-right and random token orderings. Leveraging this flexibility, we introduce a novel strided parallel generation strategy that accelerates inference by generating tokens in parallel streams while maintaining global coherence. Empirical results demonstrate that ARMD achieves state-of-the-art performance on standard language modeling benchmarks, outperforming established diffusion baselines while requiring significantly fewer training steps. Furthermore, it establishes a new benchmark for parallel text generation, effectively bridging the performance gap between parallel and sequential decoding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自回归掩码扩散模型</div>
<div class="mono" style="margin-top:8px">掩码扩散模型（MDMs）作为一种语言建模方法展现出巨大潜力，但与自回归模型（ARMs）相比存在性能差距，并且需要更多的训练迭代。在本文中，我们提出了自回归掩码扩散（ARMD）模型，该架构旨在通过将自回归模型的训练效率与基于扩散模型的并行生成能力相结合，弥合这一差距。我们的关键洞察是将掩码扩散过程重新表述为块状因果模型。这种视角使我们能够设计出一个严格因果、排列等变的架构，该架构可以在单次并行前向传递中计算多个去噪步骤中的所有条件概率。所得到的架构支持高效的自回归式解码，并采用渐进排列训练方案，使模型能够学习标准的左到右顺序和随机的标记顺序。利用这种灵活性，我们引入了一种新颖的步距并行生成策略，通过在并行流中生成标记来加速推理，同时保持全局一致性。实验证明，ARMD在标准语言建模基准上实现了最先进的性能，优于现有的扩散模型基线，同时所需的训练步骤显著减少。此外，它还为并行文本生成设立了新的基准，有效弥合了并行解码与顺序解码之间的性能差距。</div>
</details>
</div>
<div class="card">
<div class="title">BONO-Bench: A Comprehensive Test Suite for Bi-objective Numerical Optimization with Traceable Pareto Sets</div>
<div class="meta-line">Authors: Lennart Schäpermeier, Pascal Kerschke</div>
<div class="meta-line">First: 2026-01-23T18:42:20+00:00 · Latest: 2026-01-23T18:42:20+00:00</div>
<div class="meta-line">Comments: Accepted for publication in the Special Issue on Benchmarking in Multi-Criteria Optimization at ACM TELO</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16970v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16970v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The evaluation of heuristic optimizers on test problems, better known as \emph{benchmarking}, is a cornerstone of research in multi-objective optimization.
  However, most test problems used in benchmarking numerical multi-objective black-box optimizers come from one of two flawed approaches: On the one hand, problems are constructed manually, which result in problems with well-understood optimal solutions, but unrealistic properties and biases.
  On the other hand, more realistic and complex single-objective problems are composited into multi-objective problems, but with a lack of control and understanding of problem properties.
  This paper proposes an extensive problem generation approach for bi-objective numerical optimization problems consisting of the combination of theoretically well-understood convex-quadratic functions into unimodal and multimodal landscapes with and without global structure.
  It supports configuration of test problem properties, such as the number of decision variables, local optima, Pareto front shape, plateaus in the objective space, or degree of conditioning, while maintaining theoretical tractability: The optimal front can be approximated to an arbitrary degree of precision regarding Pareto-compliant performance indicators such as the hypervolume or the exact R2 indicator.
  To demonstrate the generator&#x27;s capabilities, a test suite of 20 problem categories, called \emph{BONO-Bench}, is created and subsequently used as a basis of an illustrative benchmark study.
  Finally, the general approach underlying our proposed generator, together with the associated test suite, is publicly released in the Python package \texttt{bonobench} to facilitate reproducible benchmarking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BONO-Bench：一种用于双目标数值优化的可追溯帕累托集的综合测试套件</div>
<div class="mono" style="margin-top:8px">启发式优化器在测试问题上的评估，通常称为\emph{基准测试}，是多目标优化研究的核心。然而，大多数用于基准测试的数值多目标黑盒优化问题来自两种有缺陷的方法：一方面，问题由人工构造，导致具有易于理解的最优解，但不现实的性质和偏差；另一方面，更现实且复杂的单目标问题被组合成多目标问题，但缺乏对问题性质的控制和理解。本文提出了一种广泛的问题生成方法，用于双目标数值优化问题，该方法通过将理论上易于理解的凸二次函数组合成具有和不具有全局结构的单峰和多峰景观。该方法支持配置测试问题的属性，如决策变量数量、局部最优解、帕累托前沿形状、目标空间中的平坦区域或条件数，同时保持理论上的可处理性：最优前沿可以任意精确地近似，以符合帕累托性能指标，如超体积或精确R2指标。为了展示生成器的能力，创建了一个包含20个问题类别的测试套件，称为\emph{BONO-Bench}，并随后将其作为示例基准研究的基础。最后，我们提出的生成器背后的一般方法及其相关测试套件已作为Python包\texttt{bonobench}公开发布，以促进可重复的基准测试。</div>
</details>
</div>
<div class="card">
<div class="title">On Fine-Grained I/O Complexity of Attention Backward Passes</div>
<div class="meta-line">Authors: Xiaoyu Li, Yingyu Liang, Zhenmei Shi, Zhao Song, Song Yue, Jiahao Zhang</div>
<div class="meta-line">First: 2024-10-12T07:01:30+00:00 · Latest: 2026-01-23T18:42:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.09397v2">Abs</a> · <a href="https://arxiv.org/pdf/2410.09397v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) exhibit exceptional proficiency in handling extensive context windows in natural language. Nevertheless, the quadratic scaling of attention computation relative to sequence length creates substantial efficiency bottlenecks, necessitating the development of I/O-optimized algorithms. In this work, we conduct a systematic examination of the I/O complexity inherent in attention mechanisms, with a specific emphasis on the backward pass under both small and large cache settings. By leveraging the red-blue pebble game framework, we derive tight bounds for I/O complexity across the full spectrum of cache sizes. We validate that FlashAttention, one of the current industry standards, achieves optimality in the large-cache scenario for both forward and backward passes. Conversely, for small-cache environments, we introduce a novel algorithm that outperforms contemporary methods and successfully attains theoretical tight bounds. Furthermore, we expand our investigation to include sparse attention by establishing granular lower bounds for both forward and backward passes across all cache configurations. Ultimately, our results solidify the theoretical framework regarding I/O complexity in attention mechanisms, providing critical guidance for the development of efficient LLM training and inference systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>注意力反向传播的细粒度I/O复杂度</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在处理自然语言中的长上下文窗口方面表现出卓越的能力。然而，注意力计算相对于序列长度的二次方扩展导致了显著的效率瓶颈，因此需要开发优化I/O的算法。在本工作中，我们系统地分析了注意力机制中固有的I/O复杂度，特别关注在小缓存和大缓存设置下的反向传播过程。通过利用红蓝石子游戏框架，我们推导出适用于所有缓存大小范围的紧密复杂度界限。我们验证了FlashAttention，当前行业标准之一，在大缓存场景下对于前向和反向传播均达到最优。相反，在小缓存环境中，我们提出了一种新颖的算法，其性能优于现有方法，并成功达到了理论上的紧密界限。此外，我们将研究扩展到稀疏注意力，通过为所有缓存配置下的前向和反向传播建立细粒度的下界，进一步深化了对注意力机制中I/O复杂度的理论框架的理解，为高效LLM训练和推理系统的开发提供了关键指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) exhibit exceptional proficiency in handling extensive context windows in natural language.</div>
</details>
</div>
<div class="card">
<div class="title">Q-learning with Adjoint Matching</div>
<div class="meta-line">Authors: Qiyang Li, Sergey Levine</div>
<div class="meta-line">First: 2026-01-20T18:45:34+00:00 · Latest: 2026-01-23T18:40:14+00:00</div>
<div class="meta-line">Comments: 32 pages, 8 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14234v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.14234v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic&#x27;s action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于伴随匹配的Q学习</div>
<div class="mono" style="margin-top:8px">我们提出了一种基于时间差分（TD）的强化学习（RL）新算法——伴随匹配Q学习（QAM），用于解决连续动作RL中的长期挑战：高效优化一个具有表达能力的扩散或流匹配策略，以适应参数化的Q函数。有效的优化需要利用批评者的梯度信息，但对流或扩散策略而言，直接通过其多步去噪过程进行梯度优化会导致数值不稳定。现有方法要么仅使用价值信息而丢弃梯度信息，要么依赖牺牲策略表达能力或导致学习策略偏倚的近似方法。QAM通过利用生成建模中最近提出的技术——伴随匹配，将批评者的动作梯度转换为一个无不稳定反向传播的分步目标函数，同时在最优解处提供无偏且具有表达能力的策略。结合时间差分备份进行批评者学习，QAM在离线和离线到在线的RL任务中，特别是在稀疏奖励的困难任务上，持续优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Empowering Medical Equipment Sustainability in Low-Resource Settings: An AI-Powered Diagnostic and Support Platform for Biomedical Technicians</div>
<div class="meta-line">Authors: Bernes Lorier Atabonfack, Ahmed Tahiru Issah, Mohammed Hardi Abdul Baaki, Clemence Ingabire, Tolulope Olusuyi, Maruf Adewole, Udunna C. Anazodo, Timothy X Brown</div>
<div class="meta-line">Venue: MICCAI 2025</div>
<div class="meta-line">First: 2026-01-23T18:39:55+00:00 · Latest: 2026-01-23T18:39:55+00:00</div>
<div class="meta-line">Comments: Accepted at the MIRASOL Workshop at MICCAI 2025. To appear in Lecture Notes in Computer Science (LNCS)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16967v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16967v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In low- and middle-income countries (LMICs), a significant proportion of medical diagnostic equipment remains underutilized or non-functional due to a lack of timely maintenance, limited access to technical expertise, and minimal support from manufacturers, particularly for devices acquired through third-party vendors or donations. This challenge contributes to increased equipment downtime, delayed diagnoses, and compromised patient care. This research explores the development and validation of an AI-powered support platform designed to assist biomedical technicians in diagnosing and repairing medical devices in real-time. The system integrates a large language model (LLM) with a user-friendly web interface, enabling imaging technologists/radiographers and biomedical technicians to input error codes or device symptoms and receive accurate, step-by-step troubleshooting guidance. The platform also includes a global peer-to-peer discussion forum to support knowledge exchange and provide additional context for rare or undocumented issues. A proof of concept was developed using the Philips HDI 5000 ultrasound machine, achieving 100% precision in error code interpretation and 80% accuracy in suggesting corrective actions. This study demonstrates the feasibility and potential of AI-driven systems to support medical device maintenance, with the aim of reducing equipment downtime to improve healthcare delivery in resource-constrained environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在资源匮乏环境中赋能医疗设备可持续性：一种面向生物医学技术人员的AI驱动诊断与支持平台</div>
<div class="mono" style="margin-top:8px">在低收入和中等收入国家（LMICs）中，由于缺乏及时维护、技术专家有限以及制造商支持不足，尤其是通过第三方供应商或捐赠获得的设备，大量医疗诊断设备处于未充分利用或无法运行状态。这一问题导致设备停机时间增加、诊断延迟和患者护理受损。本研究探讨了一种AI驱动的支持平台的开发与验证，旨在帮助生物医学技术人员实时诊断和维修医疗设备。该系统结合了大型语言模型（LLM）和用户友好的网页界面，使影像技术人员/放射科医生和生物医学技术人员能够输入错误代码或设备症状，获取准确的分步故障排除指导。该平台还包含一个全球性的点对点讨论论坛，以促进知识共享并为罕见或未记录的问题提供额外背景信息。使用Philips HDI 5000超声设备开发了概念验证原型，实现了错误代码解释的100%精度和纠正措施建议的80%准确性。本研究展示了AI驱动系统在支持医疗设备维护方面的可行性和潜力，旨在减少设备停机时间，从而改善资源匮乏环境中的医疗服务。</div>
</details>
</div>
<div class="card">
<div class="title">Provable Differentially Private Computation of the Cross-Attention Mechanism</div>
<div class="meta-line">Authors: Yekun Ke, Yingyu Liang, Zhenmei Shi, Zhao Song, Jiahao Zhang</div>
<div class="meta-line">First: 2024-07-20T01:02:27+00:00 · Latest: 2026-01-23T18:38:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.14717v3">Abs</a> · <a href="https://arxiv.org/pdf/2407.14717v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cross-attention has emerged as a cornerstone module in modern artificial intelligence, underpinning critical applications such as retrieval-augmented generation (RAG), system prompting, and guided stable diffusion. However, this is a rising concern about securing the privacy of cross-attention, as the underlying key and value matrices frequently encode sensitive data or private user information. In this work, we introduce a novel data structure designed to enforce differential privacy (DP) for cross-attention mechanisms, accompanied by provable theoretical guarantees. Specifically, letting $n$ denote the input sequence length, $d$ the feature dimension, $R$ the maximum magnitude of query and key matrices, $R_w$ the maximum magnitude of the value matrix, and $r, s, ε_s$ the parameters for polynomial kernel methods, our proposed structure achieves $\widetilde{O}(ndr^2)$ space and initialization complexity, with a query time of $\widetilde{O}(d r^2)$ per token. Moreover, we demonstrate that our mechanism satisfies $(ε, δ)$-DP, incurring an additive error of $\widetilde{O}((1-ε_s)^{-1} n^{-1} ε^{-1} R^{2s} R_w r^2)$ and a relative error of $2ε_s/(1-ε_s)$ with respect to the ground truth. Crucially, our framework maintains robustness against adaptive queries, ensuring security even in adversarial settings. To the best of our knowledge, this constitutes the first approach providing provable differential privacy for cross-attention, establishing a foundation for future privacy-preserving algorithms in large generative models (LGMs).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可证明的差分隐私交叉注意力机制计算</div>
<div class="mono" style="margin-top:8px">交叉注意力已成为现代人工智能中的核心模块，支撑了诸如检索增强生成（RAG）、系统提示和引导稳定扩散等关键应用。然而，如何保障交叉注意力的隐私成为一个日益关注的问题，因为底层的关键和值矩阵经常编码敏感数据或私人用户信息。在本文中，我们引入了一种新颖的数据结构，用于强制交叉注意力机制满足差分隐私（DP），并提供了可证明的理论保证。具体而言，设 $n$ 表示输入序列长度，$d$ 表示特征维度，$R$ 表示查询和键矩阵的最大绝对值，$R_w$ 表示值矩阵的最大绝对值，$r, s, ε_s$ 为多项式核方法的参数，我们提出的数据结构在空间和初始化复杂度上达到 $\widetilde{O}(ndr^2)$，每个标记的查询时间为 $\widetilde{O}(d r^2)$。此外，我们证明了我们的机制满足 $(ε, δ)$-DP，其绝对误差为 $\widetilde{O}((1-ε_s)^{-1} n^{-1} ε^{-1} R^{2s} R_w r^2)$，相对误差为 $2ε_s/(1-ε_s)$。关键的是，我们的框架能够抵御自适应查询，即使在对抗性环境中也能保证安全性。据我们所知，这是首个为交叉注意力提供可证明差分隐私的方法，为未来在大规模生成模型（LGMs）中的隐私保护算法奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">Spatial-Agent: Agentic Geo-spatial Reasoning with Scientific Core Concepts</div>
<div class="meta-line">Authors: Riyang Bao, Cheng Yang, Dazhou Yu, Zhexiang Tang, Gengchen Mai, Liang Zhao</div>
<div class="meta-line">First: 2026-01-23T18:33:45+00:00 · Latest: 2026-01-23T18:33:45+00:00</div>
<div class="meta-line">Comments: 15pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16965v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16965v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Geospatial reasoning is essential for real-world applications such as urban analytics, transportation planning, and disaster response. However, existing LLM-based agents often fail at genuine geospatial computation, relying instead on web search or pattern matching while hallucinating spatial relationships. We present Spatial-Agent, an AI agent grounded in foundational theories of spatial information science. Our approach formalizes geo-analytical question answering as a concept transformation problem, where natural-language questions are parsed into executable workflows represented as GeoFlow Graphs -- directed acyclic graphs with nodes corresponding to spatial concepts and edges representing transformations. Drawing on spatial information theory, Spatial-Agent extracts spatial concepts, assigns functional roles with principled ordering constraints, and composes transformation sequences through template-based generation. Extensive experiments on MapEval-API and MapQA benchmarks demonstrate that Spatial-Agent significantly outperforms existing baselines including ReAct and Reflexion, while producing interpretable and executable geospatial workflows.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Spatial-Agent: 基于科学核心概念的代理式地理空间推理</div>
<div class="mono" style="margin-top:8px">地理空间推理对于现实世界应用如城市分析、交通规划和灾害响应至关重要。然而，现有的基于大语言模型（LLM）的代理通常无法进行真正的地理空间计算，而是依赖网络搜索或模式匹配，并在空间关系上产生幻觉。我们提出了Spatial-Agent，这是一种基于地理空间信息科学基础理论的AI代理。我们的方法将地理分析问题回答形式化为概念转换问题，其中自然语言问题被解析为可执行的工作流，表示为GeoFlow图——一种有向无环图，节点对应空间概念，边表示转换。基于空间信息理论，Spatial-Agent通过基于模板的生成方法提取空间概念，分配功能角色并遵循原则性的顺序约束，进而组合转换序列。在MapEval-API和MapQA基准上的大量实验表明，Spatial-Agent显著优于现有的基线方法，如ReAct和Reflexion，同时生成可解释且可执行的地理空间工作流。</div>
</details>
</div>
<div class="card">
<div class="title">AgentDrive: An Open Benchmark Dataset for Agentic AI Reasoning with LLM-Generated Scenarios in Autonomous Systems</div>
<div class="meta-line">Authors: Mohamed Amine Ferrag, Abderrahmane Lakas, Merouane Debbah</div>
<div class="meta-line">First: 2026-01-23T18:33:41+00:00 · Latest: 2026-01-23T18:33:41+00:00</div>
<div class="meta-line">Comments: 16 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16964v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16964v1">PDF</a> · <a href="https://github.com/maferrag/AgentDrive">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid advancement of large language models (LLMs) has sparked growing interest in their integration into autonomous systems for reasoning-driven perception, planning, and decision-making. However, evaluating and training such agentic AI models remains challenging due to the lack of large-scale, structured, and safety-critical benchmarks. This paper introduces AgentDrive, an open benchmark dataset containing 300,000 LLM-generated driving scenarios designed for training, fine-tuning, and evaluating autonomous agents under diverse conditions. AgentDrive formalizes a factorized scenario space across seven orthogonal axes: scenario type, driver behavior, environment, road layout, objective, difficulty, and traffic density. An LLM-driven prompt-to-JSON pipeline generates semantically rich, simulation-ready specifications that are validated against physical and schema constraints. Each scenario undergoes simulation rollouts, surrogate safety metric computation, and rule-based outcome labeling. To complement simulation-based evaluation, we introduce AgentDrive-MCQ, a 100,000-question multiple-choice benchmark spanning five reasoning dimensions: physics, policy, hybrid, scenario, and comparative reasoning. We conduct a large-scale evaluation of fifty leading LLMs on AgentDrive-MCQ. Results show that while proprietary frontier models perform best in contextual and policy reasoning, advanced open models are rapidly closing the gap in structured and physics-grounded reasoning. We release the AgentDrive dataset, AgentDrive-MCQ benchmark, evaluation code, and related materials at https://github.com/maferrag/AgentDrive</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AgentDrive：用于自主系统中基于LLM生成场景的代理式AI推理的开放基准数据集</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的快速发展激发了将其整合到自主系统中用于推理驱动的感知、规划和决策的广泛兴趣。然而，由于缺乏大规模、结构化且安全关键的基准，评估和训练此类代理式AI模型仍然具有挑战性。本文介绍了AgentDrive，一个包含300,000个LLM生成驾驶场景的开放基准数据集，旨在在多样化条件下训练、微调和评估自主代理。AgentDrive在七个正交轴上形式化了场景空间：场景类型、驾驶员行为、环境、道路布局、目标、难度和交通密度。一个由LLM驱动的提示到JSON的流水线生成语义丰富且适合模拟的规范，这些规范经过物理和模式约束验证。每个场景都经过模拟回滚、代理安全度指标计算和基于规则的结果标注。为了补充基于模拟的评估，我们引入了AgentDrive-MCQ，一个包含5个推理维度（物理、政策、混合、场景和比较推理）的100,000题多选基准。我们在AgentDrive-MCQ上对五十个领先的LLM进行了大规模评估。结果表明，尽管专有前沿模型在上下文和政策推理方面表现最佳，但先进的开源模型在结构化和物理基础推理方面正迅速缩小差距。我们将在https://github.com/maferrag/AgentDrive上发布AgentDrive数据集、AgentDrive-MCQ基准、评估代码及相关材料。</div>
</details>
</div>
<div class="card">
<div class="title">Do We Know What They Know We Know? Calibrating Student Trust in AI and Human Responses Through Mutual Theory of Mind</div>
<div class="meta-line">Authors: Olivia Pal, Veda Duddu, Agam Goyal, Drishti Goel, Koustuv Saha</div>
<div class="meta-line">First: 2026-01-23T18:27:32+00:00 · Latest: 2026-01-23T18:27:32+00:00</div>
<div class="meta-line">Comments: Preprint: 1 Figure, 3 Tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16960v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16960v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Trust and reliance are often treated as coupled constructs in human-AI interaction research, with the assumption that calibrating trust will lead to appropriate reliance. We challenge this assumption in educational contexts, where students increasingly turn to AI for learning support. Through semi-structured interviews with graduate students (N=8) comparing AI-generated and human-generated responses, we find a systematic dissociation: students exhibit high trust but low reliance on human experts due to social barriers (fear of judgment, help-seeking anxiety), while showing low trust but high reliance on AI systems due to social affordances (accessibility, anonymity, judgment-free interaction). Using Mutual Theory of Mind as an analytical lens, we demonstrate that trust is shaped by epistemic evaluations while reliance is driven by social factors -- and these may operate independently.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>我们了解他们所知道的吗？通过相互心智理论校准学生对AI和人类回应的信任</div>
<div class="mono" style="margin-top:8px">信任与依赖常被视为在人机交互研究中相互关联的概念，通常假设校准信任将导致适当的依赖。我们在教育情境中挑战这一假设，因为学生越来越多地依赖AI进行学习支持。通过对8名研究生进行半结构化访谈，比较AI生成和人类生成的回应，我们发现系统性分离：学生对人类专家表现出高度信任但依赖度较低，这是由于社会障碍（害怕被评判、寻求帮助的焦虑）；而对AI系统表现出较低信任但依赖度较高，这是由于社会可利用性（可及性、匿名性、无评判互动）所致。通过相互心智理论作为分析框架，我们展示了信任由知识评价塑造，而依赖则由社会因素驱动，且这两者可能独立运作。</div>
</details>
</div>
<div class="card">
<div class="title">DataStates-LLM: Scalable Checkpointing for Transformer Models Using Composable State Providers</div>
<div class="meta-line">Authors: Avinash Maurya, M. Mustafa Rafique, Franck Cappello, Bogdan Nicolae</div>
<div class="meta-line">First: 2026-01-23T18:26:14+00:00 · Latest: 2026-01-23T18:26:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16956v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16956v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid growth of Large Transformer-based models, specifically Large Language Models (LLMs), now scaling to trillions of parameters, has necessitated training across thousands of GPUs using complex hybrid parallelism strategies (e.g., data, tensor, and pipeline parallelism). Checkpointing this massive, distributed state is critical for a wide range of use cases, such as resilience, suspend-resume, investigating undesirable training trajectories, and explaining model evolution. However, existing checkpointing solutions typically treat model state as opaque binary blobs, ignoring the ``3D heterogeneity&#x27;&#x27; of the underlying data structures--varying by memory location (GPU vs. Host), number of ``logical&#x27;&#x27; objects sharded and split across multiple files, data types (tensors vs. Python objects), and their serialization requirements. This results in significant runtime overheads due to blocking device-to-host transfers, data-oblivious serialization, and storage I/O contention. In this paper, we introduce DataStates-LLM, a novel checkpointing architecture that leverages State Providers to decouple state abstraction from data movement. DataStates-LLM exploits the immutability of model parameters during the forward and backward passes to perform ``lazy&#x27;&#x27;, non-blocking asynchronous snapshots. By introducing State Providers, we efficiently coalesce fragmented, heterogeneous shards and overlap the serialization of metadata with bulk tensor I/O. We evaluate DataStates-LLM on models up to 70B parameters on 256 A100-40GB GPUs. Our results demonstrate that DataStates-LLM achieves up to 4$\times$ higher checkpointing throughput and reduces end-to-end training time by up to 2.2$\times$ compared to state-of-the-art solutions, effectively mitigating the serialization and heterogeneity bottlenecks in extreme-scale LLM training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DataStates-LLM：使用可组合状态提供者的Transformer模型可扩展检查点机制</div>
<div class="mono" style="margin-top:8px">随着基于Transformer的大模型，特别是大型语言模型（LLMs）参数规模迅速增长至万亿级别，现在需要使用复杂的混合并行策略（如数据、张量和流水线并行）在数千块GPU上进行训练。对这种大规模分布式状态进行检查点保存对于各种应用场景至关重要，例如容错、挂起恢复、调查不良训练轨迹以及解释模型演化。然而，现有的检查点解决方案通常将模型状态视为不可变的二进制块，忽略了底层数据结构的「三维异构性」——包括内存位置（GPU与主机）、跨多个文件分割和拆分的「逻辑」对象数量、数据类型（张量与Python对象）及其序列化需求。这导致了由于设备到主机传输阻塞、数据无关的序列化以及存储I/O竞争而产生显著的运行时开销。在本文中，我们引入了DataStates-LLM，这是一种新颖的检查点架构，利用状态提供者将状态抽象与数据移动解耦。DataStates-LLM利用模型参数在前向和后向传播过程中的不可变性，执行「延迟」的非阻塞异步快照。通过引入状态提供者，我们高效地合并了碎片化、异构的状态分片，并将元数据序列化与批量张量I/O重叠。我们在256块A100-40GB GPU上对参数量高达700亿的模型进行了评估。我们的结果表明，与最先进的解决方案相比，DataStates-LLM实现了高达4倍的检查点吞吐量，并将端到端训练时间减少了高达2.2倍，有效缓解了大规模LLM训练中的序列化和异构性瓶颈。</div>
</details>
</div>
<div class="card">
<div class="title">3D Molecule Generation from Rigid Motifs via SE(3) Flows</div>
<div class="meta-line">Authors: Roman Poletukhin, Marcel Kollovieh, Eike Eberhard, Stephan Günnemann</div>
<div class="meta-line">First: 2026-01-23T18:24:57+00:00 · Latest: 2026-01-23T18:24:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16955v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16955v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Three-dimensional molecular structure generation is typically performed at the level of individual atoms, yet molecular graph generation techniques often consider fragments as their structural units. Building on the advances in frame-based protein structure generation, we extend these fragmentation ideas to 3D, treating general molecules as sets of rigid-body motifs. Utilising this representation, we employ SE(3)-equivariant generative modelling for de novo 3D molecule generation from rigid motifs. In our evaluations, we observe comparable or superior results to state-of-the-art across benchmarks, surpassing it in atom stability on GEOM-Drugs, while yielding a 2x to 10x reduction in generation steps and offering 3.5x compression in molecular representations compared to the standard atom-based methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过SE(3)流从刚性基团生成三维分子结构</div>
<div class="mono" style="margin-top:8px">三维分子结构生成通常在单个原子层面进行，而分子图生成技术则常将片段作为结构单元。基于基于框架的蛋白质结构生成技术的进展，我们将这些碎片化思想扩展到三维领域，将一般分子视为刚体基团的集合。利用这种表示方式，我们采用SE(3)等变生成模型，从刚性基团进行从头开始的三维分子生成。在我们的评估中，我们在基准测试中观察到与当前最先进的方法相当或更优的结果，在GEOM-Drugs数据集上表现出更高的原子稳定性，同时生成步骤减少了2到10倍，并且与标准基于原子的方法相比，分子表示压缩了3.5倍。</div>
</details>
</div>
<div class="card">
<div class="title">Domain-invariant Mixed-domain Semi-supervised Medical Image Segmentation with Clustered Maximum Mean Discrepancy Alignment</div>
<div class="meta-line">Authors: Ba-Thinh Lam, Thanh-Huy Nguyen, Hoang-Thien Nguyen, Quang-Khai Bui-Tran, Nguyen Lan Vi Vu, Phat K. Huynh, Ulas Bagci, Min Xu</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2026-01-23T18:23:03+00:00 · Latest: 2026-01-23T18:23:03+00:00</div>
<div class="meta-line">Comments: accepted in ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16954v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16954v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep learning has shown remarkable progress in medical image semantic segmentation, yet its success heavily depends on large-scale expert annotations and consistent data distributions. In practice, annotations are scarce, and images are collected from multiple scanners or centers, leading to mixed-domain settings with unknown domain labels and severe domain gaps. Existing semi-supervised or domain adaptation approaches typically assume either a single domain shift or access to explicit domain indices, which rarely hold in real-world deployment. In this paper, we propose a domain-invariant mixed-domain semi-supervised segmentation framework that jointly enhances data diversity and mitigates domain bias. A Copy-Paste Mechanism (CPM) augments the training set by transferring informative regions across domains, while a Cluster Maximum Mean Discrepancy (CMMD) block clusters unlabeled features and aligns them with labeled anchors via an MMD objective, encouraging domain-invariant representations. Integrated within a teacher-student framework, our method achieves robust and precise segmentation even with very few labeled examples and multiple unknown domain discrepancies. Experiments on Fundus and M&amp;Ms benchmarks demonstrate that our approach consistently surpasses semi-supervised and domain adaptation methods, establishing a potential solution for mixed-domain semi-supervised medical image segmentation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于聚类最大均值差异对齐的领域不变混合领域半监督医学图像分割</div>
<div class="mono" style="margin-top:8px">深度学习在医学图像语义分割中取得了显著进展，但其成功高度依赖于大规模专家标注和一致的数据分布。在实际应用中，标注数据稀缺，图像来自多个扫描仪或中心，导致在未知领域标签和严重领域差异的混合领域设置下进行分割。现有的半监督或领域自适应方法通常假设单一领域转移或可访问显式的领域索引，这在现实部署中很少成立。本文提出了一种领域不变的混合领域半监督分割框架，联合提升数据多样性并减轻领域偏差。通过跨领域传递信息区域的复制粘贴机制（CPM）来扩充训练集，同时通过聚类最大均值差异（CMMD）模块，利用MMD目标对未标注特征进行聚类，并将其与标注锚点对齐，从而鼓励领域不变的表示。在教师-学生框架中集成我们的方法，即使在仅有少量标注样本和多个未知领域差异的情况下，也能实现稳健且精确的分割。在视网膜图像和M&amp;Ms基准数据集上的实验表明，我们的方法在半监督和领域自适应方法中表现一致优越，为混合领域半监督医学图像分割提供了一种潜在解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond the LUMIR challenge: The pathway to foundational registration models</div>
<div class="meta-line">Authors: Junyu Chen, Shuwen Wei, Joel Honkamaa, Pekka Marttinen, Hang Zhang, Min Liu, Yichao Zhou, Zuopeng Tan, Zhuoyuan Wang, Yi Wang, Hongchao Zhou, Shunbo Hu, Yi Zhang, Qian Tao, Lukas Förner, Thomas Wendler, Bailiang Jian, Benedikt Wiestler, Tim Hable, Jin Kim, Dan Ruan, Frederic Madesta, Thilo Sentker, Wiebke Heyer, Lianrui Zuo, Yuwei Dai, Jing Wu, Jerry L. Prince, Harrison Bai, Yong Du, Yihao Liu, Alessa Hering, Reuben Dorent, Lasse Hansen, Mattias P. Heinrich, Aaron Carass</div>
<div class="meta-line">First: 2025-05-30T03:07:58+00:00 · Latest: 2026-01-23T18:17:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.24160v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.24160v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Medical image challenges have played a transformative role in advancing the field, catalyzing innovation and establishing new performance benchmarks. Image registration, a foundational task in neuroimaging, has similarly advanced through the Learn2Reg initiative. Building on this, we introduce the Large-scale Unsupervised Brain MRI Image Registration (LUMIR) challenge, a next-generation benchmark for unsupervised brain MRI registration. Previous challenges relied upon anatomical label maps, however LUMIR provides 4,014 unlabeled T1-weighted MRIs for training, encouraging biologically plausible deformation modeling through self-supervision. Evaluation includes 590 in-domain test subjects and extensive zero-shot tasks across disease populations, imaging protocols, and species. Deep learning methods consistently achieved state-of-the-art performance and produced anatomically plausible, diffeomorphic deformation fields. They outperformed several leading optimization-based methods and remained robust to most domain shifts. These findings highlight the growing maturity of deep learning in neuroimaging registration and its potential to serve as a foundation model for general-purpose medical image registration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越LUMIR挑战：基础注册模型的发展路径</div>
<div class="mono" style="margin-top:8px">医学图像挑战在推动该领域发展方面发挥了变革性作用，激发了创新并建立了新的性能基准。在神经影像学中，图像配准是一项基础任务，也通过Learn2Reg计划取得了显著进展。在此基础上，我们引入了大规模无监督脑部MRI图像配准（LUMIR）挑战，作为下一代无监督脑部MRI配准的基准。以往的挑战依赖于解剖标签图，而LUMIR提供了4,014个未标记的T1加权MRI图像用于训练，通过自监督机制鼓励生物合理的形变建模。评估包括590个域内测试受试者以及跨疾病群体、成像协议和物种的广泛零样本任务。深度学习方法持续取得最先进的性能，并生成了具有解剖合理性的微分同胚形变场。它们优于几种领先的优化方法，并且对大多数域迁移具有鲁棒性。这些发现突显了深度学习在神经影像配准中的日益成熟，并展示了其作为通用医学图像配准基础模型的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">ProveRAG: Provenance-Driven Vulnerability Analysis with Automated Retrieval-Augmented LLMs</div>
<div class="meta-line">Authors: Reza Fayyazi, Stella Hoyos Trueba, Michael Zuzak, Shanchieh Jay Yang</div>
<div class="meta-line">Venue: IEEE Access, vol. 13, pp. 212815-212826, 2025</div>
<div class="meta-line">First: 2024-10-22T20:28:57+00:00 · Latest: 2026-01-23T18:13:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.17406v3">Abs</a> · <a href="https://arxiv.org/pdf/2410.17406v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In cybersecurity, security analysts constantly face the challenge of mitigating newly discovered vulnerabilities in real-time, with over 300,000 vulnerabilities identified since 1999. The sheer volume of known vulnerabilities complicates the detection of patterns for unknown threats. While LLMs can assist, they often hallucinate and lack alignment with recent threats. Over 40,000 vulnerabilities have been identified in 2024 alone, which are introduced after most popular LLMs&#x27; (e.g., GPT-5) training data cutoff. This raises a major challenge of leveraging LLMs in cybersecurity, where accuracy and up-to-date information are paramount. Therefore, we aim to improve the adaptation of LLMs in vulnerability analysis by mimicking how an analyst performs such tasks. We propose ProveRAG, an LLM-powered system designed to assist in rapidly analyzing vulnerabilities with automated retrieval augmentation of web data while self-evaluating its responses with verifiable evidence. ProveRAG incorporates a self-critique mechanism to help alleviate the omission and hallucination common in the output of LLMs applied in cybersecurity applications. The system cross-references data from verifiable sources (NVD and CWE), giving analysts confidence in the actionable insights provided. Our results indicate that ProveRAG excels in delivering verifiable evidence to the user with over 99% and 97% accuracy in exploitation and mitigation strategies, respectively. ProveRAG guides analysts to secure their systems more effectively by overcoming temporal and context-window limitations while also documenting the process for future audits.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ProveRAG：基于溯源的漏洞分析与自动化检索增强的LLM系统</div>
<div class="mono" style="margin-top:8px">在网络安全领域，安全分析师需要实时应对新发现的漏洞，自1999年以来已识别超过30万种漏洞。已知漏洞的庞大数量使得检测未知威胁的模式变得复杂。尽管大型语言模型（LLMs）可以提供帮助，但它们常常产生幻觉，并且与最新的威胁缺乏对齐。仅在2024年就已识别超过4万种漏洞，这些漏洞是在大多数主流LLM（如GPT-5）训练数据截止日期之后引入的。这提出了一个重大挑战，即如何在网络安全领域有效利用LLMs，因为准确性和最新信息至关重要。因此，我们旨在通过模拟分析师执行此类任务的方式来改进LLM在漏洞分析中的适应性。我们提出了ProveRAG，这是一个基于LLM的系统，旨在通过自动化检索增强网络数据来快速分析漏洞，同时利用可验证的证据自我评估其响应。ProveRAG集成了自我批评机制，以帮助缓解LLM在网络安全应用中输出时常见的遗漏和幻觉问题。该系统交叉引用来自可验证来源（如NVD和CWE）的数据，使分析师能够对提供的可操作见解充满信心。我们的实验结果表明，ProveRAG在提供可验证证据方面表现出色，其利用和缓解策略的准确率分别超过99%和97%。ProveRAG通过克服时间限制和上下文窗口限制，帮助分析师更有效地保障系统安全，同时为未来的审计记录整个过程。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient semantic uncertainty quantification in language models via diversity-steered sampling</div>
<div class="meta-line">Authors: Ji Won Park, Kyunghyun Cho</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-10-24T10:06:21+00:00 · Latest: 2026-01-23T18:02:21+00:00</div>
<div class="meta-line">Comments: 10 pages (+7 appendix), 7 figures. Accepted at NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21310v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.21310v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurately estimating semantic aleatoric and epistemic uncertainties in large language models (LLMs) is particularly challenging in free-form question answering (QA), where obtaining stable estimates often requires many expensive generations. We introduce a diversity-steered sampler that discourages semantically redundant outputs during decoding, covers both autoregressive and masked diffusion paradigms, and yields substantial sample-efficiency gains. The key idea is to inject a continuous semantic-similarity penalty into the model&#x27;s proposal distribution using a natural language inference (NLI) model lightly finetuned on partial prefixes or intermediate diffusion states. We debias downstream uncertainty estimates with importance reweighting and shrink their variance with control variates. Across four QA benchmarks, our method matches or surpasses baselines while covering more semantic clusters with the same number of samples. Being modular and requiring no gradient access to the base LLM, the framework promises to serve as a drop-in enhancement for uncertainty estimation in risk-sensitive model deployments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过多样性引导采样在语言模型中实现高效的语义不确定性量化</div>
<div class="mono" style="margin-top:8px">在自由形式问答（QA）中，准确估计大型语言模型（LLMs）中的语义随机不确定性和认知不确定性尤其具有挑战性，通常需要大量昂贵的生成才能获得稳定的估计。我们引入了一种多样性引导的采样器，在解码过程中抑制语义冗余的输出，适用于自回归和掩码扩散范式，并显著提升了采样效率。其核心思想是利用一个自然语言推理（NLI）模型，在模型的提议分布中注入连续的语义相似性惩罚，该模型仅在部分前缀或中间扩散状态上进行轻量微调。我们通过重要性重加权来减少下游不确定性估计的偏差，并通过控制变量来降低其方差。在四个问答基准测试中，我们的方法在相同采样数量下覆盖了更多的语义簇，且表现与基线相当或更优。该框架模块化设计，无需访问基础LLM的梯度，因此可作为风险敏感模型部署中不确定性估计的即插即用增强方案。</div>
</details>
</div>
<div class="card">
<div class="title">Decoupling Multi-Contrast Super-Resolution: Self-Supervised Implicit Re-Representation for Unpaired Cross-Modal Synthesis</div>
<div class="meta-line">Authors: Yinzhe Wu, Hongyu Rui, Fanwen Wang, Jiahao Huang, Zhenxuan Zhang, Haosen Zhang, Zi Wang, Guang Yang</div>
<div class="meta-line">First: 2025-05-09T07:48:52+00:00 · Latest: 2026-01-23T18:01:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.05855v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.05855v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-contrast super-resolution (MCSR) is crucial for enhancing MRI but current deep learning methods are limited. They typically require large, paired low- and high-resolution (LR/HR) training datasets, which are scarce, and are trained for fixed upsampling scales. While recent self-supervised methods remove the paired data requirement, they fail to leverage valuable population-level priors. In this work, we propose a novel, decoupled MCSR framework that resolves both limitations. We reformulate MCSR into two stages: (1) an unpaired cross-modal synthesis (uCMS) module, trained once on unpaired population data to learn a robust anatomical prior; and (2) a lightweight, patient-specific implicit re-representation (IrR) module. This IrR module is optimized in a self-supervised manner to fuse the population prior with the subject&#x27;s own LR target data. This design uniquely fuses population-level knowledge with patient-specific fidelity without requiring any paired LR/HR or paired cross-modal training data. By building the IrR module on an implicit neural representation, our framework is also inherently scale-agnostic. Our method demonstrates superior quantitative performance on different datasets, with exceptional robustness at extreme scales (16x, 32x), a regime where competing methods fail. Our work presents a data-efficient, flexible, and computationally lightweight paradigm for MCSR, enabling high-fidelity, arbitrary-scale</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>解耦多对比度超分辨率：用于非配对跨模态合成的自监督隐式重表示</div>
<div class="mono" style="margin-top:8px">多对比度超分辨率（MCSR）对于提升MRI至关重要，但当前的深度学习方法存在局限。它们通常需要大量配对的低分辨率和高分辨率（LR/HR）训练数据，而这些数据稀缺，并且仅适用于固定的上采样尺度。尽管最近的自监督方法消除了对配对数据的需求，但未能有效利用有价值的群体级先验信息。在本工作中，我们提出了一种新颖的解耦MCSR框架，解决了这两个限制。我们将MCSR重新表述为两个阶段：(1) 一个基于非配对跨模态合成（uCMS）的模块，通过在非配对群体数据上进行一次训练，学习到一个稳健的解剖学先验；(2) 一个轻量级、患者特异性的隐式重表示（IrR）模块。该IrR模块以自监督方式优化，将群体先验与患者的LR目标数据融合。这种设计独特地在不依赖任何配对LR/HR或跨模态训练数据的情况下，融合了群体级知识与患者特异性保真度。通过在隐式神经表示上构建IrR模块，我们的框架也具有固有的尺度无关性。我们的方法在不同数据集上展示了优越的定量性能，在极端尺度（16x、32x）下表现出卓越的鲁棒性，这是竞争方法无法实现的领域。我们的工作提出了一个数据高效、灵活且计算轻量的MCSR范式，实现了高保真、任意尺度的图像生成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-contrast super-resolution (MCSR) is crucial for enhancing MRI but current deep learning methods are limited.</div>
</details>
</div>
<div class="card">
<div class="title">Failures of Contingent Thinking</div>
<div class="meta-line">Authors: Evan Piermont, Peio Zuazo-Garin</div>
<div class="meta-line">First: 2020-07-15T14:21:16+00:00 · Latest: 2026-01-23T17:59:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2007.07703v4">Abs</a> · <a href="https://arxiv.org/pdf/2007.07703v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a behavioral definition of an agent&#x27;s perceived implication that uniquely identifies a subjective state-space representing her view of a decision problem, and which may differ from the modeler&#x27;s. By examining belief updating within this model, we formalize the recent empirical consensus that reducing uncertainty improves contingent thinking, and propose a novel form of updating corresponding to the agent &#x27;realizing&#x27; a flaw in her own thinking. Finally, we clarify the sense in which contingent thinking makes state-bystate dominance more cognitively demanding than obvious dominance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>或然思维的失败</div>
<div class="mono" style="margin-top:8px">我们提出了一种行为定义，用于描述一个代理所感知的蕴含关系，该定义能够唯一地识别出一个主观状态空间，代表她对决策问题的看法，这可能与建模者所使用的模型不同。通过在该模型中考察信念更新，我们正式化了最近的实证共识，即减少不确定性可以提升或然思维，并提出了一种新颖的更新形式，对应于代理意识到自身思维中的缺陷。最后，我们澄清了或然思维在何种意义上使状态间主导性比明显主导性更具认知挑战性。</div>
</details>
</div>
<div class="card">
<div class="title">Is BatchEnsemble a Single Model? On Calibration and Diversity of Efficient Ensembles</div>
<div class="meta-line">Authors: Anton Zamyatin, Patrick Indri, Sagar Malhotra, Thomas Gärtner</div>
<div class="meta-line">First: 2026-01-23T17:50:50+00:00 · Latest: 2026-01-23T17:50:50+00:00</div>
<div class="meta-line">Comments: Accepted at the 1st workshop on Epistemic Intelligence in Machine Learning at EurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16936v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16936v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In resource-constrained and low-latency settings, uncertainty estimates must be efficiently obtained. Deep Ensembles provide robust epistemic uncertainty (EU) but require training multiple full-size models. BatchEnsemble aims to deliver ensemble-like EU at far lower parameter and memory cost by applying learned rank-1 perturbations to a shared base network. We show that BatchEnsemble not only underperforms Deep Ensembles but closely tracks a single model baseline in terms of accuracy, calibration and out-of-distribution (OOD) detection on CIFAR10/10C/SVHN. A controlled study on MNIST finds members are near-identical in function and parameter space, indicating limited capacity to realize distinct predictive modes. Thus, BatchEnsemble behaves more like a single model than a true ensemble.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BatchEnsemble 是一个单模型吗？关于高效集成的校准与多样性</div>
<div class="mono" style="margin-top:8px">在资源受限和低延迟的设置中，必须高效地获得不确定性估计。深度集成提供了稳健的元认知不确定性（EU），但需要训练多个完整规模的模型。BatchEnsemble 通过在共享基础网络上应用学习到的秩-1扰动，以远较低的参数和内存成本实现类似集成的不确定性估计。我们发现 BatchEnsemble 不仅在准确率、校准和分布外（OOD）检测方面表现不如深度集成，而且在 CIFAR10/10C/SVHN 上几乎与单模型基线一致。在 MNIST 上的受控研究发现，各成员在函数空间和参数空间中几乎完全相同，表明其难以实现不同的预测模式。因此，BatchEnsemble 更像一个单模型，而非真正的集成。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In resource-constrained and low-latency settings, uncertainty estimates must be efficiently obtained.</div>
</details>
</div>
<div class="card">
<div class="title">Information Representation Fairness in Long-Document Embeddings: The Peculiar Interaction of Positional and Language Bias</div>
<div class="meta-line">Authors: Elias Schuhmacher, Andrianos Michail, Juri Opitz, Rico Sennrich, Simon Clematide</div>
<div class="meta-line">First: 2026-01-23T17:48:31+00:00 · Latest: 2026-01-23T17:48:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16934v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16934v1">PDF</a> · <a href="https://github.com/impresso/fair-sentence-transformers">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To be discoverable in an embedding-based search process, each part of a document should be reflected in its embedding representation. To quantify any potential reflection biases, we introduce a permutation-based evaluation framework. With this, we observe that state-of-the-art embedding models exhibit systematic positional and language biases when documents are longer and consist of multiple segments. Specifically, early segments and segments in higher-resource languages like English are over-represented, while later segments and segments in lower-resource languages are marginalized. In our further analysis, we find that the positional bias stems from front-loaded attention distributions in pooling-token embeddings, where early tokens receive more attention. To mitigate this issue, we introduce an inference-time attention calibration method that redistributes attention more evenly across document positions, increasing discoverabiltiy of later segments. Our evaluation framework and attention calibration is available at https://github.com/impresso/fair-sentence-transformers</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>长文档嵌入中的信息表示公平性：位置偏差与语言偏差的特殊交互</div>
<div class="mono" style="margin-top:8px">为了在基于嵌入的搜索过程中被发现，文档的每个部分都应在嵌入表示中有所体现。为量化任何潜在的表示偏差，我们引入了一种基于排列的评估框架。通过该框架，我们观察到当前最先进的嵌入模型在处理较长文档且包含多个段落时，会表现出系统性的位置偏差和语言偏差。具体而言，早期段落和高资源语言（如英语）的段落被过度代表，而后期段落和低资源语言的段落则被边缘化。在进一步分析中，我们发现位置偏差源于池化标记嵌入中的前加载注意力分布，其中早期标记获得更多的注意力。为缓解这一问题，我们引入了一种推理时的注意力校准方法，使注意力在文档位置上更加均匀地分布，从而提高后期段落的可发现性。我们的评估框架和注意力校准方法可在 https://github.com/impresso/fair-sentence-transformers 获取。</div>
</details>
</div>
<div class="card">
<div class="title">Reward-Forcing: Autoregressive Video Generation with Reward Feedback</div>
<div class="meta-line">Authors: Jingran Zhang, Ning Li, Yuanhao Ban, Andrew Bai, Justin Cui</div>
<div class="meta-line">First: 2026-01-23T17:47:56+00:00 · Latest: 2026-01-23T17:47:56+00:00</div>
<div class="meta-line">Comments: preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16933v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.16933v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While most prior work in video generation relies on bidirectional architectures, recent efforts have sought to adapt these models into autoregressive variants to support near real-time generation. However, such adaptations often depend heavily on teacher models, which can limit performance, particularly in the absence of a strong autoregressive teacher, resulting in output quality that typically lags behind their bidirectional counterparts. In this paper, we explore an alternative approach that uses reward signals to guide the generation process, enabling more efficient and scalable autoregressive generation. By using reward signals to guide the model, our method simplifies training while preserving high visual fidelity and temporal consistency. Through extensive experiments on standard benchmarks, we find that our approach performs comparably to existing autoregressive models and, in some cases, surpasses similarly sized bidirectional models by avoiding constraints imposed by teacher architectures. For example, on VBench, our method achieves a total score of 84.92, closely matching state-of-the-art autoregressive methods that score 84.31 but require significant heterogeneous distillation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励强制：基于奖励反馈的自回归视频生成</div>
<div class="mono" style="margin-top:8px">尽管大多数视频生成的先前工作依赖于双向架构，但最近的研究尝试将这些模型转换为自回归变体以支持近实时生成。然而，这种转换通常高度依赖于教师模型，这可能会限制性能，尤其是在缺乏强大自回归教师模型的情况下，导致输出质量通常落后于其双向对应模型。在本文中，我们探索了一种替代方法，利用奖励信号来指导生成过程，从而实现更高效且可扩展的自回归生成。通过使用奖励信号指导模型，我们的方法简化了训练过程，同时保持了高视觉保真度和时间一致性。在标准基准上的大量实验表明，我们的方法在性能上与现有自回归模型相当，并且在某些情况下，通过避免教师架构的约束，超越了同样规模的双向模型。例如，在VBench上，我们的方法取得了84.92的总分，接近当前最先进的自回归方法84.31，但后者需要大量的异构蒸馏。</div>
</details>
</div>
<div class="card">
<div class="title">Pretraining Frame Preservation in Autoregressive Video Memory Compression</div>
<div class="meta-line">Authors: Lvmin Zhang, Shengqu Cai, Muyang Li, Chong Zeng, Beijia Lu, Anyi Rao, Song Han, Gordon Wetzstein, Maneesh Agrawala</div>
<div class="meta-line">First: 2025-12-29T20:29:21+00:00 · Latest: 2026-01-23T17:47:41+00:00</div>
<div class="meta-line">Comments: Additional Results: https://lllyasviel.github.io/pfp_gitpage/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23851v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.23851v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://lllyasviel.github.io/pfp_gitpage/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present PFP, a neural network structure to compress long videos into short contexts, with an explicit pretraining objective to preserve the high-frequency details of single frames at arbitrary temporal positions. The baseline model can compress a 20-second video into a context at about 5k length, where random frames can be retrieved with perceptually preserved appearances. Such pretrained models can be directly fine-tuned as memory encoders for autoregressive video models, enabling long history memory with low context cost and relatively low fidelity loss. We evaluate the framework with ablative settings and discuss the trade-offs of possible neural architecture designs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自回归视频记忆压缩中的预训练帧保持</div>
<div class="mono" style="margin-top:8px">我们提出了PFP，一种神经网络结构，用于将长视频压缩为短上下文，并通过显式的预训练目标来保持单帧在任意时间位置的高频细节。基线模型可以将20秒的视频压缩为约5k长度的上下文，其中随机帧可以以感知保持的外观进行检索。此类预训练模型可以直接微调作为自回归视频模型的记忆编码器，实现低上下文成本和相对较低保真度损失的长历史记忆。我们通过消融实验评估了该框架，并讨论了可能的神经架构设计的权衡。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
