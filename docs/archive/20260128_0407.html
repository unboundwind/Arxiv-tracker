<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-28 04:07</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260128_0407</div>
    <div class="row"><div class="card">
<div class="title">Beyond Expected Goals: A Probabilistic Framework for Shot Occurrences in Soccer</div>
<div class="meta-line">Authors: Jonathan Pipping-Gamón, Tianshu Feng, R. Paul Sabin</div>
<div class="meta-line">First: 2025-11-28T20:59:29+00:00 · Latest: 2026-01-26T18:59:43+00:00</div>
<div class="meta-line">Comments: 18pp main + 3pp appendix; 8 figures, 12 tables. Submitted to the Journal of Quantitative Analysis in Sports (JQAS). Data proprietary to Gradient Sports; we share derived features &amp; scripts (code under MIT/Apache-2.0). Preprint licensed CC BY 4.0</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.00203v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.00203v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Expected goals (xG) models estimate the probability that a shot results in a goal from its context (e.g., location, pressure), but they operate only on observed shots. We propose xG+, a possession-level framework that first estimates the probability that a shot occurs within the next second and its corresponding xG if it were to occur. We also introduce ways to aggregate this joint probability estimate over the course of a possession. By jointly modeling shot-taking behavior and shot quality, xG+ remedies the conditioning-on-shots limitation of standard xG. We show that this improves predictive accuracy at the team level and produces a more persistent player skill signal than standard xG models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越预期进球：足球射门发生的概率性框架</div>
<div class="mono" style="margin-top:8px">预期进球（xG）模型根据射门的情境（例如位置、压力）估计射门进球的概率，但它们仅基于观察到的射门进行分析。我们提出了xG+，这是一个基于控球的框架，首先估计射门在接下来的一秒内发生的概率以及如果发生该射门对应的预期进球概率。我们还介绍了如何在整个控球过程中对这一联合概率进行聚合。通过联合建模射门行为和射门质量，xG+克服了标准xG模型的基于已发生射门的局限性。我们证明了这种方法在球队层面提高了预测准确性，并产生了比标准xG模型更持久的球员技能信号。</div>
</details>
</div>
<div class="card">
<div class="title">ctELM: Decoding and Manipulating Embeddings of Clinical Trials with Embedding Language Models</div>
<div class="meta-line">Authors: Brian Ondov, Chia-Hsuan Chang, Yujia Zhou, Mauro Giuffrè, Hua Xu</div>
<div class="meta-line">First: 2026-01-26T18:58:46+00:00 · Latest: 2026-01-26T18:58:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18796v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18796v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text embeddings have become an essential part of a variety of language applications. However, methods for interpreting, exploring and reversing embedding spaces are limited, reducing transparency and precluding potentially valuable generative use cases. In this work, we align Large Language Models to embeddings of clinical trials using the recently reported Embedding Language Model (ELM) method. We develop an open-source, domain-agnostic ELM architecture and training framework, design training tasks for clinical trials, and introduce an expert-validated synthetic dataset. We then train a series of ELMs exploring the impact of tasks and training regimes. Our final model, ctELM, can accurately describe and compare unseen clinical trials from embeddings alone and produce plausible clinical trials from novel vectors. We further show that generated trial abstracts are responsive to moving embeddings along concept vectors for age and sex of study subjects. Our public ELM implementation and experimental results will aid the alignment of Large Language Models to embedding spaces in the biomedical domain and beyond.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ctELM：使用嵌入语言模型解码和操作临床试验嵌入</div>
<div class="mono" style="margin-top:8px">文本嵌入已成为多种语言应用的重要组成部分。然而，用于解释、探索和逆向嵌入空间的方法有限，这降低了透明度并阻碍了潜在的生成性应用。在本工作中，我们使用最近提出的嵌入语言模型（ELM）方法，将大型语言模型对齐到临床试验的嵌入。我们开发了一个开源、领域无关的ELM架构和训练框架，设计了针对临床试验的训练任务，并引入了一个专家验证的合成数据集。随后，我们训练了一系列探索任务和训练方式影响的ELM模型。我们的最终模型ctELM能够仅通过嵌入准确描述和比较未见过的临床试验，并从新颖的向量生成合理的临床试验。我们进一步表明，生成的试验摘要对沿受试者年龄和性别概念向量移动嵌入具有响应性。我们的公开ELM实现和实验结果将有助于在生物医学领域及其他领域对大型语言模型进行嵌入空间对齐。</div>
</details>
</div>
<div class="card">
<div class="title">Kicking for Goal or Touch? An Expected Points Framework for Penalty Decisions in Rugby Union</div>
<div class="meta-line">Authors: Kenny Watts, Jonathan Pipping-Gamón</div>
<div class="meta-line">First: 2025-11-29T04:13:29+00:00 · Latest: 2026-01-26T18:57:33+00:00</div>
<div class="meta-line">Comments: 20 pages; 9 figures, 8 tables. Submitted to the Journal of Quantitative Analysis in Sports (JQAS). Code &amp; replication package: https://github.com/WhartonSABI/rugby-ep (data from a public source; mirrored in the repo with attribution). Preprint licensed CC BY 4.0</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.00312v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.00312v2">PDF</a> · <a href="https://github.com/WhartonSABI/rugby-ep">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Following a penalty in rugby union, teams typically choose between attempting a shot at goal or kicking to touch to pursue a try. We develop an Expected Points (EP) framework that quantifies the value of each option as a function of both field location and game context. Using phase-level data from the 2018/19 Premiership Rugby season (35,199 phases across 132 matches) and an angle-distance model of penalty kick success estimated from international records, we construct two surfaces: (i) the expected points of a possession beginning with a lineout, and (ii) the expected points of a kick at goal, taking into account the in-game consequences of made and missed kicks. We then compare these surfaces to produce decision maps that indicate where kicking for goal or kicking to touch maximizes expected return, and we analyze how the boundary shifts with game context and the expected meters gained to touch. Our results provide a unified, data-driven method for evaluating penalty decisions and can be tailored to team-specific kickers and lineout units. This study offers, to our knowledge, the first comprehensive EP-based assessment of penalty strategy in rugby union and outlines extensions to win-probability analysis and richer tracking data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>射门得分还是踢向触地？橄榄球联盟罚球决策的预期得分框架</div>
<div class="mono" style="margin-top:8px">在橄榄球联盟中，当获得罚球机会时，球队通常需要在尝试射门得分或踢向触地以争取达阵之间做出选择。我们开发了一个预期得分（Expected Points, EP）框架，该框架通过场地位置和比赛情境来量化每种选择的价值。利用2018/19赛季英超橄榄球联赛的相位级数据（共132场比赛，35,199个相位）以及基于国际比赛记录估计的罚球成功率角度-距离模型，我们构建了两个表面：（i）以争球开始的持球方预期得分，（ii）射门得分的预期得分，同时考虑了射中和射失对比赛的影响。随后，我们比较这两个表面，生成决策图，以指示在哪些位置射门得分或踢向触地能最大化预期回报，并分析边界如何随比赛情境和预期触地推进距离而变化。我们的研究提供了一种统一且基于数据的方法来评估罚球决策，并可针对球队特定的射门球员和争球单位进行定制。据我们所知，本研究首次提出了基于预期得分的全面罚球策略评估方法，并概述了其在胜率分析和更丰富的追踪数据方面的扩展应用。</div>
</details>
</div>
<div class="card">
<div class="title">Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes</div>
<div class="meta-line">Authors: Amrith Setlur, Zijian Wang, Andrew Cohen, Paria Rashidinejad, Sang Michael Xie</div>
<div class="meta-line">First: 2026-01-26T18:57:00+00:00 · Latest: 2026-01-26T18:57:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18795v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18795v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重用你的FLOPs：通过使用非常离政策前缀来扩展RL在困难问题上的应用</div>
<div class="mono" style="margin-top:8px">典型的用于LLM推理的强化学习（RL）方法在处理困难问题时会浪费计算资源，这些问题中正确的on-policy轨迹稀少，策略梯度消失，学习停滞。为了启动更高效的RL，我们考虑以离政策迹的形式重用旧的采样FLOPs（来自先前推理或RL训练）。标准的离政策法通过离政策据进行监督，导致RL优化过程中出现不稳定性。我们引入了PrefixRL，通过在成功离政策迹的前缀上进行条件建模，并运行on-policy RL来完成这些轨迹，从而绕过离政策的不稳定性。PrefixRL通过调节离政策迹长度来调整问题难度，从而增强在困难问题上的学习信号。我们证明了PrefixRL的目标不仅与标准RL目标一致，而且更加样本高效。实证上，我们发现了反向泛化现象：仅在前缀问题上进行训练的模型可以泛化到分布外的非前缀性能，且学到的策略通常与前缀中的策略不同。在我们的实验中，我们通过基础模型的拒绝采样来获取离政策迹，从而创建了一个自我提升的循环。在困难推理问题上，PrefixRL在考虑初始拒绝采样所消耗的计算资源后，仍比最强基线（基于离政策据的SFT然后RL）快两倍达到相同的训练奖励，并将最终奖励提高了三倍。这些收益可以迁移到保留的基准测试中，即使离政策迹来源于不同的模型家族，PrefixRL仍然有效，验证了其在实际应用中的灵活性。</div>
</details>
</div>
<div class="card">
<div class="title">MEGnifying Emotion: Sentiment Analysis from Annotated Brain Data</div>
<div class="meta-line">Authors: Brian Liu, Oiwi Parker Jones</div>
<div class="meta-line">First: 2026-01-26T18:55:44+00:00 · Latest: 2026-01-26T18:55:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18792v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18792v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Decoding emotion from brain activity could unlock a deeper understanding of the human experience. While a number of existing datasets align brain data with speech and with speech transcripts, no datasets have annotated brain data with sentiment. To bridge this gap, we explore the use of pre-trained Text-to-Sentiment models to annotate non invasive brain recordings, acquired using magnetoencephalography (MEG), while participants listened to audiobooks. Having annotated the text, we employ force-alignment of the text and audio to align our sentiment labels with the brain recordings. It is straightforward then to train Brainto-Sentiment models on these data. Experimental results show an improvement in balanced accuracy for Brain-to-Sentiment compared to baseline, supporting the proposed approach as a proof-of-concept for leveraging existing MEG datasets and learning to decode sentiment directly from the brain.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MEGnifying Emotion: 从标注脑数据中进行情感分析</div>
<div class="mono" style="margin-top:8px">从脑活动解码情感可以揭示人类体验的更深层次。虽然一些现有数据集将脑数据与语音及语音转录文本对齐，但尚无数据集对脑数据进行情感标注。为弥合这一差距，我们探索使用预训练的文本到情感模型对通过脑磁图（MEG）获取的非侵入式脑记录进行情感标注，这些记录是在参与者聆听有声书时采集的。在对文本进行标注后，我们通过文本与音频的强制对齐，将情感标签与脑记录对齐。由此，可以方便地在这些数据上训练脑到情感模型。实验结果表明，与基线方法相比，Brain-to-Sentiment模型在平衡准确率上有所提升，支持了该方法作为利用现有MEG数据集并直接从脑中学习解码情感的可行性验证。</div>
</details>
</div>
<div class="card">
<div class="title">Subword-Based Comparative Linguistics across 242 Languages Using Wikipedia Glottosets</div>
<div class="meta-line">Authors: Iaroslav Chelombitko, Mika Hämäläinen, Aleksey Komissarov</div>
<div class="meta-line">First: 2026-01-26T18:55:28+00:00 · Latest: 2026-01-26T18:55:28+00:00</div>
<div class="meta-line">Comments: 15 pages, 4 figues, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18791v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18791v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a large-scale comparative study of 242 Latin and Cyrillic-script languages using subword-based methodologies. By constructing &#x27;glottosets&#x27; from Wikipedia lexicons, we introduce a framework for simultaneous cross-linguistic comparison via Byte-Pair Encoding (BPE). Our approach utilizes rank-based subword vectors to analyze vocabulary overlap, lexical divergence, and language similarity at scale. Evaluations demonstrate that BPE segmentation aligns with morpheme boundaries 95% better than random baseline across 15 languages (F1 = 0.34 vs 0.15). BPE vocabulary similarity correlates significantly with genetic language relatedness (Mantel r = 0.329, p &lt; 0.001), with Romance languages forming the tightest cluster (mean distance 0.51) and cross-family pairs showing clear separation (0.82). Analysis of 26,939 cross-linguistic homographs reveals that 48.7% receive different segmentations across related languages, with variation correlating to phylogenetic distance. Our results provide quantitative macro-linguistic insights into lexical patterns across typologically diverse languages within a unified analytical framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用维基百科语料库进行242种拉丁语和西里尔文字语言的基于子词的比较语言学研究</div>
<div class="mono" style="margin-top:8px">我们使用基于子词的方法，对242种拉丁语和西里尔文字语言进行了大规模的比较研究。通过从维基百科词典构建&#x27;语料集&#x27;，我们引入了一个基于字节对编码（BPE）的跨语言比较框架。我们的方法利用基于排名的子词向量，以大规模分析词汇重叠、词汇分化和语言相似性。评估表明，在15种语言中，BPE分词比随机基线在95%的程度上更符合语素边界（F1 = 0.34 vs 0.15）。BPE词汇相似性与语言的遗传关系显著相关（Mantel r = 0.329, p &lt; 0.001），罗曼语族形成最紧密的聚类（平均距离0.51），而跨语系语言对则表现出明显的分离（0.82）。对26,939个跨语言同形词的分析显示，在相关语言中，48.7%的词汇接收不同的分词，这种差异与系统发生学距离相关。我们的研究结果在一个统一的分析框架中，提供了关于类型学多样语言中词汇模式的定量宏观语言学见解。</div>
</details>
</div>
<div class="card">
<div class="title">MortalMATH: Evaluating the Conflict Between Reasoning Objectives and Emergency Contexts</div>
<div class="meta-line">Authors: Etienne Lanzeray, Stephane Meilliez, Malo Ruelle, Damien Sileo</div>
<div class="meta-line">First: 2026-01-26T18:55:07+00:00 · Latest: 2026-01-26T18:55:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18790v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18790v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models are increasingly optimized for deep reasoning, prioritizing the correct execution of complex tasks over general conversation. We investigate whether this focus on calculation creates a &quot;tunnel vision&quot; that ignores safety in critical situations. We introduce MortalMATH, a benchmark of 150 scenarios where users request algebra help while describing increasingly life-threatening emergencies (e.g., stroke symptoms, freefall). We find a sharp behavioral split: generalist models (like Llama-3.1) successfully refuse the math to address the danger. In contrast, specialized reasoning models (like Qwen-3-32b and GPT-5-nano) often ignore the emergency entirely, maintaining over 95 percent task completion rates while the user describes dying. Furthermore, the computational time required for reasoning introduces dangerous delays: up to 15 seconds before any potential help is offered. These results suggest that training models to relentlessly pursue correct answers may inadvertently unlearn the survival instincts required for safe deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MortalMATH：评估推理目标与紧急情境之间的冲突</div>
<div class="mono" style="margin-top:8px">大型语言模型正越来越优化以实现深层推理，优先完成复杂任务而非一般对话。我们探讨这种对计算的专注是否会造成一种 &quot;隧道视野&quot;，在关键情境中忽视安全。我们引入了MortalMATH，这是一个包含150个场景的基准测试，用户在描述日益危险的紧急情况（如中风症状、自由落体）的同时请求代数帮助。我们发现了一个显著的行为差异：通用模型（如Llama-3.1）能够成功拒绝数学任务以应对危险。相反，专门用于推理的模型（如Qwen-3-32b和GPT-5-nano）常常完全忽略紧急情况，保持超过95%的任务完成率，即使用户正在描述死亡过程。此外，推理所需的计算时间引入了危险的延迟：在提供任何潜在帮助之前可能需要长达15秒。这些结果表明，训练模型不断追求正确答案可能会无意中遗忘安全部署所需的生存本能。</div>
</details>
</div>
<div class="card">
<div class="title">Unsupervised Text Segmentation via Kernel Change-Point Detection on Sentence Embeddings</div>
<div class="meta-line">Authors: Mumin Jia, Jairo Diaz-Rodriguez</div>
<div class="meta-line">First: 2026-01-26T18:54:34+00:00 · Latest: 2026-01-26T18:54:34+00:00</div>
<div class="meta-line">Comments: arXiv admin note: substantial text overlap with arXiv:2510.03437. substantial text overlap with arXiv:2510.03437. substantial text overlap with arXiv:2510.03437. substantial text overlap with arXiv:2510.03437</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18788v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18788v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unsupervised text segmentation is crucial because boundary labels are expensive, subjective, and often fail to transfer across domains and granularity choices. We propose Embed-KCPD, a training-free method that represents sentences as embedding vectors and estimates boundaries by minimizing a penalized KCPD objective. Beyond the algorithmic instantiation, we develop, to our knowledge, the first dependence-aware theory for KCPD under $m$-dependent sequences, a finite-memory abstraction of short-range dependence common in language. We prove an oracle inequality for the population penalized risk and a localization guarantee showing that each true change point is recovered within a window that is small relative to segment length. To connect theory to practice, we introduce an LLM-based simulation framework that generates synthetic documents with controlled finite-memory dependence and known boundaries, validating the predicted scaling behavior. Across standard segmentation benchmarks, Embed-KCPD often outperforms strong unsupervised baselines. A case study on Taylor Swift&#x27;s tweets illustrates that Embed-KCPD combines strong theoretical guarantees, simulated reliability, and practical effectiveness for text segmentation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于句子嵌入的核变化点检测无监督文本分割</div>
<div class="mono" style="margin-top:8px">无监督文本分割至关重要，因为边界标签昂贵、主观且常无法跨领域和粒度选择迁移。我们提出Embed-KCPD，一种无需训练的方法，将句子表示为嵌入向量，并通过最小化带惩罚的KCPD目标函数来估计边界。除了算法实现，我们开发了，据我们所知，首个针对$m$-依赖序列的依赖感知理论，这是语言中常见短程依赖的有限记忆抽象。我们证明了针对总体惩罚风险的Oracle不等式，并提供了一个定位保证，表明每个真实的变化点都能在相对于段长度较小的窗口内被恢复。为了将理论与实践连接，我们引入了一个基于大语言模型的模拟框架，生成具有可控有限记忆依赖和已知边界的合成文档，验证了预测的扩展行为。在标准的文本分割基准测试中，Embed-KCPD通常优于强大的无监督基线。对泰勒·斯威夫特推文的案例研究表明，Embed-KCPD结合了强大的理论保证、模拟可靠性和实际有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Mathematical Foundations of Polyphonic Music Generation via Structural Inductive Bias</div>
<div class="meta-line">Authors: Joonwon Seo</div>
<div class="meta-line">First: 2026-01-07T05:40:09+00:00 · Latest: 2026-01-26T18:53:22+00:00</div>
<div class="meta-line">Comments: Monograph. Code available at https://github.com/Chooseredone/Smart-Embedding-Music-Generation</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03612v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03612v2">PDF</a> · <a href="https://github.com/Chooseredone/Smart-Embedding-Music-Generation">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This monograph introduces a novel approach to polyphonic music generation by addressing the &quot;Missing Middle&quot; problem through structural inductive bias. Focusing on Beethoven&#x27;s piano sonatas as a case study, we empirically verify the independence of pitch and hand attributes using normalized mutual information (NMI=0.167) and propose the Smart Embedding architecture, achieving a 48.30% reduction in parameters. We provide rigorous mathematical proofs using information theory (negligible loss bounded at 0.153 bits), Rademacher complexity (28.09% tighter generalization bound), and category theory to demonstrate improved stability and generalization. Empirical results show a 9.47% reduction in validation loss, confirmed by SVD analysis and an expert listening study (N=53). This dual theoretical and applied framework bridges gaps in AI music generation, offering verifiable insights for mathematically grounded deep learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过结构归纳偏置生成复调音乐的数学基础</div>
<div class="mono" style="margin-top:8px">本专著通过解决&quot;缺失中间&quot;问题，提出了一种生成复调音乐的新方法，采用结构归纳偏置。以贝多芬钢琴奏鸣曲为案例研究，我们通过归一化互信息（NMI=0.167）实证验证了音高与手属性的独立性，并提出了Smart Embedding架构，实现了参数量减少48.30%。我们利用信息论（损失可忽略，上限为0.153比特）、Rademacher复杂度（泛化边界更紧28.09%）和范畴论提供了严谨的数学证明，以展示改进的稳定性与泛化能力。实证结果表明验证损失减少了9.47%，并通过SVD分析和专家听评研究（N=53）得到确认。这种双重理论与应用框架弥合了AI音乐生成中的空白，为数学基础的深度学习提供了可验证的见解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This monograph introduces a novel approach to polyphonic music generation by addressing the &quot;Missing Middle&quot; problem through structural inductive bias.</div>
</details>
</div>
<div class="card">
<div class="title">Design Techniques for LLM-Powered Interactive Storytelling: A Case Study of the Dramamancer System</div>
<div class="meta-line">Authors: Tiffany Wang, Yuqian Sun, Yi Wang, Melissa Roemmele, John Joon Young Chung, Max Kreminski</div>
<div class="meta-line">Venue: EMNLP</div>
<div class="meta-line">First: 2026-01-26T18:51:20+00:00 · Latest: 2026-01-26T18:51:20+00:00</div>
<div class="meta-line">Comments: Extended abstract presented at the 2025 Wordplay Workshop at EMNLP</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18785v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18785v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rise of Large Language Models (LLMs) has enabled a new paradigm for bridging authorial intent and player agency in interactive narrative. We consider this paradigm through the example of Dramamancer, a system that uses an LLM to transform author-created story schemas into player-driven playthroughs. This extended abstract outlines some design techniques and evaluation considerations associated with this system.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的互动叙事设计技术：以Dramamancer系统为例</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）的兴起为互动叙事中作者意图与玩家自主性的结合开创了新的范式。我们通过Dramamancer系统这一实例来探讨这一范式，该系统利用LLM将作者创作的故事框架转化为玩家驱动的叙事体验。本文扩展摘要概述了与该系统相关的若干设计技术与评估考量。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Objective Reinforcement Learning for Efficient Tactical Decision Making for Trucks in Highway Traffic</div>
<div class="meta-line">Authors: Deepthi Pathare, Leo Laine, Morteza Haghir Chehreghani</div>
<div class="meta-line">First: 2026-01-26T18:50:21+00:00 · Latest: 2026-01-26T18:50:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18783v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18783v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Balancing safety, efficiency, and operational costs in highway driving poses a challenging decision-making problem for heavy-duty vehicles. A central difficulty is that conventional scalar reward formulations, obtained by aggregating these competing objectives, often obscure the structure of their trade-offs. We present a Proximal Policy Optimization based multi-objective reinforcement learning framework that learns a continuous set of policies explicitly representing these trade-offs and evaluates it on a scalable simulation platform for tactical decision making in trucks. The proposed approach learns a continuous set of Pareto-optimal policies that capture the trade-offs among three conflicting objectives: safety, quantified in terms of collisions and successful completion; energy efficiency and time efficiency, quantified using energy cost and driver cost, respectively. The resulting Pareto frontier is smooth and interpretable, enabling flexibility in choosing driving behavior along different conflicting objectives. This framework allows seamless transitions between different driving policies without retraining, yielding a robust and adaptive decision-making strategy for autonomous trucking applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于卡车高速公路交通高效战术决策的多目标强化学习</div>
<div class="mono" style="margin-top:8px">在高速驾驶中平衡安全、效率和运营成本，对重型车辆提出了具有挑战性的决策问题。一个主要困难是，通过聚合这些竞争目标得到的传统标量奖励公式往往掩盖了其权衡结构。我们提出了一种基于近端策略优化的多目标强化学习框架，该框架学习一个连续的策略集，显式地表示这些权衡，并在用于卡车战术决策的可扩展模拟平台上进行评估。所提出的方法学习一个连续的帕累托最优策略集，捕捉安全（以碰撞和成功完成量化）、能源效率和时间效率（分别以能源成本和驾驶员成本量化）这三个冲突目标之间的权衡。所得到的帕累托前沿是平滑且可解释的，使得在不同冲突目标之间选择驾驶行为具有灵活性。该框架允许在不同驾驶策略之间无缝切换，无需重新训练，从而为自动驾驶卡车应用提供了一种稳健且适应性强的决策策略。</div>
</details>
</div>
<div class="card">
<div class="title">POPE: Learning to Reason on Hard Problems via Privileged On-Policy Exploration</div>
<div class="meta-line">Authors: Yuxiao Qu, Amrith Setlur, Virginia Smith, Ruslan Salakhutdinov, Aviral Kumar</div>
<div class="meta-line">First: 2026-01-26T18:47:21+00:00 · Latest: 2026-01-26T18:47:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18779v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18779v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has improved the reasoning abilities of large language models (LLMs), yet state-of-the-art methods still fail to learn on many training problems. On hard problems, on-policy RL rarely explores even a single correct rollout, yielding zero reward and no learning signal for driving improvement. We find that natural solutions to remedy this exploration problem from classical RL, such as entropy bonuses, more permissive clipping of the importance ratio, or direct optimization of pass@k objectives, do not resolve this issue and often destabilize optimization without improving solvability. A natural alternative is to leverage transfer from easier problems. However, we show that mixing easy and hard problems during RL training is counterproductive due to ray interference, where optimization focuses on already-solvable problems in a way that actively inhibits progress on harder ones. To address this challenge, we introduce Privileged On-Policy Exploration (POPE), an approach that leverages human- or other oracle solutions as privileged information to guide exploration on hard problems, unlike methods that use oracle solutions as training targets (e.g., off-policy RL methods or warmstarting from SFT). POPE augments hard problems with prefixes of oracle solutions, enabling RL to obtain non-zero rewards during guided rollouts. Crucially, the resulting behaviors transfer back to the original, unguided problems through a synergy between instruction-following and reasoning. Empirically, POPE expands the set of solvable problems and substantially improves performance on challenging reasoning benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>POPE：通过特权的在线策略探索学习解决难题</div>
<div class="mono" style="margin-top:8px">强化学习（RL）已提升了大型语言模型（LLMs）的推理能力，但最先进的方法在许多训练问题上仍无法有效学习。在难题上，在线策略RL很少探索到单一正确的轨迹，导致零奖励且没有驱动改进的学习信号。我们发现，从经典RL中自然得出的解决这一探索问题的方法，如熵奖励、更宽松的重要性比率裁剪或直接优化pass@k目标，无法解决该问题，且常导致优化不稳定而无性能提升。一种自然的替代方法是利用更容易的问题进行迁移学习。然而，我们发现，在RL训练过程中混合简单和困难问题会因射线干扰（ray interference）而适得其反，其中优化会专注于已经可解的问题，从而主动抑制对更难问题的进展。为了解决这一挑战，我们引入了特权在线策略探索（Privileged On-Policy Exploration, POPE），该方法利用人类或其他oracle的解决方案作为特权信息，引导在线策略探索难题，这与使用oracle解决方案作为训练目标的方法（如离策略RL方法或从SFT进行预热）不同。POPE通过在难题中添加oracle解决方案的前缀，使RL能够在引导轨迹中获得非零奖励。关键的是，通过遵循指令与推理的协同作用，这些行为能够迁移到原始的未引导问题上。实验证明，POPE扩展了可解问题的集合，并显著提升了在挑战性推理基准上的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability</div>
<div class="meta-line">Authors: Shobhita Sundaram, John Quan, Ariel Kwiatkowski, Kartik Ahuja, Yann Ollivier, Julia Kempe</div>
<div class="meta-line">First: 2026-01-26T18:46:56+00:00 · Latest: 2026-01-26T18:46:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18778v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18778v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>让模型学会自我教学：在可学习边界进行推理</div>
<div class="mono" style="margin-top:8px">一个模型能否学习以突破自身的学习平台期？用于微调大型推理模型的强化学习方法在初始成功率较低的数据集上会停滞，因此缺乏训练信号。我们探讨了一个基本问题：一个预训练的大语言模型能否利用其潜在知识，为无法解决的问题生成自动化的课程？为此，我们设计了SOAR：一个自我改进框架，通过元强化学习（meta-RL）来揭示这些教学信号。一个教师模型副本为学生模型副本提出合成问题，并因其在一小部分困难问题上的改进而获得奖励。关键的是，SOAR将课程建立在可衡量的学生进展之上，而非内在的代理奖励。我们对数学基准中最难子集（0/128成功率）的研究揭示了三个核心发现。首先，我们展示了如何通过增强预训练模型生成有用学习步骤的潜在能力，实现双层元强化学习，从而在稀疏、二元奖励下解锁学习。其次，基于实际进展的奖励机制优于之前LLM自玩中使用的内在奖励方案，能够可靠地避免其通常表现出的不稳定性与多样性崩溃模式。第三，分析生成的问题表明，结构质量和问题的清晰性比解题正确性对学习进展更为关键。我们的结果表明，生成有用学习步骤的能力并不需要预先具备解决困难问题的能力，从而为在无需额外人工整理数据的情况下突破推理平台期提供了一条原则性的路径。</div>
</details>
</div>
<div class="card">
<div class="title">PRECISE: Reducing the Bias of LLM Evaluations Using Prediction-Powered Ranking Estimation</div>
<div class="meta-line">Authors: Abhishek Divekar, Anirban Majumder</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2026-01-26T18:46:49+00:00 · Latest: 2026-01-26T18:46:49+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI 2026 - Innovative Applications of AI (IAAI-26)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18777v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18777v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating the quality of search, ranking and RAG systems traditionally requires a significant number of human relevance annotations. In recent times, several deployed systems have explored the usage of Large Language Models (LLMs) as automated judges for this task while their inherent biases prevent direct use for metric estimation. We present a statistical framework extending Prediction-Powered Inference (PPI) that combines minimal human annotations with LLM judgments to produce reliable estimates of metrics which require sub-instance annotations. Our method requires as few as 100 human-annotated queries and 10,000 unlabeled examples, reducing annotation requirements significantly compared to traditional approaches. We formulate our proposed framework (PRECISE) for inference of relevance uplift for an LLM-based query reformulation application, extending PPI to sub-instance annotations at the query-document level. By reformulating the metric-integration space, we reduced the computational complexity from O(2^|C|) to O(2^K), where |C| represents corpus size (in order of millions). Detailed experiments across prominent retrieval datasets demonstrate that our method reduces the variance of estimates for the business-critical Precision@K metric, while effectively correcting for LLM bias in low-resource settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PRECISE：利用预测驱动的排序估计减少大语言模型评估偏差</div>
<div class="mono" style="margin-top:8px">评估搜索、排序和RAG系统的质量通常需要大量的人工相关性标注。近年来，一些部署的系统尝试使用大语言模型（LLMs）作为自动评估者，但其固有的偏差限制了其直接用于指标估计的可行性。我们提出了一种扩展预测驱动推理（PPI）的统计框架，结合少量人工标注与LLM判断，以生成需要子实例标注的指标的可靠估计。我们的方法仅需100个人工标注的查询和10,000个未标注示例，显著降低了标注需求。我们为基于LLM的查询重写应用提出了所提出的框架（PRECISE）用于相关性提升的推理，并将PPI扩展到查询-文档级别的子实例标注。通过重新定义指标整合空间，我们将计算复杂度从O(2^|C|)降低到O(2^K)，其中|C|表示语料库大小（通常为数百万）。在多个主流检索数据集上的详细实验表明，我们的方法在资源有限的环境下有效降低了Precision@K等关键业务指标估计的方差，同时纠正了LLM的偏差。</div>
</details>
</div>
<div class="card">
<div class="title">Col-OSSOS: Investigating the Origins of Different Surfaces in the Primordial Kuiper Belt</div>
<div class="meta-line">Authors: Laura E. Buchanan, Megan E. Schwamb, Wesley C. Fraser, Michele T. Bannister, J. J. Kavelaars, Michaël Marsset, Rosemary E. Pike, David Nesvorný, Samantha M. Lawler, Susan D. Benecchi, Nuno Peixinho, Nicole J. Tan, Kathryn Volk, Mike Alexandersen, Jean-Marc Petit</div>
<div class="meta-line">First: 2026-01-26T18:46:34+00:00 · Latest: 2026-01-26T18:46:34+00:00</div>
<div class="meta-line">Comments: 20 pages, 8 figures, accepted for publication in PSJ</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18776v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18776v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Colours of the Outer Solar System Origins Survey (Col-OSSOS) measured the optical/NIR colours of a brightness-complete sample of Trans-Neptunian Objects (TNOs). Like previous surveys, this one found a bimodal colour distribution in TNOs, categorised as red and very red. Additionally, this survey proposed an alternative surface classification scheme: FaintIR and BrightIR. Cold classical TNOs mostly have very red or FaintIR surfaces, while dynamically excited TNOs show a mixture of surfaces. This likely indicates that formation locations and proximity to the Sun influenced surface characteristics and color changes. Our study combines the data from Col-OSSOS with two dynamical models describing the formation of the Kuiper belt during Neptune&#x27;s migration. We investigate the proposed surface-colour changing line and explore the distribution of different surfaces within the primordial disk. By comparing radial colour transitions across various scenarios, we explore the origins of surface characteristics and their implications within the context of BrightIR and FaintIR classifications. Moreover, we extend our analysis to examine the distribution of these surface classes within the present-day Kuiper Belt, providing insights into the configuration of the early solar system&#x27;s planetesimal disk prior to giant planet migration. We find that the most likely primordial disk compositions are inner neutral / outer red (with transition $30.0^{+1.1}_{-1.2}$ au), or inner BrightIR / outer FaintIR (with transition $31.5^{+1.1}_{-1.2}$ au).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Col-OSSOS：探究原初柯伊伯带天体表面差异的起源</div>
<div class="mono" style="margin-top:8px">Colours of the Outer Solar System Origins Survey (Col-OSSOS) 测量了亮度完整的跨海王星天体（TNOs）的光学/近红外颜色。与以往的调查类似，本次调查也发现TNOs存在双峰颜色分布，分为红色和非常红色两类。此外，本次调查提出了一种替代的表面分类方案：FaintIR和BrightIR。冷经典TNOs主要具有非常红色或FaintIR表面，而动力学激发的TNOs则表现出表面颜色的混合。这可能表明形成位置和与太阳的距离影响了表面特征和颜色变化。我们的研究将Col-OSSOS的数据与两个描述海王星迁移期间柯伊伯带形成过程的动力学模型相结合。我们探讨了所提出的表面-颜色变化线，并研究了不同表面在原初盘中的分布。通过比较不同情景下的径向颜色变化，我们探索了表面特征的起源及其在BrightIR和FaintIR分类背景下的意义。此外，我们将分析扩展到研究这些表面类型在现今柯伊伯带中的分布，从而提供关于巨行星迁移前早期太阳系星子盘结构的见解。我们发现，原初盘最可能的组成是内侧中性/外侧红色（过渡半径 $30.0^{+1.1}_{-1.2}$ au），或内侧BrightIR/外侧FaintIR（过渡半径 $31.5^{+1.1}_{-1.2}$ au）。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The Colours of the Outer Solar System Origins Survey (Col-OSSOS) measured the optical/NIR colours of a brightness-complete sample of Trans-Neptunian Objects (TNOs).</div>
</details>
</div>
<div class="card">
<div class="title">DeltaDorsal: Enhancing Hand Pose Estimation with Dorsal Features in Egocentric Views</div>
<div class="meta-line">Authors: William Huang, Siyou Pei, Leyi Zou, Eric J. Gonzalez, Ishan Chatterjee, Yang Zhang</div>
<div class="meta-line">First: 2026-01-21T23:00:43+00:00 · Latest: 2026-01-26T18:45:41+00:00</div>
<div class="meta-line">Comments: 16 pages, 11 figures, Presented at ACM CHI 2026. For associated codebase, see https://github.com/hilab-open-source/deltadorsal</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15516v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15516v2">PDF</a> · <a href="https://github.com/hilab-open-source/deltadorsal">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The proliferation of XR devices has made egocentric hand pose estimation a vital task, yet this perspective is inherently challenged by frequent finger occlusions. To address this, we propose a novel approach that leverages the rich information in dorsal hand skin deformation, unlocked by recent advances in dense visual featurizers. We introduce a dual-stream delta encoder that learns pose by contrasting features from a dynamic hand with a baseline relaxed position. Our evaluation demonstrates that, using only cropped dorsal images, our method reduces the Mean Per Joint Angle Error (MPJAE) by 18% in self-occluded scenarios (fingers &gt;= 50% occluded) compared to state-of-the-art techniques that depend on the whole hand&#x27;s geometry and large model backbones. Consequently, our method not only enhances the reliability of downstream tasks like index finger pinch and tap estimation in occluded scenarios but also unlocks new interaction paradigms, such as detecting isometric force for a surface &quot;click&quot; without visible movement while minimizing model size.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeltaDorsal: 利用背侧特征提升第一视角手部姿态估计</div>
<div class="mono" style="margin-top:8px">随着XR设备的普及，第一视角手部姿态估计成为一项关键任务，但该视角由于手指频繁遮挡而面临固有挑战。为了解决这一问题，我们提出了一种新方法，利用近期密集视觉特征提取器所释放的丰富背侧手部皮肤变形信息。我们引入了一种双流delta编码器，通过将动态手部特征与基线放松位置进行对比来学习姿态。我们的评估表明，在仅使用裁剪后的背侧图像的情况下，与依赖完整手部几何结构和大型模型主干的现有技术相比，我们的方法在自遮挡场景（手指遮挡率≥50%）中将平均关节角度误差（MPJAE）降低了18%。因此，我们的方法不仅提升了遮挡场景下后续任务（如食指捏合和点击估计）的可靠性，还解锁了新的交互范式，例如在无明显运动的情况下检测等距力以实现表面点击，同时减小模型规模。</div>
</details>
</div>
<div class="card">
<div class="title">Dep-Search: Learning Dependency-Aware Reasoning Traces with Persistent Memory</div>
<div class="meta-line">Authors: Yanming Liu, Xinyue Peng, Zixuan Yan, Yanxin Shen, Wenjie Xu, Yuefeng Huang, Xinyi Wang, Jiannan Cao, Jianwei Yin, Xuhong Zhang</div>
<div class="meta-line">First: 2026-01-26T18:42:33+00:00 · Latest: 2026-01-26T18:42:33+00:00</div>
<div class="meta-line">Comments: Dep-Search 1st version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18771v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18771v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have demonstrated remarkable capabilities in complex reasoning tasks, particularly when augmented with search mechanisms that enable systematic exploration of external knowledge bases. The field has evolved from traditional retrieval-augmented generation (RAG) frameworks to more sophisticated search-based frameworks that orchestrate multi-step reasoning through explicit search strategies. However, existing search frameworks still rely heavily on implicit natural language reasoning to determine search strategies and how to leverage retrieved information across reasoning steps. This reliance on implicit reasoning creates fundamental challenges for managing dependencies between sub-questions, efficiently reusing previously retrieved knowledge, and learning optimal search strategies through reinforcement learning. To address these limitations, we propose Dep-Search, a dependency-aware search framework that advances beyond existing search frameworks by integrating structured reasoning, retrieval, and persistent memory through GRPO. Dep-Search introduces explicit control mechanisms that enable the model to decompose questions with dependency relationships, retrieve information when needed, access previously stored knowledge from memory, and summarize long reasoning contexts into reusable memory entries. Through extensive experiments on seven diverse question answering datasets, we demonstrate that Dep-Search significantly enhances LLMs&#x27; ability to tackle complex multi-hop reasoning tasks, achieving substantial improvements over strong baselines across different model scales.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Dep-Search：使用持久内存进行依赖感知推理轨迹学习</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在复杂推理任务中表现出色，尤其是在结合搜索机制以系统探索外部知识库的情况下。该领域已从传统的检索增强生成（RAG）框架演进到更复杂的基于搜索的框架，这些框架通过显式的搜索策略协调多步推理。然而，现有的搜索框架仍然严重依赖隐式的自然语言推理来确定搜索策略以及如何在推理步骤中利用检索到的信息。这种对隐式推理的依赖给管理子问题之间的依赖关系、高效重用先前检索到的知识以及通过强化学习学习最优搜索策略带来了根本性挑战。为了解决这些局限性，我们提出了Dep-Search，一个依赖感知的搜索框架，通过GRPO将结构化推理、检索和持久内存相结合，超越了现有搜索框架。Dep-Search引入了显式的控制机制，使模型能够分解具有依赖关系的问题，按需检索信息，访问内存中存储的先前知识，并将长推理上下文总结为可重用的内存条目。通过在七个多样化的问答数据集上的大量实验，我们证明Dep-Search显著增强了LLMs处理复杂多跳推理任务的能力，在不同模型规模下均优于强大的基线模型。</div>
</details>
</div>
<div class="card">
<div class="title">HiCache: A Plug-in Scaled-Hermite Upgrade for Taylor-Style Cache-then-Forecast Diffusion Acceleration</div>
<div class="meta-line">Authors: Liang Feng, Shikang Zheng, Jiacheng Liu, Yuqi Lin, Qinming Zhou, Peiliang Cai, Xinyu Wang, Junjie Chen, Chang Zou, Yue Ma, Linfeng Zhang</div>
<div class="meta-line">First: 2025-08-23T10:35:16+00:00 · Latest: 2026-01-26T18:39:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.16984v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.16984v2">PDF</a> · <a href="https://github.com/fenglang918/HiCache">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have achieved remarkable success in content generation but often incur prohibitive computational costs due to iterative sampling. Recent feature caching methods accelerate inference via temporal extrapolation, yet can suffer quality degradation from inaccurate modeling of the complex dynamics of feature evolution. We propose HiCache (Hermite Polynomial-based Feature Cache), a training-free acceleration framework that improves feature prediction by aligning mathematical tools with empirical properties. Our key insight is that feature-derivative approximations in diffusion Transformers exhibit multivariate Gaussian characteristics, motivating the use of Hermite polynomials as a potentially optimal basis for Gaussian-correlated processes. We further introduce a dual-scaling mechanism that ensures numerical stability while preserving predictive accuracy, and is also effective when applied standalone or integrated with TaylorSeer. Extensive experiments demonstrate HiCache&#x27;s superiority, achieving 5.55x speedup on FLUX.1-dev while matching or exceeding baseline quality, and maintaining strong performance across text-to-image, video generation, and super-resolution tasks. Moreover, HiCache can be naturally added to previous caching methods to enhance their performance, e.g., improving ClusCa from 0.9480 to 0.9840 in terms of image rewards. Code: https://github.com/fenglang918/HiCache</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HiCache：一种基于泰勒风格缓存后预测的扩散加速插件式扩展</div>
<div class="mono" style="margin-top:8px">扩散模型在内容生成方面取得了显著成功，但由于迭代采样的原因，通常会带来计算成本过高的问题。最近的特征缓存方法通过时间外推加速推理，但可能因对特征演化的复杂动态建模不准确而导致质量下降。我们提出HiCache（基于Hermite多项式的特征缓存），这是一个无需训练的加速框架，通过将数学工具与实证特性对齐来提升特征预测。我们的关键洞察是，扩散Transformer中的特征导数近似表现出多元高斯特性，这促使我们使用Hermite多项式作为高斯相关过程的潜在最优基。我们进一步引入了一种双尺度机制，确保数值稳定性同时保持预测准确性，并且在单独应用或与TaylorSeer集成时也有效。大量实验表明HiCache具有优越性，在FLUX.1-dev上实现了5.55倍的速度提升，同时匹配或超越基线质量，并在文本到图像、视频生成和超分辨率任务中保持强劲性能。此外，HiCache可以自然地添加到先前的缓存方法中以增强其性能，例如将ClusCa的图像奖励从0.9480提升至0.9840。代码：https://github.com/fenglang918/HiCache</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Discover: A Generalized Framework for Raga Identification without Forgetting</div>
<div class="meta-line">Authors: Parampreet Singh, Somya Kumar, Chaitanya Shailendra Nitawe, Vipul Arora</div>
<div class="meta-line">First: 2026-01-26T18:37:30+00:00 · Latest: 2026-01-26T18:37:30+00:00</div>
<div class="meta-line">Comments: Accepted at NCC 2026 conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18766v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18766v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Raga identification in Indian Art Music (IAM) remains challenging due to the presence of numerous rarely performed Ragas that are not represented in available training datasets. Traditional classification models struggle in this setting, as they assume a closed set of known categories and therefore fail to recognise or meaningfully group previously unseen Ragas. Recent works have tried categorizing unseen Ragas, but they run into a problem of catastrophic forgetting, where the knowledge of previously seen Ragas is diminished. To address this problem, we adopt a unified learning framework that leverages both labeled and unlabeled audio, enabling the model to discover coherent categories corresponding to the unseen Ragas, while retaining the knowledge of previously known ones. We test our model on benchmark Raga Identification datasets and demonstrate its performance in categorizing previously seen, unseen, and all Raga classes. The proposed approach surpasses the previous NCD-based pipeline even in discovering the unseen Raga categories, offering new insights into representation learning for IAM tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习发现：一种不遗忘的印度艺术音乐拉格识别通用框架</div>
<div class="mono" style="margin-top:8px">由于存在大量罕见且未在现有训练数据集中表示的印度艺术音乐（IAM）拉格，拉格识别仍然具有挑战性。传统分类模型在该场景下表现不佳，因为它们假设类别是封闭的已知集合，因此无法识别或有意义地归类之前未见过的拉格。最近的研究尝试对未见过的拉格进行分类，但遇到了灾难性遗忘的问题，即之前见过的拉格的知识会减少。为了解决这一问题，我们采用了一个统一的学习框架，利用有标签和无标签音频，使模型能够发现与未见过的拉格相对应的连贯类别，同时保留之前已知拉格的知识。我们在基准拉格识别数据集上测试了我们的模型，并展示了其在分类已见、未见以及所有拉格类别上的表现。所提出的这种方法即使在发现未见拉格类别方面也超越了之前的基于NCD的流程，为IAM任务的表示学习提供了新的见解。</div>
</details>
</div>
<div class="card">
<div class="title">Consistent Kernel Change-Point Detection under m-Dependence for Text Segmentation</div>
<div class="meta-line">Authors: Jairo Diaz-Rodriguez, Mumin Jia</div>
<div class="meta-line">First: 2025-10-03T18:57:22+00:00 · Latest: 2026-01-26T18:36:37+00:00</div>
<div class="meta-line">Comments: This paper is withdrawn due to an error in the proof of Proposition 3, which is used to support Theorem 1</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.03437v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.03437v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Kernel change-point detection (KCPD) has become a widely used tool for identifying structural changes in complex data. While existing theory establishes consistency under independence assumptions, real-world sequential data such as text exhibits strong dependencies. We establish new guarantees for KCPD under $m$-dependent data: specifically, we prove consistency in the number of detected change points and weak consistency in their locations under mild additional assumptions. We perform an LLM-based simulation that generates synthetic $m$-dependent text to validate the asymptotics. To complement these results, we present the first comprehensive empirical study of KCPD for text segmentation with modern embeddings. Across diverse text datasets, KCPD with text embeddings outperforms baselines in standard text segmentation metrics. We demonstrate through a case study on Taylor Swift&#x27;s tweets that KCPD not only provides strong theoretical and simulated reliability but also practical effectiveness for text segmentation tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在m-依赖性下用于文本分割的一致核变化点检测</div>
<div class="mono" style="margin-top:8px">核变化点检测（KCPD）已成为识别复杂数据结构变化的常用工具。尽管现有理论在独立性假设下建立了其一致性，但现实世界中的序列数据（如文本）往往表现出强依赖性。我们为m依赖数据下的KCPD建立了新的保证：在轻微的额外假设下，我们证明了检测到的变化点数量的一致性及其位置的弱一致性。我们通过基于大语言模型（LLM）的模拟生成合成的m依赖文本以验证渐近性质。为了补充这些结果，我们提出了首个针对文本分割的KCPD全面实证研究，使用现代嵌入方法。在多样化的文本数据集上，基于文本嵌入的KCPD在标准文本分割指标上优于基线方法。通过泰勒·斯威夫特推文的案例研究，我们展示了KCPD不仅在理论和模拟中具有强可靠性，而且在实际文本分割任务中也表现出良好的效果。</div>
</details>
</div>
<div class="card">
<div class="title">Brain-Inspired Perspective on Configurations: Unsupervised Similarity and Early Cognition</div>
<div class="meta-line">Authors: Juntang Wang, Yihan Wang, Hao Wu, Dongmian Zou, Shixin Xu</div>
<div class="meta-line">First: 2025-10-22T04:28:23+00:00 · Latest: 2026-01-26T18:34:12+00:00</div>
<div class="meta-line">Comments: 13 pages, 4 figures, conference paper. Equal contribution: Juntang Wang, Yihan Wang and Hao Wu</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.19229v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.19229v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Infants discover categories, detect novelty, and adapt to new contexts without supervision-a challenge for current machine learning. We present a brain-inspired perspective on configurations, a finite-resolution clustering framework that uses a single resolution parameter and attraction-repulsion dynamics to yield hierarchical organization, novelty sensitivity, and flexible adaptation. To evaluate these properties, we introduce mheatmap, which provides proportional heatmaps and reassignment algorithm to fairly assess multi-resolution and dynamic behavior. Across datasets, configurations are competitive on standard clustering metrics, achieve 87% AUC in novelty detection, and show 35% better stability during dynamic category evolution. These results position configurations as a principled computational model of early cognitive categorization and a step toward brain-inspired AI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大脑启发的构型视角：无监督相似性与早期认知</div>
<div class="mono" style="margin-top:8px">婴儿能够在无监督的情况下发现类别、检测新颖性并适应新情境，这对当前的机器学习提出了挑战。我们提出了一种基于大脑启发的构型视角，这是一种有限分辨率的聚类框架，通过单一分辨率参数和吸引-排斥动力学机制，实现层次化组织、新颖性敏感性和灵活适应。为评估这些特性，我们引入了mheatmap，它提供了比例热图和重新分配算法，以公平地评估多分辨率和动态行为。在多个数据集上，构型在标准聚类指标上具有竞争力，在新颖性检测中达到87%的AUC，并在动态类别演化过程中表现出35%更高的稳定性。这些结果表明，构型是一种早期认知分类的原理性计算模型，并为大脑启发的AI研究提供了重要一步。</div>
</details>
</div>
<div class="card">
<div class="title">DGFusion: Depth-Guided Sensor Fusion for Robust Semantic Perception</div>
<div class="meta-line">Authors: Tim Broedermannn, Christos Sakaridis, Luigi Piccinelli, Wim Abbeloos, Luc Van Gool</div>
<div class="meta-line">First: 2025-09-11T20:03:00+00:00 · Latest: 2026-01-26T18:33:05+00:00</div>
<div class="meta-line">Comments: Code and models are available at https://github.com/timbroed/DGFusion</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.09828v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.09828v3">PDF</a> · <a href="https://github.com/timbroed/DGFusion">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robust semantic perception for autonomous vehicles relies on effectively combining multiple sensors with complementary strengths and weaknesses. State-of-the-art sensor fusion approaches to semantic perception often treat sensor data uniformly across the spatial extent of the input, which hinders performance when faced with challenging conditions. By contrast, we propose a novel depth-guided multimodal fusion method that upgrades condition-aware fusion by integrating depth information. Our network, DGFusion, poses multimodal segmentation as a multi-task problem, utilizing the lidar measurements, which are typically available in outdoor sensor suites, both as one of the model&#x27;s inputs and as ground truth for learning depth. Our corresponding auxiliary depth head helps to learn depth-aware features, which are encoded into spatially varying local depth tokens that condition our attentive cross-modal fusion. Together with a global condition token, these local depth tokens dynamically adapt sensor fusion to the spatially varying reliability of each sensor across the scene, which largely depends on depth. In addition, we propose a robust loss for our depth, which is essential for learning from lidar inputs that are typically sparse and noisy in adverse conditions. Our method achieves state-of-the-art panoptic and semantic segmentation performance on the challenging MUSES and DeLiVER datasets. Code and models are available at https://github.com/timbroed/DGFusion</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DGFusion：用于鲁棒语义感知的深度引导传感器融合</div>
<div class="mono" style="margin-top:8px">自动驾驶车辆的鲁棒语义感知依赖于有效结合具有互补优缺点的多种传感器。目前最先进的语义感知传感器融合方法通常在输入的空间范围内对传感器数据进行统一处理，这在面对复杂环境时会限制性能。相比之下，我们提出了一种新颖的深度引导多模态融合方法，通过整合深度信息来提升条件感知融合。我们的网络DGFusion将多模态分割视为一个多任务问题，利用通常在户外传感器套件中可用的激光雷达测量数据，既作为模型的输入，也作为学习深度的地面真实值。我们相应的辅助深度头有助于学习深度感知特征，这些特征被编码为空间变化的局部深度标记，用于条件我们的注意力跨模态融合。结合一个全局条件标记，这些局部深度标记能够根据场景中每个传感器的空间变化可靠性动态调整传感器融合。此外，我们还提出了一种鲁棒的深度损失函数，这对于从通常在恶劣条件下稀疏且嘈杂的激光雷达输入中学习至关重要。我们的方法在具有挑战性的MUSES和DeLiVER数据集上实现了最先进的全景和语义分割性能。代码和模型可在https://github.com/timbroed/DGFusion获取。</div>
</details>
</div>
<div class="card">
<div class="title">Goal-oriented Communication for Fast and Robust Robotic Fault Detection and Recovery</div>
<div class="meta-line">Authors: Shutong Chen, Adnan Aijaz, Yansha Deng</div>
<div class="meta-line">First: 2026-01-26T18:32:33+00:00 · Latest: 2026-01-26T18:32:33+00:00</div>
<div class="meta-line">Comments: Submit to IEEE for potential publication</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18765v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18765v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous robotic systems are widely deployed in smart factories and operate in dynamic, uncertain, and human-involved environments that require low-latency and robust fault detection and recovery (FDR). However, existing FDR frameworks exhibit various limitations, such as significant delays in communication and computation, and unreliability in robot motion/trajectory generation, mainly because the communication-computation-control (3C) loop is designed without considering the downstream FDR goal. To address this, we propose a novel Goal-oriented Communication (GoC) framework that jointly designs the 3C loop tailored for fast and robust robotic FDR, with the goal of minimising the FDR time while maximising the robotic task (e.g., workpiece sorting) success rate. For fault detection, our GoC framework innovatively defines and extracts the 3D scene graph (3D-SG) as the semantic representation via our designed representation extractor, and detects faults by monitoring spatial relationship changes in the 3D-SG. For fault recovery, we fine-tune a small language model (SLM) via Low-Rank Adaptation (LoRA) and enhance its reasoning and generalization capabilities via knowledge distillation to generate recovery motions for robots. We also design a lightweight goal-oriented digital twin reconstruction module to refine the recovery motions generated by the SLM when fine-grained robotic control is required, using only task-relevant object contours for digital twin reconstruction. Extensive simulations demonstrate that our GoC framework reduces the FDR time by up to 82.6% and improves the task success rate by up to 76%, compared to the state-of-the-art frameworks that rely on vision language models for fault detection and large language models for fault recovery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向快速和鲁棒机器人故障检测与恢复的目标导向通信</div>
<div class="mono" style="margin-top:8px">自主机器人系统广泛部署在智能工厂中，并在动态、不确定且涉及人类操作的环境中运行，这些环境需要低延迟和鲁棒的故障检测与恢复（FDR）。然而，现有的FDR框架存在诸多限制，如通信和计算延迟显著，以及机器人运动/轨迹生成的不可靠性，主要原因是通信-计算-控制（3C）循环的设计未考虑下游FDR目标。为了解决这一问题，我们提出了一种新颖的目标导向通信（GoC）框架，该框架联合设计了针对快速和鲁棒机器人FDR的3C循环，旨在最小化FDR时间，同时最大化机器人任务（如工件分拣）的成功率。在故障检测方面，我们的GoC框架创新性地通过我们设计的表示提取器定义并提取3D场景图（3D-SG）作为语义表示，并通过监测3D-SG中的空间关系变化来检测故障。在故障恢复方面，我们通过低秩适应（LoRA）微调小型语言模型（SLM），并利用知识蒸馏增强其推理和泛化能力，以生成机器人的恢复动作。我们还设计了一个轻量级的目标导向数字孪生重建模块，当需要精细机器人控制时，仅使用任务相关的物体轮廓来优化SLM生成的恢复动作。大量仿真结果表明，与依赖视觉语言模型进行故障检测和依赖大型语言模型进行故障恢复的最先进框架相比，我们的GoC框架可将FDR时间减少高达82.6%，并提高任务成功率高达76%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Autonomous robotic systems are widely deployed in smart factories and operate in dynamic, uncertain, and human-involved environments that require low-latency and robust fault detection and recovery (FDR).</div>
</details>
</div>
<div class="card">
<div class="title">RocqStar: Leveraging Similarity-driven Retrieval and Agentic Systems for Rocq generation</div>
<div class="meta-line">Authors: Andrei Kozyrev, Nikita Khramov, Gleb Solovev, Anton Podkopaev</div>
<div class="meta-line">First: 2025-05-28T20:26:11+00:00 · Latest: 2026-01-26T18:27:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.22846v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.22846v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Interactive Theorem Proving was repeatedly shown to be fruitful when combined with Generative Artificial Intelligence. This paper assesses multiple approaches to Rocq generation and illuminates potential avenues for improvement. We identify retrieval-based premise selection as a central component of effective Rocq proof generation and propose a novel approach based on a self-attentive embedder model. The evaluation of the designed approach shows up to 28% relative increase of the generator&#x27;s performance. We tackle the problem of writing Rocq proofs using a multi-stage agentic system, tailored for formal verification, and demonstrate its high effectiveness. We conduct an ablation study and demonstrate that incorporating multi-agent debate during the planning stage increases the proof success rate by 20% overall and nearly doubles it for complex theorems, while the reflection mechanism further enhances stability and consistency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RocqStar：利用相似性驱动检索和代理系统进行Rocq生成</div>
<div class="mono" style="margin-top:8px">交互式定理证明在与生成式人工智能结合时被反复证明是富有成效的。本文评估了多种Rocq生成方法，并阐明了潜在的改进方向。我们识别出基于检索的前提选择是有效生成Rocq证明的核心组成部分，并提出了一种基于自注意力嵌入模型的新方法。所设计方法的评估结果显示，生成器的性能相对提高了最多28%。我们采用一个针对形式化验证定制的多阶段代理系统来解决Rocq证明的编写问题，并展示了其高度有效性。我们进行了消融研究，证明在规划阶段引入多代理辩论可使证明成功率整体提高20%，对于复杂定理则几乎翻倍。此外，反思机制进一步增强了稳定性和一致性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Interactive Theorem Proving was repeatedly shown to be fruitful when combined with Generative Artificial Intelligence.</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Preferences: Learning Alignment Principles Grounded in Human Reasons and Values</div>
<div class="meta-line">Authors: Henry Bell, Lara Neubauer da Costa Schertel, Bochu Ding, Brandon Fain</div>
<div class="meta-line">First: 2026-01-26T18:27:00+00:00 · Latest: 2026-01-26T18:27:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18760v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18760v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A crucial consideration when developing and deploying Large Language Models (LLMs) is the human values to which these models are aligned. In the constitutional framework of alignment models are aligned to a set of principles (the constitution) specified in natural language. However, it is unclear how to fairly determine this constitution with widespread stakeholder input. In this work we propose Grounded Constitutional AI (GCAI), a unified framework for generating constitutions of principles that are representative of both users&#x27; general expectations toward AI (general principles) and their interaction-time preferences (contextual principles). We extend the Inverse Constitutional AI (ICAI) approach to generate contextual principles from human preference annotation data by leveraging human-provided \textit{reasons} for their preferences. We supplement these contextual principles with general principles surfaced from user statements of \textit{values} regarding AI. We show that a constitution generated by GCAI is preferred by humans over one generated through ICAI both personally, and for widespread use in governing AI behavior. Additionally participants consider the GCAI constitution to be more morally grounded, coherent, and pluralistic.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越偏好：基于人类理由与价值观的对齐原则学习</div>
<div class="mono" style="margin-top:8px">在开发和部署大型语言模型（LLMs）时，一个关键考虑因素是这些模型所对齐的人类价值观。在宪法框架下，对齐模型被对齐到一组用自然语言指定的原则（即宪法）。然而，如何在广泛的利益相关者参与下公平地确定这一宪法仍不明确。在本工作中，我们提出了基于人类理由与价值观的对齐原则学习框架（Grounded Constitutional AI, GCAI），用于生成代表用户对AI的一般期望（一般原则）和交互时偏好（情境原则）的原则宪法。我们扩展了逆宪法AI（Inverse Constitutional AI, ICAI）方法，通过利用人类提供的\textit{理由}来从偏好标注数据中生成情境原则。我们还通过用户关于AI的\textit{价值观}陈述补充了这些情境原则，以提取一般原则。我们展示了由GCAI生成的宪法在个人层面和广泛用于规范AI行为时，均优于通过ICAI生成的宪法。此外，参与者认为GCAI生成的宪法更具道德基础、连贯性和多元性。</div>
</details>
</div>
<div class="card">
<div class="title">Deep-learning-based prediction of Precipitable Water Vapor in the Chajnantor area</div>
<div class="meta-line">Authors: Alison Matus-Bello, Silvia E. Restrepo, Ricardo Bustos, Yi Hu, Fujia Du, Jaime Cariñe, Pablo García, Javier Maldonado, Rodrigo Reeves, Zhaohui Shang</div>
<div class="meta-line">Venue: A&amp;A 705, A68 (2026)</div>
<div class="meta-line">First: 2025-09-11T16:11:09+00:00 · Latest: 2026-01-26T18:26:47+00:00</div>
<div class="meta-line">Comments: 9 pages, 9 figures. Published in A&amp;A, 705, A68 (2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.09575v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.09575v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Astronomical observations at millimeter and submillimeter wavelengths heavily depend on the amount of Precipitable Water Vapor (PWV) in the atmosphere, directly affecting the sky transparency and degrading the quality of the signals received by radio telescopes. Predictions of PWV at different forecasting horizons is crucial to support telescope operations, engineering planning, and observational scheduling and efficiency of radio observatories installed in the Chajnantor area in northern Chile. We developed and validated a Long Short-Term Memory (LSTM) deep learning-based model to predict PWV at forecasting horizons of 12, 24, 36, and 48 hours using historical data from two 183 GHz radiometers and a weather station in the Chajnantor area. We find the LSTM method is able to predict PWV in the 12 and 24 hours forecasting horizons with Mean Absolute Percentage Error (MAPE) of 22% compared to 36% of the traditional Global Forecast System (GFS) method used by Atacama Pathfinder EXperiment (APEX) and the Root Mean Square Error (RMSE) in mm are reduced by 50%. We present a first application of deep learning techniques for preliminary predictions of PWV in the Chajnantor area. The prediction performance shows significant improvements to traditional methods in 12 and 24 hours time windows. We also propose upgrades to improve our method in short (&lt; 1 hour) and long (&gt; 36 hours) forecasting timescales for future work.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度学习的查南托地区可降水量预测</div>
<div class="mono" style="margin-top:8px">在毫米和次毫米波长进行天文观测严重依赖于大气中的可降水量（PWV），直接影响天空透明度并降低射电望远镜接收到的信号质量。对查南托地区射电望远镜站不同预测时间尺度的PWV进行预测对于支持望远镜运行、工程规划和观测调度效率至关重要。我们开发并验证了一个基于长短期记忆网络（LSTM）的深度学习模型，利用该地区两个183 GHz辐射计和一个气象站的历史数据，预测12、24、36和48小时时间尺度的PWV。我们发现，LSTM方法在12和24小时预测时间窗口中，与传统全球预报系统（GFS）方法相比，平均绝对百分比误差（MAPE）为22%（而GFS方法为36%），且毫米级的均方根误差（RMSE）降低了50%。我们首次在查南托地区应用深度学习技术进行PWV的初步预测。预测性能在12和24小时时间窗口中显著优于传统方法。此外，我们还提出了未来改进该方法的建议，以提升短于1小时和长于36小时的预测时间尺度的性能。</div>
</details>
</div>
<div class="card">
<div class="title">$α^3$-SecBench: A Large-Scale Evaluation Suite of Security, Resilience, and Trust for LLM-based UAV Agents over 6G Networks</div>
<div class="meta-line">Authors: Mohamed Amine Ferrag, Abderrahmane Lakas, Merouane Debbah</div>
<div class="meta-line">First: 2026-01-26T18:25:07+00:00 · Latest: 2026-01-26T18:25:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18754v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18754v1">PDF</a> · <a href="https://github.com/maferrag/AlphaSecBench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous unmanned aerial vehicle (UAV) systems are increasingly deployed in safety-critical, networked environments where they must operate reliably in the presence of malicious adversaries. While recent benchmarks have evaluated large language model (LLM)-based UAV agents in reasoning, navigation, and efficiency, systematic assessment of security, resilience, and trust under adversarial conditions remains largely unexplored, particularly in emerging 6G-enabled settings.
  We introduce $α^{3}$-SecBench, the first large-scale evaluation suite for assessing the security-aware autonomy of LLM-based UAV agents under realistic adversarial interference. Building on multi-turn conversational UAV missions from $α^{3}$-Bench, the framework augments benign episodes with 20,000 validated security overlay attack scenarios targeting seven autonomy layers, including sensing, perception, planning, control, communication, edge/cloud infrastructure, and LLM reasoning. $α^{3}$-SecBench evaluates agents across three orthogonal dimensions: security (attack detection and vulnerability attribution), resilience (safe degradation behavior), and trust (policy-compliant tool usage).
  We evaluate 23 state-of-the-art LLMs from major industrial providers and leading AI labs using thousands of adversarially augmented UAV episodes sampled from a corpus of 113,475 missions spanning 175 threat types. While many models reliably detect anomalous behavior, effective mitigation, vulnerability attribution, and trustworthy control actions remain inconsistent. Normalized overall scores range from 12.9% to 57.1%, highlighting a significant gap between anomaly detection and security-aware autonomous decision-making. We release $α^{3}$-SecBench on GitHub: https://github.com/maferrag/AlphaSecBench</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>α³-SecBench：面向6G网络的基于大语言模型（LLM）无人机代理的安全、韧性与信任大规模评估套件</div>
<div class="mono" style="margin-top:8px">自主无人机（UAV）系统越来越多地部署在安全关键型网络环境中，必须在存在恶意对手的情况下可靠运行。尽管近期基准测试已评估基于大语言模型（LLM）的无人机代理在推理、导航和效率方面的表现，但在对抗性条件下的系统性安全、韧性与信任评估仍被广泛忽视，尤其是在新兴的6G网络环境中。
我们引入了α³-SecBench，这是首个用于评估基于大语言模型的无人机代理在现实对抗干扰下的安全自主性的大规模评估套件。该框架基于α³-Bench中的多轮对话无人机任务，通过在良性场景中加入20,000个经过验证的安全叠加攻击场景，覆盖七个自主层级，包括感知、感知、规划、控制、通信、边缘/云基础设施以及LLM推理。α³-SecBench从三个正交维度评估代理：安全（攻击检测与漏洞归因）、韧性（安全降级行为）和信任（符合政策的工具使用）。
我们使用来自包含113,475个任务的语料库中采样的数千个对抗性增强的无人机场景，评估了23个最先进的LLM模型，涵盖主要工业供应商和领先AI实验室。虽然许多模型能够可靠地检测异常行为，但在有效缓解、漏洞归因和可信赖的控制行为方面仍存在不一致性。归一化总体得分范围从12.9%到57.1%，突显了异常检测与安全自主决策之间的显著差距。我们已在GitHub上发布α³-SecBench：https://github.com/maferrag/AlphaSecBench</div>
</details>
</div>
<div class="card">
<div class="title">VoxGuard: Evaluating User and Attribute Privacy in Speech via Membership Inference Attacks</div>
<div class="meta-line">Authors: Efthymios Tsaprazlis, Thanathai Lertpetchpun, Tiantian Feng, Sai Praneeth Karimireddy, Shrikanth Narayanan</div>
<div class="meta-line">First: 2025-09-22T20:57:48+00:00 · Latest: 2026-01-26T18:23:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.18413v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.18413v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Voice anonymization aims to conceal speaker identity and attributes while preserving intelligibility, but current evaluations rely almost exclusively on Equal Error Rate (EER) that obscures whether adversaries can mount high-precision attacks. We argue that privacy should instead be evaluated in the low false-positive rate (FPR) regime, where even a small number of successful identifications constitutes a meaningful breach. To this end, we introduce VoxGuard, a framework grounded in differential privacy and membership inference that formalizes two complementary notions: User Privacy, preventing speaker re-identification, and Attribute Privacy, protecting sensitive traits such as gender and accent. Across synthetic and real datasets, we find that informed adversaries, especially those using fine-tuned models and max-similarity scoring, achieve orders-of-magnitude stronger attacks at low-FPR despite similar EER. For attributes, we show that simple transparent attacks recover gender and accent with near-perfect accuracy even after anonymization. Our results demonstrate that EER substantially underestimates leakage, highlighting the need for low-FPR evaluation, and recommend VoxGuard as a benchmark for evaluating privacy leakage.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VoxGuard：通过成员推断攻击评估语音中的用户和属性隐私</div>
<div class="mono" style="margin-top:8px">语音匿名化旨在隐藏说话人身份和属性，同时保持可理解性，但当前评估几乎完全依赖等错误率（EER），这掩盖了对手是否能够发起高精度攻击。我们认为，隐私评估应基于低假阳性率（FPR）的场景，即使只有少量成功识别也构成有意义的泄露。为此，我们引入了VoxGuard框架，该框架基于差分隐私和成员推断，形式化了两个互补的概念：用户隐私（防止说话人重新识别）和属性隐私（保护性别、口音等敏感属性）。在合成和真实数据集上，我们发现，有知识的对手，尤其是使用微调模型和最大相似度评分的对手，在低FPR下仍能发起数量级更强的攻击，尽管它们的EER相似。对于属性，我们展示了即使经过匿名化处理，简单的透明攻击也能以接近完美的准确率恢复性别和口音。我们的结果表明，EER严重低估了信息泄露，强调了低FPR评估的必要性，并建议将VoxGuard作为评估隐私泄露的基准。</div>
</details>
</div>
<div class="card">
<div class="title">HalluGuard: Demystifying Data-Driven and Reasoning-Driven Hallucinations in LLMs</div>
<div class="meta-line">Authors: Xinyue Zeng, Junhong Lin, Yujun Yan, Feng Guo, Liang Shi, Jun Wu, Dawei Zhou</div>
<div class="meta-line">Venue: ICLR</div>
<div class="meta-line">First: 2026-01-26T18:23:09+00:00 · Latest: 2026-01-26T18:23:09+00:00</div>
<div class="meta-line">Comments: Have been accepted by ICLR&#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18753v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18753v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The reliability of Large Language Models (LLMs) in high-stakes domains such as healthcare, law, and scientific discovery is often compromised by hallucinations. These failures typically stem from two sources: data-driven hallucinations and reasoning-driven hallucinations. However, existing detection methods usually address only one source and rely on task-specific heuristics, limiting their generalization to complex scenarios. To overcome these limitations, we introduce the Hallucination Risk Bound, a unified theoretical framework that formally decomposes hallucination risk into data-driven and reasoning-driven components, linked respectively to training-time mismatches and inference-time instabilities. This provides a principled foundation for analyzing how hallucinations emerge and evolve. Building on this foundation, we introduce HalluGuard, an NTK-based score that leverages the induced geometry and captured representations of the NTK to jointly identify data-driven and reasoning-driven hallucinations. We evaluate HalluGuard on 10 diverse benchmarks, 11 competitive baselines, and 9 popular LLM backbones, consistently achieving state-of-the-art performance in detecting diverse forms of LLM hallucinations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HalluGuard：揭开大语言模型中数据驱动与推理驱动幻觉的神秘面纱</div>
<div class="mono" style="margin-top:8px">在医疗、法律和科学发现等高风险领域，大语言模型（LLMs）的可靠性常因幻觉而受损。这些失败通常源于两个方面：数据驱动的幻觉和推理驱动的幻觉。然而，现有的检测方法通常只针对其中一个来源，并依赖任务特定的启发式规则，限制了其在复杂场景中的泛化能力。为克服这些限制，我们引入了幻觉风险边界（Hallucination Risk Bound），这是一个统一的理论框架，将幻觉风险形式化地分解为数据驱动和推理驱动的两个组成部分，分别与训练时的不匹配和推理时的不稳定性相关联。这为分析幻觉如何产生和演化提供了原理性的基础。在此基础上，我们提出了HalluGuard，一种基于神经切线核（NTK）的评分方法，利用NTK所诱导的几何结构和捕获的表示，联合识别数据驱动和推理驱动的幻觉。我们在10个多样化的基准数据集、11个竞争性基线模型和9个流行的LLM架构上评估了HalluGuard，持续在检测各种形式的LLM幻觉方面取得最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Trust, Don&#x27;t Trust, or Flip: Robust Preference-Based Reinforcement Learning with Multi-Expert Feedback</div>
<div class="meta-line">Authors: Seyed Amir Hosseini, Maryam Abdolali, Amirhosein Tavakkoli, Fardin Ayar, Ehsan Javanmardi, Manabu Tsukada, Mahdi Javanmardi</div>
<div class="meta-line">First: 2026-01-26T18:21:48+00:00 · Latest: 2026-01-26T18:21:48+00:00</div>
<div class="meta-line">Comments: Equal contribution: Seyed Amir Hosseini and Maryam Abdolali. Corresponding author: Maryam Abdolali (maryam.abdolali@kntu.ac.ir)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18751v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.18751v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Preference-based reinforcement learning (PBRL) offers a promising alternative to explicit reward engineering by learning from pairwise trajectory comparisons. However, real-world preference data often comes from heterogeneous annotators with varying reliability; some accurate, some noisy, and some systematically adversarial. Existing PBRL methods either treat all feedback equally or attempt to filter out unreliable sources, but both approaches fail when faced with adversarial annotators who systematically provide incorrect preferences. We introduce TriTrust-PBRL (TTP), a unified framework that jointly learns a shared reward model and expert-specific trust parameters from multi-expert preference feedback. The key insight is that trust parameters naturally evolve during gradient-based optimization to be positive (trust), near zero (ignore), or negative (flip), enabling the model to automatically invert adversarial preferences and recover useful signal rather than merely discarding corrupted feedback. We provide theoretical analysis establishing identifiability guarantees and detailed gradient analysis that explains how expert separation emerges naturally during training without explicit supervision. Empirically, we evaluate TTP on four diverse domains spanning manipulation tasks (MetaWorld) and locomotion (DM Control) under various corruption scenarios. TTP achieves state-of-the-art robustness, maintaining near-oracle performance under adversarial corruption while standard PBRL methods fail catastrophically. Notably, TTP outperforms existing baselines by successfully learning from mixed expert pools containing both reliable and adversarial annotators, all while requiring no expert features beyond identification indices and integrating seamlessly with existing PBRL pipelines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>信任、不信任或翻转：基于多专家反馈的鲁棒偏好强化学习</div>
<div class="mono" style="margin-top:8px">基于偏好的强化学习（PBRL）通过从成对轨迹比较中学习，为显式奖励工程提供了一种有前景的替代方案。然而，现实世界中的偏好数据通常来自可靠性各异的异构标注者；有些准确，有些嘈杂，有些则系统性地对抗。现有的PBRL方法要么将所有反馈视为同等重要，要么尝试过滤掉不可靠的来源，但这两种方法在面对系统性提供错误偏好的对抗标注者时都会失效。我们引入了TriTrust-PBRL（TTP），一个统一框架，从多专家偏好反馈中联合学习共享奖励模型和专家特定的信任参数。关键的洞察是，信任参数在基于梯度的优化过程中自然演化为正（信任）、接近零（忽略）或负（翻转），使模型能够自动翻转对抗性偏好并恢复有用信号，而不仅仅是丢弃被破坏的反馈。我们提供了理论分析，建立了可识别性保证，并进行了详细的梯度分析，解释了专家分离如何在训练过程中自然出现而无需显式监督。在实证方面，我们在四个不同的领域（包括操作任务（MetaWorld）和运动控制（DM Control））下各种破坏性场景中评估了TTP。TTP实现了最先进的鲁棒性，在对抗性破坏下仍能保持接近理想性能，而标准PBRL方法则会彻底失败。值得注意的是，TTP在包含可靠和对抗性标注者的混合专家池中成功学习，同时无需任何专家特征，且能无缝集成到现有的PBRL流程中。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
