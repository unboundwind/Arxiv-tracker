<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-15 04:01</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260215_0401</div>
    <div class="row"><div class="card">
<div class="title">Ubiquitous yet forgotten: broad absorptions in the optical spectra of low-mass X-ray binaries</div>
<div class="meta-line">Authors: D. Mata Sanchez, T. Munoz-Darias, J. Casares, M. A. P. Torres, M. Armas Padilla</div>
<div class="meta-line">First: 2026-02-12T18:59:59+00:00 · Latest: 2026-02-12T18:59:59+00:00</div>
<div class="meta-line">Comments: Main text (15 pages, 8 Figures) + Appendix (7 pages, 8 Figures). Accepted for publication in A&amp;A</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12282v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12282v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optical outburst spectra of low-mass X-ray binaries enable studies of extreme accretion and ejection phenomena. While some of their spectroscopic features have been analysed in detail, the appearance of broad absorptions in the optical regime has been traditionally neglected. In this work, we introduce the first population study dedicated to these features with the aim to understand their fundamental properties and discuss them in the context of their origin. We complement the study with a spectroscopic database of six low-mass X-ray binaries during outburst, in order to assess their evolution. We find that broad absorptions are ubiquitous, with the majority of black hole low-mass X-ray binaries exhibiting them in spite of a typically scarce outburst coverage. Their detection does not depend on the orbital inclination or the compact object nature, but they seem favoured in systems with orbital periods shorter than &lt; 11 h. They predominantly occur in the hydrogen Balmer series, being stronger at shorter wavelengths, and they are detected across all X-ray states. We find that the normalised depth of these broad absorptions is anti-correlated with the system luminosity, and that they show constant line ratios over the whole sample. Based on these properties, we favour a scenario where BAs arise from a stable, optically thick layer of the accretion disc, below the hotter chromosphere-like region producing the emission line components. Our study is consistent with the continuous presence of broad absorptions during the whole outburst, with their visibility being conditioned by the emission lines filling the broad absorption profile and veiling by the X-ray reprocessed continuum.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无处不在却被遗忘：低质量X射线双星光学光谱中的宽吸收线</div>
<div class="mono" style="margin-top:8px">低质量X射线双星的光学爆发光谱为研究极端吸积和喷射现象提供了可能。尽管它们的一些光谱特征已被详细分析，但光学波段中宽吸收线的出现传统上被忽视。在本研究中，我们首次开展针对这些特征的群体研究，旨在理解其基本性质，并在它们的起源背景下进行讨论。我们还通过一个包含六颗低质量X射线双星爆发期间光谱数据的数据库，来评估其演化过程。我们发现宽吸收线普遍存在，尽管爆发覆盖通常较少，但大多数黑洞低质量X射线双星都表现出宽吸收线。它们的检测不依赖于轨道倾角或致密天体的性质，但在轨道周期短于11小时的系统中似乎更常见。它们主要出现在氢巴尔末系列中，且在较短波长处更为显著，并且在所有X射线状态下均可检测到。我们发现这些宽吸收线的归一化深度与系统亮度呈反相关，且在整个样本中显示出恒定的线比。基于这些特性，我们倾向于认为宽吸收线来源于吸积盘中一个稳定、光学厚的层，位于产生发射线成分的较热的色球状区域下方。我们的研究结果与宽吸收线在整个爆发过程中持续存在一致，其可见性取决于发射线填充宽吸收线轮廓以及X射线再处理连续谱的遮蔽作用。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Verification Can Be More Effective than Scaling Policy Learning for Vision-Language-Action Alignment</div>
<div class="meta-line">Authors: Jacky Kwok, Xilun Zhang, Mengdi Xu, Yuejiang Liu, Azalia Mirhoseini, Chelsea Finn, Marco Pavone</div>
<div class="meta-line">First: 2026-02-12T18:59:59+00:00 · Latest: 2026-02-12T18:59:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12281v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12281v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions. Vision-Language-Action (VLA) models have made remarkable progress toward this goal, yet their generated actions can still misalign with the given instructions. In this paper, we investigate test-time verification as a means to shrink the &quot;intention-action gap.&#x27;&#x27; We first characterize the test-time scaling law for embodied instruction following and demonstrate that jointly scaling the number of rephrased instructions and generated actions greatly increases test-time sample diversity, often recovering correct actions more efficiently than scaling each dimension independently. To capitalize on these scaling laws, we present CoVer, a contrastive verifier for vision-language-action alignment, and show that our architecture scales gracefully with additional computational resources and data. We then introduce &quot;boot-time compute&quot; and a hierarchical verification inference pipeline for VLAs. At deployment, our framework precomputes a diverse set of rephrased instructions from a Vision-Language-Model (VLM), repeatedly generates action candidates for each instruction, and then uses a verifier to select the optimal high-level prompt and low-level action chunks. Compared to scaling policy pre-training on the same data, our verification approach yields 22% gains in-distribution and 13% out-of-distribution on the SIMPLER benchmark, with a further 45% improvement in real-world experiments. On the PolaRiS benchmark, CoVer achieves 14% gains in task progress and 9% in success rate.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在视觉-语言-动作对齐中，验证扩展可能比策略学习扩展更有效</div>
<div class="mono" style="margin-top:8px">通用机器人长期以来的目标依赖于其理解并执行自然语言指令的能力。视觉-语言-动作（VLA）模型在这方面取得了显著进展，但其生成的动作仍可能与给定指令不一致。本文研究了测试时验证作为缩小“意图-动作差距”的方法。我们首先刻画了具身指令遵循的测试时扩展定律，并证明联合扩展重述指令和生成动作的数量可以显著增加测试时样本的多样性，通常比单独扩展每个维度更高效地恢复正确动作。为了利用这些扩展定律，我们提出了CoVer，一种用于视觉-语言-动作对齐的对比验证器，并展示了我们的架构在增加计算资源和数据后能够优雅地扩展。随后，我们引入了“启动时计算”和一种分层验证推理管道。在部署时，我们的框架会从视觉-语言模型（VLM）中预计算一组多样化的重述指令，对每个指令重复生成动作候选，然后使用验证器选择最优的高层提示和底层动作片段。与在相同数据上扩展策略预训练相比，我们的验证方法在SIMPLER基准测试中实现了22%的分布内提升和13%的分布外提升，在现实世界实验中进一步提升了45%。在PolaRiS基准测试中，CoVer实现了任务进展的14%提升和成功率的9%提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The long-standing vision of general-purpose robots hinges on their ability to understand and act upon natural language instructions.</div>
</details>
</div>
<div class="card">
<div class="title">Stroke of Surprise: Progressive Semantic Illusions in Vector Sketching</div>
<div class="meta-line">Authors: Huai-Hsun Cheng, Siang-Ling Zhang, Yu-Lun Liu</div>
<div class="meta-line">First: 2026-02-12T18:59:54+00:00 · Latest: 2026-02-12T18:59:54+00:00</div>
<div class="meta-line">Comments: Project page: https://stroke-of-surprise.github.io/ Code: https://github.com/stroke-of-surprise/Stroke-Of-Surprise</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12280v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12280v1">PDF</a> · <a href="https://github.com/stroke-of-surprise/Stroke-Of-Surprise">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://stroke-of-surprise.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual illusions traditionally rely on spatial manipulations such as multi-view consistency. In this work, we introduce Progressive Semantic Illusions, a novel vector sketching task where a single sketch undergoes a dramatic semantic transformation through the sequential addition of strokes. We present Stroke of Surprise, a generative framework that optimizes vector strokes to satisfy distinct semantic interpretations at different drawing stages. The core challenge lies in the &quot;dual-constraint&quot;: initial prefix strokes must form a coherent object (e.g., a duck) while simultaneously serving as the structural foundation for a second concept (e.g., a sheep) upon adding delta strokes. To address this, we propose a sequence-aware joint optimization framework driven by a dual-branch Score Distillation Sampling (SDS) mechanism. Unlike sequential approaches that freeze the initial state, our method dynamically adjusts prefix strokes to discover a &quot;common structural subspace&quot; valid for both targets. Furthermore, we introduce a novel Overlay Loss that enforces spatial complementarity, ensuring structural integration rather than occlusion. Extensive experiments demonstrate that our method significantly outperforms state-of-the-art baselines in recognizability and illusion strength, successfully expanding visual anagrams from the spatial to the temporal dimension. Project page: https://stroke-of-surprise.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>惊喜笔触：矢量素描中的渐进语义错觉</div>
<div class="mono" style="margin-top:8px">视觉错觉传统上依赖于空间操控，如多视角一致性。在本工作中，我们引入了一种新颖的矢量素描任务——渐进语义错觉，其中单个素描通过连续添加笔触经历显著的语义转变。我们提出了Stroke of Surprise生成框架，该框架通过优化矢量笔触以满足不同绘画阶段的语义解释。核心挑战在于&quot;双约束&quot;：初始前缀笔触必须形成一个连贯的对象（例如鸭子），同时在添加增量笔触后，又能作为第二个概念（例如绵羊）的结构基础。为了解决这一问题，我们提出了一种基于双分支得分蒸馏采样（SDS）机制的序列感知联合优化框架。与冻结初始状态的顺序方法不同，我们的方法动态调整前缀笔触，以发现适用于两个目标的&quot;公共结构子空间&quot;。此外，我们引入了一种新颖的叠加损失函数，强制执行空间互补性，确保结构整合而非遮挡。大量实验表明，我们的方法在可识别性和错觉强度方面显著优于最先进的基线方法，成功地将视觉文字游戏从空间维度扩展到时间维度。</div>
</details>
</div>
<div class="card">
<div class="title">UniT: Unified Multimodal Chain-of-Thought Test-time Scaling</div>
<div class="meta-line">Authors: Leon Liangyu Chen, Haoyu Ma, Zhipeng Fan, Ziqi Huang, Animesh Sinha, Xiaoliang Dai, Jialiang Wang, Zecheng He, Jianwei Yang, Chunyuan Li, Junzhe Sun, Chu Wang, Serena Yeung-Levy, Felix Juefei-Xu</div>
<div class="meta-line">First: 2026-02-12T18:59:49+00:00 · Latest: 2026-02-12T18:59:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12279v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12279v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs. Many multimodal tasks, especially those involving complex spatial compositions, multiple interacting objects, or evolving instructions, require decomposing instructions, verifying intermediate results, and making iterative corrections. While test-time scaling (TTS) has demonstrated that allocating additional inference compute for iterative reasoning substantially improves language model performance, extending this paradigm to unified multimodal models remains an open challenge. We introduce UniT, a framework for multimodal chain-of-thought test-time scaling that enables a single unified model to reason, verify, and refine across multiple rounds. UniT combines agentic data synthesis, unified model training, and flexible test-time inference to elicit cognitive behaviors including verification, subgoal decomposition, and content memory. Our key findings are: (1) unified models trained on short reasoning trajectories generalize to longer inference chains at test time; (2) sequential chain-of-thought reasoning provides a more scalable and compute-efficient TTS strategy than parallel sampling; (3) training on generation and editing trajectories improves out-of-distribution visual reasoning. These results establish multimodal test-time scaling as an effective paradigm for advancing both generation and understanding in unified models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UniT：统一的多模态链式思维测试时扩展</div>
<div class="mono" style="margin-top:8px">统一模型可以在单一架构中处理多模态理解和生成，但它们通常仅进行单次推理，而不进行输出的迭代优化。许多多模态任务，尤其是涉及复杂空间组合、多个交互对象或逐步演进指令的任务，需要分解指令、验证中间结果并进行迭代修正。尽管测试时扩展（TTS）已证明为迭代推理分配额外的推理计算资源可以显著提升语言模型的性能，但将这一范式扩展到统一的多模态模型仍然是一个开放性挑战。我们提出了UniT，这是一个用于多模态链式思维测试时扩展的框架，使单一的统一模型能够在多轮中进行推理、验证和优化。UniT结合了代理数据合成、统一模型训练和灵活的测试时推理，以激发包括验证、子目标分解和内容记忆在内的认知行为。我们的主要发现包括：(1) 在短推理轨迹上训练的统一模型在测试时能够泛化到更长的推理链；(2) 顺序链式思维推理比并行采样提供了更可扩展且计算更高效的TTS策略；(3) 在生成和编辑轨迹上训练可以提升模型在分布外视觉推理中的表现。这些结果确立了多模态测试时扩展作为一种有效范式，用于推动统一模型在生成和理解方面的发展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Unified models can handle both multimodal understanding and generation within a single architecture, yet they typically operate in a single pass without iteratively refining their outputs.</div>
</details>
</div>
<div class="card">
<div class="title">AttentionRetriever: Attention Layers are Secretly Long Document Retrievers</div>
<div class="meta-line">Authors: David Jiahao Fu, Lam Thanh Do, Jiayu Li, Kevin Chen-Chuan Chang</div>
<div class="meta-line">First: 2026-02-12T18:59:35+00:00 · Latest: 2026-02-12T18:59:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12278v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12278v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval augmented generation (RAG) has been widely adopted to help Large Language Models (LLMs) to process tasks involving long documents. However, existing retrieval models are not designed for long document retrieval and fail to address several key challenges of long document retrieval, including context-awareness, causal dependence, and scope of retrieval. In this paper, we proposed AttentionRetriever, a novel long document retrieval model that leverages attention mechanism and entity-based retrieval to build context-aware embeddings for long document and determine the scope of retrieval. With extensive experiments, we found AttentionRetriever is able to outperform existing retrieval models on long document retrieval datasets by a large margin while remaining as efficient as dense retrieval models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AttentionRetriever：注意力层实际上是秘密的长文档检索器</div>
<div class="mono" style="margin-top:8px">检索增强生成（RAG）已被广泛采用，以帮助大型语言模型（LLMs）处理涉及长文档的任务。然而，现有的检索模型并未专门设计用于长文档检索，也未能有效解决长文档检索中的几个关键挑战，包括上下文感知、因果依赖和检索范围。在本文中，我们提出了AttentionRetriever，这是一种新颖的长文档检索模型，利用注意力机制和基于实体的检索来构建长文档的上下文感知嵌入，并确定检索范围。通过大量实验，我们发现AttentionRetriever在长文档检索数据集上显著优于现有检索模型，同时保持与密集检索模型相当的效率。</div>
</details>
</div>
<div class="card">
<div class="title">Agentic Test-Time Scaling for WebAgents</div>
<div class="meta-line">Authors: Nicholas Lee, Lutfi Eren Erdogan, Chris Joseph John, Surya Krishnapillai, Michael W. Mahoney, Kurt Keutzer, Amir Gholami</div>
<div class="meta-line">First: 2026-02-12T18:58:30+00:00 · Latest: 2026-02-12T18:58:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12276v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12276v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Test-time scaling has become a standard way to improve performance and boost reliability of neural network models. However, its behavior on agentic, multi-step tasks remains less well-understood: small per-step errors can compound over long horizons; and we find that naive policies that uniformly increase sampling show diminishing returns. In this work, we present CATTS, a simple technique for dynamically allocating compute for multi-step agents. We first conduct an empirical study of inference-time scaling for web agents. We find that uniformly increasing per-step compute quickly saturates in long-horizon environments. We then investigate stronger aggregation strategies, including an LLM-based Arbiter that can outperform naive voting, but that can overrule high-consensus decisions. We show that uncertainty statistics derived from the agent&#x27;s own vote distribution (entropy and top-1/top-2 margin) correlate with downstream success and provide a practical signal for dynamic compute allocation. Based on these findings, we introduce Confidence-Aware Test-Time Scaling (CATTS), which uses vote-derived uncertainty to allocate compute only when decisions are genuinely contentious. CATTS improves performance on WebArena-Lite and GoBrowse by up to 9.1% over React while using up to 2.3x fewer tokens than uniform scaling, providing both efficiency gains and an interpretable decision rule.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向WebAgent的代理测试时扩展</div>
<div class="mono" style="margin-top:8px">测试时扩展已成为提升神经网络模型性能和增强可靠性的标准方法。然而，其在代理、多步骤任务中的行为仍不完全清楚：在长时域环境中，每一步的小误差可能累积；我们发现，简单地统一增加采样量的策略会产生递减的回报。在本工作中，我们提出了CATTS，一种用于动态分配计算资源的简单技术。我们首先对WebAgent的推理时扩展进行了实证研究，发现统一增加每一步的计算量在长时域环境中会迅速饱和。随后，我们研究了更强的聚合策略，包括基于LLM的仲裁者，该策略可以超越简单的投票，但可能覆盖高共识决策。我们展示了从代理自身投票分布中得出的不确定性统计量（熵和top-1/top-2边界）与下游成功之间的相关性，并提供了用于动态计算资源分配的实用信号。基于这些发现，我们引入了基于置信度的测试时扩展（CATTS），该方法仅在决策真正存在争议时才分配计算资源。CATTS在WebArena-Lite和GoBrowse上相比React提升了最多9.1%的性能，同时使用的token数量最多减少2.3倍，实现了效率提升和可解释的决策规则。</div>
</details>
</div>
<div class="card">
<div class="title">On-Policy Context Distillation for Language Models</div>
<div class="meta-line">Authors: Tianzhu Ye, Li Dong, Xun Wu, Shaohan Huang, Furu Wei</div>
<div class="meta-line">First: 2026-02-12T18:58:28+00:00 · Latest: 2026-02-12T18:58:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12275v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12275v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Context distillation enables language models to internalize in-context knowledge into their parameters. In our work, we propose On-Policy Context Distillation (OPCD), a framework that bridges on-policy distillation with context distillation by training a student model on its own generated trajectories while minimizing reverse Kullback-Leibler divergence against a context-conditioned teacher. We demonstrate the effectiveness of OPCD on two important applications: experiential knowledge distillation, where models extract and consolidate transferable knowledge from their historical solution traces, and system prompt distillation, where models internalize beneficial behaviors encoded in optimized prompts. Across mathematical reasoning, text-based games, and domain-specific tasks, OPCD consistently outperforms baseline methods, achieving higher task accuracy while better preserving out-of-distribution capabilities. We further show that OPCD enables effective cross-size distillation, where smaller student models can internalize experiential knowledge from larger teachers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于策略的上下文蒸馏用于语言模型</div>
<div class="mono" style="margin-top:8px">上下文蒸馏使语言模型能够将上下文中的知识内化到其参数中。在我们的工作中，我们提出了基于策略的上下文蒸馏（OPCD），该框架通过在学生模型上训练其自身生成的轨迹，并最小化与条件上下文教师的反向Kullback-Leibler散度，将基于策略的蒸馏与上下文蒸馏相结合。我们在两个重要应用中展示了OPCD的有效性：经验知识蒸馏，其中模型从其历史解决方案轨迹中提取并整合可迁移的知识；以及系统提示蒸馏，其中模型内化优化提示中编码的有益行为。在数学推理、基于文本的游戏和领域特定任务中，OPCD始终优于基线方法，在提高任务准确性的同时更好地保留了分布外能力。我们进一步表明，OPCD能够实现有效的跨规模蒸馏，其中较小的学生模型可以从较大的教师模型中内化经验知识。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Context distillation enables language models to internalize in-context knowledge into their parameters.</div>
</details>
</div>
<div class="card">
<div class="title">Function-Space Decoupled Diffusion for Forward and Inverse Modeling in Carbon Capture and Storage</div>
<div class="meta-line">Authors: Xin Ju, Jiachen Yao, Anima Anandkumar, Sally M. Benson, Gege Wen</div>
<div class="meta-line">First: 2026-02-12T18:58:12+00:00 · Latest: 2026-02-12T18:58:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12274v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12274v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate characterization of subsurface flow is critical for Carbon Capture and Storage (CCS) but remains challenged by the ill-posed nature of inverse problems with sparse observations. We present Fun-DDPS, a generative framework that combines function-space diffusion models with differentiable neural operator surrogates for both forward and inverse modeling. Our approach learns a prior distribution over geological parameters (geomodel) using a single-channel diffusion model, then leverages a Local Neural Operator (LNO) surrogate to provide physics-consistent guidance for cross-field conditioning on the dynamics field. This decoupling allows the diffusion prior to robustly recover missing information in parameter space, while the surrogate provides efficient gradient-based guidance for data assimilation. We demonstrate Fun-DDPS on synthetic CCS modeling datasets, achieving two key results: (1) For forward modeling with only 25% observations, Fun-DDPS achieves 7.7% relative error compared to 86.9% for standard surrogates (an 11x improvement), proving its capability to handle extreme data sparsity where deterministic methods fail. (2) We provide the first rigorous validation of diffusion-based inverse solvers against asymptotically exact Rejection Sampling (RS) posteriors. Both Fun-DDPS and the joint-state baseline (Fun-DPS) achieve Jensen-Shannon divergence less than 0.06 against the ground truth. Crucially, Fun-DDPS produces physically consistent realizations free from the high-frequency artifacts observed in joint-state baselines, achieving this with 4x improved sample efficiency compared to rejection sampling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于碳捕集与封存正向与反向建模的功能空间解耦扩散模型</div>
<div class="mono" style="margin-top:8px">对地下流体流动的准确表征对于碳捕集与封存（CCS）至关重要，但受限于观测数据稀疏的反向问题的不适定性仍面临挑战。我们提出了Fun-DDPS，这是一种生成式框架，结合了功能空间扩散模型和可微神经算子代理模型，用于正向和反向建模。我们的方法使用单通道扩散模型学习地质参数（geomodel）的先验分布，然后利用局部神经算子（LNO）代理模型在动态场中提供物理一致的跨领域条件化指导。这种解耦使得扩散先验能够稳健地恢复参数空间中的缺失信息，而代理模型则为数据同化提供高效的梯度指导。我们在合成的CCS建模数据集上验证了Fun-DDPS，取得了两个关键结果：(1) 在仅使用25%观测数据的正向建模中，Fun-DDPS的相对误差为7.7%，而标准代理模型为86.9%（提升了11倍），证明其在确定性方法失效的极端数据稀疏情况下仍具有处理能力。(2) 我们首次对基于扩散的反向求解器与渐近精确的拒绝抽样（RS）后验分布进行了严格验证。Fun-DDPS和联合状态基线（Fun-DPS）均与真实值的Jensen-Shannon散度小于0.06。重要的是，Fun-DDPS生成的实现具有物理一致性，避免了联合状态基线中出现的高频伪影，且在采样效率上比拒绝抽样提高了4倍。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Control: The iUzawa-Net for Nonsmooth Optimal Control of Linear PDEs</div>
<div class="meta-line">Authors: Yongcun Song, Xiaoming Yuan, Hangrui Yue, Tianyou Zeng</div>
<div class="meta-line">First: 2026-02-12T18:57:43+00:00 · Latest: 2026-02-12T18:57:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12273v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12273v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose an optimization-informed deep neural network approach, named iUzawa-Net, aiming for the first solver that enables real-time solutions for a class of nonsmooth optimal control problems of linear partial differential equations (PDEs). The iUzawa-Net unrolls an inexact Uzawa method for saddle point problems, replacing classical preconditioners and PDE solvers with specifically designed learnable neural networks. We prove universal approximation properties and establish the asymptotic $\varepsilon$-optimality for the iUzawa-Net, and validate its promising numerical efficiency through nonsmooth elliptic and parabolic optimal control problems. Our techniques offer a versatile framework for designing and analyzing various optimization-informed deep learning approaches to optimal control and other PDE-constrained optimization problems. The proposed learning-to-control approach synergizes model-based optimization algorithms and data-driven deep learning techniques, inheriting the merits of both methodologies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习控制：用于线性偏微分方程非光滑最优控制的iUzawa网络</div>
<div class="mono" style="margin-top:8px">我们提出了一种基于优化的深度神经网络方法，称为iUzawa-Net，旨在实现首个能够实时求解线性偏微分方程（PDE）一类非光滑最优控制问题的求解器。iUzawa-Net展开了一个用于鞍点问题的非精确Uzawa方法，用专门设计的可学习神经网络替代了传统的预处理子和PDE求解器。我们证明了iUzawa-Net的通用逼近性质，并建立了其渐近$\varepsilon$-最优性。通过非光滑椭圆和抛物型最优控制问题，我们验证了其有前景的数值效率。我们的技术为设计和分析各种基于优化的深度学习方法用于最优控制及其他PDE约束优化问题提供了灵活的框架。所提出的“学习控制”方法结合了基于模型的优化算法和数据驱动的深度学习技术，继承了两种方法的优点。</div>
</details>
</div>
<div class="card">
<div class="title">MonarchRT: Efficient Attention for Real-Time Video Generation</div>
<div class="meta-line">Authors: Krish Agarwal, Zhuoming Chen, Cheng Luo, Yongqi Chen, Haizhong Zheng, Xun Huang, Atri Rudra, Beidi Chen</div>
<div class="meta-line">First: 2026-02-12T18:56:53+00:00 · Latest: 2026-02-12T18:56:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12271v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12271v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Real-time video generation with Diffusion Transformers is bottlenecked by the quadratic cost of 3D self-attention, especially in real-time regimes that are both few-step and autoregressive, where errors compound across time and each denoising step must carry substantially more information. In this setting, we find that prior sparse-attention approximations break down, despite showing strong results for bidirectional, many-step diffusion. Specifically, we observe that video attention is not reliably sparse, but instead combines pronounced periodic structure driven by spatiotemporal position with dynamic, sparse semantic correspondences and dense mixing, exceeding the representational capacity of even oracle top-k attention. Building on this insight, we propose Monarch-RT, a structured attention parameterization for video diffusion models that factorizes attention using Monarch matrices. Through appropriately aligned block structure and our extended tiled Monarch parameterization, we achieve high expressivity while preserving computational efficiency. We further overcome the overhead of parameterization through finetuning, with custom Triton kernels. We first validate the high efficacy of Monarch-RT over existing sparse baselines designed only for bidirectional models. We further observe that Monarch-RT attains up to 95% attention sparsity with no loss in quality when applied to the state-of-the-art model Self-Forcing, making Monarch-RT a pioneering work on highly-capable sparse attention parameterization for real-time video generation. Our optimized implementation outperforms FlashAttention-2, FlashAttention-3, and FlashAttention-4 kernels on Nvidia RTX 5090, H100, and B200 GPUs respectively, providing kernel speedups in the range of 1.4-11.8X. This enables us, for the first time, to achieve true real-time video generation with Self-Forcing at 16 FPS on a single RTX 5090.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MonarchRT：高效注意力用于实时视频生成</div>
<div class="mono" style="margin-top:8px">使用扩散变压器进行实时视频生成受到三维自注意力的二次成本的限制，尤其是在少步且自回归的实时场景中，误差随时间累积，每个去噪步骤必须携带大量信息。在此设置下，我们发现先前的稀疏注意力近似方法失效，尽管它们在双向、多步扩散中表现出色。具体而言，我们观察到视频注意力并不稳定地稀疏，而是结合了由时空位置驱动的显著周期性结构，以及动态、稀疏的语义对应关系和密集混合，超出了甚至 oracle top-k 注意力的表示能力。基于这一洞察，我们提出了 Monarch-RT，这是一种为视频扩散模型设计的结构化注意力参数化方法，利用 Monarch 矩阵对注意力进行分解。通过适当对齐的块结构和我们扩展的分块 Monarch 参数化方法，我们在保持计算效率的同时实现了高表达能力。我们进一步通过微调克服了参数化带来的开销，使用了定制的 Triton 内核。我们首先验证了 Monarch-RT 在现有仅针对双向模型设计的稀疏基线上的高效性。我们还观察到，当应用于最先进的 Self-Forcing 模型时，Monarch-RT 可实现高达 95% 的注意力稀疏度，且不损失质量，使 Monarch-RT 成为实时视频生成中高度能力稀疏注意力参数化方法的开创性工作。我们的优化实现分别在 Nvidia RTX 5090、H100 和 B200 GPU 上超越了 FlashAttention-2、FlashAttention-3 和 FlashAttention-4 内核，提供了 1.4-11.8 倍的内核加速。这使我们首次能够在单块 RTX 5090 上以 16 FPS 的速度实现真正的实时视频生成。</div>
</details>
</div>
<div class="card">
<div class="title">Decoupled Diffusion Sampling for Inverse Problems on Function Spaces</div>
<div class="meta-line">Authors: Thomas Y. L. Lin, Jiachen Yao, Lufang Chiang, Julius Berner, Anima Anandkumar</div>
<div class="meta-line">First: 2026-01-30T18:54:49+00:00 · Latest: 2026-02-12T18:56:52+00:00</div>
<div class="meta-line">Comments: Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.23280v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.23280v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a data-efficient, physics-aware generative framework in function space for inverse PDE problems. Existing plug-and-play diffusion posterior samplers represent physics implicitly through joint coefficient-solution modeling, requiring substantial paired supervision. In contrast, our Decoupled Diffusion Inverse Solver (DDIS) employs a decoupled design: an unconditional diffusion learns the coefficient prior, while a neural operator explicitly models the forward PDE for guidance. This decoupling enables superior data efficiency and effective physics-informed learning, while naturally supporting Decoupled Annealing Posterior Sampling (DAPS) to avoid over-smoothing in Diffusion Posterior Sampling (DPS). Theoretically, we prove that DDIS avoids the guidance attenuation failure of joint models when training data is scarce. Empirically, DDIS achieves state-of-the-art performance under sparse observation, improving $l_2$ error by 11% and spectral error by 54% on average; when data is limited to 1%, DDIS maintains accuracy with 40% advantage in $l_2$ error compared to joint models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>函数空间逆问题的解耦扩散采样</div>
<div class="mono" style="margin-top:8px">我们提出了一种在函数空间中用于逆偏微分方程问题的数据高效且物理感知的生成框架。现有的插件式扩散后验采样器通过联合系数-解建模隐式地表示物理规律，需要大量的配对监督数据。相比之下，我们的解耦扩散逆求解器（DDIS）采用了解耦设计：一个无条件扩散学习系数先验，而一个神经算子则显式建模前向PDE以提供指导。这种解耦设计实现了更优的数据效率和有效的物理感知学习，同时自然支持解耦退火后验采样（DAPS）以避免扩散后验采样（DPS）中的过度平滑问题。理论上，我们证明了在训练数据稀缺的情况下，DDIS能够避免联合模型的引导衰减失败。实验证明，DDIS在稀疏观测条件下实现了最先进的性能，平均提高了$ l_2 $误差11%和谱误差54%；当数据仅限于1%时，DDIS在$ l_2 $误差上相比联合模型仍保持40%的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Creative Ownership in the Age of AI</div>
<div class="meta-line">Authors: Annie Liang, Jay Lu</div>
<div class="meta-line">First: 2026-02-12T18:56:42+00:00 · Latest: 2026-02-12T18:56:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12270v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12270v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Copyright law focuses on whether a new work is &quot;substantially similar&quot; to an existing one, but generative AI can closely imitate style without copying content, a capability now central to ongoing litigation. We argue that existing definitions of infringement are ill-suited to this setting and propose a new criterion: a generative AI output infringes on an existing work if it could not have been generated without that work in its training corpus. To operationalize this definition, we model generative systems as closure operators mapping a corpus of existing works to an output of new works. AI generated outputs are \emph{permissible} if they do not infringe on any existing work according to our criterion. Our results characterize structural properties of permissible generation and reveal a sharp asymptotic dichotomy: when the process of organic creations is light-tailed, dependence on individual works eventually vanishes, so that regulation imposes no limits on AI generation; with heavy-tailed creations, regulation can be persistently constraining.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>人工智能时代的创意所有权</div>
<div class="mono" style="margin-top:8px">版权法关注新作品是否与现有作品『实质性相似』，但生成式AI可以在不复制内容的情况下紧密模仿风格，这一能力已成为当前诉讼的核心。我们认为，现有的侵权定义不适用于这种情境，并提出一个新的标准：如果生成式AI的输出无法在不依赖现有作品的情况下生成，则构成侵权。为了将这一定义具体化，我们将生成系统建模为闭包算子，将现有作品的语料库映射为新作品的输出。根据我们的标准，AI生成的输出如果未侵犯任何现有作品，则是『可允许的』。我们的研究结果描述了可允许生成的结构特性，并揭示了一个尖锐的渐近二分法：当有机创作过程是轻尾分布时，对个体作品的依赖最终会消失，因此监管对AI生成不施加限制；而在重尾创作情况下，监管可以持续地起到限制作用。</div>
</details>
</div>
<div class="card">
<div class="title">Certification of linear optical quantum state preparation</div>
<div class="meta-line">Authors: Riko Schadow, Naomi Spier, Stefan N. van den Hoven, Malaquias Correa Anguita, Redlef B. G. Braamhaar, Sara Marzban, Jens Eisert, Jelmer J. Renema, Nathan Walk</div>
<div class="meta-line">First: 2026-02-12T18:55:49+00:00 · Latest: 2026-02-12T18:55:49+00:00</div>
<div class="meta-line">Comments: 32 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12269v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12269v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Certification is important to guarantee the correct functioning of quantum devices. A key certification task is verifying that a device has produced a desired output state. In this work, we study this task in the context of photonic platforms, where single photons are propagated through linear optical interferometers to create large, entangled resource states for metrology, communication, quantum advantage demonstrations and for so-called linear optical quantum computing (LOQC). This setting derives its computational power from the indistinguishability of the photons, i.e., their relative overlap. Therefore, standard fidelity witnesses developed for distinguishable particles (including qubits) do not apply directly, because they merely certify the closeness to some fixed target state. We introduce a measure of fidelity suitable for this setting and show several different ways to witness it, based on earlier proposals for measuring genuine multi-photon indistinguishability. We argue that a witness based upon the discrete Fourier transform is an optimal choice. We experimentally implement this witness and certify the fidelity of several multi-photon states.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>线性光学量子态制备的认证</div>
<div class="mono" style="margin-top:8px">认证对于确保量子设备的正常运行至关重要。一个关键的认证任务是验证设备是否产生了期望的输出态。在本文中，我们研究了在光子平台上这一任务，其中单光子通过线性光学干涉仪传播，以制备用于计量、通信、量子优势演示以及所谓的线性光学量子计算（LOQC）的大规模纠缠资源态。该设置的计算能力来源于光子的不可区分性，即它们的相对重叠。因此，为可区分粒子（包括量子比特）开发的标准保真度见证方法并不直接适用，因为它们仅能证明与某个固定目标态的接近程度。我们引入了一种适用于该设置的保真度度量，并展示了基于先前用于测量真实多光子不可区分性的提案的几种不同见证方法。我们论证了基于离散傅里叶变换的见证方法是最优选择。我们实验实现了该见证方法，并认证了若干多光子态的保真度。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Certification is important to guarantee the correct functioning of quantum devices.</div>
</details>
</div>
<div class="card">
<div class="title">CM2: Reinforcement Learning with Checklist Rewards for Multi-Turn and Multi-Step Agentic Tool Use</div>
<div class="meta-line">Authors: Zhen Zhang, Kaiqiang Song, Xun Wang, Yebowen Hu, Weixiang Yan, Chenyang Zhao, Henry Peng Zou, Haoyun Deng, Sathish Reddy Indurthi, Shujian Liu, Simin Ma, Xiaoyang Wang, Xin Eric Wang, Song Wang</div>
<div class="meta-line">First: 2026-02-12T18:55:09+00:00 · Latest: 2026-02-12T18:55:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12268v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12268v1">PDF</a> · <a href="https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI agents are increasingly used to solve real-world tasks by reasoning over multi-turn user interactions and invoking external tools. However, applying reinforcement learning to such settings remains difficult: realistic objectives often lack verifiable rewards and instead emphasize open-ended behaviors; moreover, RL for multi-turn, multi-step agentic tool use is still underexplored; and building and maintaining executable tool environments is costly, limiting scale and coverage. We propose CM2, an RL framework that replaces verifiable outcome rewards with checklist rewards. CM2 decomposes each turn&#x27;s intended behavior into fine-grained binary criteria with explicit evidence grounding and structured metadata, turning open-ended judging into more stable classification-style decisions. To balance stability and informativeness, our method adopts a strategy of sparse reward assignment but dense evaluation criteria. Training is performed in a scalable LLM-simulated tool environment, avoiding heavy engineering for large tool sets. Experiments show that CM2 consistently improves over supervised fine-tuning. Starting from an 8B Base model and training on an 8k-example RL dataset, CM2 improves over the SFT counterpart by 8 points on tau^-Bench, by 10 points on BFCL-V4, and by 12 points on ToolSandbox. The results match or even outperform similarly sized open-source baselines, including the judging model. CM2 thus provides a scalable recipe for optimizing multi-turn, multi-step tool-using agents without relying on verifiable rewards. Code provided by the open-source community: https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CM2：基于清单奖励的多轮多步骤代理工具使用强化学习</div>
<div class="mono" style="margin-top:8px">AI代理越来越多地用于通过推理多轮用户交互并调用外部工具来解决现实世界任务。然而，将强化学习应用于此类场景仍然具有挑战性：现实目标通常缺乏可验证的奖励，而是强调开放式的操作行为；此外，针对多轮多步骤代理工具使用的强化学习仍处于探索阶段；构建和维护可执行的工具环境成本高昂，限制了其规模和覆盖范围。我们提出了CM2，一种将可验证结果奖励替换为清单奖励的强化学习框架。CM2将每一轮的预期行为分解为细粒度的二元标准，并通过显式的证据锚定和结构化元数据进行处理，将开放式的判断转化为更稳定的分类式决策。为了在稳定性和信息性之间取得平衡，我们的方法采用了稀疏奖励分配但密集评估标准的策略。训练在可扩展的LLM模拟工具环境中进行，避免了对大型工具集进行繁重工程的需求。实验表明，CM2在监督微调基础上持续提升性能。从8B基础模型开始，在8k示例的强化学习数据集上训练，CM2在tau^-Bench上比SFT模型提升8分，在BFCL-V4上提升10分，在ToolSandbox上提升12分。结果与同样规模的开源基线模型相当，甚至在某些方面表现更优，包括判断模型。因此，CM2为优化多轮多步骤工具使用代理提供了一种可扩展的方法，而无需依赖可验证奖励。开源社区提供的代码：https://github.com/namezhenzhang/CM2-RLCR-Tool-Agent.</div>
</details>
</div>
<div class="card">
<div class="title">Self-Supervised Learning via Flow-Guided Neural Operator on Time-Series Data</div>
<div class="meta-line">Authors: Duy Nguyen, Jiachen Yao, Jiayun Wang, Julius Berner, Animashree Anandkumar</div>
<div class="meta-line">First: 2026-02-12T18:54:57+00:00 · Latest: 2026-02-12T18:54:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12267v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12267v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Self-supervised learning (SSL) is a powerful paradigm for learning from unlabeled time-series data. However, popular methods such as masked autoencoders (MAEs) rely on reconstructing inputs from a fixed, predetermined masking ratio. Instead of this static design, we propose treating the corruption level as a new degree of freedom for representation learning, enhancing flexibility and performance. To achieve this, we introduce the Flow-Guided Neural Operator (FGNO), a novel framework combining operator learning with flow matching for SSL training. FGNO learns mappings in functional spaces by using Short-Time Fourier Transform to unify different time resolutions. We extract a rich hierarchy of features by tapping into different network layers and flow times that apply varying strengths of noise to the input data. This enables the extraction of versatile representations, from low-level patterns to high-level global features, using a single model adaptable to specific tasks. Unlike prior generative SSL methods that use noisy inputs during inference, we propose using clean inputs for representation extraction while learning representations with noise; this eliminates randomness and boosts accuracy. We evaluate FGNO across three biomedical domains, where it consistently outperforms established baselines. Our method yields up to 35% AUROC gains in neural signal decoding (BrainTreeBank), 16% RMSE reductions in skin temperature prediction (DREAMT), and over 20% improvement in accuracy and macro-F1 on SleepEDF under low-data regimes. These results highlight FGNO&#x27;s robustness to data scarcity and its superior capacity to learn expressive representations for diverse time series.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过流引导神经算子进行时间序列数据的自监督学习</div>
<div class="mono" style="margin-top:8px">自监督学习（SSL）是一种从无标签时间序列数据中学习的强大范式。然而，流行的掩码自编码器（MAEs）方法依赖于从固定的预设掩码比例中重建输入。与这种静态设计不同，我们提出将破坏程度视为表示学习的新自由度，从而增强灵活性和性能。为此，我们引入了流引导神经算子（FGNO），这是一种结合算子学习与流匹配的新框架，用于SSL训练。FGNO通过使用短时傅里叶变换统一不同时间分辨率，从而在函数空间中学习映射。我们通过利用不同网络层和流时间，对输入数据施加不同强度的噪声，提取丰富的特征层次。这使得模型能够从低级模式到高级全局特征提取多样化表示，且模型可适应特定任务。与之前使用噪声输入进行推理的生成式SSL方法不同，我们提出在学习表示时使用噪声输入，而在提取表示时使用干净输入，从而消除随机性并提高准确性。我们在三个生物医学领域中评估FGNO，结果表明其在这些领域中始终优于现有基线方法。在低数据情况下，我们的方法在神经信号解码（BrainTreeBank）中实现了高达35%的AUROC提升，在皮肤温度预测（DREAMT）中减少了16%的RMSE，在SleepEDF数据集上准确率和macro-F1均提升了超过20%。这些结果突显了FGNO在数据稀缺情况下的鲁棒性，以及其在学习多样化时间序列的表达性表示方面的卓越能力。</div>
</details>
</div>
<div class="card">
<div class="title">Holographic Equidistribution</div>
<div class="meta-line">Authors: Nico Cooper</div>
<div class="meta-line">First: 2026-02-12T18:54:04+00:00 · Latest: 2026-02-12T18:54:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12265v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12265v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hecke operators acting on modular functions arise naturally in the context of 2d conformal field theory, but in seemingly disparate areas, including permutation orbifold theories, ensembles of code CFTs, and more recently in the context of the AdS$_3$/RMT$_2$ program. We use an equidistribution theorem for Hecke operators to show that in each of these large $N$ limits, an entire heavy sector of the partition function gets integrated out, leaving only contributions from Poincaré series of light states. This gives an immediate holographic interpretation as a sum over semiclassical handlebody geometries. We speculate on further physical interpretations for equidistribution, including a potential ergodicity statement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>全息等分布</div>
<div class="mono" style="margin-top:8px">Hecke算符作用于模函数在二维共形场论的背景下自然出现，但也在看似不同的领域中出现，包括置换轨道共形场论、代码CFT集合，以及最近在AdS$_3$/RMT$_2$项目中。我们利用Hecke算符的等分布定理，表明在这些大$N$极限中，整个重态部分会被积分出去，只留下轻态庞加莱级数的贡献。这立即给出了一个全息解释，即对半经典柄体几何的求和。我们推测等分布可能还有其他物理解释，包括可能的遍历性陈述。</div>
</details>
</div>
<div class="card">
<div class="title">T3D: Few-Step Diffusion Language Models via Trajectory Self-Distillation with Direct Discriminative Optimization</div>
<div class="meta-line">Authors: Tunyu Zhang, Xinxi Zhang, Ligong Han, Haizhou Shi, Xiaoxiao He, Zhuowei Li, Hao Wang, Kai Xu, Akash Srivastava, Hao Wang, Vladimir Pavlovic, Dimitris N. Metaxas</div>
<div class="meta-line">First: 2026-02-12T18:52:35+00:00 · Latest: 2026-02-12T18:52:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12262v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12262v1">PDF</a> · <a href="https://github.com/Tyrion58/T3D">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion large language models (DLLMs) have the potential to enable fast text generation by decoding multiple tokens in parallel. However, in practice, their inference efficiency is constrained by the need for many refinement steps, while aggressively reducing the number of steps leads to a substantial degradation in generation quality. To alleviate this, we propose a trajectory self-distillation framework that improves few-step decoding by distilling the model&#x27;s own generative trajectories. We incorporate Direct Discriminative Optimization (DDO), a reverse-KL objective that promotes mode-seeking distillation and encourages the student to concentrate on high-probability teacher modes. Across benchmarks, our approach consistently outperforms strong few-step baselines and standard training under tight step budgets. Although full-step decoding remains superior, we substantially narrow the gap, establishing a strong foundation towards practical few-step DLLMs. The source code is available at https://github.com/Tyrion58/T3D.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>T3D：通过轨迹自蒸馏与直接判别优化实现的少步扩散语言模型</div>
<div class="mono" style="margin-top:8px">扩散大语言模型（DLLMs）有潜力通过并行解码多个标记来实现快速文本生成。然而，在实践中，其推理效率受到需要大量细化步骤的限制，而激进地减少步骤数会导致生成质量显著下降。为了解决这一问题，我们提出了一种轨迹自蒸馏框架，通过蒸馏模型自身的生成轨迹来提升少步解码性能。我们引入了直接判别优化（DDO），这是一种反向KL目标，促进模式寻求的蒸馏，并鼓励学生模型专注于高概率教师模型的模式。在多个基准测试中，我们的方法在严格的步骤预算下持续优于强大的少步基线和标准训练方法。尽管全步解码仍然更优，但我们显著缩小了差距，为实用的少步DLLMs奠定了坚实基础。源代码可在 https://github.com/Tyrion58/T3D 获取。</div>
</details>
</div>
<div class="card">
<div class="title">Think like a Scientist: Physics-guided LLM Agent for Equation Discovery</div>
<div class="meta-line">Authors: Jianke Yang, Ohm Venkatachalam, Mohammad Kianezhad, Sharvaree Vadgama, Rose Yu</div>
<div class="meta-line">First: 2026-02-12T18:49:27+00:00 · Latest: 2026-02-12T18:49:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12259v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12259v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Explaining observed phenomena through symbolic, interpretable formulas is a fundamental goal of science. Recently, large language models (LLMs) have emerged as promising tools for symbolic equation discovery, owing to their broad domain knowledge and strong reasoning capabilities. However, most existing LLM-based systems try to guess equations directly from data, without modeling the multi-step reasoning process that scientists often follow: first inferring physical properties such as symmetries, then using these as priors to restrict the space of candidate equations. We introduce KeplerAgent, an agentic framework that explicitly follows this scientific reasoning process. The agent coordinates physics-based tools to extract intermediate structure and uses these results to configure symbolic regression engines such as PySINDy and PySR, including their function libraries and structural constraints. Across a suite of physical equation benchmarks, KeplerAgent achieves substantially higher symbolic accuracy and greater robustness to noisy data than both LLM and traditional baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>像科学家一样思考：基于物理的LLM代理用于方程发现</div>
<div class="mono" style="margin-top:8px">通过符号化、可解释的公式解释观察到的现象是科学的基本目标。最近，大型语言模型（LLMs）因其广泛的领域知识和强大的推理能力，成为符号方程发现的有前景工具。然而，大多数现有的基于LLM的系统直接从数据猜测方程，而没有建模科学家通常遵循的多步推理过程：首先推断物理属性如对称性，然后利用这些属性作为先验来限制候选方程的空间。我们引入了KeplerAgent，这是一个明确遵循科学推理过程的代理框架。该代理协调基于物理的工具以提取中间结构，并利用这些结果配置符号回归引擎，如PySINDy和PySR，包括其函数库和结构约束。在一系列物理方程基准测试中，KeplerAgent在符号准确性方面显著优于LLM和传统基线，并且对噪声数据具有更强的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Optimal Parallel Algorithms for Convex Hulls in 2D and 3D under Noisy Primitive Operations</div>
<div class="meta-line">Authors: Michael T. Goodrich, Vinesh Sridhar</div>
<div class="meta-line">Venue: In Proceedings of the 37th Canadian Conference on Computational Geometry, pages 36-52, 2025</div>
<div class="meta-line">First: 2025-06-20T23:09:23+00:00 · Latest: 2026-02-12T18:47:03+00:00</div>
<div class="meta-line">Comments: 17 pages, 3 figures. Accepted at the 37th Canadian Conference on Computational Geometry, 2025. This version fixes a bug in the analysis of our 3D hull algorithm</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.17507v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.17507v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the noisy primitives model, each primitive comparison performed by an algorithm, e.g., testing whether one value is greater than another, returns the incorrect answer with random, independent probability p &lt; 1/2 and otherwise returns a correct answer. This model was first applied in the context of sorting and searching, and recent work by Eppstein, Goodrich, and Sridhar extends this model to sequential algorithms involving geometric primitives such as orientation and sidedness tests. However, their approaches appear to be inherently sequential; hence, in this paper, we study parallel computational geometry algorithms for 2D and 3D convex hulls in the noisy primitives model. We give the first optimal parallel algorithms in the noisy primitives model for 2D and 3D convex hulls in the CREW PRAM model. The main technical contribution of our work concerns our ability to detect and fix errors during intermediate steps of our algorithm using a generalization of the failure sweeping technique.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在噪声原语模型下的二维和三维凸包的最优并行算法</div>
<div class="mono" style="margin-top:8px">在噪声原语模型中，算法执行的每个原语比较操作，例如判断一个值是否大于另一个值，以随机且独立的概率 p &lt; 1/2 返回错误答案，否则返回正确答案。该模型最初应用于排序和搜索问题，最近 Eppstein、Goodrich 和 Sridhar 的工作将其扩展到涉及几何原语（如方向性和边侧性测试）的顺序算法中。然而，他们的方法似乎本质上是顺序的；因此，本文研究了在噪声原语模型下的二维和三维凸包的并行计算几何算法。我们给出了在 CREW PRAM 模型下噪声原语模型中二维和三维凸包的第一个最优并行算法。本文的主要技术贡献在于我们能够利用失败扫除技术的推广，在算法中间步骤中检测并修复错误。</div>
</details>
</div>
<div class="card">
<div class="title">Privacy Risks in Time Series Forecasting: User- and Record-Level Membership Inference</div>
<div class="meta-line">Authors: Nicolas Johansson, Tobias Olsson, Daniel Nilsson, Johan Östman, Fazeleh Hoseini</div>
<div class="meta-line">First: 2025-09-04T12:43:45+00:00 · Latest: 2026-02-12T18:46:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.04169v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.04169v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Membership inference attacks (MIAs) aim to determine whether specific data were used to train a model. While extensively studied on classification models, their impact on time series forecasting remains largely unexplored. We address this gap by introducing two new attacks: (i) an adaptation of multivariate LiRA, a state-of-the-art MIA originally developed for classification models, to the time-series forecasting setting, and (ii) a novel end-to-end learning approach called Deep Time Series (DTS) attack. We benchmark these methods against adapted versions of other leading attacks from the classification setting.
  We evaluate all attacks in realistic settings on the TUH-EEG and ELD datasets, targeting two strong forecasting architectures, LSTM and the state-of-the-art N-HiTS, under both record- and user-level threat models. Our results show that forecasting models are vulnerable, with user-level attacks often achieving perfect detection. The proposed methods achieve the strongest performance in several settings, establishing new baselines for privacy risk assessment in time series forecasting. Furthermore, vulnerability increases with longer prediction horizons and smaller training populations, echoing trends observed in large language models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>时间序列预测中的隐私风险：用户级和记录级成员推断攻击</div>
<div class="mono" style="margin-top:8px">成员推断攻击（MIAs）旨在确定特定数据是否被用于训练模型。尽管在分类模型上已有大量研究，但其对时间序列预测的影响仍鲜有探索。我们通过引入两种新攻击方法来填补这一空白：(i) 多变量LiRA攻击的适应版本，这是一种最初为分类模型设计的最先进的MIA方法，现被应用于时间序列预测场景；(ii) 一种名为Deep Time Series（DTS）的新型端到端学习方法。我们针对适应自分类场景的其他领先攻击方法对这些方法进行了基准测试。
我们在一个现实场景中评估了所有攻击方法，使用TUH-EEG和ELD数据集，针对两种强大的预测架构LSTM和最先进的N-HiTS，在记录级和用户级威胁模型下进行测试。我们的结果表明，预测模型存在隐私风险，用户级攻击常常能够实现完美的检测。所提出的方法在多个场景中表现出最强的性能，为时间序列预测中的隐私风险评估建立了新的基准。此外，隐私漏洞随着预测时间范围的增加和训练数据量的减少而增加，这与在大型语言模型中观察到的趋势相呼应。</div>
</details>
</div>
<div class="card">
<div class="title">On the implicit regularization of Langevin dynamics with projected noise</div>
<div class="meta-line">Authors: Govind Menon, Austin J. Stromme, Adrien Vacher</div>
<div class="meta-line">First: 2026-02-12T18:45:42+00:00 · Latest: 2026-02-12T18:45:42+00:00</div>
<div class="meta-line">Comments: 30 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12257v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12257v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study Langevin dynamics with noise projected onto the directions orthogonal to an isometric group action. This mathematical model is introduced to shed new light on the effects of symmetry on stochastic gradient descent for over-parametrized models. Our main result identifies a novel form of implicit regularization: when the initial and target density are both invariant under the group action, Langevin dynamics with projected noise is equivalent in law to Langevin dynamics with isotropic diffusion but with an additional drift term proportional to the negative log volume of the group orbit. We prove this result by constructing a coupling of the two processes via a third process on the group itself, and identify the additional drift as the mean curvature of the orbits.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于投影噪声下朗之万动力学的隐式正则化</div>
<div class="mono" style="margin-top:8px">我们研究了噪声被投影到等距群作用方向正交方向上的朗之万动力学。该数学模型被引入以提供新的视角，探讨对称性对过参数化模型中随机梯度下降的影响。我们的主要结果识别了一种新的隐式正则化形式：当初始和目标密度在群作用下不变时，带有投影噪声的朗之万动力学在分布上等价于带有各向同性扩散的朗之万动力学，但额外包含一个与群轨道负对数体积成比例的漂移项。我们通过构造一个在群本身上的第三过程来耦合这两个过程，从而证明了这一结果，并将额外的漂移项识别为轨道的平均曲率。</div>
</details>
</div>
<div class="card">
<div class="title">Is Online Linear Optimization Sufficient for Strategic Robustness?</div>
<div class="meta-line">Authors: Yang Cai, Haipeng Luo, Chen-Yu Wei, Weiqiang Zheng</div>
<div class="meta-line">First: 2026-02-12T18:41:55+00:00 · Latest: 2026-02-12T18:41:55+00:00</div>
<div class="meta-line">Comments: 26 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12253v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12253v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider bidding in repeated Bayesian first-price auctions. Bidding algorithms that achieve optimal regret have been extensively studied, but their strategic robustness to the seller&#x27;s manipulation remains relatively underexplored. Bidding algorithms based on no-swap-regret algorithms achieve both desirable properties, but are suboptimal in terms of statistical and computational efficiency. In contrast, online gradient ascent is the only algorithm that achieves $O(\sqrt{TK})$ regret and strategic robustness [KSS24], where $T$ denotes the number of auctions and $K$ the number of bids.
  In this paper, we explore whether simple online linear optimization (OLO) algorithms suffice for bidding algorithms with both desirable properties. Our main result shows that sublinear linearized regret is sufficient for strategic robustness. Specifically, we construct simple black-box reductions that convert any OLO algorithm into a strategically robust no-regret bidding algorithm, in both known and unknown value distribution settings. For the known value distribution case, our reduction yields a bidding algorithm that achieves $O(\sqrt{T \log K})$ regret and strategic robustness (with exponential improvement on the $K$-dependence compared to [KSS24]). For the unknown value distribution case, our reduction gives a bidding algorithm with high-probability $O(\sqrt{T (\log K+\log(T/δ)})$ regret and strategic robustness, while removing the bounded density assumption made in [KSS24].</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在线线性优化是否足以实现策略鲁棒性？</div>
<div class="mono" style="margin-top:8px">我们研究在重复的贝叶斯第一价格拍卖中的出价策略。实现最优遗憾的出价算法已被广泛研究，但它们对卖家操控的策略鲁棒性仍相对未被充分探索。基于无交换遗憾算法的出价算法能够实现这两种理想性质，但在统计和计算效率方面是次优的。相比之下，在线梯度上升是唯一能够实现 $O(\sqrt{TK})$ 遗憾和策略鲁棒性的算法 [KSS24]，其中 $T$ 表示拍卖次数，$K$ 表示出价数量。
在本文中，我们探讨简单的在线线性优化（OLO）算法是否足以实现同时具备这两种理想性质的出价策略。我们的主要结果表明，子线性线性化遗憾足以保证策略鲁棒性。具体而言，我们构建了简单的黑盒归约方法，将任何 OLO 算法转换为具有策略鲁棒性的无遗憾出价算法，适用于已知和未知价值分布的场景。在已知价值分布的情况下，我们的归约方法得到的出价算法实现了 $O(\sqrt{T \log K})$ 遗憾和策略鲁棒性（相比 [KSS24] 在 $K$ 依赖性上实现了指数级改进）。在未知价值分布的情况下，我们的归约方法得到的出价算法具有高概率的 $O(\sqrt{T (\log K + \log(T/δ))})$ 遗憾和策略鲁棒性，同时消除了 [KSS24] 中的有界密度假设。</div>
</details>
</div>
<div class="card">
<div class="title">EGG-SR: Embedding Symbolic Equivalence into Symbolic Regression via Equality Graph</div>
<div class="meta-line">Authors: Nan Jiang, Ziyi Wang, Yexiang Xue</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-11-08T04:39:11+00:00 · Latest: 2026-02-12T18:38:11+00:00</div>
<div class="meta-line">Comments: Camera-ready version accepted for ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.05849v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.05849v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://nan-jiang-group.github.io/egg-sr">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Symbolic regression seeks to uncover physical laws from experimental data by searching for closed-form expressions, which is an important task in AI-driven scientific discovery. Yet the exponential growth of the search space of expression renders the task computationally challenging. A promising yet underexplored direction for reducing the search space and accelerating training lies in *symbolic equivalence*: many expressions, although syntactically different, define the same function -- for example, $\log(x_1^2x_2^3)$, $\log(x_1^2)+\log(x_2^3)$, and $2\log(x_1)+3\log(x_2)$. Existing algorithms treat such variants as distinct outputs, leading to redundant exploration and slow learning. We introduce EGG-SR, a unified framework that integrates symbolic equivalence into a class of modern symbolic regression methods, including Monte Carlo Tree Search (MCTS), Deep Reinforcement Learning (DRL), and Large Language Models (LLMs). EGG-SR compactly represents equivalent expressions through the proposed EGG module (via equality graphs), accelerating learning by: (1) pruning redundant subtree exploration in EGG-MCTS, (2) aggregating rewards across equivalent generated sequences in EGG-DRL, and (3) enriching feedback prompts in EGG-LLM. Theoretically, we show the benefit of embedding EGG into learning: it tightens the regret bound of MCTS and reduces the variance of the DRL gradient estimator. Empirically, EGG-SR consistently enhances a class of symbolic regression models across several benchmarks, discovering more accurate expressions within the same time limit. Project page is at: https://nan-jiang-group.github.io/egg-sr.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EGG-SR：通过等式图将符号等价性融入符号回归</div>
<div class="mono" style="margin-top:8px">符号回归旨在通过搜索闭式表达式从实验数据中揭示物理定律，是AI驱动科学发现中的重要任务。然而，表达式搜索空间的指数级增长使该任务在计算上具有挑战性。一种有前景但尚未充分探索的方向是*符号等价性*：许多表达式虽然语法不同，但定义相同的函数，例如$\log(x_1^2x_2^3)$, $\log(x_1^2)+\log(x_2^3)$, 和 $2\log(x_1)+3\log(x_2)$。现有算法将这些变体视为不同的输出，导致冗余探索和缓慢学习。我们引入EGG-SR，一个统一框架，将符号等价性整合到一类现代符号回归方法中，包括蒙特卡洛树搜索（MCTS）、深度强化学习（DRL）和大语言模型（LLMs）。EGG-SR通过提出的EGG模块（等式图）紧凑地表示等价表达式，从而加速学习：(1) 在EGG-MCTS中剪枝冗余的子树探索；(2) 在EGG-DRL中跨等价生成序列聚合奖励；(3) 在EGG-LLM中丰富反馈提示。理论上，我们展示了将EGG嵌入学习中的优势：它收紧了MCTS的遗憾界限，并降低了DRL梯度估计器的方差。实证上，EGG-SR在多个基准测试中持续提升符号回归模型的性能，在相同时间内发现更准确的表达式。项目页面：https://nan-jiang-group.github.io/egg-sr。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Symbolic regression seeks to uncover physical laws from experimental data by searching for closed-form expressions, which is an important task in AI-driven scientific discovery.</div>
</details>
</div>
<div class="card">
<div class="title">A technical curriculum on language-oriented artificial intelligence in translation and specialised communication</div>
<div class="meta-line">Authors: Ralph Krüger</div>
<div class="meta-line">First: 2026-02-12T18:37:23+00:00 · Latest: 2026-02-12T18:37:23+00:00</div>
<div class="meta-line">Comments: 10 pages, 1 figure, EAMT 2026, TAITT Workshop</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12251v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12251v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a technical curriculum on language-oriented artificial intelligence (AI) in the language and translation (L&amp;T) industry. The curriculum aims to foster domain-specific technical AI literacy among stakeholders in the fields of translation and specialised communication by exposing them to the conceptual and technical/algorithmic foundations of modern language-oriented AI in an accessible way. The core curriculum focuses on 1) vector embeddings, 2) the technical foundations of neural networks, 3) tokenization and 4) transformer neural networks. It is intended to help users develop computational thinking as well as algorithmic awareness and algorithmic agency, ultimately contributing to their digital resilience in AI-driven work environments. The didactic suitability of the curriculum was tested in an AI-focused MA course at the Institute of Translation and Multilingual Communication at TH Koeln. Results suggest the didactic effectiveness of the curriculum, but participant feedback indicates that it should be embedded into higher-level didactic scaffolding - e.g., in the form of lecturer support - in order to enable optimal learning conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向翻译与专业交流的语言导向型人工智能技术课程</div>
<div class="mono" style="margin-top:8px">本文提出了一项面向翻译与专业交流领域的语言导向型人工智能（AI）技术课程。该课程旨在通过以一种易于理解的方式介绍现代语言导向型AI的概念和技术/算法基础，提升相关行业利益相关者在特定领域内的技术AI素养。核心课程内容包括：1）向量嵌入，2）神经网络的技术基础，3）分词，4）Transformer神经网络。该课程旨在帮助学习者培养计算思维、算法意识和算法自主性，最终增强其在AI驱动工作环境中的数字韧性。该课程的教学适用性已在科隆大学翻译与多语种交流研究所的AI硕士课程中进行了测试。结果显示该课程具有良好的教学效果，但参与者反馈表明，为了实现最佳学习效果，该课程应嵌入到更高层次的教学框架中，例如通过讲师支持的形式。</div>
</details>
</div>
<div class="card">
<div class="title">Community Concealment from Unsupervised Graph Learning-Based Clustering</div>
<div class="meta-line">Authors: Dalyapraz Manatova, Pablo Moriano, L. Jean Camp</div>
<div class="meta-line">First: 2026-02-12T18:36:19+00:00 · Latest: 2026-02-12T18:36:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12250v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12250v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph neural networks (GNNs) are designed to use attributed graphs to learn representations. Such representations are beneficial in the unsupervised learning of clusters and community detection. Nonetheless, such inference may reveal sensitive groups, clustered systems, or collective behaviors, raising concerns regarding group-level privacy. Community attribution in social and critical infrastructure networks, for example, can expose coordinated asset groups, operational hierarchies, and system dependencies that could be used for profiling or intelligence gathering. We study a defensive setting in which a data publisher (defender) seeks to conceal a community of interest while making limited, utility-aware changes in the network. Our analysis indicates that community concealment is strongly influenced by two quantifiable factors: connectivity at the community boundary and feature similarity between the protected community and adjacent communities. Informed by these findings, we present a perturbation strategy that rewires a set of selected edges and modifies node features to reduce the distinctiveness leveraged by GNN message passing. The proposed method outperforms DICE in our experiments on synthetic benchmarks and real network graphs under identical perturbation budgets. Overall, it achieves median relative concealment improvements of approximately 20-45% across the evaluated settings. These findings demonstrate a mitigation strategy against GNN-based community learning and highlight group-level privacy risks intrinsic to graph learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于无监督图学习聚类的社区隐藏</div>
<div class="mono" style="margin-top:8px">图神经网络（GNNs）被设计用于利用带属性的图来学习表示。这些表示在无监督学习聚类和社区检测中具有优势。然而，这种推断可能会暴露敏感群体、集群系统或集体行为，引发群体层面隐私方面的担忧。例如，在社交网络和关键基础设施网络中，社区属性可能揭示协调资产组、操作层级和系统依赖关系，这些信息可能被用于画像或情报收集。我们研究了一种防御性场景，其中数据发布者（防御者）希望在对网络进行有限且具有效用意识的修改的同时，隐藏其感兴趣的社区。我们的分析表明，社区隐藏受到两个可量化的因素的强烈影响：社区边界处的连通性以及被保护社区与相邻社区之间的特征相似性。基于这些发现，我们提出了一种扰动策略，通过重新布线选定的边并修改节点特征，以降低GNN消息传递机制所利用的区分性。在相同扰动预算下，我们的实验结果表明，所提出的方法在合成基准和真实网络图上均优于DICE。总体而言，该方法在评估的各类场景中实现了约20-45%的中位相对隐藏性能提升。这些发现展示了一种针对基于GNN的社区学习的缓解策略，并突显了图学习固有的群体层面隐私风险。</div>
</details>
</div>
<div class="card">
<div class="title">&quot;Sorry, I Didn&#x27;t Catch That&quot;: How Speech Models Miss What Matters Most</div>
<div class="meta-line">Authors: Kaitlyn Zhou, Martijn Bartelds, Federico Bianchi, James Zou</div>
<div class="meta-line">First: 2026-02-12T18:36:09+00:00 · Latest: 2026-02-12T18:36:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12249v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12249v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite speech recognition systems achieving low word error rates on standard benchmarks, they often fail on short, high-stakes utterances in real-world deployments. Here, we study this failure mode in a high-stakes task: the transcription of U.S. street names as spoken by U.S. participants. We evaluate 15 models from OpenAI, Deepgram, Google, and Microsoft on recordings from linguistically diverse U.S. speakers and find an average transcription error rate of 44%. We quantify the downstream impact of failed transcriptions by geographic locations and show that mis-transcriptions systematically cause errors for all speakers, but that routing distance errors are twice as large for non-English primary speakers compared to English primary speakers. To mitigate this harm, we introduce a synthetic data generation approach that produces diverse pronunciations of named entities using open-source text-to-speech models. Fine-tuning with less than 1,000 synthetic samples improves street name transcription accuracy by nearly 60% (relative to base models) for non-English primary speakers. Our results highlight a critical gap between benchmark performance and real-world reliability in speech systems and demonstrate a simple, scalable path to reducing high-stakes transcription errors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>&quot;抱歉，我没听清&quot;: 语音模型为何错过关键信息</div>
<div class="mono" style="margin-top:8px">尽管语音识别系统在标准基准测试中实现了较低的词错误率，但在现实部署中，它们经常在短而关键的语句上失败。本文研究了这一失败模式，以美国参与者口述的美国街道名称转录为高风险任务。我们评估了来自OpenAI、Deepgram、Google和Microsoft的15个模型，在语言多样性的美国说话人录音上的表现，发现平均转录错误率为44%。我们通过地理区域量化了转录失败的下游影响，并发现误转录系统性地导致所有说话人的错误，但对非英语母语者而言，路线距离错误是英语母语者的两倍。为减轻这种危害，我们引入了一种合成数据生成方法，利用开源文本到语音模型生成命名实体的多样化发音。使用不到1000个合成样本进行微调，可使非英语母语者的街道名称转录准确率提高近60%（相对于基础模型）。我们的结果突显了语音系统在基准性能与现实可靠性之间的关键差距，并展示了一条简单且可扩展的路径，以减少高风险转录错误。</div>
</details>
</div>
<div class="card">
<div class="title">ExtractBench: A Benchmark and Evaluation Methodology for Complex Structured Extraction</div>
<div class="meta-line">Authors: Nick Ferguson, Josh Pennington, Narek Beghian, Aravind Mohan, Douwe Kiela, Sheshansh Agrawal, Thien Hang Nguyen</div>
<div class="meta-line">First: 2026-02-12T18:31:37+00:00 · Latest: 2026-02-12T18:31:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12247v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12247v1">PDF</a> · <a href="https://github.com/ContextualAI/extract-bench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unstructured documents like PDFs contain valuable structured information, but downstream systems require this data in reliable, standardized formats. LLMs are increasingly deployed to automate this extraction, making accuracy and reliability paramount. However, progress is bottlenecked by two gaps. First, no end-to-end benchmark evaluates PDF-to-JSON extraction under enterprise-scale schema breadth. Second, no principled methodology captures the semantics of nested extraction, where fields demand different notions of correctness (exact match for identifiers, tolerance for quantities, semantic equivalence for names), arrays require alignment, and omission must be distinguished from hallucination. We address both gaps with ExtractBench, an open-source benchmark and evaluation framework for PDF-to-JSON structured extraction. The benchmark pairs 35 PDF documents with JSON Schemas and human-annotated gold labels across economically valuable domains, yielding 12,867 evaluatable fields spanning schema complexities from tens to hundreds of fields. The evaluation framework treats the schema as an executable specification: each field declares its scoring metric. Baseline evaluations reveal that frontier models (GPT-5/5.2, Gemini-3 Flash/Pro, Claude 4.5 Opus/Sonnet) remain unreliable on realistic schemas. Performance degrades sharply with schema breadth, culminating in 0% valid output on a 369-field financial reporting schema across all tested models. We release ExtractBench at https://github.com/ContextualAI/extract-bench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ExtractBench：一种复杂结构化提取的基准和评估方法</div>
<div class="mono" style="margin-top:8px">非结构化文档如PDF包含有价值的信息，但下游系统需要这些数据以可靠且标准化的格式呈现。大型语言模型（LLMs）正越来越多地被用于自动化这一提取过程，因此准确性和可靠性变得尤为重要。然而，进展受到两个方面的限制。首先，缺乏一个端到端的基准来评估企业级规模下的PDF到JSON提取。其次，没有一种系统的方法能够捕捉嵌套提取的语义，其中字段对正确性的要求各不相同（标识符需要精确匹配，数量可容忍误差，名称需要语义等价），数组需要对齐，而遗漏必须与幻觉区分开。我们通过ExtractBench这一开源基准和评估框架来解决这两个问题。该基准将35份PDF文档与JSON Schema及人工标注的黄金标签配对，涵盖具有经济价值的领域，从而生成了12,867个可评估字段，覆盖从几十到几百个字段的Schema复杂度。评估框架将Schema视为可执行规范：每个字段声明其评分指标。基线评估表明，前沿模型（如GPT-5/5.2、Gemini-3 Flash/Pro、Claude 4.5 Opus/Sonnet）在现实Schema上仍不可靠。随着Schema复杂度的增加，性能急剧下降，所有测试模型在包含369个字段的财务报告Schema上均输出0%的有效结果。我们已在https://github.com/ContextualAI/extract-bench发布ExtractBench。</div>
</details>
</div>
<div class="card">
<div class="title">Intrinsic-Energy Joint Embedding Predictive Architectures Induce Quasimetric Spaces</div>
<div class="meta-line">Authors: Anthony Kobanda, Waris Radji</div>
<div class="meta-line">First: 2026-02-12T18:30:27+00:00 · Latest: 2026-02-12T18:30:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.12245v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.12245v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Joint-Embedding Predictive Architectures (JEPAs) aim to learn representations by predicting target embeddings from context embeddings, inducing a scalar compatibility energy in a latent space. In contrast, Quasimetric Reinforcement Learning (QRL) studies goal-conditioned control through directed distance values (cost-to-go) that support reaching goals under asymmetric dynamics. In this short article, we connect these viewpoints by restricting attention to a principled class of JEPA energy functions : intrinsic (least-action) energies, defined as infima of accumulated local effort over admissible trajectories between two states. Under mild closure and additivity assumptions, any intrinsic energy is a quasimetric. In goal-reaching control, optimal cost-to-go functions admit exactly this intrinsic form ; inversely, JEPAs trained to model intrinsic energies lie in the quasimetric value class targeted by QRL. Moreover, we observe why symmetric finite energies are structurally mismatched with one-way reachability, motivating asymmetric (quasimetric) energies when directionality matters.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>内在能量联合嵌入预测架构诱导准度量空间</div>
<div class="mono" style="margin-top:8px">联合嵌入预测架构（JEPAs）旨在通过从上下文嵌入预测目标嵌入来学习表示，从而在潜在空间中诱导一个标量兼容性能量。相比之下，准度量强化学习（QRL）通过支持在非对称动态下达到目标的定向距离值（成本到目标）研究目标条件控制。在本文中，我们通过关注一个原理性的联合嵌入能量函数类——内在（最小作用）能量，将这两种观点联系起来。内在能量被定义为两个状态之间可允许轨迹上累积局部努力的下确界。在轻微的闭合性和可加性假设下，任何内在能量都是一个准度量。在目标到达控制中，最优成本到目标函数恰好具有这种内在形式；反过来，训练以建模内在能量的JEPAs属于QRL所针对的准度量值类。此外，我们观察到对称的有限能量在结构上与单向可达性不匹配，这促使我们在方向性重要的情况下使用非对称（准度量）能量。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Autonomous Mathematics Research</div>
<div class="meta-line">Authors: Tony Feng, Trieu H. Trinh, Garrett Bingham, Dawsen Hwang, Yuri Chervonyi, Junehyuk Jung, Joonkyung Lee, Carlo Pagano, Sang-hyun Kim, Federico Pasqualotto, Sergei Gukov, Jonathan N. Lee, Junsu Kim, Kaiying Hou, Golnaz Ghiasi, Yi Tay, YaGuang Li, Chenkai Kuang, Yuan Liu, Hanzhao Lin, Evan Zheran Liu, Nigamaa Nayakanti, Xiaomeng Yang, Heng-Tze Cheng, Demis Hassabis, Koray Kavukcuoglu, Quoc V. Le, Thang Luong</div>
<div class="meta-line">First: 2026-02-10T18:50:15+00:00 · Latest: 2026-02-12T18:27:29+00:00</div>
<div class="meta-line">Comments: 35 pages. Accompanied blog post https://deepmind.google/blog/accelerating-mathematical-and-scientific-discovery-with-gemini-deep-think/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.10177v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.10177v2">PDF</a> · <a href="https://github.com/google-deepmind/superhuman/tree/main/aletheia">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in foundational models have yielded reasoning systems capable of achieving a gold-medal standard at the International Mathematical Olympiad. The transition from competition-level problem-solving to professional research, however, requires navigating vast literature and constructing long-horizon proofs. In this work, we introduce Aletheia, a math research agent that iteratively generates, verifies, and revises solutions end-to-end in natural language. Specifically, Aletheia is powered by an advanced version of Gemini Deep Think for challenging reasoning problems, a novel inference-time scaling law that extends beyond Olympiad-level problems, and intensive tool use to navigate the complexities of mathematical research. We demonstrate the capability of Aletheia from Olympiad problems to PhD-level exercises and most notably, through several distinct milestones in AI-assisted mathematics research: (a) a research paper (Feng26) generated by AI without any human intervention in calculating certain structure constants in arithmetic geometry called eigenweights; (b) a research paper (LeeSeo26) demonstrating human-AI collaboration in proving bounds on systems of interacting particles called independent sets; and (c) an extensive semi-autonomous evaluation (Feng et al., 2026a) of 700 open problems on Bloom&#x27;s Erdos Conjectures database, including autonomous solutions to four open questions. In order to help the public better understand the developments pertaining to AI and mathematics, we suggest quantifying standard levels of autonomy and novelty of AI-assisted results, as well as propose a novel concept of human-AI interaction cards for transparency. We conclude with reflections on human-AI collaboration in mathematics and share all prompts as well as model outputs at https://github.com/google-deepmind/superhuman/tree/main/aletheia.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向自主数学研究</div>
<div class="mono" style="margin-top:8px">基础模型的最新进展已经产生了能够在国际数学奥林匹克竞赛中达到金牌水平的推理系统。然而，从竞赛级问题解决过渡到专业研究，需要处理大量文献并构建长期推理证明。在本工作中，我们引入了Aletheia，这是一个数学研究代理，能够以自然语言生成、验证和修订解决方案。具体而言，Aletheia由改进版的Gemini Deep Think驱动，用于解决具有挑战性的推理问题，结合了一种新颖的推理时间扩展定律，超越了奥林匹克级别的问题，并通过密集的工具使用来应对数学研究的复杂性。我们展示了Aletheia从奥林匹克问题到博士水平练习的能力，并特别通过AI辅助数学研究中的几个不同里程碑：(a) 由AI生成的无任何人类干预参与的论文（Feng26），用于计算算术几何中某些结构常数（称为特征权重）；(b) 展示人类与AI协作证明相互作用粒子系统（称为独立集）界限的论文（LeeSeo26）；(c) 对Bloom的Erdos猜想数据库中的700个开放问题进行广泛半自主评估（Feng et al., 2026a），包括对四个开放问题的自主解决方案。为了帮助公众更好地理解AI与数学发展的关系，我们建议量化AI辅助成果的标准自主性和新颖性，并提出了一种新的透明度概念——人类-AI交互卡片。最后，我们反思了数学领域中的人机协作，并在https://github.com/google-deepmind/superhuman/tree/main/aletheia上分享了所有提示和模型输出。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Pareto Reinforcement Learning for Multi-Objective Recommender Systems</div>
<div class="meta-line">Authors: Pan Li, Alexander Tuzhilin</div>
<div class="meta-line">First: 2024-07-04T02:19:49+00:00 · Latest: 2026-02-12T18:21:18+00:00</div>
<div class="meta-line">Comments: This is a preliminary version of the paper accepted at MISQ: https://doi.org/10.25300/MISQ/2025/19488 Please do not cite this version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.03580v4">Abs</a> · <a href="https://arxiv.org/pdf/2407.03580v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimizing multiple objectives simultaneously is an important task for recommendation platforms to improve their performance. However, this task is particularly challenging since the relationships between different objectives are heterogeneous across different consumers and dynamically fluctuating according to different contexts. Especially in those cases when objectives become conflicting with each other, the result of recommendations will form a pareto-frontier, where the improvements of any objective comes at the cost of a performance decrease of another objective. Existing multi-objective recommender systems do not systematically consider such dynamic relationships; instead, they balance between these objectives in a static and uniform manner, resulting in only suboptimal multi-objective recommendation performance. In this paper, we propose a Deep Pareto Reinforcement Learning (DeepPRL) approach, where we (1) comprehensively model the complex relationships between multiple objectives in recommendations; (2) effectively capture personalized and contextual consumer preference for each objective to provide better recommendations; (3) optimize both the short-term and the long-term performance of multi-objective recommendations. As a result, our method achieves significant pareto-dominance over the state-of-the-art baselines in the offline experiments. Furthermore, we conducted a controlled experiment at the video streaming platform of Alibaba, where our method simultaneously improved three conflicting business objectives over the latest production system significantly, demonstrating its tangible economic impact in practice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向多目标推荐系统的深度帕累托强化学习</div>
<div class="mono" style="margin-top:8px">同时优化多个目标是提升推荐平台性能的重要任务。然而，这一任务极具挑战性，因为不同消费者之间目标关系是异质的，并且会根据不同的上下文动态变化。尤其是在目标之间出现冲突的情况下，推荐结果将形成帕累托前沿，其中任何一个目标的改进都意味着另一个目标性能的下降。现有的多目标推荐系统并未系统地考虑这些动态关系，而是以静态和统一的方式在这些目标之间进行权衡，导致多目标推荐性能只能达到次优。在本文中，我们提出了一种深度帕累托强化学习（DeepPRL）方法，具体包括：(1) 全面建模推荐中多个目标之间的复杂关系；(2) 有效捕捉每个目标下的个性化和上下文相关的消费者偏好，以提供更优质的推荐；(3) 优化多目标推荐的短期和长期性能。实验结果表明，我们的方法在离线实验中显著优于现有基线方法。此外，我们在阿里巴巴的视频流媒体平台进行了受控实验，结果表明我们的方法在最新生产系统上同时显著提升了三个相互冲突的业务目标，证明了其在实际应用中的显著经济价值。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260213_0421.html">20260213_0421</a>
<a href="archive/20260212_0425.html">20260212_0425</a>
<a href="archive/20260210_0435.html">20260210_0435</a>
<a href="archive/20260209_0404.html">20260209_0404</a>
<a href="archive/20260208_0353.html">20260208_0353</a>
<a href="archive/20260207_0410.html">20260207_0410</a>
<a href="archive/20260206_0412.html">20260206_0412</a>
<a href="archive/20260205_0417.html">20260205_0417</a>
<a href="archive/20260204_0421.html">20260204_0421</a>
<a href="archive/20260202_0402.html">20260202_0402</a>
<a href="archive/20260201_0354.html">20260201_0354</a>
<a href="archive/20260131_0408.html">20260131_0408</a>
<a href="archive/20260130_0406.html">20260130_0406</a>
<a href="archive/20260129_0407.html">20260129_0407</a>
<a href="archive/20260128_0407.html">20260128_0407</a>
<a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
