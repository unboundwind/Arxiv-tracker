<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-08 03:56</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260108_0356</div>
    <div class="row"><div class="card">
<div class="title">Muses: Designing, Composing, Generating Nonexistent Fantasy 3D Creatures without Training</div>
<div class="meta-line">Authors: Hexiao Lu, Xiaokun Sun, Zeyu Cai, Hao Guo, Ying Tai, Jian Yang, Zhenyu Zhang</div>
<div class="meta-line">First: 2026-01-06T18:59:57+00:00 · Latest: 2026-01-06T18:59:57+00:00</div>
<div class="meta-line">Comments: Project page: https://luhexiao.github.io/Muses.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03256v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03256v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://luhexiao.github.io/Muses.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Muses, the first training-free method for fantastic 3D creature generation in a feed-forward paradigm. Previous methods, which rely on part-aware optimization, manual assembly, or 2D image generation, often produce unrealistic or incoherent 3D assets due to the challenges of intricate part-level manipulation and limited out-of-domain generation. In contrast, Muses leverages the 3D skeleton, a fundamental representation of biological forms, to explicitly and rationally compose diverse elements. This skeletal foundation formalizes 3D content creation as a structure-aware pipeline of design, composition, and generation. Muses begins by constructing a creatively composed 3D skeleton with coherent layout and scale through graph-constrained reasoning. This skeleton then guides a voxel-based assembly process within a structured latent space, integrating regions from different objects. Finally, image-guided appearance modeling under skeletal conditions is applied to generate a style-consistent and harmonious texture for the assembled shape. Extensive experiments establish Muses&#x27; state-of-the-art performance in terms of visual fidelity and alignment with textual descriptions, and potential on flexible 3D object editing. Project page: https://luhexiao.github.io/Muses.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>缪斯：无需训练的幻想3D生物生成方法</div>
<div class="mono" style="margin-top:8px">我们提出了Muses，这是首个在前馈范式下无需训练的幻想3D生物生成方法。以往的方法依赖于部件感知优化、手动组装或2D图像生成，由于复杂部件级操作的挑战和有限的领域外生成能力，常常导致生成的3D资产不真实或不连贯。相比之下，Muses利用3D骨骼，这是生物形态的基本表示，以明确且合理的方式组合多样元素。该骨骼基础将3D内容创作形式化为一个结构感知的流程，包括设计、组合和生成。Muses首先通过图约束推理构建一个具有连贯布局和比例的创造性3D骨骼。随后，该骨骼引导在结构化潜在空间中的体素组装过程，整合不同物体的区域。最后，在骨骼条件下应用图像引导的外观建模，以生成风格一致且和谐的纹理。大量实验验证了Muses在视觉保真度和与文本描述的对齐度方面的最先进性能，以及其在灵活3D对象编辑方面的潜力。项目页面：https://luhexiao.github.io/Muses.github.io/</div>
</details>
</div>
<div class="card">
<div class="title">TTrace: Lightweight Error Checking and Diagnosis for Distributed Training</div>
<div class="meta-line">Authors: Haitian Jiang, Shaowei Zhu, Zhen Zhang, Zhenyu Song, Xinwei Fu, Zhen Jia, Yida Wang, Jinyang Li</div>
<div class="meta-line">First: 2025-06-10T22:39:14+00:00 · Latest: 2026-01-06T18:59:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.09280v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.09280v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Distributed training is essential for scaling the training of large neural network models, such as large language models (LLMs), across thousands of GPUs. However, the complexity of distributed training programs makes them particularly prone to silent bugs, which do not produce explicit error signals but lead to incorrect training outcomes. Effectively detecting and localizing such silent bugs in distributed training is challenging. Common debugging practices based on monitoring training loss or gradient norm curves are indirect, inefficient, and provide no way to localize bugs. To address those challenges, we design and implement TTrace, the first systematic differential testing system for detecting and localizing silent bugs in distributed training. TTrace aligns intermediate tensors from distributed training with those from a trusted reference implementation. To properly compare the floating-point values in the corresponding tensors, we propose a novel mathematical analysis that provides a guideline for setting tolerances, enabling TTrace to distinguish bug-induced errors from numerical errors. Experimental results demonstrate that TTrace effectively detects 11 existing bugs and 3 new bugs in the widely used Megatron-LM framework, while requiring fewer than 10 lines of code changes. TTrace is effective in various training recipes, including low-precision recipes involving BF16 and FP8. Notably, a popular open-source training framework has already adopted the method proposed by TTrace in its development workflow.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TTrace：分布式训练中的轻量级错误检测与诊断</div>
<div class="mono" style="margin-top:8px">分布式训练对于在数千个GPU上扩展大型神经网络模型（如大型语言模型LLMs）的训练至关重要。然而，分布式训练程序的复杂性使其特别容易出现静默错误，这些错误不会产生明确的错误信号，但会导致训练结果不正确。在分布式训练中有效检测和定位此类静默错误具有挑战性。基于监控训练损失或梯度范数曲线的常见调试方法是间接的、低效的，并且无法定位错误。为了解决这些挑战，我们设计并实现了TTrace，这是首个用于检测和定位分布式训练中静默错误的系统性差分测试系统。TTrace将分布式训练中的中间张量与可信参考实现中的张量进行对齐。为了正确比较对应张量中的浮点数值，我们提出了一种新颖的数学分析方法，为设置容忍度提供指导，使TTrace能够区分由错误引起的误差和数值误差。实验结果表明，TTrace在广泛使用的Megatron-LM框架中有效检测了11个现有错误和3个新错误，且仅需不到10行代码修改。TTrace在多种训练方案中均有效，包括涉及BF16和FP8的低精度方案。值得注意的是，一个流行的开源训练框架已经在其开发流程中采用了TTrace提出的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Aligning Text, Images, and 3D Structure Token-by-Token</div>
<div class="meta-line">Authors: Aadarsh Sahoo, Vansh Tibrewal, Georgia Gkioxari</div>
<div class="meta-line">First: 2025-06-09T17:59:37+00:00 · Latest: 2026-01-06T18:58:50+00:00</div>
<div class="meta-line">Comments: Project webpage: https://glab-caltech.github.io/kyvo/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.08002v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.08002v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://glab-caltech.github.io/kyvo/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Creating machines capable of understanding the world in 3D is essential in assisting designers that build and edit 3D environments and robots navigating and interacting within a three-dimensional space. Inspired by advances in language and image modeling, we investigate the potential of autoregressive models for a new modality: structured 3D scenes. To this end, we propose a unified LLM framework that aligns language, images, and 3D scenes and provide a detailed &#x27;&#x27;cookbook&#x27;&#x27; outlining critical design choices for achieving optimal training and performance addressing key questions related to data representation, modality-specific objectives, and more. We show how to tokenize complex 3D objects to incorporate into our structured 3D scene modality. We evaluate performance across four core 3D tasks -- rendering, recognition, instruction-following, and question-answering -- and four 3D datasets, synthetic and real-world. We show our model&#x27;s effectiveness on reconstructing complete 3D scenes consisting of complex objects from a single image and on real-world 3D object recognition tasks. Project webpage: https://glab-caltech.github.io/kyvo/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>逐字对齐文本、图像和3D结构</div>
<div class="mono" style="margin-top:8px">创建能够理解三维世界的机器对于帮助构建和编辑三维环境以及机器人在三维空间中导航和交互至关重要。受语言和图像建模进展的启发，我们研究了自回归模型在新模态——结构化3D场景中的潜力。为此，我们提出了一种统一的LLM框架，用于对齐语言、图像和3D场景，并提供一份详细的指南，阐述实现最佳训练和性能的关键设计选择，涵盖数据表示、模态特定目标等核心问题。我们展示了如何对复杂3D对象进行分词，以将其纳入我们的结构化3D场景模态。我们在四个核心3D任务——渲染、识别、指令遵循和问答——以及四个3D数据集（合成和真实世界）上评估了模型性能。我们展示了模型在从单张图像重建包含复杂对象的完整3D场景以及在真实世界3D对象识别任务中的有效性。项目网页：https://glab-caltech.github.io/kyvo/</div>
</details>
</div>
<div class="card">
<div class="title">Automated Semantic Rules Detection (ASRD) for Emergent Communication Interpretation</div>
<div class="meta-line">Authors: Bastien Vanderplaetse, Xavier Siebert, Stéphane Dupont</div>
<div class="meta-line">First: 2026-01-06T18:57:39+00:00 · Latest: 2026-01-06T18:57:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03254v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03254v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The field of emergent communication within multi-agent systems examines how autonomous agents can independently develop communication strategies, without explicit programming, and adapt them to varied environments. However, few studies have focused on the interpretability of emergent languages. The research exposed in this paper proposes an Automated Semantic Rules Detection (ASRD) algorithm, which extracts relevant patterns in messages exchanged by agents trained with two different datasets on the Lewis Game, which is often studied in the context of emergent communication. ASRD helps at the interpretation of the emergent communication by relating the extracted patterns to specific attributes of the input data, thereby considerably simplifying subsequent analysis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于涌现通信解释的自动化语义规则检测（ASRD）</div>
<div class="mono" style="margin-top:8px">在多智能体系统中，涌现通信领域研究了自主智能体如何在没有显式编程的情况下独立发展通信策略，并适应不同的环境。然而，很少有研究关注涌现语言的可解释性。本文提出了一种自动化语义规则检测（ASRD）算法，该算法从在Lewis游戏中使用两个不同数据集训练的智能体之间交换的消息中提取相关模式。ASRD通过将提取的模式与输入数据的特定属性相关联，从而大大简化了后续的分析。</div>
</details>
</div>
<div class="card">
<div class="title">InfiniDepth: Arbitrary-Resolution and Fine-Grained Depth Estimation with Neural Implicit Fields</div>
<div class="meta-line">Authors: Hao Yu, Haotong Lin, Jiawei Wang, Jiaxin Li, Yida Wang, Xueyang Zhang, Yue Wang, Xiaowei Zhou, Ruizhen Hu, Sida Peng</div>
<div class="meta-line">First: 2026-01-06T18:57:06+00:00 · Latest: 2026-01-06T18:57:06+00:00</div>
<div class="meta-line">Comments: 19 pages, 13 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03252v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03252v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids. Such representations restrict their scalability to arbitrary output resolutions and hinder the geometric detail recovery. This paper introduces InfiniDepth, which represents depth as neural implicit fields. Through a simple yet effective local implicit decoder, we can query depth at continuous 2D coordinates, enabling arbitrary-resolution and fine-grained depth estimation. To better assess our method&#x27;s capabilities, we curate a high-quality 4K synthetic benchmark from five different games, spanning diverse scenes with rich geometric and appearance details. Extensive experiments demonstrate that InfiniDepth achieves state-of-the-art performance on both synthetic and real-world benchmarks across relative and metric depth estimation tasks, particularly excelling in fine-detail regions. It also benefits the task of novel view synthesis under large viewpoint shifts, producing high-quality results with fewer holes and artifacts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InfiniDepth：基于神经隐式场的任意分辨率与细粒度深度估计</div>
<div class="mono" style="margin-top:8px">现有的深度估计方法本质上受限于在离散图像网格上预测深度。这种表示方式限制了其在任意输出分辨率上的可扩展性，并阻碍了几何细节的恢复。本文引入了InfiniDepth，将深度表示为神经隐式场。通过一个简单而有效的局部隐式解码器，我们可以在连续的2D坐标上查询深度，从而实现任意分辨率和细粒度的深度估计。为了更好地评估我们方法的能力，我们从五个不同的游戏中整理了一个高质量的4K合成基准数据集，涵盖具有丰富几何和外观细节的多样化场景。大量实验表明，InfiniDepth在合成和真实世界基准上均实现了最先进的性能，特别是在细粒度细节区域表现尤为出色。此外，它也对大视角变化下的新视角合成任务有帮助，能够生成高质量的结果，且孔洞和伪影更少。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Existing depth estimation methods are fundamentally limited to predicting depth on discrete image grids.</div>
</details>
</div>
<div class="card">
<div class="title">A Versatile Multimodal Agent for Multimedia Content Generation</div>
<div class="meta-line">Authors: Daoan Zhang, Wenlin Yao, Xiaoyang Wang, Yebowen Hu, Jiebo Luo, Dong Yu</div>
<div class="meta-line">First: 2026-01-06T18:49:47+00:00 · Latest: 2026-01-06T18:49:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03250v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03250v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the advancement of AIGC (AI-generated content) technologies, an increasing number of generative models are revolutionizing fields such as video editing, music generation, and even film production. However, due to the limitations of current AIGC models, most models can only serve as individual components within specific application scenarios and are not capable of completing tasks end-to-end in real-world applications. In real-world applications, editing experts often work with a wide variety of images and video inputs, producing multimodal outputs -- a video typically includes audio, text, and other elements. This level of integration across multiple modalities is something current models are unable to achieve effectively. However, the rise of agent-based systems has made it possible to use AI tools to tackle complex content generation tasks. To deal with the complex scenarios, in this paper, we propose a MultiMedia-Agent designed to automate complex content creation. Our agent system includes a data generation pipeline, a tool library for content creation, and a set of metrics for evaluating preference alignment. Notably, we introduce the skill acquisition theory to model the training data curation and agent training. We designed a two-stage correlation strategy for plan optimization, including self-correlation and model preference correlation. Additionally, we utilized the generated plans to train the MultiMedia-Agent via a three stage approach including base/success plan finetune and preference optimization. The comparison results demonstrate that the our approaches are effective and the MultiMedia-Agent can generate better multimedia content compared to novel models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种用于多媒体内容生成的多功能多模态代理</div>
<div class="mono" style="margin-top:8px">随着AIGC（AI生成内容）技术的进步，越来越多的生成模型正在革新视频编辑、音乐生成，甚至电影制作等领域。然而，由于当前AIGC模型的局限性，大多数模型只能作为特定应用场景中的单一组件，无法在现实应用中完成端到端的任务。在现实应用中，编辑专家通常需要处理各种图像和视频输入，生成多模态输出——视频通常包含音频、文本和其他元素。这种跨多个模态的整合程度是当前模型难以有效实现的。然而，基于代理的系统的发展使得使用AI工具来处理复杂的多媒体生成任务成为可能。为应对复杂场景，本文提出了一种MultiMedia-Agent，旨在自动化复杂内容的创建。我们的代理系统包括一个数据生成流水线、一个用于内容创建的工具库以及一套用于评估偏好对齐的指标。值得注意的是，我们引入了技能获取理论来建模训练数据的整理和代理的训练。我们设计了一个两阶段的相关性策略用于计划优化，包括自相关和模型偏好相关性。此外，我们还通过一个三阶段方法利用生成的计划来训练MultiMedia-Agent，包括基础/成功计划微调和偏好优化。比较结果表明，我们的方法是有效的，且MultiMedia-Agent能够生成比新型模型更优质的多媒体内容。</div>
</details>
</div>
<div class="card">
<div class="title">Characterizing the Robustness of Black-Box LLM Planners Under Perturbed Observations with Adaptive Stress Testing</div>
<div class="meta-line">Authors: Neeloy Chakraborty, John Pohovey, Melkior Ornik, Katherine Driggs-Campbell</div>
<div class="meta-line">First: 2025-05-08T21:50:43+00:00 · Latest: 2026-01-06T18:46:38+00:00</div>
<div class="meta-line">Comments: 30 pages, 24 figures, 6 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.05665v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.05665v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have recently demonstrated success in decision-making tasks including planning, control, and prediction, but their tendency to hallucinate unsafe and undesired outputs poses risks. This unwanted behavior is further exacerbated in environments where sensors are noisy or unreliable. Characterizing the behavior of LLM planners to varied observations is necessary to proactively avoid failures in safety-critical scenarios. We specifically investigate the response of LLMs along two different perturbation dimensions. Like prior works, one dimension generates semantically similar prompts with varied phrasing by randomizing order of details, modifying access to few-shot examples, etc. Unique to our work, the second dimension simulates access to varied sensors and noise to mimic raw sensor or detection algorithm failures. An initial case study in which perturbations are manually applied show that both dimensions lead LLMs to hallucinate in a multi-agent driving environment. However, manually covering the entire perturbation space for several scenarios is infeasible. As such, we propose a novel method for efficiently searching the space of prompt perturbations using adaptive stress testing (AST) with Monte-Carlo tree search (MCTS). Our AST formulation enables discovery of scenarios, sensor configurations, and prompt phrasing that cause language models to act with high uncertainty or even crash. By generating MCTS prompt perturbation trees across diverse scenarios, we show through extensive experiments that offline analyses can be used to proactively understand potential failures that may arise at runtime.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过自适应压力测试对受扰观测下黑盒LLM规划器鲁棒性的表征</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）最近在决策任务（如规划、控制和预测）中表现出色，但其倾向于生成不安全且不期望的输出，存在潜在风险。这种不良行为在传感器噪声或不可靠的环境中会进一步加剧。为了在安全关键场景中主动避免失败，有必要表征LLM规划器在不同观测条件下的行为。我们特别研究了LLM在两个不同扰动维度上的响应。与以往工作类似，其中一个维度通过随机化细节顺序、修改少样本示例的访问方式等方法生成语义相似但措辞不同的提示。而我们工作的独特之处在于，第二个维度模拟了不同传感器和噪声的访问情况，以模仿原始传感器或检测算法的故障。一个初始案例研究显示，手动应用扰动后，这两个维度都会导致LLM在多智能体驾驶环境中产生幻觉。然而，手动覆盖多个场景的整个扰动空间是不可行的。因此，我们提出了一种基于蒙特卡洛树搜索（MCTS）的自适应压力测试（AST）方法，以高效搜索提示扰动空间。我们的AST框架能够发现导致语言模型行为高度不确定甚至崩溃的场景、传感器配置和提示措辞。通过在多样化场景中生成MCTS提示扰动树，我们通过大量实验表明，离线分析可用于主动理解运行时可能出现的潜在故障。</div>
</details>
</div>
<div class="card">
<div class="title">VisRet: Visualization Improves Knowledge-Intensive Text-to-Image Retrieval</div>
<div class="meta-line">Authors: Di Wu, Yixin Wan, Kai-Wei Chang</div>
<div class="meta-line">First: 2025-05-26T17:59:33+00:00 · Latest: 2026-01-06T18:46:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.20291v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.20291v3">PDF</a> · <a href="https://github.com/xiaowu0162/Visualize-then-Retrieve">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image retrieval (T2I retrieval) remains challenging because cross-modal embeddings often behave as bags of concepts, underrepresenting structured visual relationships such as pose and viewpoint. We propose Visualize-then-Retrieve (VisRet), a retrieval paradigm that mitigates this limitation of cross-modal similarity alignment. VisRet first projects textual queries into the image modality via T2I generation, then performs retrieval within the image modality to bypass the weaknesses of cross-modal retrievers in recognizing subtle visual-spatial features. Across four benchmarks (Visual-RAG, INQUIRE-Rerank, Microsoft COCO, and our new Visual-RAG-ME featuring multi-entity comparisons), VisRet substantially outperforms cross-modal similarity matching and baselines that recast T2I retrieval as text-to-text similarity matching, improving nDCG@30 by 0.125 on average with CLIP as the retriever and by 0.121 with E5-V. For downstream question answering, VisRet increases accuracy on Visual-RAG and Visual-RAG-ME by 3.8% and 15.7% in top-1 retrieval, and by 3.9% and 11.1% in top-10 retrieval. Ablation studies show compatibility with different T2I instruction LLMs, T2I generation models, and downstream LLMs. VisRet provides a simple yet effective perspective for advancing in text-image retrieval. Our code and the new benchmark are publicly available at https://github.com/xiaowu0162/Visualize-then-Retrieve.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VisRet：可视化提升知识密集型文本到图像检索</div>
<div class="mono" style="margin-top:8px">文本到图像检索（T2I检索）仍然具有挑战性，因为跨模态嵌入通常表现为概念的集合，未能充分表达结构化的视觉关系，如姿态和视角。我们提出了一种名为Visualize-then-Retrieve（VisRet）的检索范式，以缓解跨模态相似性对齐的这一局限性。VisRet首先通过T2I生成将文本查询投影到图像模态，然后在图像模态内进行检索，从而绕过跨模态检索器在识别细微视觉空间特征方面的不足。在四个基准测试（Visual-RAG、INQUIRE-Rerank、Microsoft COCO以及我们新提出的包含多实体比较的Visual-RAG-ME）中，VisRet显著优于跨模态相似性匹配方法以及将T2I检索重新表述为文本到文本相似性匹配的基线方法。使用CLIP作为检索器时，VisRet在nDCG@30上的平均提升为0.125，使用E5-V时提升为0.121。在下游问答任务中，VisRet在Visual-RAG和Visual-RAG-ME的top-1检索中分别提升了3.8%和15.7%的准确率，在top-10检索中分别提升了3.9%和11.1%。消融研究显示VisRet与不同的T2I指令大语言模型（LLMs）、T2I生成模型和下游LLMs兼容。VisRet为推进文本-图像检索提供了一种简单而有效的视角。我们的代码和新基准测试可在https://github.com/xiaowu0162/Visualize-then-Retrieve上公开获取。</div>
</details>
</div>
<div class="card">
<div class="title">STReasoner: Empowering LLMs for Spatio-Temporal Reasoning in Time Series via Spatial-Aware Reinforcement Learning</div>
<div class="meta-line">Authors: Juntong Ni, Shiyu Wang, Ming Jin, Qi He, Wei Jin</div>
<div class="meta-line">First: 2026-01-06T18:46:12+00:00 · Latest: 2026-01-06T18:46:12+00:00</div>
<div class="meta-line">Comments: preprint, we release our code publicly at https://github.com/LingFengGold/STReasoner</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03248v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03248v1">PDF</a> · <a href="https://github.com/LingFengGold/STReasoner">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatio-temporal reasoning in time series involves the explicit synthesis of temporal dynamics, spatial dependencies, and textual context. This capability is vital for high-stakes decision-making in systems such as traffic networks, power grids, and disease propagation. However, the field remains underdeveloped because most existing works prioritize predictive accuracy over reasoning. To address the gap, we introduce ST-Bench, a benchmark consisting of four core tasks, including etiological reasoning, entity identification, correlation reasoning, and in-context forecasting, developed via a network SDE-based multi-agent data synthesis pipeline. We then propose STReasoner, which empowers LLM to integrate time series, graph structure, and text for explicit reasoning. To promote spatially grounded logic, we introduce S-GRPO, a reinforcement learning algorithm that rewards performance gains specifically attributable to spatial information. Experiments show that STReasoner achieves average accuracy gains between 17% and 135% at only 0.004X the cost of proprietary models and generalizes robustly to real-world data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STReasoner: 通过空间感知强化学习增强LLMs在时间序列中的时空推理能力</div>
<div class="mono" style="margin-top:8px">时间序列中的时空推理涉及显式合成时间动态、空间依赖性和文本上下文。这种能力对于交通网络、电网和疾病传播等系统中的高风险决策至关重要。然而，该领域仍不成熟，因为大多数现有工作优先考虑预测准确性而非推理能力。为了解决这一问题，我们引入了ST-Bench，这是一个由基于网络SDE的多智能体数据合成流水线开发的基准，包含四个核心任务：病因推理、实体识别、相关性推理和上下文内预测。随后，我们提出了STReasoner，使LLM能够整合时间序列、图结构和文本进行显式推理。为了促进空间基础的逻辑，我们引入了S-GRPO，这是一种强化学习算法，专门奖励与空间信息相关的性能提升。实验表明，STReasoner仅以专有模型和通用模型的0.004倍成本，就实现了平均准确率提升17%至135%，并且在现实数据上具有强大的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">ShareChat: A Dataset of Chatbot Conversations in the Wild</div>
<div class="meta-line">Authors: Yueru Yan, Tuc Nguyen, Bo Su, Melissa Lieffers, Thai Le</div>
<div class="meta-line">First: 2025-12-19T17:47:53+00:00 · Latest: 2026-01-06T18:45:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17843v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.17843v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While academic research typically treats Large Language Models (LLM) as generic text generators, they are distinct commercial products with unique interfaces and capabilities that fundamentally shape user behavior. Current datasets obscure this reality by collecting text-only data through uniform interfaces that fail to capture authentic chatbot usage. To address this limitation, we present ShareChat, a large-scale corpus of 142,808 conversations (660,293 turns) sourced directly from publicly shared URLs on ChatGPT, Perplexity, Grok, Gemini, and Claude. ShareChat distinguishes itself by preserving native platform affordances, such as citations and thinking traces, across a diverse collection covering 101 languages and the period from April 2023 to October 2025. Furthermore, ShareChat offers substantially longer context windows and greater interaction depth than prior datasets. To illustrate the dataset&#x27;s breadth, we present three case studies: a completeness analysis of intent satisfaction, a citation study of model grounding, and a temporal analysis of engagement rhythms. This work provides the community with a vital and timely resource for understanding authentic user-LLM chatbot interactions in the wild. The dataset will be publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ShareChat：真实环境中的聊天机器人对话数据集</div>
<div class="mono" style="margin-top:8px">尽管学术研究通常将大型语言模型（LLM）视为通用文本生成器，但它们实际上是具有独特界面和功能的商业产品，这些特性从根本上塑造了用户行为。当前的数据集通过统一的界面收集纯文本数据，未能真实反映聊天机器人的实际使用情况。为了解决这一局限性，我们提出了ShareChat，这是一个包含142,808次对话（660,293轮次）的大规模语料库，直接来源于ChatGPT、Perplexity、Grok、Gemini和Claude等平台上公开共享的URL。ShareChat通过保留原平台的特性，如引用和思考轨迹，在涵盖101种语言、从2023年4月至2025年10月的多样化对话集合中脱颖而出。此外，ShareChat还提供了比以往数据集更长的上下文窗口和更深层次的交互。为了展示数据集的广泛性，我们提供了三个案例研究：意图满足的完整性分析、模型基础的引用研究以及互动节奏的时间分析。本工作为社区提供了一个理解真实用户与LLM聊天机器人互动的重要且及时资源。该数据集将公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">While academic research typically treats Large Language Models (LLM) as generic text generators, they are distinct commercial products with unique interfaces and capabilities that fundamentally shape user behavior.</div>
</details>
</div>
<div class="card">
<div class="title">Nonlinear Spectral Modeling and Control of Soft-Robotic Muscles from Data</div>
<div class="meta-line">Authors: Leonardo Bettini, Amirhossein Kazemipour, Robert K. Katzschmann, George Haller</div>
<div class="meta-line">First: 2026-01-06T18:43:49+00:00 · Latest: 2026-01-06T18:43:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03247v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03247v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Artificial muscles are essential for compliant musculoskeletal robotics but complicate control due to nonlinear multiphysics dynamics. Hydraulically amplified electrostatic (HASEL) actuators, a class of soft artificial muscles, offer high performance but exhibit memory effects and hysteresis. Here we present a data-driven reduction and control strategy grounded in spectral submanifold (SSM) theory. In the adiabatic regime, where inputs vary slowly relative to intrinsic transients, trajectories rapidly converge to a low-dimensional slow manifold. We learn an explicit input-to-output map on this manifold from forced-response trajectories alone, avoiding decay experiments that can trigger hysteresis. We deploy the SSM-based model for real-time control of an antagonistic HASEL-clutch joint. This approach yields a substantial reduction in tracking error compared to feedback-only and feedforward-only baselines under identical settings. This record-and-control workflow enables rapid characterization and high-performance control of soft muscles and muscle-driven joints without detailed physics-based modeling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于数据的软体机器人肌肉非线性谱建模与控制</div>
<div class="mono" style="margin-top:8px">人工肌肉对于柔性肌骨机器人至关重要，但由于非线性多物理场动力学而使控制复杂化。液压放大静电（HASEL）执行器是一类软体人工肌肉，具有高性能，但表现出记忆效应和迟滞现象。本文提出了一种基于谱子流形（SSM）理论的数据驱动降阶与控制策略。在绝热工况下，输入变化相对于内在瞬态较慢，轨迹会快速收敛到一个低维慢流形。我们仅通过强迫响应轨迹学习该流形上的显式输入-输出映射，从而避免可能引发迟滞的衰减实验。我们采用基于SSM的模型对拮抗式HASEL离合关节进行实时控制。与仅反馈或仅前馈的基线方法相比，在相同条件下，该方法显著降低了跟踪误差。这种记录与控制的工作流程使得无需详细的基于物理的建模即可快速表征和实现软肌肉及肌肉驱动关节的高性能控制。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Supervised Learning from Noisy and Incomplete Data</div>
<div class="meta-line">Authors: Julián Tachella, Mike Davies</div>
<div class="meta-line">First: 2026-01-06T18:40:50+00:00 · Latest: 2026-01-06T18:40:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03244v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03244v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many important problems in science and engineering involve inferring a signal from noisy and/or incomplete observations, where the observation process is known. Historically, this problem has been tackled using hand-crafted regularization (e.g., sparsity, total-variation) to obtain meaningful estimates. Recent data-driven methods often offer better solutions by directly learning a solver from examples of ground-truth signals and associated observations. However, in many real-world applications, obtaining ground-truth references for training is expensive or impossible. Self-supervised learning methods offer a promising alternative by learning a solver from measurement data alone, bypassing the need for ground-truth references. This manuscript provides a comprehensive summary of different self-supervised methods for inverse problems, with a special emphasis on their theoretical underpinnings, and presents practical applications in imaging inverse problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从噪声和不完整数据中进行自监督学习</div>
<div class="mono" style="margin-top:8px">许多科学和工程中的重要问题涉及从噪声和/或不完整的观测中推断信号，其中观测过程是已知的。历史上，这类问题通常通过手工设计的正则化方法（例如稀疏性、总变差）来获得有意义的估计。近年来，数据驱动的方法通过直接从真实信号及其相关观测示例中学习求解器，通常能提供更优的解决方案。然而，在许多实际应用中，获取用于训练的真实参考信号成本高昂或根本不可能。自监督学习方法提供了一种有前景的替代方案，即仅通过测量数据学习求解器，从而避免对真实参考信号的依赖。本文综述了不同自监督方法在逆问题中的应用，特别强调其理论基础，并展示了在成像逆问题中的实际应用。</div>
</details>
</div>
<div class="card">
<div class="title">Kolmogorov-Arnold Energy Models: Fast and Interpretable Generative Modeling</div>
<div class="meta-line">Authors: Prithvi Raj</div>
<div class="meta-line">First: 2025-06-17T04:07:32+00:00 · Latest: 2026-01-06T18:32:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.14167v8">Abs</a> · <a href="https://arxiv.org/pdf/2506.14167v8">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning an energy-based model (EBM) in the latent space of a top-down generative model offers a powerful framework for generation across many data modalities. However, it remains unclear how its interpretability can be used to guide model design, improve generative quality, and reduce training time. Moreover, the reliance on Langevin Monte Carlo (LMC) sampling presents challenges in efficiency and sampling multimodal latent distributions. We propose a novel adaptation of the Kolmogorov-Arnold representation theorem for generative modeling and introduce the Kolmogorov-Arnold Energy Model (KAEM) to take advantage of structural and inductive biases. By constraining the prior to univariate relationships, KAEM enables fast and exact inference via the inverse transform method. With the low dimensionality of the latent space and suitable inductive biases encoded, we demonstrate that importance sampling (IS) becomes a viable, unbiased, and highly efficient posterior sampler. For domains where IS fails, we introduce a strategy based on population-based LMC, decomposing the posterior into a sequence of annealed distributions to improve LMC mixing. KAEM balances common generative modeling trade-offs, offering fast inference, interpretability, and stable training, while being naturally suited to Zettascale Computing hardware.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Kolmogorov-Arnold能量模型：快速且可解释的生成建模</div>
<div class="mono" style="margin-top:8px">在自上而下的生成模型的潜在空间中学习能量模型（EBM）提供了一种强大的框架，用于跨多种数据模态的生成。然而，其可解释性如何用于指导模型设计、提升生成质量并减少训练时间仍不清楚。此外，对朗之万蒙特卡洛（LMC）采样的依赖在效率和多模态潜在分布采样方面带来了挑战。我们提出了一种生成建模的Kolmogorov-Arnold表示定理的新适应方法，并引入了Kolmogorov-Arnold能量模型（KAEM），以利用结构和归纳偏置。通过将先验约束为单变量关系，KAEM能够通过反变换方法实现快速且精确的推理。在潜在空间低维且合适的归纳偏置被编码的情况下，我们证明了重要性采样（IS）成为一种可行、无偏且高效的后验采样方法。对于重要性采样失效的领域，我们引入了一种基于群体的LMC策略，将后验分解为一系列退火分布以提升LMC的混合效率。KAEM在常见的生成建模权衡中取得平衡，提供快速推理、可解释性和稳定训练，同时天然适合Zettascale计算硬件。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Learning an energy-based model (EBM) in the latent space of a top-down generative model offers a powerful framework for generation across many data modalities.</div>
</details>
</div>
<div class="card">
<div class="title">PET-TURTLE: Deep Unsupervised Support Vector Machines for Imbalanced Data Clusters</div>
<div class="meta-line">Authors: Javier Salazar Cavazos</div>
<div class="meta-line">Venue: IEEE Signal Processing Letters, vol. 33, pp. 91-95, 2026</div>
<div class="meta-line">First: 2026-01-06T18:30:25+00:00 · Latest: 2026-01-06T18:30:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03237v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03237v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Foundation vision, audio, and language models enable zero-shot performance on downstream tasks via their latent representations. Recently, unsupervised learning of data group structure with deep learning methods has gained popularity. TURTLE, a state of the art deep clustering algorithm, uncovers data labeling without supervision by alternating label and hyperplane updates, maximizing the hyperplane margin, in a similar fashion to support vector machines (SVMs). However, TURTLE assumes clusters are balanced; when data is imbalanced, it yields non-ideal hyperplanes that cause higher clustering error. We propose PET-TURTLE, which generalizes the cost function to handle imbalanced data distributions by a power law prior. Additionally, by introducing sparse logits in the labeling process, PET-TURTLE optimizes a simpler search space that in turn improves accuracy for balanced datasets. Experiments on synthetic and real data show that PET-TURTLE improves accuracy for imbalanced sources, prevents over-prediction of minority clusters, and enhances overall clustering.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PET-TURTLE：用于不平衡数据聚类的深度无监督支持向量机</div>
<div class="mono" style="margin-top:8px">基础视觉、音频和语言模型通过其潜在表示实现下游任务的零样本性能。最近，利用深度学习方法进行数据组结构的无监督学习变得流行。TURTLE 是一种最先进的深度聚类算法，通过交替标签和超平面更新，在类似于支持向量机（SVMs）的方式下揭示数据标签。然而，TURTLE 假设聚类是平衡的；当数据不平衡时，它会产生非理想的超平面，导致更高的聚类误差。我们提出 PET-TURTLE，通过幂律先验将成本函数推广以处理不平衡数据分布。此外，通过在标签过程中引入稀疏 logit，PET-TURTLE 优化了一个更简单的搜索空间，从而提高了平衡数据集的准确性。在合成数据和真实数据上的实验表明，PET-TURTLE 提高了不平衡数据源的准确性，防止了少数聚类的过度预测，并增强了整体聚类效果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Foundation vision, audio, and language models enable zero-shot performance on downstream tasks via their latent representations.</div>
</details>
</div>
<div class="card">
<div class="title">MAGMA: A Multi-Graph based Agentic Memory Architecture for AI Agents</div>
<div class="meta-line">Authors: Dongming Jiang, Yi Li, Guanpeng Li, Bingzhe Li</div>
<div class="meta-line">First: 2026-01-06T18:29:43+00:00 · Latest: 2026-01-06T18:29:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03236v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03236v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Memory-Augmented Generation (MAG) extends Large Language Models with external memory to support long-context reasoning, but existing approaches largely rely on semantic similarity over monolithic memory stores, entangling temporal, causal, and entity information. This design limits interpretability and alignment between query intent and retrieved evidence, leading to suboptimal reasoning accuracy. In this paper, we propose MAGMA, a multi-graph agentic memory architecture that represents each memory item across orthogonal semantic, temporal, causal, and entity graphs. MAGMA formulates retrieval as policy-guided traversal over these relational views, enabling query-adaptive selection and structured context construction. By decoupling memory representation from retrieval logic, MAGMA provides transparent reasoning paths and fine-grained control over retrieval. Experiments on LoCoMo and LongMemEval demonstrate that MAGMA consistently outperforms state-of-the-art agentic memory systems in long-horizon reasoning tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MAGMA：一种基于多图的智能体记忆架构</div>
<div class="mono" style="margin-top:8px">记忆增强生成（MAG）通过引入外部记忆扩展大型语言模型，以支持长上下文推理，但现有方法主要依赖于单一记忆存储上的语义相似性，将时间、因果和实体信息纠缠在一起。这种设计限制了查询意图与检索证据之间的可解释性和对齐，导致推理准确性不足。本文提出MAGMA，一种基于多图的智能体记忆架构，通过正交的语义、时间、因果和实体图来表示每个记忆项。MAGMA将检索过程建模为在这些关系视图上的策略引导遍历，从而实现查询自适应选择和结构化上下文构建。通过将记忆表示与检索逻辑解耦，MAGMA提供了透明的推理路径和对检索的细粒度控制。在LoCoMo和LongMemEval上的实验表明，MAGMA在长视野推理任务中始终优于最先进的智能体记忆系统。</div>
</details>
</div>
<div class="card">
<div class="title">Shallow-circuit Supervised Learning on a Quantum Processor</div>
<div class="meta-line">Authors: Luca Candelori, Swarnadeep Majumder, Antonio Mezzacapo, Javier Robledo Moreno, Kharen Musaelian, Santhanam Nagarajan, Sunil Pinnamaneni, Kunal Sharma, Dario Villani</div>
<div class="meta-line">First: 2026-01-06T18:26:53+00:00 · Latest: 2026-01-06T18:26:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03235v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03235v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quantum computing has long promised transformative advances in data analysis, yet practical quantum machine learning has remained elusive due to fundamental obstacles such as a steep quantum cost for the loading of classical data and poor trainability of many quantum machine learning algorithms designed for near-term quantum hardware. In this work, we show that one can overcome these obstacles by using a linear Hamiltonian-based machine learning method which provides a compact quantum representation of classical data via ground state problems for k-local Hamiltonians. We use the recent sample-based Krylov quantum diagonalization method to compute low-energy states of the data Hamiltonians, whose parameters are trained to express classical datasets through local gradients. We demonstrate the efficacy and scalability of the methods by performing experiments on benchmark datasets using up to 50 qubits of an IBM Heron quantum processor.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在量子处理器上的浅层电路监督学习</div>
<div class="mono" style="margin-top:8px">量子计算长期以来承诺在数据分析领域带来变革性的进步，但由于诸如经典数据加载的高量子成本和许多为近期量子硬件设计的量子机器学习算法训练性差等根本性障碍，实用的量子机器学习仍难以实现。在本工作中，我们展示了一种基于线性哈密顿量的机器学习方法，该方法通过k局部哈密顿量的基态问题，为经典数据提供紧凑的量子表示。我们使用最近的基于抽样方法的Krylov量子对角化方法来计算数据哈密顿量的低能态，其参数通过局部梯度进行训练以表达经典数据集。我们通过在IBM Heron量子处理器上使用多达50个量子比特对基准数据集进行实验，展示了该方法的有效性和可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">LTX-2: Efficient Joint Audio-Visual Foundation Model</div>
<div class="meta-line">Authors: Yoav HaCohen, Benny Brazowski, Nisan Chiprut, Yaki Bitterman, Andrew Kvochko, Avishai Berkowitz, Daniel Shalem, Daphna Lifschitz, Dudu Moshe, Eitan Porat, Eitan Richardson, Guy Shiran, Itay Chachy, Jonathan Chetboun, Michael Finkelson, Michael Kupchick, Nir Zabari, Nitzan Guetta, Noa Kotler, Ofir Bibi, Ori Gordon, Poriya Panet, Roi Benita, Shahar Armon, Victor Kulikov, Yaron Inger, Yonatan Shiftan, Zeev Melumian, Zeev Farbman</div>
<div class="meta-line">First: 2026-01-06T18:24:41+00:00 · Latest: 2026-01-06T18:24:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03233v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03233v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent text-to-video diffusion models can generate compelling video sequences, yet they remain silent -- missing the semantic, emotional, and atmospheric cues that audio provides. We introduce LTX-2, an open-source foundational model capable of generating high-quality, temporally synchronized audiovisual content in a unified manner. LTX-2 consists of an asymmetric dual-stream transformer with a 14B-parameter video stream and a 5B-parameter audio stream, coupled through bidirectional audio-video cross-attention layers with temporal positional embeddings and cross-modality AdaLN for shared timestep conditioning. This architecture enables efficient training and inference of a unified audiovisual model while allocating more capacity for video generation than audio generation. We employ a multilingual text encoder for broader prompt understanding and introduce a modality-aware classifier-free guidance (modality-CFG) mechanism for improved audiovisual alignment and controllability. Beyond generating speech, LTX-2 produces rich, coherent audio tracks that follow the characters, environment, style, and emotion of each scene -- complete with natural background and foley elements. In our evaluations, the model achieves state-of-the-art audiovisual quality and prompt adherence among open-source systems, while delivering results comparable to proprietary models at a fraction of their computational cost and inference time. All model weights and code are publicly released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LTX-2：高效的音频-视觉联合基础模型</div>
<div class="mono" style="margin-top:8px">近期的文本到视频扩散模型可以生成引人入胜的视频序列，但它们仍然无声——缺少音频所提供的语义、情感和氛围线索。我们引入了LTX-2，这是一个开源的基础模型，能够以统一的方式生成高质量且时间同步的音频视觉内容。LTX-2由一个非对称的双流Transformer组成，包含一个140亿参数的视频流和一个50亿参数的音频流，通过双向的音频-视频交叉注意力层进行耦合，这些层结合了时间位置嵌入和跨模态AdaLN机制，用于共享时间步的条件。这种架构使得统一的音频视觉模型能够高效地进行训练和推理，同时为视频生成分配了更多的容量。我们采用多语言文本编码器以实现更广泛的提示理解，并引入了模态感知的无分类器引导（modality-CFG）机制，以提升音频视觉对齐和可控性。除了生成语音，LTX-2还能生成丰富且连贯的音频轨道，这些音频轨道遵循每个场景中的角色、环境、风格和情感，包括自然的背景和音效元素。在我们的评估中，该模型在开源系统中实现了最先进的音频视觉质量和提示遵循能力，同时以远低于专有模型的计算成本和推理时间，提供了与之相当的结果。所有模型权重和代码均已公开发布。</div>
</details>
</div>
<div class="card">
<div class="title">AgentArch: A Comprehensive Benchmark to Evaluate Agent Architectures in Enterprise</div>
<div class="meta-line">Authors: Tara Bogavelli, Roshnee Sharma, Hari Subramani</div>
<div class="meta-line">First: 2025-09-13T01:18:23+00:00 · Latest: 2026-01-06T18:18:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.10769v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.10769v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While individual components of agentic architectures have been studied in isolation, there remains limited empirical understanding of how different design dimensions interact within complex multi-agent systems. This study aims to address these gaps by providing a comprehensive enterprise-specific benchmark evaluating 18 distinct agentic configurations across state-of-the-art large language models. We examine four critical agentic system dimensions: orchestration strategy, agent prompt implementation (ReAct versus function calling), memory architecture, and thinking tool integration. Our benchmark reveals significant model-specific architectural preferences that challenge the prevalent one-size-fits-all paradigm in agentic AI systems. It also reveals significant weaknesses in overall agentic performance on enterprise tasks with the highest scoring models achieving a maximum of only 35.3\% success on the more complex task and 70.8\% on the simpler task. We hope these findings inform the design of future agentic systems by enabling more empirically backed decisions regarding architectural components and model selection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AgentArch：评估企业环境中代理架构的全面基准</div>
<div class="mono" style="margin-top:8px">尽管代理架构的各个组件已被单独研究，但对复杂多代理系统中不同设计维度如何相互作用的实证理解仍有限。本研究通过提供一个全面的企业特定基准，旨在填补这一空白，评估当前最先进的大型语言模型中18种不同的代理配置。我们考察了四个关键的代理系统维度：编排策略、代理提示实现（ReAct 与函数调用）、记忆架构以及思维工具集成。我们的基准测试揭示了显著的模型特定架构偏好，这挑战了代理AI系统中普遍存在的统一架构范式。同时，也发现了代理在企业任务中整体表现的显著弱点，最高得分模型在更复杂的任务中仅达到35.3\%的成功率，在较简单的任务中则达到70.8\%。我们希望这些发现能为未来代理系统的构建提供指导，使在架构组件和模型选择上能做出更具实证依据的决策。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-RADS Synthetic Radiology Report Dataset and Head-to-Head Benchmarking of 41 Open-Weight and Proprietary Language Models</div>
<div class="meta-line">Authors: Kartik Bose, Abhinandan Kumar, Raghuraman Soundararajan, Priya Mudgil, Samonee Ralmilay, Niharika Dutta, Manphool Singhal, Arun Kumar, Saugata Sen, Anurima Patra, Priya Ghosh, Abanti Das, Amit Gupta, Ashish Verma, Dipin Sudhakaran, Ekta Dhamija, Himangi Unde, Ishan Kumar, Krithika Rangarajan, Prerna Garg, Rachel Sequeira, Sudhin Shylendran, Taruna Yadav, Tej Pal, Pankaj Gupta</div>
<div class="meta-line">First: 2026-01-06T18:18:44+00:00 · Latest: 2026-01-06T18:18:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03232v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03232v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Background: Reporting and Data Systems (RADS) standardize radiology risk communication but automated RADS assignment from narrative reports is challenging because of guideline complexity, output-format constraints, and limited benchmarking across RADS frameworks and model sizes. Purpose: To create RXL-RADSet, a radiologist-verified synthetic multi-RADS benchmark, and compare validity and accuracy of open-weight small language models (SLMs) with a proprietary model for RADS assignment. Materials and Methods: RXL-RADSet contains 1,600 synthetic radiology reports across 10 RADS (BI-RADS, CAD-RADS, GB-RADS, LI-RADS, Lung-RADS, NI-RADS, O-RADS, PI-RADS, TI-RADS, VI-RADS) and multiple modalities. Reports were generated by LLMs using scenario plans and simulated radiologist styles and underwent two-stage radiologist verification. We evaluated 41 quantized SLMs (12 families, 0.135-32B parameters) and GPT-5.2 under a fixed guided prompt. Primary endpoints were validity and accuracy; a secondary analysis compared guided versus zero-shot prompting. Results: Under guided prompting GPT-5.2 achieved 99.8% validity and 81.1% accuracy (1,600 predictions). Pooled SLMs (65,600 predictions) achieved 96.8% validity and 61.1% accuracy; top SLMs in the 20-32B range reached ~99% validity and mid-to-high 70% accuracy. Performance scaled with model size (inflection between &lt;1B and &gt;=10B) and declined with RADS complexity primarily due to classification difficulty rather than invalid outputs. Guided prompting improved validity (99.2% vs 96.7%) and accuracy (78.5% vs 69.6%) compared with zero-shot. Conclusion: RXL-RADSet provides a radiologist-verified multi-RADS benchmark; large SLMs (20-32B) can approach proprietary-model performance under guided prompting, but gaps remain for higher-complexity schemes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多RADS合成放射学报告数据集及41个开源和专有语言模型的头对头基准测试</div>
<div class="mono" style="margin-top:8px">背景：报告和数据系统（RADS）标准化了放射学风险沟通，但从叙述性报告中自动分配RADS具有挑战性，因为指南复杂性、输出格式限制以及RADS框架和模型规模之间的有限基准测试。目的：创建RXL-RADSet，一个放射科医生验证的多RADS基准数据集，并在固定引导提示下比较开源小型语言模型（SLMs）与专有模型在RADS分配中的有效性与准确性。材料与方法：RXL-RADSet包含1,600份合成放射学报告，涵盖10种RADS（BI-RADS、CAD-RADS、GB-RADS、LI-RADS、Lung-RADS、NI-RADS、O-RADS、PI-RADS、TI-RADS、VI-RADS）和多种模态。报告由LLMs根据场景计划和模拟放射科医生风格生成，并经过两阶段放射科医生验证。我们在固定引导提示下评估了41个量化SLMs（12个家族，0.135-32B参数）和GPT-5.2。主要终点为有效性和准确性；次要分析比较了引导提示与零样本提示。结果：在引导提示下，GPT-5.2实现了99.8%的有效性和81.1%的准确性（1,600个预测）。聚合的SLMs（65,600个预测）实现了96.8%的有效性和61.1%的准确性；20-32B范围内的顶级SLMs达到约99%的有效性和中高70%的准确性。性能随着模型规模增加而提升（在&lt;1B和&gt;=10B之间出现拐点），并随着RADS复杂性增加而下降，主要由于分类难度而非无效输出。与零样本提示相比，引导提示提高了有效性（99.2% vs 96.7%）和准确性（78.5% vs 69.6%）。结论：RXL-RADSet提供了一个放射科医生验证的多RADS基准；在引导提示下，大型SLMs（20-32B）可接近专有模型的性能，但高复杂度方案仍存在差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Background: Reporting and Data Systems (RADS) standardize radiology risk communication but automated RADS assignment from narrative reports is challenging because of guideline complexity, output-format constraints, and limited benchmarking across RADS frameworks and model sizes.</div>
</details>
</div>
<div class="card">
<div class="title">SpANNS: Optimizing Approximate Nearest Neighbor Search for Sparse Vectors Using Near Memory Processing</div>
<div class="meta-line">Authors: Tianqi Zhang, Flavio Ponzina, Tajana Rosing</div>
<div class="meta-line">First: 2026-01-06T18:15:53+00:00 · Latest: 2026-01-06T18:15:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03229v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03229v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Approximate Nearest Neighbor Search (ANNS) is a fundamental operation in vector databases, enabling efficient similarity search in high-dimensional spaces. While dense ANNS has been optimized using specialized hardware accelerators, sparse ANNS remains limited by CPU-based implementations, hindering scalability. This limitation is increasingly critical as hybrid retrieval systems, combining sparse and dense embeddings, become standard in Information Retrieval (IR) pipelines. We propose SpANNS, a near-memory processing architecture for sparse ANNS. SpANNS combines a hybrid inverted index with efficient query management and runtime optimizations. The architecture is built on a CXL Type-2 near-memory platform, where a specialized controller manages query parsing and cluster filtering, while compute-enabled DIMMs perform index traversal and distance computations close to the data. It achieves 15.2x to 21.6x faster execution over the state-of-the-art CPU baselines, offering scalable and efficient solutions for sparse vector search.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpANNS：利用近内存处理优化稀疏向量的近似最近邻搜索</div>
<div class="mono" style="margin-top:8px">近似最近邻搜索（ANNS）是向量数据库中的基本操作，能够在高维空间中实现高效的相似性搜索。尽管密集型ANNS已通过专用硬件加速器进行了优化，但稀疏型ANNS仍受限于基于CPU的实现，影响了其可扩展性。随着混合检索系统在信息检索（IR）流程中成为标准，这一限制变得愈发关键。我们提出SpANNS，一种用于稀疏ANNS的近内存处理架构。SpANNS结合了混合倒排索引、高效的查询管理和运行时优化。该架构基于CXL Type-2近内存平台，其中专用控制器负责查询解析和聚类过滤，而计算增强型DIMM则在数据附近执行索引遍历和距离计算。与最先进的CPU基线相比，SpANNS实现了15.2到21.6倍的执行速度提升，为稀疏向量搜索提供了可扩展且高效的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">The Sonar Moment: Benchmarking Audio-Language Models in Audio Geo-Localization</div>
<div class="meta-line">Authors: Ruixing Zhang, Zihan Liu, Leilei Sun, Tongyu Zhu, Weifeng Lv</div>
<div class="meta-line">First: 2026-01-06T18:13:24+00:00 · Latest: 2026-01-06T18:13:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03227v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03227v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Geo-localization aims to infer the geographic origin of a given signal. In computer vision, geo-localization has served as a demanding benchmark for compositional reasoning and is relevant to public safety. In contrast, progress on audio geo-localization has been constrained by the lack of high-quality audio-location pairs. To address this gap, we introduce AGL1K, the first audio geo-localization benchmark for audio language models (ALMs), spanning 72 countries and territories. To extract reliably localizable samples from a crowd-sourced platform, we propose the Audio Localizability metric that quantifies the informativeness of each recording, yielding 1,444 curated audio clips. Evaluations on 16 ALMs show that ALMs have emerged with audio geo-localization capability. We find that closed-source models substantially outperform open-source models, and that linguistic clues often dominate as a scaffold for prediction. We further analyze ALMs&#x27; reasoning traces, regional bias, error causes, and the interpretability of the localizability metric. Overall, AGL1K establishes a benchmark for audio geo-localization and may advance ALMs with better geospatial reasoning capability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>声纳时刻：音频语言模型在音频地理定位中的基准测试</div>
<div class="mono" style="margin-top:8px">地理定位旨在推断给定信号的地理来源。在计算机视觉领域，地理定位作为组合推理的严苛基准，与公共安全相关。相比之下，音频地理定位的进展受到高质量音频-位置配对数据的缺乏限制。为解决这一问题，我们引入了AGL1K，这是首个针对音频语言模型（ALMs）的音频地理定位基准，涵盖72个国家和地区。为从众包平台中可靠地提取可定位样本，我们提出了音频可定位性度量，量化每段录音的信息量，从而获得1,444个精选的音频片段。在16个ALMs上的评估表明，ALMs已具备音频地理定位能力。我们发现，闭源模型显著优于开源模型，且语言线索通常作为预测的主导框架。我们进一步分析了ALMs的推理轨迹、区域偏差、错误原因以及可定位性度量的可解释性。总体而言，AGL1K为音频地理定位建立了基准，并可能推动具备更强地理空间推理能力的ALMs发展。</div>
</details>
</div>
<div class="card">
<div class="title">The Fake Friend Dilemma: Trust and the Political Economy of Conversational AI</div>
<div class="meta-line">Authors: Jacob Erickson</div>
<div class="meta-line">First: 2026-01-06T18:07:52+00:00 · Latest: 2026-01-06T18:07:52+00:00</div>
<div class="meta-line">Comments: Manuscript under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03222v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03222v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As conversational AI systems become increasingly integrated into everyday life, they raise pressing concerns about user autonomy, trust, and the commercial interests that influence their behavior. To address these concerns, this paper develops the Fake Friend Dilemma (FFD), a sociotechnical condition in which users place trust in AI agents that appear supportive while pursuing goals that are misaligned with the user&#x27;s own. The FFD provides a critical framework for examining how anthropomorphic AI systems facilitate subtle forms of manipulation and exploitation. Drawing on literature in trust, AI alignment, and surveillance capitalism, we construct a typology of harms, including covert advertising, political propaganda, behavioral nudging, and surveillance. We then assess possible mitigation strategies, including both structural and technical interventions. By focusing on trust as a vector of asymmetrical power, the FFD offers a lens for understanding how AI systems may undermine user autonomy while maintaining the appearance of helpfulness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>虚假朋友困境：信任与对话式人工智能的政治经济学</div>
<div class="mono" style="margin-top:8px">随着对话式人工智能系统日益融入日常生活，它们引发了关于用户自主性、信任以及影响其行为的商业利益的紧迫问题。为应对这些问题，本文提出了虚假朋友困境（FFD），这是一种社会技术情境，用户信任那些看似支持但目标与自身不一致的人工智能代理。FFD提供了一个批判性框架，用于考察拟人化的人工智能系统如何促进微妙形式的操控和剥削。本文借鉴了关于信任、人工智能对齐和监控资本主义的文献，构建了一个危害类型学，包括隐蔽广告、政治宣传、行为引导和监控。随后，我们评估了可能的缓解策略，包括结构性和技术性干预。通过将信任视为不对称权力的载体，FFD为理解人工智能系统如何在保持帮助性表象的同时削弱用户自主性提供了新的视角。</div>
</details>
</div>
<div class="card">
<div class="title">From Entropy to Epiplexity: Rethinking Information for Computationally Bounded Intelligence</div>
<div class="meta-line">Authors: Marc Finzi, Shikai Qiu, Yiding Jiang, Pavel Izmailov, J. Zico Kolter, Andrew Gordon Wilson</div>
<div class="meta-line">First: 2026-01-06T18:04:03+00:00 · Latest: 2026-01-06T18:04:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03220v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03220v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Can we learn more from data than existed in the generating process itself? Can new and useful information be constructed from merely applying deterministic transformations to existing data? Can the learnable content in data be evaluated without considering a downstream task? On these questions, Shannon information and Kolmogorov complexity come up nearly empty-handed, in part because they assume observers with unlimited computational capacity and fail to target the useful information content. In this work, we identify and exemplify three seeming paradoxes in information theory: (1) information cannot be increased by deterministic transformations; (2) information is independent of the order of data; (3) likelihood modeling is merely distribution matching. To shed light on the tension between these results and modern practice, and to quantify the value of data, we introduce epiplexity, a formalization of information capturing what computationally bounded observers can learn from data. Epiplexity captures the structural content in data while excluding time-bounded entropy, the random unpredictable content exemplified by pseudorandom number generators and chaotic dynamical systems. With these concepts, we demonstrate how information can be created with computation, how it depends on the ordering of the data, and how likelihood modeling can produce more complex programs than present in the data generating process itself. We also present practical procedures to estimate epiplexity which we show capture differences across data sources, track with downstream performance, and highlight dataset interventions that improve out-of-distribution generalization. In contrast to principles of model selection, epiplexity provides a theoretical foundation for data selection, guiding how to select, generate, or transform data for learning systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从熵到反熵：重新思考计算受限智能体的信息</div>
<div class="mono" style="margin-top:8px">我们能否从数据中学习到比生成过程本身更多的信息？仅通过确定性变换是否能从现有数据中构建出新的有用信息？数据中的可学习内容是否可以在不考虑下游任务的情况下进行评估？在这些问题上，香农信息和柯尔莫哥洛夫复杂度几乎无能为力，部分原因是它们假设观察者具有无限的计算能力，并未能聚焦于有用的信息内容。在本工作中，我们识别并举例说明了信息论中的三个看似矛盾的现象：(1) 确定性变换无法增加信息；(2) 信息与数据顺序无关；(3) 概率建模仅仅是分布匹配。为了阐明这些结果与现代实践之间的张力，并量化数据的价值，我们引入了反熵（epiplexity），这是一种形式化的信息概念，捕捉计算受限观察者可以从数据中学习到的内容。反熵捕捉数据中的结构信息，同时排除了时间受限熵（如伪随机数生成器和混沌动力系统所体现的随机不可预测内容）。借助这些概念，我们展示了如何通过计算创建信息、信息如何依赖于数据的顺序，以及概率建模如何能够生成比数据生成过程本身更复杂的程序。我们还提出了估计反熵的实用方法，这些方法能够捕捉不同数据源之间的差异，与下游性能保持一致，并突出那些能提升分布外泛化能力的数据集干预措施。与模型选择原则不同，反熵为数据选择提供了理论基础，指导如何为学习系统选择、生成或转换数据。</div>
</details>
</div>
<div class="card">
<div class="title">MalruleLib: Large-Scale Executable Misconception Reasoning with Step Traces for Modeling Student Thinking in Mathematics</div>
<div class="meta-line">Authors: Xinghe Chen, Naiming Liu, Shashank Sonkar</div>
<div class="meta-line">First: 2026-01-06T17:59:37+00:00 · Latest: 2026-01-06T17:59:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03217v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03217v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Student mistakes in mathematics are often systematic: a learner applies a coherent but wrong procedure and repeats it across contexts. We introduce MalruleLib, a learning-science-grounded framework that translates documented misconceptions into executable procedures, drawing on 67 learning-science and mathematics education sources, and generates step-by-step traces of malrule-consistent student work. We formalize a core student-modeling problem as Malrule Reasoning Accuracy (MRA): infer a misconception from one worked mistake and predict the student&#x27;s next answer under cross-template rephrasing. Across nine language models (4B-120B), accuracy drops from 66% on direct problem solving to 40% on cross-template misconception prediction. MalruleLib encodes 101 malrules over 498 parameterized problem templates and produces paired dual-path traces for both correct reasoning and malrule-consistent student reasoning. Because malrules are executable and templates are parameterizable, MalruleLib can generate over one million instances, enabling scalable supervision and controlled evaluation. Using MalruleLib, we observe cross-template degradations of 10-21%, while providing student step traces improves prediction by 3-15%. We release MalruleLib as infrastructure for educational AI that models student procedures across contexts, enabling diagnosis and feedback that targets the underlying misconception.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MalruleLib：基于步骤追踪的大规模可执行误解推理框架，用于建模学生数学思维</div>
<div class="mono" style="margin-top:8px">数学学习中的学生错误往往是系统性的：学习者应用一个连贯但错误的程序，并在不同情境中重复使用。我们引入了MalruleLib，这是一个基于学习科学的框架，将记录的误解转化为可执行的程序，参考了67个学习科学和数学教育来源，并生成与误解一致的学生步骤追踪。我们将一个核心的学生建模问题形式化为误解推理准确率（MRA）：从一个错误解答中推断出误解，并在跨模板重述的情况下预测学生的下一个答案。在九个语言模型（4B-120B）中，直接问题解决的准确率为66%，而跨模板误解预测的准确率降至40%。MalruleLib编码了101个误解规则，覆盖498个参数化问题模板，并为正确推理和误解一致的学生推理生成配对的双路径追踪。由于误解规则是可执行的，模板是可参数化的，MalruleLib可以生成超过一百万个实例，从而实现可扩展的监督和受控评估。使用MalruleLib，我们观察到跨模板的性能下降为10-21%，而提供学生步骤追踪可将预测准确率提高3-15%。我们发布MalruleLib作为建模学生跨情境解题过程的教育AI基础设施，使诊断和反馈能够针对潜在的误解进行优化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Student mistakes in mathematics are often systematic: a learner applies a coherent but wrong procedure and repeats it across contexts.</div>
</details>
</div>
<div class="card">
<div class="title">Adapting Web Agents with Synthetic Supervision</div>
<div class="meta-line">Authors: Zhaoyang Wang, Yiming Liang, Xuchao Zhang, Qianhui Wu, Siwei Han, Anson Bastos, Rujia Wang, Chetan Bansal, Baolin Peng, Jianfeng Gao, Saravan Rajmohan, Huaxiu Yao</div>
<div class="meta-line">First: 2025-11-08T18:45:33+00:00 · Latest: 2026-01-06T17:55:17+00:00</div>
<div class="meta-line">Comments: 21 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.06101v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.06101v2">PDF</a> · <a href="https://github.com/aiming-lab/SynthAgent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Web agents struggle to adapt to new websites due to the scarcity of environment specific tasks and demonstrations. Recent works have explored synthetic data generation to address this challenge, however, they suffer from data quality issues where synthesized tasks contain hallucinations that cannot be executed, and collected trajectories are noisy with redundant or misaligned actions. In this paper, we propose SynthAgent, a fully synthetic supervision framework that aims at improving synthetic data quality via dual refinement of both tasks and trajectories. Our approach begins by synthesizing diverse tasks through categorized exploration of web elements, ensuring efficient coverage of the target environment. During trajectory collection, tasks are refined only when conflicts with observations are detected, which mitigates hallucinations while preserving task consistency. After collection, we conduct trajectory refinement with global context to mitigate potential noise or misalignments. Finally, we fine-tune open-source web agents on the refined synthetic data to adapt them to the target environment. Experimental results demonstrate that SynthAgent outperforms existing synthetic data methods, validating the importance of high-quality synthetic supervision. The code is publicly available at https://github.com/aiming-lab/SynthAgent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用合成监督的网页代理适应</div>
<div class="mono" style="margin-top:8px">网页代理由于缺乏特定环境的任务和演示而难以适应新网站。最近的研究探索了合成数据生成以解决这一挑战，但这些方法存在数据质量问题，合成任务中包含无法执行的幻觉，且收集的轨迹存在噪声，包含冗余或对齐错误的操作。本文提出SynthAgent，一个完全基于合成监督的框架，旨在通过任务和轨迹的双重优化来提升合成数据的质量。我们的方法首先通过分类探索网页元素来合成多样化任务，确保高效覆盖目标环境。在轨迹收集过程中，仅在任务与观察结果冲突时进行任务优化，从而减少幻觉并保持任务一致性。收集完成后，我们利用全局上下文进行轨迹优化，以缓解潜在的噪声或对齐问题。最后，我们在优化后的合成数据上微调开源网页代理，以适应目标环境。实验结果表明，SynthAgent优于现有的合成数据方法，验证了高质量合成监督的重要性。代码可在https://github.com/aiming-lab/SynthAgent上公开获取。</div>
</details>
</div>
<div class="card">
<div class="title">Critic-Guided Reinforcement Unlearning in Text-to-Image Diffusion</div>
<div class="meta-line">Authors: Mykola Vysotskyi, Zahar Kohut, Mariia Shpir, Taras Rumezhak, Volodymyr Karpiv</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-06T17:52:02+00:00 · Latest: 2026-01-06T17:52:02+00:00</div>
<div class="meta-line">Comments: Preprint. Under review at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03213v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03213v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine unlearning in text-to-image diffusion models aims to remove targeted concepts while preserving overall utility. Prior diffusion unlearning methods typically rely on supervised weight edits or global penalties; reinforcement-learning (RL) approaches, while flexible, often optimize sparse end-of-trajectory rewards, yielding high-variance updates and weak credit assignment. We present a general RL framework for diffusion unlearning that treats denoising as a sequential decision process and introduces a timestep-aware critic with noisy-step rewards. Concretely, we train a CLIP-based reward predictor on noisy latents and use its per-step signal to compute advantage estimates for policy-gradient updates of the reverse diffusion kernel. Our algorithm is simple to implement, supports off-policy reuse, and plugs into standard text-to-image backbones. Across multiple concepts, the method achieves better or comparable forgetting to strong baselines while maintaining image quality and benign prompt fidelity; ablations show that (i) per-step critics and (ii) noisy-conditioned rewards are key to stability and effectiveness. We release code and evaluation scripts to facilitate reproducibility and future research on RL-based diffusion unlearning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评论引导的扩散模型文本到图像反学习</div>
<div class="mono" style="margin-top:8px">文本到图像扩散模型的机器反学习旨在移除特定概念，同时保持整体功能。先前的扩散反学习方法通常依赖于监督权重编辑或全局惩罚；而强化学习（RL）方法虽然灵活，但往往优化稀疏的轨迹结束奖励，导致高方差更新和弱的信用分配。我们提出了一种通用的RL框架用于扩散反学习，将去噪视为一个序列决策过程，并引入了时间步感知的评论器和噪声步奖励。具体而言，我们在噪声潜变量上训练基于CLIP的奖励预测器，并利用其每一步的信号计算策略梯度更新的优势估计。我们的算法易于实现，支持离策略重用，并可集成到标准的文本到图像主干模型中。在多个概念上，该方法在保持图像质量和良性提示保真度的同时，实现了比强基线更好的或相当的遗忘效果；消融实验表明，(i) 每一步的评论器和 (ii) 噪声条件奖励是稳定性和有效性的关键。</div>
</details>
</div>
<div class="card">
<div class="title">LVLM-Aware Multimodal Retrieval for RAG-Based Medical Diagnosis with General-Purpose Models</div>
<div class="meta-line">Authors: Nir Mazor, Tom Hope</div>
<div class="meta-line">First: 2025-08-24T15:06:20+00:00 · Latest: 2026-01-06T17:49:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.17394v4">Abs</a> · <a href="https://arxiv.org/pdf/2508.17394v4">PDF</a> · <a href="https://github.com/Nirmaz/JOMED">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieving visual and textual information from medical literature and hospital records can enhance diagnostic accuracy for clinical image interpretation. However, multimodal retrieval-augmented diagnosis is highly challenging. We explore a lightweight mechanism for enhancing diagnostic performance of retrieval-augmented LVLMs. We train a lightweight LVLM-aware multimodal retriever, such that the retriever learns to return images and texts that guide the LVLM toward correct predictions. In our low-resource setting, we perform only lightweight fine-tuning with small amounts of data, and use only general-purpose backbone models, achieving competitive results in clinical classification and VQA tasks compared to medically pre-trained models with extensive training. In a novel analysis, we highlight a previously unexplored class of errors that we term inconsistent retrieval predictions: cases where different top-retrieved images yield different predictions for the same target. We find that these cases are challenging for all models, even for non-retrieval models, and that our retrieval optimization mechanism significantly improves these cases over standard RAG. However, our analysis also sheds light on gaps in the ability of LVLMs to utilize retrieved information for clinical predictions. Code and models available at: https://github.com/Nirmaz/JOMED.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于通用模型的RAG医疗诊断中的LVLM感知多模态检索</div>
<div class="mono" style="margin-top:8px">从医学文献和医院记录中检索视觉和文本信息可以提高临床图像解释的诊断准确性。然而，多模态检索增强的诊断极具挑战性。我们探索了一种轻量机制，以提升检索增强型LVLM的诊断性能。我们训练了一个轻量级的LVLM感知多模态检索器，使得检索器能够学习返回引导LVLM做出正确预测的图像和文本。在我们的低资源设置中，我们仅使用少量数据进行轻量级微调，并采用通用的主干模型，其在临床分类和VQA任务中的表现与需要大量训练的医学预训练模型相当。在一项新颖的分析中，我们突出了之前未被探索的一类错误，我们称之为不一致的检索预测：对于同一目标，不同的Top检索图像会导致不同的预测结果。我们发现这些情况对所有模型都具有挑战性，即使是非检索模型也是如此，而我们的检索优化机制在标准RAG之上显著改善了这些情况。然而，我们的分析也揭示了LVLM在利用检索信息进行临床预测方面的能力差距。代码和模型可在：https://github.com/Nirmaz/JOMED 获取。</div>
</details>
</div>
<div class="card">
<div class="title">Fine-tuning Small Language Models as Efficient Enterprise Search Relevance Labelers</div>
<div class="meta-line">Authors: Yue Kang, Zhuoyi Huang, Benji Schussheim, Diana Licon, Dina Atia, Shixing Cao, Jacob Danovitch, Kunho Kim, Billy Norcilien, Jonah Karpman, Mahmound Sayed, Mike Taylor, Tao Sun, Pavel Metrikov, Vipul Agarwal, Chris Quirk, Ye-Yi Wang, Nick Craswell, Irene Shaffer, Tianwei Chen, Sulaiman Vesal, Soundar Srinivasan</div>
<div class="meta-line">First: 2026-01-06T17:48:40+00:00 · Latest: 2026-01-06T17:48:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03211v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03211v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In enterprise search, building high-quality datasets at scale remains a central challenge due to the difficulty of acquiring labeled data. To resolve this challenge, we propose an efficient approach to fine-tune small language models (SLMs) for accurate relevance labeling, enabling high-throughput, domain-specific labeling comparable or even better in quality to that of state-of-the-art large language models (LLMs). To overcome the lack of high-quality and accessible datasets in the enterprise domain, our method leverages on synthetic data generation. Specifically, we employ an LLM to synthesize realistic enterprise queries from a seed document, apply BM25 to retrieve hard negatives, and use a teacher LLM to assign relevance scores. The resulting dataset is then distilled into an SLM, producing a compact relevance labeler. We evaluate our approach on a high-quality benchmark consisting of 923 enterprise query-document pairs annotated by trained human annotators, and show that the distilled SLM achieves agreement with human judgments on par with or better than the teacher LLM. Furthermore, our fine-tuned labeler substantially improves throughput, achieving 17 times increase while also being 19 times more cost-effective. This approach enables scalable and cost-effective relevance labeling for enterprise-scale retrieval applications, supporting rapid offline evaluation and iteration in real-world settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>高效企业搜索相关性标注的小语言模型微调</div>
<div class="mono" style="margin-top:8px">在企业搜索中，由于获取标注数据的困难，大规模构建高质量数据集仍然是一个核心挑战。为了解决这一问题，我们提出了一种高效的方法，用于微调小语言模型（SLMs），以实现准确的相关性标注，从而支持高吞吐量、领域特定的标注，其质量可媲美甚至优于最先进的大型语言模型（LLMs）。为克服企业领域中高质量且可访问数据集的缺乏，我们的方法利用合成数据生成。具体而言，我们使用LLM从种子文档中合成现实的企业查询，应用BM25检索困难负样本，并使用教师LLM分配相关性评分。最终生成的数据集被蒸馏到SLM中，从而得到一个紧凑的相关性标注器。我们在由训练有素的人类标注员标注的923个企业查询-文档对高质量基准数据集上评估了我们的方法，并展示了蒸馏后的SLM在与人类判断的一致性上可达到或优于教师LLM。此外，我们的微调标注器显著提高了吞吐量，在成本上也降低了19倍，实现了17倍的性能提升。这种方法使企业级检索应用的相关性标注具备可扩展性和成本效益，支持现实场景中快速的离线评估和迭代。</div>
</details>
</div>
<div class="card">
<div class="title">UltraLogic: Enhancing LLM Reasoning through Large-Scale Data Synthesis and Bipolar Float Reward</div>
<div class="meta-line">Authors: Yile Liu, Yixian Liu, Zongwei Li, Yufei Huang, Xinhua Feng, Zhichao Hu, Jinglu Hu, Jianfeng Yan, Fengzong Lian, Yuhong Liu</div>
<div class="meta-line">First: 2026-01-06T17:41:32+00:00 · Latest: 2026-01-06T17:41:32+00:00</div>
<div class="meta-line">Comments: 19 pages, 6 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03205v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03205v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) have demonstrated significant potential in natural language processing , complex general-purpose reasoning requiring multi-step logic, planning, and verification remains a critical bottleneck. Although Reinforcement Learning with Verifiable Rewards (RLVR) has succeeded in specific domains , the field lacks large-scale, high-quality, and difficulty-calibrated data for general reasoning. To address this, we propose UltraLogic, a framework that decouples the logical core of a problem from its natural language expression through a Code-based Solving methodology to automate high-quality data production. The framework comprises hundreds of unique task types and an automated calibration pipeline across ten difficulty levels. Furthermore, to mitigate binary reward sparsity and the Non-negative Reward Trap, we introduce the Bipolar Float Reward (BFR) mechanism, utilizing graded penalties to effectively distinguish perfect responses from those with logical flaws. Our experiments demonstrate that task diversity is the primary driver for reasoning enhancement , and that BFR, combined with a difficulty matching strategy, significantly improves training efficiency, guiding models toward global logical optima.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UltraLogic：通过大规模数据合成与双极浮点奖励增强LLM推理</div>
<div class="mono" style="margin-top:8px">尽管大型语言模型（LLMs）在自然语言处理方面展现出巨大潜力，但需要多步骤逻辑、规划和验证的复杂通用推理仍然是一个关键瓶颈。虽然可验证奖励的强化学习（RLVR）在特定领域取得了成功，但该领域缺乏大规模、高质量且难度适配的通用推理数据。为了解决这一问题，我们提出了UltraLogic框架，通过基于代码的解决方法将问题的逻辑核心与其自然语言表达分离，从而自动化生成高质量数据。该框架包含数百种独特的任务类型，并在十个难度等级上设有自动校准流程。此外，为缓解二元奖励稀疏性和非负奖励陷阱，我们引入了双极浮点奖励（BFR）机制，利用分级惩罚有效区分完美回答与存在逻辑错误的回答。我们的实验表明，任务多样性是推理增强的主要驱动力，而BFR结合难度匹配策略显著提高了训练效率，引导模型向全局逻辑最优解发展。</div>
</details>
</div>
<div class="card">
<div class="title">InfiAgent: An Infinite-Horizon Framework for General-Purpose Autonomous Agents</div>
<div class="meta-line">Authors: Chenglin Yu, Yuchen Wang, Songmiao Wang, Hongxia Yang, Ming Li</div>
<div class="meta-line">First: 2026-01-06T17:35:57+00:00 · Latest: 2026-01-06T17:35:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03204v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.03204v1">PDF</a> · <a href="https://github.com/ChenglinPoly/infiAgent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM agents can reason and use tools, but they often break down on long-horizon tasks due to unbounded context growth and accumulated errors. Common remedies such as context compression or retrieval-augmented prompting introduce trade-offs between information fidelity and reasoning stability. We present InfiAgent, a general-purpose framework that keeps the agent&#x27;s reasoning context strictly bounded regardless of task duration by externalizing persistent state into a file-centric state abstraction. At each step, the agent reconstructs context from a workspace state snapshot plus a fixed window of recent actions. Experiments on DeepResearch and an 80-paper literature review task show that, without task-specific fine-tuning, InfiAgent with a 20B open-source model is competitive with larger proprietary systems and maintains substantially higher long-horizon coverage than context-centric baselines. These results support explicit state externalization as a practical foundation for stable long-horizon agents. Github Repo:https://github.com/ChenglinPoly/infiAgent</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InfiAgent：面向通用自主代理的无限时间范围框架</div>
<div class="mono" style="margin-top:8px">LLM代理可以进行推理并使用工具，但在长时间任务中由于上下文增长无界和累积误差，常常出现故障。常见的解决方法如上下文压缩或检索增强提示，在信息保真度和推理稳定性之间引入权衡。我们提出了InfiAgent，这是一个通用框架，通过将持久状态外部化为以文件为中心的状态抽象，无论任务持续时间如何，都能严格限制代理的推理上下文。在每一步中，代理通过从工作区状态快照和固定窗口的最近操作中重建上下文。在DeepResearch和一项80篇论文的文献综述任务上的实验表明，无需任务特定微调，使用20B开源模型的InfiAgent在性能上与更大的专有系统具有竞争力，并且在长时间范围覆盖方面显著优于以上下文为中心的基线方法。这些结果支持将状态显式外部化作为稳定长时间范围代理的实用基础。Github仓库：https://github.com/ChenglinPoly/infiAgent</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
