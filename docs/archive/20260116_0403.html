<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-16 04:03</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260116_0403</div>
    <div class="row"><div class="card">
<div class="title">Fast-ThinkAct: Efficient Vision-Language-Action Reasoning via Verbalizable Latent Planning</div>
<div class="meta-line">Authors: Chi-Pin Huang, Yunze Man, Zhiding Yu, Min-Hung Chen, Jan Kautz, Yu-Chiang Frank Wang, Fu-En Yang</div>
<div class="meta-line">First: 2026-01-14T18:59:59+00:00 · Latest: 2026-01-14T18:59:59+00:00</div>
<div class="meta-line">Comments: Project page: https://jasper0314-huang.github.io/fast-thinkact/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09708v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09708v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://jasper0314-huang.github.io/fast-thinkact/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) tasks require reasoning over complex visual scenes and executing adaptive actions in dynamic environments. While recent studies on reasoning VLAs show that explicit chain-of-thought (CoT) can improve generalization, they suffer from high inference latency due to lengthy reasoning traces. We propose Fast-ThinkAct, an efficient reasoning framework that achieves compact yet performant planning through verbalizable latent reasoning. Fast-ThinkAct learns to reason efficiently with latent CoTs by distilling from a teacher, driven by a preference-guided objective to align manipulation trajectories that transfers both linguistic and visual planning capabilities for embodied control. This enables reasoning-enhanced policy learning that effectively connects compact reasoning to action execution. Extensive experiments across diverse embodied manipulation and reasoning benchmarks demonstrate that Fast-ThinkAct achieves strong performance with up to 89.3\% reduced inference latency over state-of-the-art reasoning VLAs, while maintaining effective long-horizon planning, few-shot adaptation, and failure recovery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Fast-ThinkAct: 通过可言化的潜在规划实现高效的视觉-语言-动作推理</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）任务需要对复杂视觉场景进行推理，并在动态环境中执行适应性动作。尽管近期关于推理VLA的研究表明，显式的思维链（CoT）可以提升泛化能力，但它们由于推理轨迹较长而面临高推理延迟的问题。我们提出Fast-ThinkAct，这是一种高效的推理框架，通过可言化的潜在推理实现紧凑且高效的规划。Fast-ThinkAct通过从教师模型中蒸馏学习，利用偏好引导的目标对操作轨迹进行对齐，从而迁移语言和视觉规划能力，用于具身控制。这使得推理增强的策略学习能够有效连接紧凑推理与动作执行。在多种具身操作和推理基准上的广泛实验表明，Fast-ThinkAct在推理延迟上比最先进的推理VLA减少了高达89.3\%，同时保持了有效的长时规划、少样本适应和失败恢复能力。</div>
</details>
</div>
<div class="card">
<div class="title">Value-Aware Numerical Representations for Transformer Language Models</div>
<div class="meta-line">Authors: Andreea Dutulescu, Stefan Ruseti, Mihai Dascalu</div>
<div class="meta-line">First: 2026-01-14T18:59:14+00:00 · Latest: 2026-01-14T18:59:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09706v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09706v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformer-based language models often achieve strong results on mathematical reasoning benchmarks while remaining fragile on basic numerical understanding and arithmetic operations. A central limitation is that numbers are processed as symbolic tokens whose embeddings do not explicitly encode numerical value, leading to systematic errors. We introduce a value-aware numerical representation that augments standard tokenized inputs with a dedicated prefix token whose embedding is explicitly conditioned on the underlying numerical value. This mechanism injects magnitude information directly into the model&#x27;s input space while remaining compatible with existing tokenizers and decoder-only Transformer architectures. Evaluation on arithmetic tasks shows that the proposed approach outperforms baselines across numerical formats, tasks, and operand lengths. These results indicate that explicitly encoding numerical value is an effective and efficient way to improve fundamental numerical robustness in language models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向Transformer语言模型的价值感知数值表示</div>
<div class="mono" style="margin-top:8px">基于Transformer的语言模型通常在数学推理基准测试中表现出色，但在基本数值理解与算术运算方面却较为脆弱。一个核心限制是数值被处理为符号标记，其嵌入并未显式编码数值意义，从而导致系统性错误。我们引入了一种价值感知的数值表示方法，通过在标准标记化输入中添加专用的前缀标记，其嵌入显式地依赖于底层数值。该机制将数值大小信息直接注入模型的输入空间，同时保持与现有标记器和解码器-only Transformer架构的兼容性。在算术任务上的评估表明，所提出的方法在不同数值格式、任务和操作数长度上均优于基线方法。这些结果表明，显式编码数值意义是提升语言模型基本数值鲁棒性的一种有效且高效的方式。</div>
</details>
</div>
<div class="card">
<div class="title">ShortCoder: Knowledge-Augmented Syntax Optimization for Token-Efficient Code Generation</div>
<div class="meta-line">Authors: Sicong Liu, Yanxian Huang, Mingwei Liu, Jiachi Chen, Ensheng Shi, Yuchi Ma, Hongyu Zhang, Yin Zhang, Yanlin Wang</div>
<div class="meta-line">First: 2026-01-14T18:57:31+00:00 · Latest: 2026-01-14T18:57:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09703v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09703v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Code generation tasks aim to automate the conversion of user requirements into executable code, significantly reducing manual development efforts and enhancing software productivity. The emergence of large language models (LLMs) has significantly advanced code generation, though their efficiency is still impacted by certain inherent architectural constraints. Each token generation necessitates a complete inference pass, requiring persistent retention of contextual information in memory and escalating resource consumption. While existing research prioritizes inference-phase optimizations such as prompt compression and model quantization, the generation phase remains underexplored. To tackle these challenges, we propose a knowledge-infused framework named ShortCoder, which optimizes code generation efficiency while preserving semantic equivalence and readability. In particular, we introduce: (1) ten syntax-level simplification rules for Python, derived from AST-preserving transformations, achieving 18.1% token reduction without functional compromise; (2) a hybrid data synthesis pipeline integrating rule-based rewriting with LLM-guided refinement, producing ShorterCodeBench, a corpus of validated tuples of original code and simplified code with semantic consistency; (3) a fine-tuning strategy that injects conciseness awareness into the base LLMs. Extensive experimental results demonstrate that ShortCoder consistently outperforms state-of-the-art methods on HumanEval, achieving an improvement of 18.1%-37.8% in generation efficiency over previous methods while ensuring the performance of code generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ShortCoder: 基于知识的语法优化用于高效代码生成</div>
<div class="mono" style="margin-top:8px">代码生成任务旨在自动化地将用户需求转换为可执行代码，显著减少手动开发工作量并提升软件生产力。尽管大型语言模型（LLMs）的出现极大地推动了代码生成的发展，但其效率仍受到某些固有架构限制的影响。每个token的生成都需要一次完整的推理过程，这要求在内存中持续保留上下文信息，从而增加资源消耗。虽然现有研究主要关注推理阶段的优化，如提示压缩和模型量化，但生成阶段仍被忽视。为了解决这些问题，我们提出了一种名为ShortCoder的知识注入框架，该框架在提升代码生成效率的同时保持语义等价性和可读性。具体而言，我们引入了以下内容：(1) 十条基于AST保持变换的Python语法简化规则，实现18.1%的token减少而不会影响功能；(2) 一种混合数据合成流水线，结合基于规则的重写与LLM引导的优化，生成ShorterCodeBench，这是一个经过验证的原始代码与简化代码语义一致的元组语料库；(3) 一种微调策略，将简洁性意识注入基础LLMs中。大量实验结果表明，ShortCoder在HumanEval上持续优于最先进的方法，相比之前的方法在生成效率上提升了18.1%-37.8%，同时确保了代码生成的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Code generation tasks aim to automate the conversion of user requirements into executable code, significantly reducing manual development efforts and enhancing software productivity.</div>
</details>
</div>
<div class="card">
<div class="title">SAM3-DMS: Decoupled Memory Selection for Multi-target Video Segmentation of SAM3</div>
<div class="meta-line">Authors: Ruiqi Shen, Chang Liu, Henghui Ding</div>
<div class="meta-line">First: 2026-01-14T18:52:14+00:00 · Latest: 2026-01-14T18:52:14+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/FudanCVL/SAM3-DMS</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09699v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09699v1">PDF</a> · <a href="https://github.com/FudanCVL/SAM3-DMS">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Segment Anything 3 (SAM3) has established a powerful foundation that robustly detects, segments, and tracks specified targets in videos. However, in its original implementation, its group-level collective memory selection is suboptimal for complex multi-object scenarios, as it employs a synchronized decision across all concurrent targets conditioned on their average performance, often overlooking individual reliability. To this end, we propose SAM3-DMS, a training-free decoupled strategy that utilizes fine-grained memory selection on individual objects. Experiments demonstrate that our approach achieves robust identity preservation and tracking stability. Notably, our advantage becomes more pronounced with increased target density, establishing a solid foundation for simultaneous multi-target video segmentation in the wild.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SAM3-DMS: 为SAM3的多目标视频分割解耦记忆选择</div>
<div class="mono" style="margin-top:8px">Segment Anything 3 (SAM3) 已建立了一个强大的基础，能够稳健地检测、分割和跟踪视频中的指定目标。然而，在其原始实现中，其组级集体记忆选择在复杂多目标场景下表现不佳，因为它基于所有并发目标的平均性能进行同步决策，常常忽略了个体的可靠性。为此，我们提出了SAM3-DMS，这是一种无需训练的解耦策略，利用对单个目标的细粒度记忆选择。实验表明，我们的方法实现了稳健的身份保持和跟踪稳定性。值得注意的是，随着目标密度的增加，我们的优势更加明显，为野外同时多目标视频分割奠定了坚实的基础。</div>
</details>
</div>
<div class="card">
<div class="title">Causality-enhanced Decision-Making for Autonomous Mobile Robots in Dynamic Environments</div>
<div class="meta-line">Authors: Luca Castri, Gloria Beraldo, Nicola Bellotto</div>
<div class="meta-line">First: 2025-04-16T09:26:04+00:00 · Latest: 2026-01-14T18:52:06+00:00</div>
<div class="meta-line">Comments: Causal Discovery and Inference - Robot Autonomy - Human-Robot Spatial Interaction - Decision-Making</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.11901v4">Abs</a> · <a href="https://arxiv.org/pdf/2504.11901v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The growing integration of robots in shared environments - such as warehouses, shopping centres, and hospitals - demands a deep understanding of the underlying dynamics and human behaviours, including how, when, and where individuals engage in various activities and interactions. This knowledge goes beyond simple correlation studies and requires a more comprehensive causal analysis. By leveraging causal inference to model cause-and-effect relationships, we can better anticipate critical environmental factors and enable autonomous robots to plan and execute tasks more effectively. To this end, we propose a novel causality-based decision-making framework that reasons over a learned causal model to assist the robot in deciding when and how to complete a given task. In the examined use case - i.e., a warehouse shared with people - we exploit the causal model to estimate battery usage and human obstructions as factors influencing the robot&#x27;s task execution. This reasoning framework supports the robot in making informed decisions about task timing and strategy. To achieve this, we developed also PeopleFlow, a new Gazebo-based simulator designed to model context-sensitive human-robot spatial interactions in shared workspaces. PeopleFlow features realistic human and robot trajectories influenced by contextual factors such as time, environment layout, and robot state, and can simulate a large number of agents. While the simulator is general-purpose, in this paper we focus on a warehouse-like environment as a case study, where we conduct an extensive evaluation benchmarking our causal approach against a non-causal baseline. Our findings demonstrate the efficacy of the proposed solutions, highlighting how causal reasoning enables autonomous robots to operate more efficiently and safely in dynamic environments shared with humans.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>动态环境中基于因果关系的自主移动机器人决策</div>
<div class="mono" style="margin-top:8px">随着机器人在共享环境（如仓库、购物中心和医院）中的集成度不断提高，需要深入理解其背后的动态和人类行为，包括个体在不同活动和互动中何时、何地以及如何参与。这种知识超越了简单的相关性研究，需要更全面的因果分析。通过利用因果推断来建模因果关系，我们可以更好地预测关键环境因素，并使自主机器人更有效地规划和执行任务。为此，我们提出了一种新颖的基于因果关系的决策框架，该框架通过学习的因果模型进行推理，帮助机器人决定何时以及如何完成特定任务。在所研究的用例——即与人类共享的仓库环境中——我们利用因果模型来估计电池使用和人类障碍等因素对机器人任务执行的影响。该推理框架支持机器人在任务时间和策略上做出明智的决策。为实现这一目标，我们还开发了PeopleFlow，一个基于Gazebo的新模拟器，用于建模共享工作空间中具有情境敏感性的人机空间交互。PeopleFlow具有受时间、环境布局和机器人状态等情境因素影响的逼真人类和机器人轨迹，并且可以模拟大量智能体。虽然该模拟器具有通用性，但在本文中我们聚焦于类似仓库的环境作为案例研究，并进行了广泛的评估，将我们的因果方法与非因果基线进行对比。我们的研究结果展示了所提出解决方案的有效性，突显了因果推理如何使自主机器人在与人类共享的动态环境中更高效、更安全地运行。</div>
</details>
</div>
<div class="card">
<div class="title">LLM-Based Emulation of the Radio Resource Control Layer: Towards AI-Native RAN Protocols</div>
<div class="meta-line">Authors: Ziming Liu, Bryan Liu, Alvaro Valcarce, Xiaoli Chu</div>
<div class="meta-line">First: 2025-05-22T15:55:56+00:00 · Latest: 2026-01-14T18:50:49+00:00</div>
<div class="meta-line">Comments: This work has been submitted to the IEEE for possible publication. Focuses on applying LLMs to 5G RRC protocol generation; primary: cs.NI; cross-list: eess.SP, cs.LG</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.16821v4">Abs</a> · <a href="https://arxiv.org/pdf/2505.16821v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Integrating Large AI Models (LAMs) into 6G mobile networks is a key enabler of the AI-Native Air Interface (AI-AI), where protocol intelligence must scale beyond handcrafted logic. This paper presents, to our knowledge, the first standards-compliant emulation of the Radio Resource Control (RRC) layer using a decoder-only LAM (LLAMA-class) fine-tuned with Low-Rank Adaptation (LoRA) on a multi-vendor corpus of real-world traces spanning both 5G and 4G systems. We treat RRC as a domain-specific language and construct a segmentation-safe, question-answer (Question-and-Answer (QA)) dataset that preserves Abstract Syntax Notation (ASN.1) structure through linearization prior to Byte Pair Encoding (BPE) tokenization. The proposed approach combines parameter-efficient adaptation with schema-bounded prompting to ensure syntactic and procedural fidelity. Evaluation introduces a standards-aware triad -- ASN.1 conformance, field-level coverage analysis, and uplink-to-downlink state-machine checks -- alongside semantic similarity and latency profiling across 120 configurations. On 30k 5G request-response pairs plus an additional 4.8k QA turns from 4G sessions, our 8B model achieves a median cosine similarity of 0.97, a 61% relative gain over a zero-shot baseline, while sustaining high conformance rates. These results demonstrate that LAMs, when augmented with protocol-aware reasoning, can directly orchestrate control-plane procedures, laying the foundation for the future Artificial Intelligence (AI)-native Radio Access Network (RAN).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的无线资源控制层模拟：迈向AI原生的RAN协议</div>
<div class="mono" style="margin-top:8px">将大型AI模型（LAMs）集成到6G移动网络中是实现AI原生无线接口（AI-AI）的关键，其中协议智能必须超越手工编写的逻辑。本文提出了，据我们所知，首个符合标准的基于解码器-only LAM（如LLAMA类）的RRC层模拟，该模型在涵盖5G和4G系统的多供应商真实轨迹数据集上，通过低秩适应（LoRA）进行微调。我们将RRC视为一种领域特定语言，并构建了一个分段安全、问答（QA）数据集，通过线性化处理在字节对编码（BPE）分词前保留抽象语法表示（ASN.1）结构。所提出的方法结合了参数高效适应与模式边界提示，以确保语法和过程的保真度。评估引入了一个标准感知的三元组——ASN.1符合性、字段级覆盖分析以及上行-下行状态机检查——并结合语义相似性和延迟分析，在120种配置中进行测试。在3万个5G请求-响应对以及额外4800个4G会话中的问答轮次上，我们的8B模型实现了中位数余弦相似度为0.97，相较零样本基线有61%的相对提升，同时保持了高符合率。这些结果表明，当大语言模型（LAMs）结合协议感知推理时，可以直接协调控制平面过程，为未来AI原生的无线接入网（RAN）奠定基础。</div>
</details>
</div>
<div class="card">
<div class="title">COMPOSE: Hypergraph Cover Optimization for Multi-view 3D Human Pose Estimation</div>
<div class="meta-line">Authors: Tony Danjun Wang, Tolga Birdal, Nassir Navab, Lennart Bastian</div>
<div class="meta-line">First: 2026-01-14T18:50:17+00:00 · Latest: 2026-01-14T18:50:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09698v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09698v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">3D pose estimation from sparse multi-views is a critical task for numerous applications, including action recognition, sports analysis, and human-robot interaction. Optimization-based methods typically follow a two-stage pipeline, first detecting 2D keypoints in each view and then associating these detections across views to triangulate the 3D pose. Existing methods rely on mere pairwise associations to model this correspondence problem, treating global consistency between views (i.e., cycle consistency) as a soft constraint. Yet, reconciling these constraints for multiple views becomes brittle when spurious associations propagate errors. We thus propose COMPOSE, a novel framework that formulates multi-view pose correspondence matching as a hypergraph partitioning problem rather than through pairwise association. While the complexity of the resulting integer linear program grows exponentially in theory, we introduce an efficient geometric pruning strategy to substantially reduce the search space. COMPOSE achieves improvements of up to 23% in average precision over previous optimization-based methods and up to 11% over self-supervised end-to-end learned methods, offering a promising solution to a widely studied problem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>COMPOSE：用于多视角三维人体姿态估计的超图覆盖优化</div>
<div class="mono" style="margin-top:8px">从稀疏多视角中进行三维姿态估计是许多应用中的关键任务，包括动作识别、体育分析和人机交互。基于优化的方法通常遵循一个两阶段流程，首先在每个视角中检测二维关键点，然后通过关联这些检测结果来三角化三维姿态。现有方法仅依赖于成对关联来建模这一对应问题，将视角间的全局一致性（即循环一致性）视为软约束。然而，当错误通过错误关联传播时，这些约束的协调变得脆弱。因此，我们提出COMPOSE，一种新颖的框架，将多视角姿态对应匹配问题建模为超图划分问题，而非成对关联。虽然由此产生的整数线性规划问题在理论上复杂度呈指数增长，但我们引入了一种高效的几何剪枝策略，显著减少了搜索空间。COMPOSE在平均精度上比之前的基于优化的方法提高了最多23%，比自监督端到端学习方法提高了最多11%，为一个广泛研究的问题提供了一个有前景的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Camera-Controlled Video Generation of Static Scenes via Sparse Diffusion and 3D Rendering</div>
<div class="meta-line">Authors: Jieying Chen, Jeffrey Hu, Joan Lasenby, Ayush Tewari</div>
<div class="meta-line">First: 2026-01-14T18:50:06+00:00 · Latest: 2026-01-14T18:50:06+00:00</div>
<div class="meta-line">Comments: Project page: https://ayushtewari.com/projects/srender/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09697v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09697v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ayushtewari.com/projects/srender/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern video generative models based on diffusion models can produce very realistic clips, but they are computationally inefficient, often requiring minutes of GPU time for just a few seconds of video. This inefficiency poses a critical barrier to deploying generative video in applications that require real-time interactions, such as embodied AI and VR/AR. This paper explores a new strategy for camera-conditioned video generation of static scenes: using diffusion-based generative models to generate a sparse set of keyframes, and then synthesizing the full video through 3D reconstruction and rendering. By lifting keyframes into a 3D representation and rendering intermediate views, our approach amortizes the generation cost across hundreds of frames while enforcing geometric consistency. We further introduce a model that predicts the optimal number of keyframes for a given camera trajectory, allowing the system to adaptively allocate computation. Our final method, SRENDER, uses very sparse keyframes for simple trajectories and denser ones for complex camera motion. This results in video generation that is more than 40 times faster than the diffusion-based baseline in generating 20 seconds of video, while maintaining high visual fidelity and temporal stability, offering a practical path toward efficient and controllable video synthesis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过稀疏扩散和3D渲染高效生成静态场景的摄像机控制视频</div>
<div class="mono" style="margin-top:8px">基于扩散模型的现代视频生成模型可以生成非常逼真的视频片段，但计算效率低下，通常生成几秒钟的视频需要数分钟的GPU时间。这种低效性成为在需要实时交互的应用（如具身AI和VR/AR）中部署生成视频的关键障碍。本文探讨了一种新的静态场景摄像机条件视频生成策略：使用基于扩散的生成模型生成稀疏的关键帧，然后通过3D重建和渲染合成完整视频。通过将关键帧提升为3D表示并渲染中间视图，我们的方法在数百帧上摊薄生成成本，同时强制几何一致性。我们进一步引入了一个模型，用于预测给定摄像机轨迹所需的最优关键帧数量，使系统能够自适应地分配计算资源。我们的最终方法SRENDER在简单轨迹中使用非常稀疏的关键帧，在复杂摄像机运动中使用更密集的关键帧。这使得生成20秒视频的速度比基于扩散的基线方法快40倍以上，同时保持了高视觉保真度和时间稳定性，为高效可控的视频合成提供了一条实用路径。</div>
</details>
</div>
<div class="card">
<div class="title">LLMs can Compress LLMs: Adaptive Pruning by Agents</div>
<div class="meta-line">Authors: Sai Varun Kodathala, Rakesh Vunnam</div>
<div class="meta-line">First: 2026-01-14T18:45:36+00:00 · Latest: 2026-01-14T18:45:36+00:00</div>
<div class="meta-line">Comments: 17 Pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09694v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09694v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As Large Language Models (LLMs) continue to scale, post-training pruning has emerged as a promising approach to reduce computational costs while preserving performance. Existing methods such as SparseGPT and Wanda achieve high sparsity through layer-wise weight reconstruction or activation-aware magnitude pruning, but rely on uniform or hand-crafted heuristics to determine per-layer sparsity ratios. Moreover, recent work has shown that pruned LLMs suffer from severe factual knowledge degradation, with structured pruning methods experiencing near-total collapse in factual question-answering capabilities. We introduce agent-guided pruning, where a foundation model acts as an adaptive pruning agent to intelligently select which layers to prune at each iteration while preserving critical knowledge pathways. Our method constructs layer-wise sensitivity profiles by combining Wanda-inspired weight-activation metrics with gradient importance scores, normalized as z-scores for model-agnostic comparison. These statistics are processed by an LLM agent equipped with self-reflection capabilities, enabling it to learn from previous pruning outcomes and iteratively refine its strategy. A checkpoint rollback mechanism maintains model quality by reverting when perplexity degradation exceeds a threshold. We evaluate our approach on Qwen3 models (4B and 8B parameters) at approximately 45% sparsity, demonstrating substantial improvements over structured pruning baselines: 56% relative improvement in MMLU accuracy, 19x better factual knowledge retention on FreebaseQA, and 69% lower perplexity degradation. Notably, our framework requires no retraining, operates in a model-agnostic manner, and exhibits effective self-correction with only 2-4 rollbacks across 21-40 iterations, demonstrating that foundation models can effectively guide the compression of other foundation models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLMs可以压缩LLMs：通过智能体实现的自适应剪枝</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLMs）的持续扩展，后训练剪枝作为一种有前景的方法，被提出以降低计算成本同时保持性能。现有的方法如SparseGPT和Wanda通过逐层权重重建或基于激活的幅度剪枝实现高稀疏性，但依赖于统一或手工设计的启发式方法来确定每层的稀疏比例。此外，最近的研究表明，剪枝后的LLMs会严重丧失事实知识，其中结构化剪枝方法在事实性问答任务中的能力几乎完全崩溃。我们引入了智能体引导的剪枝方法，其中基础模型作为自适应剪枝智能体，在每次迭代中智能选择哪些层进行剪枝，同时保留关键的知识路径。我们的方法通过结合受Wanda启发的权重-激活度指标与梯度重要性分数，构建逐层敏感性分析，并将其标准化为z分数以实现模型无关的比较。这些统计信息由具备自我反思能力的LLM智能体处理，使其能够从之前的剪枝结果中学习，并迭代优化其策略。一个检查点回滚机制通过在困惑度下降超过阈值时回退，来维持模型质量。我们在Qwen3模型（4B和8B参数）上以约45%的稀疏度评估了我们的方法，结果表明相较于结构化剪枝基线有显著提升：MMLU准确率相对提升56%，在FreebaseQA任务中事实知识保留效果提高了19倍，困惑度下降降低了69%。值得注意的是，我们的框架无需重新训练，以模型无关的方式运行，并且在21-40次迭代中仅需2-4次回滚即可实现有效的自我修正，证明了基础模型可以有效地指导其他基础模型的压缩。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">As Large Language Models (LLMs) continue to scale, post-training pruning has emerged as a promising approach to reduce computational costs while preserving performance.</div>
</details>
</div>
<div class="card">
<div class="title">Contrastive Geometric Learning Unlocks Unified Structure- and Ligand-Based Drug Design</div>
<div class="meta-line">Authors: Lisa Schneckenreiter, Sohvi Luukkonen, Lukas Friedrich, Daniel Kuhn, Günter Klambauer</div>
<div class="meta-line">First: 2026-01-14T18:45:08+00:00 · Latest: 2026-01-14T18:45:08+00:00</div>
<div class="meta-line">Comments: ELLIS ML4Molecules Workshop 2025, ELLIS Unconference, Copenhagen 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09693v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09693v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Structure-based and ligand-based computational drug design have traditionally relied on disjoint data sources and modeling assumptions, limiting their joint use at scale. In this work, we introduce Contrastive Geometric Learning for Unified Computational Drug Design (ConGLUDe), a single contrastive geometric model that unifies structure- and ligand-based training. ConGLUDe couples a geometric protein encoder that produces whole-protein representations and implicit embeddings of predicted binding sites with a fast ligand encoder, removing the need for pre-defined pockets. By aligning ligands with both global protein representations and multiple candidate binding sites through contrastive learning, ConGLUDe supports ligand-conditioned pocket prediction in addition to virtual screening and target fishing, while being trained jointly on protein-ligand complexes and large-scale bioactivity data. Across diverse benchmarks, ConGLUDe achieves state-of-the-art zero-shot virtual screening performance in settings where no binding pocket information is provided as input, substantially outperforms existing methods on a challenging target fishing task, and demonstrates competitive ligand-conditioned pocket selection. These results highlight the advantages of unified structure-ligand training and position ConGLUDe as a step toward general-purpose foundation models for drug discovery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对比几何学习解锁统一的基于结构和配体的药物设计</div>
<div class="mono" style="margin-top:8px">基于结构和基于配体的计算药物设计传统上依赖于分离的数据源和建模假设，限制了它们在大规模联合应用中的使用。在本工作中，我们引入了用于统一计算药物设计的对比几何学习模型（ConGLUDe），这是一个单一的对比几何模型，能够统一基于结构和基于配体的训练。ConGLUDe结合了一个几何蛋白质编码器，该编码器生成完整的蛋白质表示和预测结合位点的隐式嵌入，并与一个快速的配体编码器相耦合，从而消除了对预定义口袋的需求。通过对比学习将配体与全局蛋白质表示和多个候选结合位点对齐，ConGLUDe支持配体条件下的口袋预测，同时还能进行虚拟筛选和靶点挖掘，并且在蛋白质-配体复合物和大规模生物活性数据上进行联合训练。在多种基准测试中，ConGLUDe在不提供结合口袋信息的零样本虚拟筛选任务中实现了最先进的性能，在具有挑战性的靶点挖掘任务中显著优于现有方法，并展示了在配体条件下口袋选择方面的竞争力。这些结果突显了统一结构-配体训练的优势，并将ConGLUDe定位为迈向通用药物发现基础模型的重要一步。</div>
</details>
</div>
<div class="card">
<div class="title">Routing with Generated Data: Annotation-Free LLM Skill Estimation and Expert Selection</div>
<div class="meta-line">Authors: Tianyi Niu, Justin Chih-Yao Chen, Genta Indra Winata, Shi-Xiong Zhang, Supriyo Chakraborty, Sambit Sahu, Yue Zhang, Elias Stengel-Eskin, Mohit Bansal</div>
<div class="meta-line">First: 2026-01-14T18:43:32+00:00 · Latest: 2026-01-14T18:43:32+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/tianyiniu/RoutingGenData</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09692v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09692v1">PDF</a> · <a href="https://github.com/tianyiniu/RoutingGenData">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Model (LLM) routers dynamically select optimal models for given inputs. Existing approaches typically assume access to ground-truth labeled data, which is often unavailable in practice, especially when user request distributions are heterogeneous and unknown. We introduce Routing with Generated Data (RGD), a challenging setting in which routers are trained exclusively on generated queries and answers produced from high-level task descriptions by generator LLMs. We evaluate query-answer routers (using both queries and labels) and query-only routers across four diverse benchmarks and 12 models, finding that query-answer routers degrade faster than query-only routers as generator quality decreases. Our analysis reveals two crucial characteristics of effective generators: they must accurately respond to their own questions, and their questions must produce sufficient performance differentiation among the model pool. We then show how filtering for these characteristics can improve the quality of generated data. We further propose CASCAL, a novel query-only router that estimates model correctness through consensus voting and identifies model-specific skill niches via hierarchical clustering. CASCAL is substantially more robust to generator quality, outperforming the best query-answer router by 4.6% absolute accuracy when trained on weak generator data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用生成数据的路由：无标注的LLM技能评估与专家选择</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）路由器根据输入动态选择最优模型。现有方法通常假设可以访问真实标注数据，但在实践中这种数据往往不可用，尤其是在用户请求分布异质且未知的情况下。我们引入了使用生成数据的路由（RGD），这是一种具有挑战性的设置，其中路由器仅基于由生成器LLM从高层次任务描述中生成的查询和答案进行训练。我们在四个多样化的基准和十二个模型上评估了查询-答案路由器（使用查询和标签）和仅查询路由器，发现当生成器质量下降时，查询-答案路由器的性能下降速度比仅查询路由器更快。我们的分析揭示了有效生成器的两个关键特征：它们必须准确回答自己的问题，且其生成的问题必须在模型池中产生足够的性能差异。我们随后展示了如何通过筛选这些特征来提升生成数据的质量。我们进一步提出了CASCAL，这是一种新颖的仅查询路由器，它通过共识投票估计模型的正确性，并通过分层聚类识别模型特定的技能领域。CASCAL对生成器质量具有更高的鲁棒性，在使用弱生成器数据训练时，其准确率比最佳的查询-答案路由器高出4.6%。</div>
</details>
</div>
<div class="card">
<div class="title">Provable Acceleration of Distributed Optimization with Local Updates</div>
<div class="meta-line">Authors: Zuang Wang, Yongqiang Wang</div>
<div class="meta-line">First: 2026-01-06T22:10:11+00:00 · Latest: 2026-01-14T18:40:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.03442v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.03442v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In conventional distributed optimization, each agent performs a single local update between two communication rounds with its neighbors to synchronize solutions. Inspired by the success of using multiple local updates in federated learning, incorporating local updates into distributed optimization has recently attracted increasing attention. However, unlike federated learning, where multiple local updates can accelerate learning by improving gradient estimation under mini-batch settings, it remains unclear whether similar benefits hold in distributed optimization when gradients are exact. Moreover, existing theoretical results typically require reducing the step size when multiple local updates are employed, which can entirely offset any potential benefit of these additional local updates and obscure their true impact on convergence. In this paper, we focus on the classic DIGing algorithm and leverage the tight performance bounds provided by Performance Estimation Problems (PEP) to show that incorporating local updates can indeed accelerate distributed optimization. To the best of our knowledge, this is the first rigorous demonstration of such acceleration for a broad class of objective functions. Our analysis further reveals that, under an appropriate step size, performing only two local updates is sufficient to achieve the maximal possible improvement, and that additional local updates provide no further gains. Because more updates increase computational cost, these findings offer practical guidance for efficient implementation. Extensive experiments on both synthetic and real-world datasets corroborate the theoretical findings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>可证明的分布式优化中带有本地更新的加速</div>
<div class="mono" style="margin-top:8px">在传统的分布式优化中，每个代理在其邻居之间进行一次本地更新以同步解决方案。受联邦学习中使用多个本地更新成功的启发，将本地更新纳入分布式优化最近引起了越来越多的关注。然而，与联邦学习不同，在小批量设置下，多个本地更新可以通过改进梯度估计来加速学习，但在梯度精确的情况下，这种类似的优势是否仍然存在仍不清楚。此外，现有的理论结果通常要求在使用多个本地更新时减小步长，这可能会完全抵消这些额外本地更新的潜在优势，并掩盖其对收敛的真实影响。在本文中，我们专注于经典的DIGing算法，并利用性能估计问题（PEP）提供的紧密性能界限，证明了在分布式优化中引入本地更新确实可以加速优化过程。据我们所知，这是对广泛目标函数类中此类加速的首次严格证明。我们的分析进一步表明，在适当的步长下，仅进行两次本地更新就足以实现最大可能的改进，而额外的本地更新不会带来进一步的收益。由于更多的更新会增加计算成本，这些发现为高效实现提供了实际指导。我们在合成数据集和现实世界数据集上的大量实验验证了这些理论发现。</div>
</details>
</div>
<div class="card">
<div class="title">Quantum graphs of homomorphisms</div>
<div class="meta-line">Authors: Andre Kornell, Bert Lindenhovius</div>
<div class="meta-line">First: 2026-01-14T18:36:43+00:00 · Latest: 2026-01-14T18:36:43+00:00</div>
<div class="meta-line">Comments: 32 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09685v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09685v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce a category $\mathsf{qGph}$ of quantum graphs, whose definition is motivated entirely from noncommutative geometry. For all quantum graphs $G$ and $H$ in $\mathsf{qGph}$, we then construct a quantum graph $[G,H]$ of homomorphisms from $G$ to $H$, making $\mathsf{qGph}$ a closed symmetric monoidal category. We prove that for all finite graphs $G$ and $H$, the quantum graph $[G,H]$ is nonempty iff the $(G,H)$-homomorphism game has a winning quantum strategy, directly generalizing the classical case.
  The finite quantum graphs in $\mathsf{qGph}$ are tracial, real, and self-adjoint, and the morphisms between them are CP morphisms that are adjoint to a unital $*$-homomorphism. We show that Weaver&#x27;s two notions of a CP morphism coincide in this context. We also show that every finite reflexive quantum graph is the confusability quantum graph of a quantum channel, answering a question of Daws.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>同构量子图</div>
<div class="mono" style="margin-top:8px">我们引入了一个量子图的范畴 $\mathsf{qGph}$，其定义完全受到非交换几何的启发。对于所有量子图 $G$ 和 $H$，我们构造了一个从 $G$ 到 $H$ 的同构量子图 $[G,H]$，使得 $\mathsf{qGph}$ 成为一个闭合的对称单范畴。我们证明，对于所有有限图 $G$ 和 $H$，量子图 $[G,H]$ 非空当且仅当 $(G,H)$-同构游戏存在获胜的量子策略，这直接推广了经典情形。
  $\mathsf{qGph}$ 中的有限量子图是迹类、实数和自伴的，它们之间的态射是伴随于单 $*$-同态的CP态射。我们展示了Weaver提出的两种CP态射概念在此上下文中是相同的。我们还证明了每个有限的反射量子图都是某个量子信道的混淆量子图，回答了Daws提出的问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We introduce a category $\mathsf{qGph}$ of quantum graphs, whose definition is motivated entirely from noncommutative geometry.</div>
</details>
</div>
<div class="card">
<div class="title">Disentangling Task Conflicts in Multi-Task LoRA via Orthogonal Gradient Projection</div>
<div class="meta-line">Authors: Ziyu Yang, Guibin Chen, Yuxin Yang, Aoxiong Zeng, Xiangquan Yang</div>
<div class="meta-line">First: 2026-01-14T18:36:22+00:00 · Latest: 2026-01-14T18:36:22+00:00</div>
<div class="meta-line">Comments: preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09684v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09684v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-Task Learning (MTL) combined with Low-Rank Adaptation (LoRA) has emerged as a promising direction for parameter-efficient deployment of Large Language Models (LLMs). By sharing a single adapter across multiple tasks, one can significantly reduce storage overhead. However, this approach suffers from negative transfer, where conflicting gradient updates from distinct tasks degrade the performance of individual tasks compared to single-task fine-tuning. This problem is exacerbated in LoRA due to the low-rank constraint, which limits the optimization landscape&#x27;s capacity to accommodate diverse task requirements. In this paper, we propose Ortho-LoRA, a gradient projection method specifically tailored for the bipartite structure of LoRA. Ortho-LoRA dynamically projects conflicting task gradients onto the orthogonal complement of each other within the intrinsic LoRA subspace. Extensive experiments on the GLUE benchmark demonstrate that Ortho-LoRA effectively mitigates task interference, outperforming standard joint training and recovering 95\% of the performance gap between multi-task and single-task baselines with negligible computational overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过正交梯度投影在多任务LoRA中解耦任务冲突</div>
<div class="mono" style="margin-top:8px">多任务学习（MTL）与低秩适应（LoRA）相结合，已成为一种在大型语言模型（LLMs）中实现参数高效部署的有前景方向。通过在多个任务间共享一个适配器，可以显著减少存储开销。然而，这种方法存在负迁移问题，即来自不同任务的冲突梯度更新会降低单个任务的性能，相较于单任务微调。在LoRA中，由于低秩约束，这一问题更加严重，因为它限制了优化空间容纳多样化任务需求的能力。本文提出了一种名为正交LoRA（Ortho-LoRA）的梯度投影方法，专门针对LoRA的二分结构进行优化。正交LoRA在内在LoRA子空间内动态地将冲突任务的梯度投影到彼此的正交补空间上。在GLUE基准上的大量实验表明，正交LoRA能有效缓解任务干扰，优于标准联合训练，并在计算开销可忽略的情况下恢复了多任务与单任务基线之间95\%的性能差距。</div>
</details>
</div>
<div class="card">
<div class="title">Automating Supply Chain Disruption Monitoring via an Agentic AI Approach</div>
<div class="meta-line">Authors: Sara AlMahri, Liming Xu, Alexandra Brintrup</div>
<div class="meta-line">First: 2026-01-14T18:28:31+00:00 · Latest: 2026-01-14T18:28:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09680v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09680v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern supply chains are increasingly exposed to disruptions from geopolitical events, demand shocks, trade restrictions, to natural disasters. While many of these disruptions originate deep in the supply network, most companies still lack visibility beyond Tier-1 suppliers, leaving upstream vulnerabilities undetected until the impact cascades downstream. To overcome this blind-spot and move from reactive recovery to proactive resilience, we introduce a minimally supervised agentic AI framework that autonomously monitors, analyses, and responds to disruptions across extended supply networks. The architecture comprises seven specialised agents powered by large language models and deterministic tools that jointly detect disruption signals from unstructured news, map them to multi-tier supplier networks, evaluate exposure based on network structure, and recommend mitigations such as alternative sourcing options. \rev{We evaluate the framework across 30 synthesised scenarios covering three automotive manufacturers and five disruption classes. The system achieves high accuracy across core tasks, with F1 scores between 0.962 and 0.991, and performs full end-to-end analyses in a mean of 3.83 minutes at a cost of \$0.0836 per disruption. Relative to industry benchmarks of multi-day, analyst-driven assessments, this represents a reduction of more than three orders of magnitude in response time. A real-world case study of the 2022 Russia-Ukraine conflict further demonstrates operational applicability. This work establishes a foundational step toward building resilient, proactive, and autonomous supply chains capable of managing disruptions across deep-tier networks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过代理型人工智能方法实现供应链中断监测的自动化</div>
<div class="mono" style="margin-top:8px">现代供应链日益受到地缘政治事件、需求冲击、贸易限制和自然灾害等中断因素的影响。尽管许多这些中断源自供应链网络的深层，但大多数公司仍缺乏对一级供应商以上的可见性，导致上游脆弱性在影响向下游扩散后才被发现。为克服这一盲点并从被动恢复转向主动韧性，我们引入了一个最小监督的代理型人工智能框架，该框架能够自主监测、分析和响应扩展供应链网络中的中断。该架构由七个由大型语言模型和确定性工具驱动的专用代理组成，共同从非结构化新闻中检测中断信号，将其映射到多级供应商网络，基于网络结构评估暴露程度，并推荐缓解措施，如替代采购方案。我们评估了该框架在涵盖三家汽车制造商和五类中断的30个合成场景中的表现。系统在核心任务中实现了高准确度，F1分数在0.962到0.991之间，并在平均3.83分钟内完成完整的端到端分析，每起中断的成本为0.0836美元。与行业基准的多日分析师驱动评估相比，这代表了响应时间减少了三个数量级。2022年俄乌冲突的现实案例研究进一步证明了该框架的操作适用性。本工作为构建具有韧性和自主性的供应链奠定了基础，使其能够管理深层网络中的中断。</div>
</details>
</div>
<div class="card">
<div class="title">OptiMind: Teaching LLMs to Think Like Optimization Experts</div>
<div class="meta-line">Authors: Xinzhi Zhang, Zeyi Chen, Humishka Zope, Hugo Barbalho, Konstantina Mellou, Marco Molinaro, Janardhan Kulkarni, Ishai Menache, Sirui Li</div>
<div class="meta-line">First: 2025-09-26T22:23:12+00:00 · Latest: 2026-01-14T18:26:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.22979v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.22979v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mathematical programming -- the task of expressing operations and decision-making problems in precise mathematical language -- is fundamental across domains, yet remains a skill-intensive process requiring operations research expertise. Recent advances in large language models for complex reasoning have spurred interest in automating this task, translating natural language into executable optimization models. Current approaches, however, achieve limited accuracy, hindered by scarce and noisy training data without leveraging domain knowledge. In this work, we systematically integrate optimization expertise to improve formulation accuracy for mixed-integer linear programming, a key family of mathematical programs. Our OptiMind framework leverages semi-automated, class-based error analysis to guide both training and inference, explicitly preventing common mistakes within each optimization class. Our resulting fine-tuned LLM significantly improves formulation accuracy by 20.7% across multiple optimization benchmarks, with consistent gains under test-time scaling methods such as self-consistency and multi-turn feedback, enabling further progress toward robust LLM-assisted optimization formulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OptiMind：教会大语言模型像优化专家一样思考</div>
<div class="mono" style="margin-top:8px">数学规划——将操作和决策问题用精确的数学语言表达——是跨领域的基础任务，但仍然是一个需要运筹学专业知识的技能密集型过程。近年来，大语言模型在复杂推理方面的进展激发了人们对其自动完成这一任务的兴趣，即将自然语言转化为可执行的优化模型。然而，当前的方法在准确性方面存在局限，受限于稀缺且嘈杂的训练数据，并未充分利用领域知识。在本工作中，我们系统地整合了优化专业知识，以提高混合整数线性规划这一关键数学规划类别的建模准确性。我们的OptiMind框架利用半自动、基于类别的错误分析来指导训练和推理过程，明确防止每类优化中常见的错误。我们的最终微调大语言模型在多个优化基准测试中将建模准确性提高了20.7%，并在诸如自一致性与多轮反馈等测试时扩展方法下保持一致的提升，从而推动了稳健的大语言模型辅助优化建模的进一步发展。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Hybrid Model for Region of Interest Detection in Omnidirectional Videos</div>
<div class="meta-line">Authors: Sana Alamgeer, Mylene Farias, Marcelo Carvalho</div>
<div class="meta-line">First: 2025-11-24T07:52:06+00:00 · Latest: 2026-01-14T18:25:35+00:00</div>
<div class="meta-line">Comments: I need to withdraw this as it contains some confidential information related to FAPESP funding agency</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.18856v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.18856v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The main goal of the project is to design a new model that predicts regions of interest in 360$^{\circ}$ videos. The region of interest (ROI) plays an important role in 360$^{\circ}$ video streaming. For example, ROIs are used to predict view-ports, intelligently cut the videos for live streaming, etc so that less bandwidth is used. Detecting view-ports in advance helps reduce the movement of the head while streaming and watching a video via the head-mounted device. Whereas, intelligent cuts of the videos help improve the efficiency of streaming the video to users and enhance the quality of their viewing experience. This report illustrates the secondary task to identify ROIs, in which, we design, train, and test a hybrid saliency model. In this work, we refer to saliency regions to represent the regions of interest. The method includes the processes as follows: preprocessing the video to obtain frames, developing a hybrid saliency model for predicting the region of interest, and finally post-processing the output predictions of the hybrid saliency model to obtain the output region of interest for each frame. Then, we compare the performance of the proposed method with the subjective annotations of the 360RAT dataset.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于全方位视频中感兴趣区域检测的深度混合模型</div>
<div class="mono" style="margin-top:8px">本项目的主要目标是设计一种新模型，用于预测全方位视频中的感兴趣区域（ROI）。ROI在全方位视频流中起着重要作用。例如，ROI可用于预测视口、智能裁剪视频以进行直播，从而减少带宽使用。提前检测视口有助于减少佩戴头戴设备观看视频时的头部移动。此外，智能裁剪视频可提高向用户传输视频的效率，并提升他们的观看体验质量。本报告阐述了识别ROI的次要任务，其中我们设计、训练并测试了一种混合显著性模型。在本工作中，我们将显著性区域作为感兴趣区域的代表。该方法包括以下步骤：对视频进行预处理以获取帧，开发用于预测感兴趣区域的混合显著性模型，最后对混合显著性模型的输出预测进行后处理，以获得每帧的输出感兴趣区域。然后，我们将所提方法的性能与360RAT数据集的主观注释进行比较。</div>
</details>
</div>
<div class="card">
<div class="title">DNN Modularization via Activation-Driven Training</div>
<div class="meta-line">Authors: Tuan Ngo, Abid Hassan, Saad Shafiq, Nenad Medvidovic</div>
<div class="meta-line">First: 2024-11-01T23:07:33+00:00 · Latest: 2026-01-14T18:22:31+00:00</div>
<div class="meta-line">Comments: Accepted at International Conference on Software Engineering (ICSE) 2026 - Research Track</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.01074v4">Abs</a> · <a href="https://arxiv.org/pdf/2411.01074v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep Neural Networks (DNNs) tend to accrue technical debt and suffer from significant retraining costs when adapting to evolving requirements. Modularizing DNNs offers the promise of improving their reusability. Previous work has proposed techniques to decompose DNN models into modules both during and after training. However, these strategies yield several shortcomings, including significant weight overlaps and accuracy losses across modules, restricted focus on convolutional layers only, and added complexity and training time by introducing auxiliary masks to control modularity. In this work, we propose MODA, an activation-driven modular training approach. MODA promotes inherent modularity within a DNN model by directly regulating the activation outputs of its layers based on three modular objectives: intra-class affinity, inter-class dispersion, and compactness. MODA is evaluated using three well-known DNN models and five datasets with varying sizes. This evaluation indicates that, compared to the existing state-of-the-art, using MODA yields several advantages: (1) MODA accomplishes modularization with 22% less training time; (2) the resultant modules generated by MODA comprise up to 24x fewer weights and 37x less weight overlap while (3) preserving the original model&#x27;s accuracy without additional fine-tuning; in module replacement scenarios, (4) MODA improves the accuracy of a target class by 12% on average while ensuring minimal impact on the accuracy of other classes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过激活驱动训练实现DNN模块化</div>
<div class="mono" style="margin-top:8px">深度神经网络（DNNs）在适应不断变化的需求时，往往积累技术债务并面临显著的再训练成本。对DNN进行模块化可以提高其可重用性。以往的工作提出了在训练过程中和训练后将DNN模型分解为模块的技术。然而，这些策略存在一些缺点，包括模块间显著的权重重叠和准确率损失、仅关注卷积层、以及通过引入辅助掩码来控制模块化而增加的复杂性和训练时间。在本工作中，我们提出了一种名为MODA的激活驱动模块化训练方法。MODA通过三个模块化目标（类内亲和性、类间分散性和紧凑性）直接调节DNN各层的激活输出，以促进模型内部的固有模块化。我们使用三个知名DNN模型和五个不同规模的数据集对MODA进行了评估。评估结果表明，与现有最先进的方法相比，MODA具有以下优势：(1) MODA实现模块化的训练时间减少了22%；(2) MODA生成的模块包含最多24倍更少的权重和37倍更少的权重重叠，同时(3) 无需额外微调即可保持原始模型的准确率；在模块替换场景中，(4) MODA平均提高了目标类别的准确率12%，同时确保对其他类别的准确率影响最小。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Deep Neural Networks (DNNs) tend to accrue technical debt and suffer from significant retraining costs when adapting to evolving requirements.</div>
</details>
</div>
<div class="card">
<div class="title">Template-Based Probes Are Imperfect Lenses for Counterfactual Bias Evaluation in LLMs</div>
<div class="meta-line">Authors: Farnaz Kohankhaki, D. B. Emerson, Jacob-Junqi Tian, Laleh Seyyed-Kalantari, Faiza Khan Khattak</div>
<div class="meta-line">First: 2024-04-04T14:24:06+00:00 · Latest: 2026-01-14T18:20:19+00:00</div>
<div class="meta-line">Comments: 22 Pages, 6 Figures, 5 Tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2404.03471v5">Abs</a> · <a href="https://arxiv.org/pdf/2404.03471v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bias in large language models (LLMs) has many forms, from overt discrimination to implicit stereotypes. Counterfactual bias evaluation is a widely used approach to quantifying bias and often relies on template-based probes that explicitly state group membership. It aims to measure whether the outcome of a task performed by an LLM is invariant to a change in group membership. In this work, we find that template-based probes can introduce systematic distortions in bias measurements. Specifically, we consistently find that such probes suggest that LLMs classify text associated with White race as negative at disproportionately elevated rates. This is observed consistently across a large collection of LLMs, over several diverse template-based probes, and with different classification approaches. We hypothesize that this arises artificially due to linguistic asymmetries present in LLM pretraining data, in the form of markedness, (e.g., Black president vs. president) and templates used for bias measurement (e.g., Black president vs. White president). These findings highlight the need for more rigorous methodologies in counterfactual bias evaluation, ensuring that observed disparities reflect genuine biases rather than artifacts of linguistic conventions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于模板的探针在LLM中的反事实偏差评估中并非完美的工具</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）中的偏差形式多样，从明显的歧视到隐含的刻板印象。反事实偏差评估是一种广泛使用的量化偏差的方法，通常依赖于基于模板的探针，这些探针明确陈述群体成员身份。其目标是衡量LLM执行任务的结果是否对群体成员身份的变化保持不变。在本研究中，我们发现基于模板的探针可能在偏差测量中引入系统性偏差。具体而言，我们一致发现这些探针表明LLMs对与白人种族相关的文本分类为负面的比例异常偏高。这一现象在大量LLMs中被观察到，且在多个不同的基于模板的探针和分类方法中均保持一致。我们假设这种现象是由于LLMs预训练数据中存在语言不对称性，例如标记性（如“黑人总统”与“总统”）以及用于偏差测量的模板（如“黑人总统”与“白人总统”）所导致的。这些发现强调了在反事实偏差评估中需要更严谨的方法论，以确保观察到的差异反映真实的偏差，而非语言惯例的产物。</div>
</details>
</div>
<div class="card">
<div class="title">VIGIL: Defending LLM Agents Against Tool Stream Injection via Verify-Before-Commit</div>
<div class="meta-line">Authors: Junda Lin, Zhaomeng Zhou, Zhi Zheng, Shuochen Liu, Tong Xu, Yong Chen, Enhong Chen</div>
<div class="meta-line">First: 2026-01-09T12:19:49+00:00 · Latest: 2026-01-14T18:19:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05755v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.05755v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM agents operating in open environments face escalating risks from indirect prompt injection, particularly within the tool stream where manipulated metadata and runtime feedback hijack execution flow. Existing defenses encounter a critical dilemma as advanced models prioritize injected rules due to strict alignment while static protection mechanisms sever the feedback loop required for adaptive reasoning. To reconcile this conflict, we propose \textbf{VIGIL}, a framework that shifts the paradigm from restrictive isolation to a verify-before-commit protocol. By facilitating speculative hypothesis generation and enforcing safety through intent-grounded verification, \textbf{VIGIL} preserves reasoning flexibility while ensuring robust control. We further introduce \textbf{SIREN}, a benchmark comprising 959 tool stream injection cases designed to simulate pervasive threats characterized by dynamic dependencies. Extensive experiments demonstrate that \textbf{VIGIL} outperforms state-of-the-art dynamic defenses by reducing the attack success rate by over 22\% while more than doubling the utility under attack compared to static baselines, thereby achieving an optimal balance between security and utility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VIGIL：通过Verify-Before-Commit机制防御LLM代理中的工具流注入</div>
<div class="mono" style="margin-top:8px">在开放环境中运行的LLM代理面临日益严重的间接提示注入风险，尤其是在工具流中，被操控的元数据和运行时反馈会劫持执行流程。现有防御方法面临关键困境，因为先进模型由于严格对齐而优先采用注入的规则，而静态保护机制则切断了适应性推理所需的反馈循环。为解决这一冲突，我们提出\textbf{VIGIL}框架，将防御范式从严格的隔离转向Verify-Before-Commit协议。通过促进推测性假设生成并借助意图基础的验证确保安全性，\textbf{VIGIL}在保持推理灵活性的同时实现了稳健的控制。我们进一步引入\textbf{SIREN}，这是一个包含959个工具流注入案例的基准测试集，旨在模拟具有动态依赖关系的普遍性威胁。大量实验表明，\textbf{VIGIL}在降低攻击成功率超过22\%的同时，相较于静态基线，其在遭受攻击时的实用性提升了超过一倍，从而在安全性和实用性之间实现了最佳平衡。</div>
</details>
</div>
<div class="card">
<div class="title">A supermassive black hole under the radar: repeating X-ray variability in a Seyfert galaxy</div>
<div class="meta-line">Authors: Matteo Imbrogno, Andrea Sacchi, Giovanni Miniutti, Francesco Tombesi, Gian Luca Israel, Enrico Piconcelli, Roberta Amato</div>
<div class="meta-line">First: 2025-10-03T09:10:08+00:00 · Latest: 2026-01-14T18:18:25+00:00</div>
<div class="meta-line">Comments: 14 pages (9 main text, 5 appendices). 9 figure in main text. Accepted for publication on A&amp;A</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.02832v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.02832v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the last few years, a few supermassive black holes (SMBHs) have shown short-term (of the order of hours) X-ray variability. Given the limited size of the sample, every new addition to this class of SMBHs can bring invaluable information. Within the context of an automated search for X-ray sources showing flux variability in the \textit{Chandra} archive, we identified peculiar variability patterns in 2MASX J12571076+2724177 (J1257), a SMBH in the Coma cluster, during observations performed in 2020. We investigated the long-term evolution of the flux, together with the evolution of the spectral parameters throughout the \textit{Chandra} and \textit{XMM-Newton} observations, which cover a time span of approximately 20 years. We found that J1257 has repeatedly shown peculiar variability over the last 20 years, on typical timescales of $\simeq20-30$ ks. From our spectral analysis, we found hints of a softer-when-brighter behaviour and of two well-separated flux states. We suggest that J1257 might represent a new addition to the ever-growing size of relatively low mass SMBHs ($M\simeq10^6-10^7\mathrm{M}_\odot$) showing extreme, possibly quasi-periodic X-ray variability on short time scales. The available dataset does not allow for a definitive classification of the nature of the variability. However, given the observed properties, it could either represent a quasi-periodic oscillation at particularly low frequency or be associated with quasi-periodic eruptions in an AGN with peculiar spectral properties.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>雷达之下的一颗超大质量黑洞：塞弗特星系中的重复X射线变化</div>
<div class="mono" style="margin-top:8px">近年来，一些超大质量黑洞（SMBHs）表现出短时间尺度（约几小时）的X射线变化。由于样本数量有限，每一颗新发现的此类SMBH都能提供宝贵的信息。在一项针对\textit{Chandra}档案中X射线源流量变化的自动搜索中，我们发现2MASX J12571076+2724177（J1257）这颗位于室女座星系团中的SMBH在2020年的观测中表现出奇特的流量变化模式。我们研究了流量的长期演化，以及在\textit{Chandra}和\textit{XMM-Newton}观测中谱参数的变化，这些观测覆盖了大约20年的时间跨度。我们发现，在过去20年中，J1257反复表现出奇特的流量变化，其典型时间尺度为$\simeq20-30$ ks。从我们的谱分析中，我们发现了一些软当明亮的行为迹象，以及两个明显分离的流量状态。我们建议J1257可能是相对低质量的SMBH（$M\simeq10^6-10^7\mathrm{M}_\odot$）中的一员，这些黑洞在短时间尺度上表现出极端的、可能是准周期性的X射线变化。目前可用的数据集尚不足以对变化的性质进行明确分类。然而，根据观测特性，它可能代表一种特别低频率的准周期振荡，或者与具有奇特谱特性的活动星系核（AGN）中的准周期爆发有关。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In the last few years, a few supermassive black holes (SMBHs) have shown short-term (of the order of hours) X-ray variability.</div>
</details>
</div>
<div class="card">
<div class="title">Counting and Entropy Bounds for Structure-Avoiding Spatially-Coupled LDPC Constructions</div>
<div class="meta-line">Authors: Lei Huang</div>
<div class="meta-line">First: 2026-01-14T18:15:46+00:00 · Latest: 2026-01-14T18:15:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09674v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09674v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Designing large coupling memory quasi-cyclic spatially-coupled LDPC (QC-SC-LDPC) codes with low error floors requires eliminating specific harmful substructures (e.g., short cycles) induced by edge spreading and lifting. Building on our work~\cite{r15} that introduced a Clique Lovász Local Lemma (CLLL)-based design principle and a Moser--Tardos (MT)-type constructive approach, this work quantifies the size and structure of the feasible design space. Using the quantitative CLLL, we derive explicit lower bounds on the number of partition matrices satisfying a given family of structure-avoidance constraints, and further obtain bounds on the number of non-equivalent solutions under row/column permutations. Moreover, via Rényi-entropy bounds for the MT distribution, we provide a computable lower bound on the number of distinct solutions that the MT algorithm can output, giving a concrete diversity guarantee for randomized constructions. Specializations for eliminating 4-cycle candidates yield closed-form bounds as functions of system parameters, offering a principled way to size memory/lifting and to estimate the remaining search space.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结构避免空间耦合LDPC构造的计数与熵界</div>
<div class="mono" style="margin-top:8px">设计具有低错误地板的大耦合记忆准循环空间耦合LDPC (QC-SC-LDPC) 代码，需要消除由边扩展和提升引起的特定有害子结构（例如短环）。基于我们之前的工作~\cite{r15}，该工作引入了基于团Lovász局部引理（CLLL）的设计原则和Moser--Tardos（MT）型构造方法，量化了可行设计空间的大小和结构。利用定量CLLL，我们推导出满足给定结构避免约束族的划分矩阵数量的显式下界，并进一步获得在行/列排列下非等价解的数量界。此外，通过MT分布的Rényi熵界，我们提供了MT算法可以输出的不同解数量的可计算下界，为随机化构造提供了具体的多样性保证。针对消除4-环候选的特例，我们得到以系统参数为函数的闭式界，从而提供了一种基于原理的方法来确定记忆/提升的大小，并估计剩余的搜索空间。</div>
</details>
</div>
<div class="card">
<div class="title">Policy Compatible Skill Incremental Learning via Lazy Learning Interface</div>
<div class="meta-line">Authors: Daehee Lee, Dongsu Lee, TaeYoon Kwack, Wonje Choi, Honguk Woo</div>
<div class="meta-line">Venue: NeurIPS 2025 Spotlight</div>
<div class="meta-line">First: 2025-09-24T23:34:01+00:00 · Latest: 2026-01-14T18:11:20+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 Spotlight</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.20612v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.20612v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Skill Incremental Learning (SIL) is the process by which an embodied agent expands and refines its skill set over time by leveraging experience gained through interaction with its environment or by the integration of additional data. SIL facilitates efficient acquisition of hierarchical policies grounded in reusable skills for downstream tasks. However, as the skill repertoire evolves, it can disrupt compatibility with existing skill-based policies, limiting their reusability and generalization. In this work, we propose SIL-C, a novel framework that ensures skill-policy compatibility, allowing improvements in incrementally learned skills to enhance the performance of downstream policies without requiring policy re-training or structural adaptation. SIL-C employs a bilateral lazy learning-based mapping technique to dynamically align the subtask space referenced by policies with the skill space decoded into agent behaviors. This enables each subtask, derived from the policy&#x27;s decomposition of a complex task, to be executed by selecting an appropriate skill based on trajectory distribution similarity. We evaluate SIL-C across diverse SIL scenarios and demonstrate that it maintains compatibility between evolving skills and downstream policies while ensuring efficiency throughout the learning process.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过惰性学习接口实现策略兼容的技能增量学习</div>
<div class="mono" style="margin-top:8px">技能增量学习（Skill Incremental Learning, SIL）是具身智能体通过与环境交互获得的经验或整合额外数据来逐步扩展和优化其技能集的过程。SIL有助于高效获取基于可重用技能的分层策略，以应对下游任务。然而，随着技能集的演变，可能会破坏与现有基于技能策略的兼容性，从而限制其重用性和泛化能力。在本文中，我们提出了一种名为SIL-C的新框架，确保技能策略的兼容性，使得增量学习的技能改进能够提升下游策略的性能，而无需重新训练策略或进行结构上的调整。SIL-C采用双边惰性学习映射技术，动态对齐策略所引用的子任务空间与解码为智能体行为的技能空间。这使得每个子任务，由策略对复杂任务的分解产生，可以通过基于轨迹分布相似性的技能选择来执行。我们在多种SIL场景中评估了SIL-C，并展示了其在保持技能与下游策略兼容性的同时，确保整个学习过程的高效性。</div>
</details>
</div>
<div class="card">
<div class="title">A Novel Hybrid Deep Learning Technique for Speech Emotion Detection using Feature Engineering</div>
<div class="meta-line">Authors: Shahana Yasmin Chowdhury, Bithi Banik, Md Tamjidul Hoque, Shreya Banerjee</div>
<div class="meta-line">Venue: HHAI-WS 2025 Workshops at the Fourth International Conference on Hybrid Human-Artificial Intelligence (HHAI), June, 2025, Pisa, Italy</div>
<div class="meta-line">First: 2025-07-09T17:07:45+00:00 · Latest: 2026-01-14T18:07:43+00:00</div>
<div class="meta-line">Comments: 17 pages, 11 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.07046v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.07046v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Nowadays, speech emotion recognition (SER) plays a vital role in the field of human-computer interaction (HCI) and the evolution of artificial intelligence (AI). Our proposed DCRF-BiLSTM model is used to recognize seven emotions: neutral, happy, sad, angry, fear, disgust, and surprise, which are trained on five datasets: RAVDESS (R), TESS (T), SAVEE (S), EmoDB (E), and Crema-D (C). The model achieves high accuracy on individual datasets, including 97.83% on RAVDESS, 97.02% on SAVEE, 95.10% for CREMA-D, and a perfect 100% on both TESS and EMO-DB. For the combined (R+T+S) datasets, it achieves 98.82% accuracy, outperforming previously reported results. To our knowledge, no existing study has evaluated a single SER model across all five benchmark datasets (i.e., R+T+S+C+E) simultaneously. In our work, we introduce this comprehensive combination and achieve a remarkable overall accuracy of 93.76%. These results confirm the robustness and generalizability of our DCRF-BiLSTM framework across diverse datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种用于语音情感识别的新型混合深度学习技术与特征工程</div>
<div class="mono" style="margin-top:8px">如今，语音情感识别（SER）在人机交互（HCI）和人工智能（AI）的发展中起着至关重要的作用。我们提出的DCRF-BiLSTM模型用于识别七种情感：中性、快乐、悲伤、愤怒、恐惧、厌恶和惊讶，这些模型在五个数据集上进行训练：RAVDESS（R）、TESS（T）、SAVEE（S）、EmoDB（E）和Crema-D（C）。该模型在各个数据集上均取得了高准确率，包括在RAVDESS上达到97.83%、在SAVEE上达到97.02%、在Crema-D上达到95.10%，并在TESS和EMO-DB上达到了完美的100%。对于组合数据集（R+T+S），其准确率达到98.82%，优于之前报告的结果。据我们所知，目前尚无研究同时评估单个SER模型在所有五个基准数据集（即R+T+S+C+E）上的表现。在我们的工作中，我们引入了这种全面的组合，并实现了93.76%的总体准确率。这些结果证实了我们的DCRF-BiLSTM框架在不同数据集上的鲁棒性和泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Nowadays, speech emotion recognition (SER) plays a vital role in the field of human-computer interaction (HCI) and the evolution of artificial intelligence (AI).</div>
</details>
</div>
<div class="card">
<div class="title">STEP3-VL-10B Technical Report</div>
<div class="meta-line">Authors: Ailin Huang, Chengyuan Yao, Chunrui Han, Fanqi Wan, Hangyu Guo, Haoran Lv, Hongyu Zhou, Jia Wang, Jian Zhou, Jianjian Sun, Jingcheng Hu, Kangheng Lin, Liang Zhao, Mitt Huang, Song Yuan, Wenwen Qu, Xiangfeng Wang, Yanlin Lai, Yingxiu Zhao, Yinmin Zhang, Yukang Shi, Yuyang Chen, Zejia Weng, Ziyang Meng, Ang Li, Aobo Kong, Bo Dong, Changyi Wan, David Wang, Di Qi, Dingming Li, En Yu, Guopeng Li, Haiquan Yin, Han Zhou, Hanshan Zhang, Haolong Yan, Hebin Zhou, Hongbo Peng, Jiaran Zhang, Jiashu Lv, Jiayi Fu, Jie Cheng, Jie Zhou, Jisheng Yin, Jingjing Xie, Jingwei Wu, Jun Zhang, Junfeng Liu, Kaijun Tan, Kaiwen Yan, Liangyu Chen, Lina Chen, Mingliang Li, Qian Zhao, Quan Sun, Shaoliang Pang, Shengjie Fan, Shijie Shang, Siyuan Zhang, Tianhao You, Wei Ji, Wuxun Xie, Xiaobo Yang, Xiaojie Hou, Xiaoran Jiao, Xiaoxiao Ren, Xiangwen Kong, Xin Huang, Xin Wu, Xing Chen, Xinran Wang, Xuelin Zhang, Yana Wei, Yang Li, Yanming Xu, Yeqing Shen, Yuang Peng, Yue Peng, Yu Zhou, Yusheng Li, Yuxiang Yang, Yuyang Zhang, Zhe Xie, Zhewei Huang, Zhenyi Lu, Zhimin Fan, Zihui Cheng, Daxin Jiang, Qi Han, Xiangyu Zhang, Yibo Zhu, Zheng Ge</div>
<div class="meta-line">First: 2026-01-14T17:58:24+00:00 · Latest: 2026-01-14T17:58:24+00:00</div>
<div class="meta-line">Comments: 50 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09668v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09668v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence. STEP3-VL-10B is realized through two strategic shifts: first, a unified, fully unfrozen pre-training strategy on 1.2T multimodal tokens that integrates a language-aligned Perception Encoder with a Qwen3-8B decoder to establish intrinsic vision-language synergy; and second, a scaled post-training pipeline featuring over 1k iterations of reinforcement learning. Crucially, we implement Parallel Coordinated Reasoning (PaCoRe) to scale test-time compute, allocating resources to scalable perceptual reasoning that explores and synthesizes diverse visual hypotheses. Consequently, despite its compact 10B footprint, STEP3-VL-10B rivals or surpasses models 10$\times$-20$\times$ larger (e.g., GLM-4.6V-106B, Qwen3-VL-235B) and top-tier proprietary flagships like Gemini 2.5 Pro and Seed-1.5-VL. Delivering best-in-class performance, it records 92.2% on MMBench and 80.11% on MMMU, while excelling in complex reasoning with 94.43% on AIME2025 and 75.95% on MathVision. We release the full model suite to provide the community with a powerful, efficient, and reproducible baseline.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>STEP3-VL-10B 技术报告</div>
<div class="mono" style="margin-top:8px">我们提出了STEP3-VL-10B，这是一个轻量级的开源基础模型，旨在重新定义紧凑效率与前沿级多模态智能之间的权衡。STEP3-VL-10B通过两个战略转变实现：首先，在1.2万亿个多模态标记上采用统一且完全解冻的预训练策略，将语言对齐的感知编码器与Qwen3-8B解码器结合，以建立内在的视觉-语言协同效应；其次，采用了一个经过超过1000次迭代强化学习的扩展后训练流程。关键的是，我们实现了并行协调推理（PaCoRe），以扩展测试时的计算能力，将资源分配给可扩展的感知推理，探索和综合多样化的视觉假设。因此，尽管其参数量仅为100亿，STEP3-VL-10B在性能上与参数量大10-20倍的模型（如GLM-4.6V-106B、Qwen3-VL-235B）以及顶级的专有旗舰模型（如Gemini 2.5 Pro和Seed-1.5-VL）相媲美甚至超越。在MMBench上达到92.2%，在MMMU上达到80.11%，在AIME2025上达到94.43%，在MathVision上达到75.95%。我们发布完整的模型套件，为社区提供一个强大、高效且可复现的基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present STEP3-VL-10B, a lightweight open-source foundation model designed to redefine the trade-off between compact efficiency and frontier-level multimodal intelligence.</div>
</details>
</div>
<div class="card">
<div class="title">Collaborative Multi-Agent Test-Time Reinforcement Learning for Reasoning</div>
<div class="meta-line">Authors: Zhiyuan Hu, Yunhai Hu, Juncheng Liu, Shuyue Stella Li, Yucheng Wang, Zhen Xu, See-Kiong Ng, Anh Tuan Luu, Xinxing Xu, Bryan Hooi, Cynthia Breazeal, Hae Won Park</div>
<div class="meta-line">First: 2026-01-14T17:57:43+00:00 · Latest: 2026-01-14T17:57:43+00:00</div>
<div class="meta-line">Comments: Work in Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09667v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09667v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking. However, multi-agent RL (MARL) training is resource-intensive and unstable: co-adapting teammates induce non-stationarity, and rewards are often sparse and high-variance. Therefore, we introduce \textbf{Multi-Agent Test-Time Reinforcement Learning (MATTRL)}, a framework that injects structured textual experience into multi-agent deliberation at inference time. MATTRL forms a multi-expert team of specialists for multi-turn discussions, retrieves and integrates test-time experiences, and reaches consensus for final decision-making. We also study credit assignment for constructing a turn-level experience pool, then reinjecting it into the dialogue. Across challenging benchmarks in medicine, math, and education, MATTRL improves accuracy by an average of 3.67\% over a multi-agent baseline, and by 8.67\% over comparable single-agent baselines. Ablation studies examine different credit-assignment schemes and provide a detailed comparison of how they affect training outcomes. MATTRL offers a stable, effective and efficient path to distribution-shift-robust multi-agent reasoning without tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于推理的协作多智能体测试时强化学习</div>
<div class="mono" style="margin-top:8px">多智能体系统已发展为许多应用中的实用LLM驱动协作伙伴，通过多样性和相互验证获得了鲁棒性。然而，多智能体强化学习（MARL）训练资源密集且不稳定：队友的协同适应会导致非平稳性，奖励通常稀疏且方差高。因此，我们引入\textbf{多智能体测试时强化学习（MATTRL）}，这是一种在推理时将结构化文本经验注入多智能体决策的框架。MATTRL为多轮讨论构建一个由专家组成的多专家团队，检索并整合测试时经验，并在最终决策中达成共识。我们还研究了信用分配方法，以构建轮级经验池，然后将其重新注入对话中。在医学、数学和教育等具有挑战性的基准测试中，MATTRL在多智能体基线基础上平均提升了3.67\%，在可比的单智能体基线基础上平均提升了8.67\%。消融研究探讨了不同的信用分配方案，并详细比较了它们对训练结果的影响。MATTRL提供了一条无需调参即可实现分布偏移鲁棒的多智能体推理的稳定、有效且高效路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-agent systems have evolved into practical LLM-driven collaborators for many applications, gaining robustness from diversity and cross-checking.</div>
</details>
</div>
<div class="card">
<div class="title">SCE-SLAM: Scale-Consistent Monocular SLAM via Scene Coordinate Embeddings</div>
<div class="meta-line">Authors: Yuchen Wu, Jiahe Li, Xiaohan Yu, Lina Yu, Jin Zheng, Xiao Bai</div>
<div class="meta-line">First: 2026-01-14T17:57:08+00:00 · Latest: 2026-01-14T17:57:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09665v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09665v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Monocular visual SLAM enables 3D reconstruction from internet video and autonomous navigation on resource-constrained platforms, yet suffers from scale drift, i.e., the gradual divergence of estimated scale over long sequences. Existing frame-to-frame methods achieve real-time performance through local optimization but accumulate scale drift due to the lack of global constraints among independent windows. To address this, we propose SCE-SLAM, an end-to-end SLAM system that maintains scale consistency through scene coordinate embeddings, which are learned patch-level representations encoding 3D geometric relationships under a canonical scale reference. The framework consists of two key modules: geometry-guided aggregation that leverages 3D spatial proximity to propagate scale information from historical observations through geometry-modulated attention, and scene coordinate bundle adjustment that anchors current estimates to the reference scale through explicit 3D coordinate constraints decoded from the scene coordinate embeddings. Experiments on KITTI, Waymo, and vKITTI demonstrate substantial improvements: our method reduces absolute trajectory error by 8.36m on KITTI compared to the best prior approach, while maintaining 36 FPS and achieving scale consistency across large-scale scenes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SCE-SLAM：通过场景坐标嵌入实现尺度一致的单目SLAM</div>
<div class="mono" style="margin-top:8px">单目视觉SLAM能够在互联网视频中实现3D重建，并在资源受限的平台上支持自主导航，但存在尺度漂移问题，即在长序列中估计的尺度逐渐偏离。现有的帧到帧方法通过局部优化实现实时性能，但由于独立窗口之间缺乏全局约束，导致尺度漂移的累积。为了解决这一问题，我们提出了SCE-SLAM，这是一个端到端的SLAM系统，通过场景坐标嵌入来维持尺度一致性，这些嵌入是学习到的基于补丁的表示，编码了在标准尺度参考下的3D几何关系。该框架包含两个关键模块：几何引导的聚合模块，利用3D空间邻近性通过几何调制的注意力机制将历史观测的尺度信息传播到当前；场景坐标捆绑调整模块，通过从场景坐标嵌入中解码出的显式3D坐标约束，将当前估计锚定到参考尺度上。在KITTI、Waymo和vKITTI数据集上的实验表明，我们的方法取得了显著改进：与最佳先前方法相比，在KITTI数据集上绝对轨迹误差减少了8.36米，同时保持了36帧每秒的实时性能，并在大规模场景中实现了尺度一致性。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Supervised Animal Identification for Long Videos</div>
<div class="meta-line">Authors: Xuyang Fang, Sion Hannuna, Edwin Simpson, Neill Campbell</div>
<div class="meta-line">First: 2026-01-14T17:53:59+00:00 · Latest: 2026-01-14T17:53:59+00:00</div>
<div class="meta-line">Comments: 11 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09663v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09663v1">PDF</a> · <a href="https://huggingface.co/datasets/tonyFang04/8-calves}{here">Code1</a> · <a href="https://huggingface.co/datasets/tonyFang04/8-calves">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Identifying individual animals in long-duration videos is essential for behavioral ecology, wildlife monitoring, and livestock management. Traditional methods require extensive manual annotation, while existing self-supervised approaches are computationally demanding and ill-suited for long sequences due to memory constraints and temporal error propagation. We introduce a highly efficient, self-supervised method that reframes animal identification as a global clustering task rather than a sequential tracking problem. Our approach assumes a known, fixed number of individuals within a single video -- a common scenario in practice -- and requires only bounding box detections and the total count. By sampling pairs of frames, using a frozen pre-trained backbone, and employing a self-bootstrapping mechanism with the Hungarian algorithm for in-batch pseudo-label assignment, our method learns discriminative features without identity labels. We adapt a Binary Cross Entropy loss from vision-language models, enabling state-of-the-art accuracy ($&gt;$97\%) while consuming less than 1 GB of GPU memory per batch -- an order of magnitude less than standard contrastive methods. Evaluated on challenging real-world datasets (3D-POP pigeons and 8-calves feeding videos), our framework matches or surpasses supervised baselines trained on over 1,000 labeled frames, effectively removing the manual annotation bottleneck. This work enables practical, high-accuracy animal identification on consumer-grade hardware, with broad applicability in resource-constrained research settings. All code written for this paper are \href{https://huggingface.co/datasets/tonyFang04/8-calves}{here}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>长视频的自监督动物识别</div>
<div class="mono" style="margin-top:8px">在长时序视频中识别个体动物对于行为生态学、野生动物监测和家畜管理至关重要。传统方法需要大量手动标注，而现有的自监督方法计算成本高，且由于内存限制和时间误差传播，难以处理长序列。我们提出了一种高效、自监督的方法，将动物识别重新定义为全局聚类任务，而非序列跟踪问题。我们的方法假设单个视频中个体数量已知且固定——这是实践中常见的场景——仅需边界框检测和总数量。通过采样帧对、使用冻结的预训练主干网络，并结合匈牙利算法的自引导机制进行同一批次内的伪标签分配，我们的方法无需身份标签即可学习判别性特征。我们采用视觉-语言模型中的二元交叉熵损失函数，实现了超过97%的准确率，同时每批次仅消耗不到1 GB的GPU内存——比标准对比方法低一个数量级。在具有挑战性的现实数据集（3D-POP鸽子和8头小牛喂食视频）上评估，我们的框架在超过1000个标注帧训练的监督基线模型上表现相当或更优，有效消除了手动标注的瓶颈。本工作使消费者级硬件上实现实用且高精度的动物识别成为可能，在资源受限的研究环境中具有广泛的应用前景。本文所使用的所有代码均可在\href{https://huggingface.co/datasets/tonyFang04/8-calves}{此处}找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Identifying individual animals in long-duration videos is essential for behavioral ecology, wildlife monitoring, and livestock management.</div>
</details>
</div>
<div class="card">
<div class="title">Controlled Self-Evolution for Algorithmic Code Optimization</div>
<div class="meta-line">Authors: Tu Hu, Ronghao Chen, Shuo Zhang, Jianghao Yin, Mou Xiao Feng, Jingping Liu, Shaolei Zhang, Wenqi Jiang, Yuqi Fang, Sen Hu, Yi Xu, Huacan Wang</div>
<div class="meta-line">First: 2026-01-12T09:23:13+00:00 · Latest: 2026-01-14T17:53:44+00:00</div>
<div class="meta-line">Comments: 27 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07348v3">Abs</a> · <a href="https://arxiv.org/pdf/2601.07348v3">PDF</a> · <a href="https://github.com/QuantaAlpha/EvoControl">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Self-evolution methods enhance code generation through iterative &quot;generate-verify-refine&quot; cycles, yet existing approaches suffer from low exploration efficiency, failing to discover solutions with superior complexity within limited budgets. This inefficiency stems from initialization bias trapping evolution in poor solution regions, uncontrolled stochastic operations lacking feedback guidance, and insufficient experience utilization across tasks. To address these bottlenecks, we propose Controlled Self-Evolution (CSE), which consists of three key components. Diversified Planning Initialization generates structurally distinct algorithmic strategies for broad solution space coverage. Genetic Evolution replaces stochastic operations with feedback-guided mechanisms, enabling targeted mutation and compositional crossover. Hierarchical Evolution Memory captures both successful and failed experiences at inter-task and intra-task levels. Experiments on EffiBench-X demonstrate that CSE consistently outperforms all baselines across various LLM backbones. Furthermore, CSE achieves higher efficiency from early generations and maintains continuous improvement throughout evolution. Our code is publicly available at https://github.com/QuantaAlpha/EvoControl.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于算法代码优化的受控自我演化</div>
<div class="mono" style="margin-top:8px">自我演化方法通过迭代的&quot;生成-验证-优化&quot;循环增强代码生成，但现有方法在探索效率方面存在不足，无法在有限预算内发现具有更优复杂度的解决方案。这种低效性源于初始化偏差使演化陷入较差的解空间，无反馈指导的随机操作以及跨任务经验利用不足。为了解决这些瓶颈，我们提出了受控自我演化（CSE），包含三个关键组成部分。多样化规划初始化生成结构上不同的算法策略，以覆盖广泛的解空间。遗传演化用反馈引导机制替代随机操作，实现有针对性的突变和组合交叉。分层演化记忆在跨任务和任务内层面记录成功与失败的经验。在EffiBench-X上的实验表明，CSE在各种LLM架构上均优于所有基线方法。此外，CSE在早期世代就表现出更高的效率，并在整个演化过程中保持持续改进。我们的代码可在https://github.com/QuantaAlpha/EvoControl上公开获取。</div>
</details>
</div>
<div class="card">
<div class="title">LiteEmbed: Adapting CLIP to Rare Classes</div>
<div class="meta-line">Authors: Aishwarya Agarwal, Srikrishna Karanam, Vineet Gandhi</div>
<div class="meta-line">First: 2026-01-14T17:53:11+00:00 · Latest: 2026-01-14T17:53:11+00:00</div>
<div class="meta-line">Comments: 14 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09661v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.09661v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories. We introduce LiteEmbed, a lightweight framework for few-shot personalization of CLIP that enables new classes to be added without retraining its encoders. LiteEmbed performs subspace-guided optimization of text embeddings within CLIP&#x27;s vocabulary, leveraging a PCA-based decomposition that disentangles coarse semantic directions from fine-grained variations. Two complementary objectives, coarse alignment and fine separation, jointly preserve global semantic consistency while enhancing discriminability among visually similar classes. Once optimized, the embeddings are plug-and-play, seamlessly substituting CLIP&#x27;s original text features across classification, retrieval, segmentation, and detection tasks. Extensive experiments demonstrate substantial gains over prior methods, establishing LiteEmbed as an effective approach for adapting CLIP to underrepresented, rare, or unseen classes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LiteEmbed: 将CLIP适配到稀有类别</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型如CLIP在零样本识别方面表现出色，但在预训练期间很少见的类别（包括新出现的实体和文化特定类别）上表现不佳。我们引入LiteEmbed，这是一个轻量级框架，用于CLIP的少样本个性化，使得可以在不重新训练其编码器的情况下添加新类别。LiteEmbed在CLIP词汇表内执行子空间引导的文本嵌入优化，利用基于PCA的分解方法，将粗粒度语义方向与细粒度变化分离。两个互补的目标——粗对齐和细分离——共同保持全局语义一致性，同时增强视觉相似类别之间的可区分性。一旦优化完成，这些嵌入即可即插即用，无缝替代CLIP原有的文本特征，适用于分类、检索、分割和检测任务。大量实验表明，LiteEmbed在性能上显著优于先前方法，确立了其作为将CLIP适配到欠代表、稀有或未见过类别的有效方法的地位。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large-scale vision-language models such as CLIP achieve strong zero-shot recognition but struggle with classes that are rarely seen during pretraining, including newly emerging entities and culturally specific categories.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
