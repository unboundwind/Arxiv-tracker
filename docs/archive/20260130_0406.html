<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-30 04:06</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260130_0406</div>
    <div class="row"><div class="card">
<div class="title">Recursive Language Models</div>
<div class="meta-line">Authors: Alex L. Zhang, Tim Kraska, Omar Khattab</div>
<div class="meta-line">First: 2025-12-31T03:43:41+00:00 · Latest: 2026-01-28T18:59:39+00:00</div>
<div class="meta-line">Comments: 9 pages, 33 with Appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.24601v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.24601v2">PDF</a> · <a href="https://github.com/alexzhang13/rlm">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study allowing large language models (LLMs) to process arbitrarily long prompts through the lens of inference-time scaling. We propose Recursive Language Models (RLMs), a general inference paradigm that treats long prompts as part of an external environment and allows the LLM to programmatically examine, decompose, and recursively call itself over snippets of the prompt. We find that RLMs can successfully process inputs up to two orders of magnitude beyond model context windows and, even for shorter prompts, dramatically outperform the quality of vanilla frontier LLMs and common long-context scaffolds across four diverse long-context tasks while having comparable cost. At a small scale, we post-train the first natively recursive language model. Our model, RLM-Qwen3-8B, outperforms the underlying Qwen3-8B model by $28.3\%$ on average and even approaches the quality of vanilla GPT-5 on three long-context tasks. Code is available at https://github.com/alexzhang13/rlm.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>递归语言模型</div>
<div class="mono" style="margin-top:8px">我们通过推理时的扩展性研究，探讨让大型语言模型（LLMs）处理任意长度提示的可行性。我们提出了递归语言模型（RLMs），这是一种通用的推理范式，将长提示视为外部环境的一部分，并允许LLM以编程方式检查、分解并递归地在提示片段上调用自身。我们发现RLMs能够成功处理长度超过模型上下文窗口两个数量级的输入，并且即使对于较短的提示，在四个不同的长上下文任务中，其表现也显著优于原始前沿LLMs和常见的长上下文框架，同时具有相当的成本。在小规模上，我们对首个原生递归语言模型进行了微调。我们的模型RLM-Qwen3-8B在平均上比基础的Qwen3-8B模型表现提升28.3%，并在三个长上下文任务中甚至接近原始GPT-5的水平。代码可在https://github.com/alexzhang13/rlm获取。</div>
</details>
</div>
<div class="card">
<div class="title">Evolutionary Strategies lead to Catastrophic Forgetting in LLMs</div>
<div class="meta-line">Authors: Immanuel Abdi, Akshat Gupta, Micah Mok, Alexander Lu, Nicholas Lee, Gopala Anumanchipalli</div>
<div class="meta-line">First: 2026-01-28T18:59:34+00:00 · Latest: 2026-01-28T18:59:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20861v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20861v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One of the biggest missing capabilities in current AI systems is the ability to learn continuously after deployment. Implementing such continually learning systems have several challenges, one of which is the large memory requirement of gradient-based algorithms that are used to train state-of-the-art LLMs. Evolutionary Strategies (ES) have recently re-emerged as a gradient-free alternative to traditional learning algorithms and have shown encouraging performance on specific tasks in LLMs. In this paper, we perform a comprehensive analysis of ES and specifically evaluate its forgetting curves when training for an increasing number of update steps. We first find that ES is able to reach performance numbers close to GRPO for math and reasoning tasks with a comparable compute budget. However, and most importantly for continual learning, the performance gains in ES is accompanied by significant forgetting of prior abilities, limiting its applicability for training models online. We also explore the reason behind this behavior and show that the updates made using ES are much less sparse and have orders of magnitude larger $\ell_2$ norm compared to corresponding GRPO updates, explaining the contrasting forgetting curves between the two algorithms. With this study, we aim to highlight the issue of forgetting in gradient-free algorithms like ES and hope to inspire future work to mitigate these issues.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>进化策略在LLMs中导致灾难性遗忘</div>
<div class="mono" style="margin-top:8px">当前AI系统中最大的缺失能力之一是在部署后持续学习的能力。实现这种持续学习系统有几个挑战，其中之一是用于训练最先进LLMs的基于梯度的算法需要大量内存。进化策略（ES）最近重新出现为传统学习算法的无梯度替代方案，并在LLMs的特定任务中表现出令人鼓舞的性能。在本文中，我们对ES进行了全面分析，并特别评估了在增加更新步数训练时其遗忘曲线。我们首先发现，ES在数学和推理任务中能够以相当的计算预算达到接近GRPO的性能水平。然而，对于持续学习而言，更重要的是，ES的性能提升伴随着先前能力的显著遗忘，这限制了其在在线训练模型中的适用性。我们还探讨了这种行为背后的原因，并表明使用ES进行的更新要远比相应的GRPO更新更加密集，且其$\ell_2$范数大几个数量级，这解释了两种算法之间遗忘曲线的差异。通过这项研究，我们旨在突出像ES这样的无梯度算法中的遗忘问题，并希望激发未来工作以缓解这些问题。</div>
</details>
</div>
<div class="card">
<div class="title">LLMStinger: Jailbreaking LLMs using RL fine-tuned LLMs</div>
<div class="meta-line">Authors: Piyush Jha, Arnav Arora, Vijay Ganesh</div>
<div class="meta-line">Venue: AAAI 2025</div>
<div class="meta-line">First: 2024-11-13T18:44:30+00:00 · Latest: 2026-01-28T18:58:57+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.08862v2">Abs</a> · <a href="https://arxiv.org/pdf/2411.08862v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce LLMStinger, a novel approach that leverages Large Language Models (LLMs) to automatically generate adversarial suffixes for jailbreak attacks. Unlike traditional methods, which require complex prompt engineering or white-box access, LLMStinger uses a reinforcement learning (RL) loop to fine-tune an attacker LLM, generating new suffixes based on existing attacks for harmful questions from the HarmBench benchmark. Our method significantly outperforms existing red-teaming approaches (we compared against 15 of the latest methods), achieving a +57.2% improvement in Attack Success Rate (ASR) on LLaMA2-7B-chat and a +50.3% ASR increase on Claude 2, both models known for their extensive safety measures. Additionally, we achieved a 94.97% ASR on GPT-3.5 and 99.4% on Gemma-2B-it, demonstrating the robustness and adaptability of LLMStinger across open and closed-source models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLMStinger：使用强化学习微调的LLM进行LLM越狱攻击</div>
<div class="mono" style="margin-top:8px">我们介绍了LLMStinger，这是一种新颖的方法，利用大型语言模型（LLMs）自动生成对抗性后缀以实施越狱攻击。与传统方法不同，LLMStinger通过强化学习（RL）循环微调攻击模型，基于HarmBench基准中的现有攻击生成有害问题的新后缀。我们的方法在攻击成功率（ASR）上显著优于现有的红队攻击方法（我们对比了15种最新方法），在LLaMA2-7B-chat模型上实现了+57.2%的ASR提升，在Claude 2模型上实现了+50.3%的ASR提升，这两个模型以广泛的安全措施著称。此外，我们在GPT-3.5和Gemma-2B-it模型上分别实现了94.97%和99.4%的ASR，展示了LLMStinger在开源和闭源模型上的鲁棒性和适应性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We introduce LLMStinger, a novel approach that leverages Large Language Models (LLMs) to automatically generate adversarial suffixes for jailbreak attacks.</div>
</details>
</div>
<div class="card">
<div class="title">ArchesClimate: Probabilistic Decadal Ensemble Generation With Flow Matching</div>
<div class="meta-line">Authors: Graham Clyne, Guillaume Couairon, Guillaume Gastineau, Claire Monteleoni, Anastase Charantonis</div>
<div class="meta-line">First: 2025-09-19T12:53:24+00:00 · Latest: 2026-01-28T18:58:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.15942v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.15942v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Climate projections have uncertainties related to components of the climate system and their interactions. A typical approach to quantifying these uncertainties is to use climate models to create ensembles of repeated simulations under different initial conditions. Due to the complexity of these simulations, generating such ensembles of projections is computationally expensive. In this work, we present ArchesClimate, a deep learning-based climate model emulator that aims to reduce this cost. ArchesClimate is trained on decadal hindcasts of the IPSL-CM6A-LR climate model at a spatial resolution of approximately 2.5x1.25 degrees. We train a flow matching model following ArchesWeatherGen, which we adapt to predict near-term climate. Once trained, the model generates states at a one-month lead time and can be used to auto-regressively emulate climate model simulations of any length. We show that for up to 10 years, these generations are stable and physically consistent. We also show that for several important climate variables, ArchesClimate generates simulations that are interchangeable with the IPSL model. This work suggests that climate model emulators could significantly reduce the cost of climate model simulations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ArchesClimate：基于流匹配的十年概率集合生成</div>
<div class="mono" style="margin-top:8px">气候预测与气候系统各组成部分及其相互作用有关的不确定性。一种典型的量化这些不确定性的方法是使用气候模型在不同初始条件下进行重复模拟生成集合。由于这些模拟的复杂性，生成这样的预测集合在计算上成本很高。本文中，我们提出了ArchesClimate，这是一种基于深度学习的气候模型模拟器，旨在降低这种成本。ArchesClimate是在IPSL-CM6A-LR气候模型的十年回算数据上训练的，空间分辨率为约2.5x1.25度。我们按照ArchesWeatherGen的方法训练了一个流匹配模型，并将其调整用于预测短期气候。训练完成后，该模型可以在一个月的提前期生成状态，并可用于自回归地模拟任意长度的气候模型输出。我们展示了在最多十年的时间范围内，这些生成结果是稳定且物理上一致的。我们还展示了对于一些重要的气候变量，ArchesClimate生成的模拟结果可以与IPSL模型互换使用。这项工作表明，气候模型模拟器可以显著降低气候模型模拟的成本。</div>
</details>
</div>
<div class="card">
<div class="title">Do Teachers Dream of GenAI Widening Educational (In)equality? Envisioning the Future of K-12 GenAI Education from Global Teachers&#x27; Perspectives</div>
<div class="meta-line">Authors: Ruiwei Xiao, Qing Xiao, Xinying Hou, Phenyo Phemelo Moletsane, Hanqi Jane Li, Hong Shen, John Stamper</div>
<div class="meta-line">First: 2025-09-13T02:03:06+00:00 · Latest: 2026-01-28T18:58:26+00:00</div>
<div class="meta-line">Comments: Accepted to CHI&#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.10782v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.10782v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative artificial intelligence (GenAI) is rapidly entering K-12 classrooms worldwide, initiating urgent debates about its potential to either reduce or exacerbate educational inequalities. Drawing on interviews with 30 K-12 teachers across the United States, South Africa, and Taiwan, this study examines how teachers navigate this GenAI tension around educational equalities. We found teachers actively framed GenAI education as an equality-oriented practice: they used it to alleviate pre-existing inequalities while simultaneously working to prevent new inequalities from emerging. Despite these efforts, teachers confronted persistent systemic barriers, i.e., unequal infrastructure, insufficient professional training, and restrictive social norms, that individual initiative alone could not overcome. Teachers thus articulated normative visions for more inclusive GenAI education. By centering teachers&#x27; practices, constraints, and future envisions, this study contributes a global account of how GenAI education is being integrated into K-12 contexts and highlights what is required to make its adoption genuinely equal.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>教师是否梦想着生成式人工智能扩大教育（不）平等？从全球教师视角展望K-12生成式人工智能教育的未来</div>
<div class="mono" style="margin-top:8px">生成式人工智能（GenAI）正迅速进入全球K-12课堂，引发了关于其是否能减少或加剧教育不平等的紧迫讨论。本研究基于对来自美国、南非和台湾的30名K-12教师的访谈，探讨教师如何应对教育平等性方面的GenAI矛盾。我们发现，教师积极将GenAI教育视为一种以平等为导向的实践：他们利用其缓解既有的不平等，同时努力防止新的不平等产生。尽管如此，教师仍面临系统性的持续障碍，如基础设施不均、专业培训不足以及限制性的社会规范，这些障碍单靠个人努力无法克服。因此，教师提出了更具包容性的GenAI教育规范性愿景。通过聚焦教师的实践、限制和未来展望，本研究提供了全球视角下GenAI教育如何融入K-12环境的分析，并强调实现其真正平等采用所需的条件。</div>
</details>
</div>
<div class="card">
<div class="title">DCP-Bench-Open: Evaluating LLMs for Constraint Modelling of Discrete Combinatorial Problems</div>
<div class="meta-line">Authors: Kostis Michailidis, Dimos Tsouros, Tias Guns</div>
<div class="meta-line">First: 2025-06-06T12:56:02+00:00 · Latest: 2026-01-28T18:58:23+00:00</div>
<div class="meta-line">Comments: This version is currently submitted and it is under review. For CP-Bench (the paper accepted at ECAI25), please refer to the previous version of this entry (v2)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.06052v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.06052v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Discrete Combinatorial Problems (DCPs) are prevalent in industrial decision-making and optimisation. However, while constraint solving technologies for DCPs have advanced significantly, the core process of formalising them, namely constraint modelling, requires significant expertise and remains a bottleneck for wider adoption. Aiming to alleviate this bottleneck, recent studies have explored using Large Language Models (LLMs) to transform combinatorial problem descriptions into executable constraint models. However, the existing evaluation datasets for discrete constraint modelling are often limited to small, homogeneous, or domain-specific problems, which do not capture the diversity of real-world scenarios. This work addresses this gap by introducing DCP-Bench-Open, a novel benchmark that includes a diverse set of well-known discrete combinatorial problems sourced from the Constraint Programming (CP) and Operations Research (OR) communities, structured explicitly for evaluating LLM-driven constraint modelling. With this dataset, and given the variety of modelling frameworks, we compare and evaluate the modelling capabilities of LLMs for three distinct constraint modelling systems, which vary in abstraction level and underlying syntax. Notably, the results show higher performance when modelling with a high-level Python-based framework. Additionally, we systematically evaluate the use of prompt-based and inference-time compute methods across different LLMs, which further increase accuracy, reaching up to 91% on this highly challenging benchmark. DCP-Bench-Open is publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DCP-Bench-Open：评估大型语言模型在离散组合问题建模中的表现</div>
<div class="mono" style="margin-top:8px">离散组合问题（DCPs）在工业决策和优化中非常普遍。然而，尽管DCP的约束求解技术有了显著进步，但其核心建模过程——约束建模——仍需要专业知识，是更广泛应用的瓶颈。为缓解这一瓶颈，近期研究探索利用大型语言模型（LLMs）将组合问题描述转换为可执行的约束模型。然而，现有的离散约束建模评估数据集往往局限于小型、同质或特定领域的题目，无法反映现实场景的多样性。本文通过引入DCP-Bench-Open这一新型基准，填补了这一空白。该基准包含来自约束编程（CP）和运筹学（OR）社区的多种知名离散组合问题，并明确结构化以评估基于LLM的约束建模能力。借助该数据集，我们比较和评估了三种不同约束建模系统中LLMs的建模能力，这些系统在抽象层次和底层语法上有所不同。值得注意的是，使用基于Python的高级框架进行建模时，性能更高。此外，我们系统地评估了不同LLMs在提示词驱动和推理时计算方法上的应用，进一步提高了准确性，最高可达91%。DCP-Bench-Open已公开发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Discrete Combinatorial Problems (DCPs) are prevalent in industrial decision-making and optimisation.</div>
</details>
</div>
<div class="card">
<div class="title">FreeFix: Boosting 3D Gaussian Splatting via Fine-Tuning-Free Diffusion Models</div>
<div class="meta-line">Authors: Hongyu Zhou, Zisen Shao, Sheng Miao, Pan Wang, Dongfeng Bai, Bingbing Liu, Yiyi Liao</div>
<div class="meta-line">First: 2026-01-28T18:56:03+00:00 · Latest: 2026-01-28T18:56:03+00:00</div>
<div class="meta-line">Comments: Our project page is at https://xdimlab.github.io/freefix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20857v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20857v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://xdimlab.github.io/freefix">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural Radiance Fields and 3D Gaussian Splatting have advanced novel view synthesis, yet still rely on dense inputs and often degrade at extrapolated views. Recent approaches leverage generative models, such as diffusion models, to provide additional supervision, but face a trade-off between generalization and fidelity: fine-tuning diffusion models for artifact removal improves fidelity but risks overfitting, while fine-tuning-free methods preserve generalization but often yield lower fidelity. We introduce FreeFix, a fine-tuning-free approach that pushes the boundary of this trade-off by enhancing extrapolated rendering with pretrained image diffusion models. We present an interleaved 2D-3D refinement strategy, showing that image diffusion models can be leveraged for consistent refinement without relying on costly video diffusion models. Furthermore, we take a closer look at the guidance signal for 2D refinement and propose a per-pixel confidence mask to identify uncertain regions for targeted improvement. Experiments across multiple datasets show that FreeFix improves multi-frame consistency and achieves performance comparable to or surpassing fine-tuning-based methods, while retaining strong generalization ability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FreeFix：通过无需微调的扩散模型提升3D高斯点云渲染</div>
<div class="mono" style="margin-top:8px">神经辐射场和3D高斯点云技术在新视角合成方面取得了进展，但仍依赖密集输入，并且在扩展视角下常常出现性能下降。近期方法利用生成模型（如扩散模型）提供额外监督，但面临泛化能力与保真度之间的权衡：对扩散模型进行微调以去除伪影可以提高保真度，但可能造成过拟合；而无需微调的方法则保持较好的泛化能力，但通常保真度较低。我们提出FreeFix，一种无需微调的方法，通过增强预训练图像扩散模型的扩展渲染能力，突破这一权衡。我们提出了一种交错的2D-3D优化策略，表明图像扩散模型可以在不依赖昂贵的视频扩散模型的情况下实现一致的优化。此外，我们深入研究了2D优化的引导信号，并提出了一种逐像素置信度掩膜，以识别不确定区域并进行针对性优化。在多个数据集上的实验表明，FreeFix在多帧一致性方面有所提升，并且其性能可与基于微调的方法相媲美甚至超越，同时保持了较强的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">SokoBench: Evaluating Long-Horizon Planning and Reasoning in Large Language Models</div>
<div class="meta-line">Authors: Sebastiano Monti, Carlo Nicolini, Gianni Pellegrini, Jacopo Staiano, Bruno Lepri</div>
<div class="meta-line">First: 2026-01-28T18:56:00+00:00 · Latest: 2026-01-28T18:56:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20856v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20856v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated. In this work, we provide a systematic assessment of the planning and long-horizon reasoning capabilities of state-of-the-art Large Reasoning Models (LRMs). We propose a novel benchmark based on Sokoban puzzles, intentionally simplified to isolate long-horizon planning from state persistence. Our findings reveal a consistent degradation in planning performance when more than 25 moves are required to reach the solution, suggesting a fundamental constraint on forward planning capacity. We show that equipping LRMs with Planning Domain Definition Language (PDDL) parsing, validation, and solving tools allows for modest improvements, suggesting inherent architectural limitations which might not be overcome by test-time scaling approaches alone.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SokoBench：评估大型语言模型中的长视距规划与推理能力</div>
<div class="mono" style="margin-top:8px">尽管大型语言模型的能力在复杂的推理任务中已被越来越多地测试，但其长视距规划能力尚未被广泛研究。在本工作中，我们对最先进的大型推理模型（LRMs）的规划和长视距推理能力进行了系统评估。我们提出了一种基于Sokoban谜题的新基准，故意简化以将长视距规划与状态持久性分离。我们的研究结果表明，当解决谜题所需的移动步骤超过25步时，规划性能会出现一致下降，这表明前向规划能力存在根本性限制。我们展示了为LRMs配备规划领域定义语言（PDDL）解析、验证和求解工具可以带来适度的性能提升，这表明其内在的架构限制可能无法仅通过测试时的扩展方法来克服。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Although the capabilities of large language models have been increasingly tested on complex reasoning tasks, their long-horizon planning abilities have not yet been extensively investigated.</div>
</details>
</div>
<div class="card">
<div class="title">From Specialist to Generalist: Unlocking SAM&#x27;s Learning Potential on Unlabeled Medical Images</div>
<div class="meta-line">Authors: Vi Vu, Thanh-Huy Nguyen, Tien-Thinh Nguyen, Ba-Thinh Lam, Hoang-Thien Nguyen, Tianyang Wang, Xingjian Li, Min Xu</div>
<div class="meta-line">Venue: ISBI 2026</div>
<div class="meta-line">First: 2026-01-25T18:13:48+00:00 · Latest: 2026-01-28T18:55:46+00:00</div>
<div class="meta-line">Comments: Accepted to ISBI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17934v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.17934v2">PDF</a> · <a href="https://github.com/vnlvi2k3/SC-SAM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Foundation models like the Segment Anything Model (SAM) show strong generalization, yet adapting them to medical images remains difficult due to domain shift, scarce labels, and the inability of Parameter-Efficient Fine-Tuning (PEFT) to exploit unlabeled data. While conventional models like U-Net excel in semi-supervised medical learning, their potential to assist a PEFT SAM has been largely overlooked. We introduce SC-SAM, a specialist-generalist framework where U-Net provides point-based prompts and pseudo-labels to guide SAM&#x27;s adaptation, while SAM serves as a powerful generalist supervisor to regularize U-Net. This reciprocal guidance forms a bidirectional co-training loop that allows both models to effectively exploit the unlabeled data. Across prostate MRI and polyp segmentation benchmarks, our method achieves state-of-the-art results, outperforming other existing semi-supervised SAM variants and even medical foundation models like MedSAM, highlighting the value of specialist-generalist cooperation for label-efficient medical image segmentation. Our code is available at https://github.com/vnlvi2k3/SC-SAM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从专家模型到通用模型：解锁SAM在未标记医学图像上的学习潜力</div>
<div class="mono" style="margin-top:8px">像Segment Anything Model (SAM)这样的基础模型表现出强大的泛化能力，但由于领域迁移、标签稀缺以及参数高效微调(PEFT)无法有效利用未标记数据，将其适配到医学图像上仍具挑战。尽管传统模型如U-Net在半监督医学学习中表现优异，但其协助PEFT SAM的潜力却常被忽视。我们提出SC-SAM，一个专家-通用模型框架，其中U-Net提供基于点的提示和伪标签以指导SAM的适配，而SAM则作为强大的通用模型监督者来正则化U-Net。这种相互指导形成了双向协同训练循环，使两个模型都能有效利用未标记数据。在前列腺MRI和息肉分割基准测试中，我们的方法取得了最先进的结果，优于其他现有的半监督SAM变体，甚至超越了医学基础模型如MedSAM，突显了专家-通用模型协作在标签高效医学图像分割中的价值。我们的代码可在https://github.com/vnlvi2k3/SC-SAM获取。</div>
</details>
</div>
<div class="card">
<div class="title">Exploring Transformer Placement in Variational Autoencoders for Tabular Data Generation</div>
<div class="meta-line">Authors: Aníbal Silva, Moisés Santos, André Restivo, Carlos Soares</div>
<div class="meta-line">First: 2026-01-28T18:54:27+00:00 · Latest: 2026-01-28T18:54:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20854v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20854v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tabular data remains a challenging domain for generative models. In particular, the standard Variational Autoencoder (VAE) architecture, typically composed of multilayer perceptrons, struggles to model relationships between features, especially when handling mixed data types. In contrast, Transformers, through their attention mechanism, are better suited for capturing complex feature interactions. In this paper, we empirically investigate the impact of integrating Transformers into different components of a VAE. We conduct experiments on 57 datasets from the OpenML CC18 suite and draw two main conclusions. First, results indicate that positioning Transformers to leverage latent and decoder representations leads to a trade-off between fidelity and diversity. Second, we observe a high similarity between consecutive blocks of a Transformer in all components. In particular, in the decoder, the relationship between the input and output of a Transformer is approximately linear.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探索变分自编码器中Transformer的位置用于表格数据生成</div>
<div class="mono" style="margin-top:8px">表格数据仍然是生成模型的一个具有挑战性的领域。特别是，标准的变分自编码器（VAE）架构通常由多层感知机组成，在建模特征之间的关系时表现不佳，尤其是在处理混合数据类型时。相比之下，Transformer通过其注意力机制更适合捕捉复杂的特征交互。本文通过实证研究探讨将Transformer集成到VAE不同组件中的影响。我们在OpenML CC18套件的57个数据集上进行了实验，并得出两个主要结论。首先，结果表明，将Transformer定位在利用潜在表示和解码器表示的位置会导致保真度与多样性之间的权衡。其次，我们观察到Transformer在所有组件中连续块之间具有高度相似性。特别是，在解码器中，Transformer的输入和输出之间的关系近似为线性。</div>
</details>
</div>
<div class="card">
<div class="title">HeuriGym: An Agentic Benchmark for LLM-Crafted Heuristics in Combinatorial Optimization</div>
<div class="meta-line">Authors: Hongzheng Chen, Yingheng Wang, Yaohui Cai, Hins Hu, Jiajie Li, Shirley Huang, Chenhui Deng, Rongjian Liang, Shufeng Kong, Haoxing Ren, Samitha Samaranayake, Carla P. Gomes, Zhiru Zhang</div>
<div class="meta-line">Venue: ICLR</div>
<div class="meta-line">First: 2025-06-09T17:46:47+00:00 · Latest: 2026-01-28T18:52:54+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR&#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.07972v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.07972v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Large Language Models (LLMs) have demonstrated significant advancements in reasoning and agent-based problem-solving, current evaluation methodologies fail to adequately assess their capabilities: existing benchmarks either rely on closed-ended questions prone to saturation and memorization, or subjective comparisons that lack consistency and rigor. In this work, we introduce HeuriGym, an agentic framework designed for evaluating heuristic algorithms generated by LLMs for combinatorial optimization problems, characterized by clearly defined objectives and expansive solution spaces. HeuriGym empowers LLMs to propose heuristics, receive evaluative feedback via code execution, and iteratively refine their solutions. We evaluate nine state-of-the-art models on nine problems across domains such as computer systems, logistics, and biology, exposing persistent limitations in tool use, planning, and adaptive reasoning. To quantify performance, we propose the Quality-Yield Index (QYI), a metric that captures both solution pass rate and quality. Even top models like GPT-o4-mini-high and Gemini-2.5-Pro attain QYI scores of only 0.6, well below the expert baseline of 1. Our open-source benchmark aims to guide the development of LLMs toward more effective and realistic problem-solving in scientific and engineering domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HeuriGym：一种用于评估LLM生成启发式算法的代理基准</div>
<div class="mono" style="margin-top:8px">尽管大型语言模型（LLMs）在推理和基于代理的问题解决方面取得了显著进展，但当前的评估方法未能充分检验其能力：现有基准要么依赖容易饱和和记忆的封闭式问题，要么是缺乏一致性和严谨性的主观比较。在本工作中，我们引入了HeuriGym，这是一个专门设计用于评估LLM在组合优化问题中生成启发式算法的代理框架，这些问题具有明确的目标和广阔的解空间。HeuriGym使LLM能够提出启发式方法，通过代码执行获得评估反馈，并迭代优化其解决方案。我们在计算机系统、物流和生物学等领域的问题上对九个最先进的模型进行了评估，揭示了在工具使用、规划和自适应推理方面持续存在的局限性。为了量化性能，我们提出了质量产出指数（QYI），该指标同时衡量了解的通过率和质量。即使是像GPT-o4-mini-high和Gemini-2.5-Pro这样的顶级模型，其QYI得分也只有0.6，远低于专家基准的1。我们的开源基准旨在引导LLM在科学和工程领域向更有效和现实的问题解决方向发展。</div>
</details>
</div>
<div class="card">
<div class="title">C3Box: A CLIP-based Class-Incremental Learning Toolbox</div>
<div class="meta-line">Authors: Hao Sun, Da-Wei Zhou</div>
<div class="meta-line">First: 2026-01-28T18:52:36+00:00 · Latest: 2026-01-28T18:52:36+00:00</div>
<div class="meta-line">Comments: The code is available at https://github.com/LAMDA-CL/C3Box</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20852v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20852v1">PDF</a> · <a href="https://github.com/LAMDA-CL/C3Box">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Traditional machine learning systems are typically designed for static data distributions, which suffer from catastrophic forgetting when learning from evolving data streams. Class-Incremental Learning (CIL) addresses this challenge by enabling learning systems to continuously learn new classes while preserving prior knowledge. With the rise of pre-trained models (PTMs) such as CLIP, leveraging their strong generalization and semantic alignment capabilities has become a promising direction in CIL. However, existing CLIP-based CIL methods are often scattered across disparate codebases, rely on inconsistent configurations, hindering fair comparisons, reproducibility, and practical adoption. Therefore, we propose C3Box (CLIP-based Class-inCremental learning toolBOX), a modular and comprehensive Python toolbox. C3Box integrates representative traditional CIL methods, ViT-based CIL methods, and state-of-the-art CLIP-based CIL methods into a unified CLIP-based framework. By inheriting the streamlined design of PyCIL, C3Box provides a JSON-based configuration and standardized execution pipeline. This design enables reproducible experimentation with low engineering overhead and makes C3Box a reliable benchmark platform for continual learning research. Designed to be user-friendly, C3Box relies only on widely used open-source libraries and supports major operating systems. The code is available at https://github.com/LAMDA-CL/C3Box.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>C3Box：基于CLIP的类别增量学习工具箱</div>
<div class="mono" style="margin-top:8px">传统的机器学习系统通常设计用于静态数据分布，在从演进数据流中学习时会遭受灾难性遗忘。类别增量学习（CIL）通过使学习系统能够持续学习新类别同时保留先前知识来解决这一挑战。随着预训练模型（PTMs）如CLIP的兴起，利用其强大的泛化能力和语义对齐能力已成为CIL的一个有前景的研究方向。然而，现有的基于CLIP的CIL方法往往分散在不同的代码库中，依赖不一致的配置，阻碍了公平比较、可重复性和实际应用。因此，我们提出了C3Box（基于CLIP的类别增量学习工具箱），一个模块化且全面的Python工具箱。C3Box将代表性的传统CIL方法、基于ViT的CIL方法以及最先进的基于CLIP的CIL方法整合到一个统一的CLIP框架中。通过继承PyCIL的简洁设计，C3Box提供了基于JSON的配置和标准化的执行流程。这种设计使得实验可重复且工程负担低，使C3Box成为持续学习研究的可靠基准平台。C3Box设计为用户友好，仅依赖广泛使用的开源库，并支持主要操作系统。代码可在https://github.com/LAMDA-CL/C3Box获取。</div>
</details>
</div>
<div class="card">
<div class="title">Splat Feature Solver</div>
<div class="meta-line">Authors: Butian Xiong, Rong Liu, Kenneth Xu, Meida Chen, Andrew Feng</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-08-17T03:13:06+00:00 · Latest: 2026-01-28T18:51:46+00:00</div>
<div class="meta-line">Comments: ICLR 2026 Accepted</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.12216v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.12216v2">PDF</a> · <a href="https://github.com/saliteta/splat-distiller/tree/main}{\textcolor{blue}{GitHub">Code1</a> · <a href="https://github.com/saliteta/splat-distiller/tree/main">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Feature lifting has emerged as a crucial component in 3D scene understanding, enabling the attachment of rich image feature descriptors (e.g., DINO, CLIP) onto splat-based 3D representations. The core challenge lies in optimally assigning rich general attributes to 3D primitives while addressing the inconsistency issues from multi-view images. We present a unified, kernel- and feature-agnostic formulation of the feature lifting problem as a sparse linear inverse problem, which can be solved efficiently in closed form. Our approach admits a provable upper bound on the global optimal error under convex losses for delivering high quality lifted features. To address inconsistencies and noise in multi-view observations, we introduce two complementary regularization strategies to stabilize the solution and enhance semantic fidelity. Tikhonov Guidance enforces numerical stability through soft diagonal dominance, while Post-Lifting Aggregation filters noisy inputs via feature clustering. Extensive experiments demonstrate that our approach achieves state-of-the-art performance on open-vocabulary 3D segmentation benchmarks, outperforming training-based, grouping-based, and heuristic-forward baselines while producing lifted features in minutes. Our \textbf{code} is available in the \href{https://github.com/saliteta/splat-distiller/tree/main}{\textcolor{blue}{GitHub}}. We provide additional \href{https://splat-distiller.pages.dev/}{\textcolor{blue}{website}} for more visualization, as well as the \href{https://www.youtube.com/watch?v=CH-G5hbvArM}{\textcolor{blue}{video}}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Splat 特征求解器</div>
<div class="mono" style="margin-top:8px">特征提升已成为3D场景理解中的关键组成部分，能够将丰富的图像特征描述符（如DINO、CLIP）附加到基于点云的3D表示上。核心挑战在于如何在多视角图像中为3D基本体分配丰富的通用属性，并解决由此产生的不一致性问题。我们提出了一种统一的、与核函数和特征无关的特征提升问题形式化方法，将其建模为稀疏线性逆问题，并可通过闭合形式高效求解。我们的方法在凸损失下提供了可证明的全局最优误差上界，以实现高质量的特征提升。为了解决多视角观测中的不一致性和噪声问题，我们引入了两种互补的正则化策略，以稳定解并增强语义保真度。Tikhonov 引导通过软对角占优性强制数值稳定性，而后提升聚合则通过特征聚类过滤噪声输入。大量实验表明，我们的方法在开放词汇3D分割基准测试中达到了最先进的性能，优于基于训练、基于分组和启发式前向的基线方法，同时在几分钟内即可生成提升后的特征。我们的\textbf{代码}可在\href{https://github.com/saliteta/splat-distiller/tree/main}{\textcolor{blue}{GitHub}}上获取。我们还提供了额外的\href{https://splat-distiller.pages.dev/}{\textcolor{blue}{网站}}用于更多可视化展示，以及\href{https://www.youtube.com/watch?v=CH-G5hbvArM}{\textcolor{blue}{视频}}。</div>
</details>
</div>
<div class="card">
<div class="title">Post-Training Fairness Control: A Single-Train Framework for Dynamic Fairness in Recommendation</div>
<div class="meta-line">Authors: Weixin Chen, Li Chen, Yuhan Zhao</div>
<div class="meta-line">Venue: WWW 2026 Oral Presentation</div>
<div class="meta-line">First: 2026-01-28T18:48:43+00:00 · Latest: 2026-01-28T18:48:43+00:00</div>
<div class="meta-line">Comments: Accepted to WWW 2026 Workshop on HCRS (Oral Presentation)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20848v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20848v1">PDF</a> · <a href="https://github.com/weixinchen98/Cofair">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite growing efforts to mitigate unfairness in recommender systems, existing fairness-aware methods typically fix the fairness requirement at training time and provide limited post-training flexibility. However, in real-world scenarios, diverse stakeholders may demand differing fairness requirements over time, so retraining for different fairness requirements becomes prohibitive. To address this limitation, we propose Cofair, a single-train framework that enables post-training fairness control in recommendation. Specifically, Cofair introduces a shared representation layer with fairness-conditioned adapter modules to produce user embeddings specialized for varied fairness levels, along with a user-level regularization term that guarantees user-wise monotonic fairness improvements across these levels. We theoretically establish that the adversarial objective of Cofair upper bounds demographic parity and the regularization term enforces progressive fairness at user level. Comprehensive experiments on multiple datasets and backbone models demonstrate that our framework provides dynamic fairness at different levels, delivering comparable or better fairness-accuracy curves than state-of-the-art baselines, without the need to retrain for each new fairness requirement. Our code is publicly available at https://github.com/weixinchen98/Cofair.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>后训练公平性控制：一种用于推荐系统动态公平性的单训练框架</div>
<div class="mono" style="margin-top:8px">尽管在推荐系统中缓解不公平性的工作不断增多，但现有的公平性感知方法通常在训练时固定公平性要求，且在训练后缺乏灵活性。然而，在现实场景中，不同利益相关者可能随时间提出不同的公平性需求，因此为不同公平性需求重新训练变得不可行。为了解决这一限制，我们提出了Cofair，一种单训练框架，能够在推荐系统训练后进行公平性控制。具体而言，Cofair引入了一个共享表示层和带有公平性条件的适配器模块，以生成适用于不同公平性级别的用户嵌入，同时引入了一个用户级的正则化项，确保这些级别之间用户层面的单调公平性提升。我们在理论上证明了Cofair的对抗性目标能够上界约束人口统计学公平性，而正则化项则在用户层面强制执行渐进式公平性。在多个数据集和主干模型上的综合实验表明，我们的框架能够在不同级别实现动态公平性，其公平性-准确性曲线优于或至少与最先进的基线方法相当，而无需为每个新的公平性需求重新训练。</div>
</details>
</div>
<div class="card">
<div class="title">Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs</div>
<div class="meta-line">Authors: Rui Pan, Zhuofu Chen, Hongyi Liu, Arvind Krishnamurthy, Ravi Netravali</div>
<div class="meta-line">First: 2025-12-23T18:16:58+00:00 · Latest: 2026-01-28T18:48:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20573v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.20573v3">PDF</a> · <a href="https://github.com/ruipeterpan/failfast">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM&#x27;s speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It &quot;fails fast&quot; by spending minimal compute in hard-to-speculate regions to shrink speculation latency and &quot;wins big&quot; by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\times$ speedup over vanilla decoding, 1.7$\times$ over the best naive dLLM drafter, and 1.7$\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>快速失败，大幅收益：通过扩散大语言模型重新思考投机解码策略</div>
<div class="mono" style="margin-top:8px">扩散大语言模型（dLLMs）能够实现快速、并行的token生成，但其独立使用存在固有的效率与质量的权衡。我们证明，如果仔细应用，dLLMs的特性实际上可以成为在使用自回归（AR）验证器进行投机解码时的优势。我们的核心洞察是，dLLM的并行解码速度大幅降低了昂贵拒绝的风险，提供了一种实用机制，以实现（难以实现的）长篇草稿，从而在投机解码中获得显著的速度提升。我们提出了FailFast，这是一个基于dLLM的投机解码框架，通过动态调整其投机长度来实现这一方法。它在难以投机的区域花费极少的计算资源，以减少投机延迟（即&quot;快速失败&quot;），并在容易投机的区域积极延长草稿长度，以减少验证延迟（在许多情况下，每次可推测并接受70个token！）。无需任何微调，FailFast即可实现自回归LLM的无损加速，并在多种模型和任务中，相比传统解码方法实现高达4.9倍的速度提升，相比最佳的朴素dLLM草稿生成器实现1.7倍的速度提升，相比EAGLE-3实现1.7倍的速度提升。我们已在https://github.com/ruipeterpan/failfast开源FailFast。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff.</div>
</details>
</div>
<div class="card">
<div class="title">A New Dataset and Framework for Robust Road Surface Classification via Camera-IMU Fusion</div>
<div class="meta-line">Authors: Willams de Lima Costa, Thifany Ketuli Silva de Souza, Jonas Ferreira Silva, Carlos Gabriel Bezerra Pereira, Bruno Reis Vila Nova, Leonardo Silvino Brito, Rafael Raider Leoni, Juliano Silva, Valter Ferreira, Sibele Miguel Soares Neto, Samantha Uehara, Daniel Giacomo, João Marcelo Teixeira, Veronica Teichrieb, Cristiano Coelho de Araújo</div>
<div class="meta-line">First: 2026-01-28T18:46:29+00:00 · Latest: 2026-01-28T18:46:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20847v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20847v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Road surface classification (RSC) is a key enabler for environment-aware predictive maintenance systems. However, existing RSC techniques often fail to generalize beyond narrow operational conditions due to limited sensing modalities and datasets that lack environmental diversity. This work addresses these limitations by introducing a multimodal framework that fuses images and inertial measurements using a lightweight bidirectional cross-attention module followed by an adaptive gating layer that adjusts modality contributions under domain shifts. Given the limitations of current benchmarks, especially regarding lack of variability, we introduce ROAD, a new dataset composed of three complementary subsets: (i) real-world multimodal recordings with RGB-IMU streams synchronized using a gold-standard industry datalogger, captured across diverse lighting, weather, and surface conditions; (ii) a large vision-only subset designed to assess robustness under adverse illumination and heterogeneous capture setups; and (iii) a synthetic subset generated to study out-of-distribution generalization in scenarios difficult to obtain in practice. Experiments show that our method achieves a +1.4 pp improvement over the previous state-of-the-art on the PVS benchmark and an +11.6 pp improvement on our multimodal ROAD subset, with consistently higher F1-scores on minority classes. The framework also demonstrates stable performance across challenging visual conditions, including nighttime, heavy rain, and mixed-surface transitions. These findings indicate that combining affordable camera and IMU sensors with multimodal attention mechanisms provides a scalable, robust foundation for road surface understanding, particularly relevant for regions where environmental variability and cost constraints limit the adoption of high-end sensing suites.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过相机-IMU融合的新数据集和框架实现稳健的路面分类</div>
<div class="mono" style="margin-top:8px">路面分类（RSC）是环境感知预测性维护系统的关键使能技术。然而，现有的RSC技术由于传感模式有限以及数据集缺乏环境多样性，往往难以泛化到狭窄的操作条件之外。本文通过引入一种多模态框架来解决这些限制，该框架使用轻量级双向交叉注意力模块融合图像和惯性测量，并通过自适应门控层调整模态贡献，以应对领域转移带来的挑战。鉴于当前基准数据集在缺乏多样性方面的局限性，我们提出了ROAD数据集，该数据集包含三个互补的子集：(i) 使用行业标准数据记录器同步采集的RGB-IMU流的真实世界多模态记录，涵盖多样化的光照、天气和路面条件；(ii) 一个大型视觉-only子集，用于评估在恶劣光照和异构采集设置下的鲁棒性；(iii) 一个合成子集，用于研究在实际难以获取的场景中模型的分布外泛化能力。实验表明，我们的方法在PVS基准上比先前的最先进方法提升了1.4个百分点，在我们的多模态ROAD子集上提升了11.6个百分点，并在少数类别上保持了更高的F1分数。该框架在夜间、大雨和混合路面过渡等具有挑战性的视觉条件下也表现出稳定的性能。这些结果表明，结合经济实惠的相机和IMU传感器与多模态注意力机制，为路面理解提供了一个可扩展且稳健的基础，特别是在环境变化大且成本限制较高的地区具有特别重要的意义。</div>
</details>
</div>
<div class="card">
<div class="title">PatchFormer: A Patch-Based Time Series Foundation Model with Hierarchical Masked Reconstruction and Cross-Domain Transfer Learning for Zero-Shot Multi-Horizon Forecasting</div>
<div class="meta-line">Authors: Olaf Yunus Laitinen Imanov, Derya Umut Kulali, Taner Yilmaz</div>
<div class="meta-line">First: 2026-01-28T18:45:45+00:00 · Latest: 2026-01-28T18:45:45+00:00</div>
<div class="meta-line">Comments: 5 pages; 2 figures; 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20845v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20845v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time series forecasting is a fundamental problem with applications in climate, energy, healthcare, and finance. Many existing approaches require domain-specific feature engineering and substantial labeled data for each task. We introduce PatchFormer, a patch-based time series foundation model that uses hierarchical masked reconstruction for self-supervised pretraining and lightweight adapters for efficient transfer. PatchFormer segments time series into patches and learns multiscale temporal representations with learnable aggregation across temporal scales. Pretraining uses masked patch reconstruction with dynamic masking and objectives that encourage both local accuracy and global consistency, followed by cross-domain knowledge distillation. Experiments on 24 benchmark datasets spanning weather, energy, traffic, finance, and healthcare demonstrate state-of-the-art zero-shot multi-horizon forecasting, reducing mean squared error by 27.3 percent relative to strong baselines while requiring 94 percent less task-specific training data. The model exhibits near log-linear scaling with more pretraining data up to 100 billion points and processes length-512 sequences 3.8x faster than full-sequence transformers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PatchFormer：一种基于片段的时间序列基础模型，结合分层掩码重建和跨领域迁移学习，用于零样本多步预测</div>
<div class="mono" style="margin-top:8px">时间序列预测是一个基础问题，在气候、能源、医疗和金融等领域有广泛应用。许多现有方法需要领域特定的特征工程和大量标注数据。我们引入了PatchFormer，一种基于片段的时间序列基础模型，利用分层掩码重建进行自监督预训练，并采用轻量级适配器实现高效迁移。PatchFormer将时间序列分割为片段，并通过可学习的跨时间尺度聚合机制学习多尺度时间表示。预训练阶段采用动态掩码和鼓励局部准确性和全局一致性的目标进行掩码片段重建，随后进行跨领域知识蒸馏。在涵盖天气、能源、交通、金融和医疗的24个基准数据集上的实验表明，PatchFormer在零样本多步预测中达到最先进的效果，相较强基线模型将均方误差降低了27.3%，同时仅需94%更少的领域特定训练数据。该模型在预训练数据达到1000亿点时表现出近对数线性扩展能力，并且处理长度为512的序列速度比完整序列Transformer快3.8倍。</div>
</details>
</div>
<div class="card">
<div class="title">$\mathbb{R}^{2k}$ is Theoretically Large Enough for Embedding-based Top-$k$ Retrieval</div>
<div class="meta-line">Authors: Zihao Wang, Hang Yin, Lihui Liu, Hanghang Tong, Yangqiu Song, Ginny Wong, Simon See</div>
<div class="meta-line">First: 2026-01-28T18:45:43+00:00 · Latest: 2026-01-28T18:45:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20844v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20844v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper studies the minimal dimension required to embed subset memberships ($m$ elements and ${m\choose k}$ subsets of at most $k$ elements) into vector spaces, denoted as Minimal Embeddable Dimension (MED). The tight bounds of MED are derived theoretically and supported empirically for various notions of &quot;distances&quot; or &quot;similarities,&quot; including the $\ell_2$ metric, inner product, and cosine similarity. In addition, we conduct numerical simulation in a more achievable setting, where the ${m\choose k}$ subset embeddings are chosen as the centroid of the embeddings of the contained elements. Our simulation easily realizes a logarithmic dependency between the MED and the number of elements to embed. These findings imply that embedding-based retrieval limitations stem primarily from learnability challenges, not geometric constraints, guiding future algorithm design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>向量空间中基于嵌入的前$k$检索理论上需要的维度$\mathbb{R}^{2k}$足够大</div>
<div class="mono" style="margin-top:8px">本文研究了将子集成员关系（$m$个元素和最多包含$k$个元素的${m\choose k}$个子集）嵌入到向量空间中所需的最小维度，称为最小可嵌入维度（MED）。我们从理论上推导了MED的紧致界限，并通过实证支持了这些界限，适用于各种&quot;距离&quot;或&quot;相似性&quot;的定义，包括$\ell_2$度量、内积和余弦相似性。此外，我们在一个更可行的设置中进行了数值模拟，其中${m\choose k}$个子集嵌入被选择为所包含元素嵌入的质心。我们的模拟结果表明，MED与嵌入元素数量之间存在对数依赖关系。这些发现表明，基于嵌入的检索限制主要源于可学习性挑战，而非几何约束，为未来算法设计提供了指导。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Researcher with Sequential Plan Reflection and Candidates Crossover (Deep Researcher Reflect Evolve)</div>
<div class="meta-line">Authors: Saurav Prateek</div>
<div class="meta-line">First: 2026-01-28T18:45:39+00:00 · Latest: 2026-01-28T18:45:39+00:00</div>
<div class="meta-line">Comments: 11 pages, 6 figures, 2 tables, source code: https://github.com/SauravP97/deep-researcher-reflect-evolve/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20843v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20843v1">PDF</a> · <a href="https://github.com/SauravP97/deep-researcher-reflect-evolve/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces a novel Deep Researcher architecture designed to generate detailed research reports on complex PhD level topics by addressing the inherent limitations of the Parallel Scaling paradigm. Our system utilizes two key innovations: Sequential Research Plan Refinement via Reflection and a Candidates Crossover algorithm. The sequential refinement process is demonstrated as an efficient method that allows the agent to maintain a centralized Global Research Context, enabling it to look back at current progress, reason about the research plan, and intelligently make changes at runtime. This dynamic adaptation contrasts with parallel approaches, which often suffer from siloed knowledge. The Candidates Crossover algorithm further enhances search efficiency by deploying multiple LLM candidates with varied parameters to explore a larger search space, with their findings synthesized to curate a comprehensive final research response. The process concludes with One Shot Report Generation, ensuring the final document is informed by a unified narrative and high fact density. Powered by the Gemini 2.5 Pro model, our Deep Researcher was evaluated on the DeepResearch Bench, a globally recognized benchmark of 100 doctoral level research tasks. Our architecture achieved an overall score of 46.21, demonstrating superior performance by surpassing leading deep research agents such as Claude Researcher, Nvidia AIQ Research Assistant, Perplexity Research, Kimi Researcher and Grok Deeper Search present on the DeepResearch Bench actively running leaderboard. This performance marginally exceeds our previous work, Static DRA, and reinforces the finding that sequential scaling consistently outperforms the parallel self consistency paradigm.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于顺序计划反思和候选者交叉的深度研究者（Deep Researcher Reflect Evolve）</div>
<div class="mono" style="margin-top:8px">本文介绍了一种新颖的深度研究者架构，旨在通过解决并行扩展范式的固有局限性，生成关于复杂博士级主题的详细研究报告。我们的系统采用了两项关键技术：通过反思实现的顺序研究计划优化和候选者交叉算法。顺序优化过程被证明是一种高效的方法，使代理能够维护一个集中的全局研究上下文，从而回顾当前进展、推理研究计划，并在运行时智能地进行调整。这种动态适应与并行方法形成对比，后者往往面临知识孤岛的问题。候选者交叉算法进一步通过部署多个参数各异的LLM候选模型，探索更大的搜索空间，并综合其结果以生成全面的最终研究回答。整个过程以一次生成报告（One Shot Report Generation）结束，确保最终文档具有统一的叙述和高事实密度。该系统基于Gemini 2.5 Pro模型，已在DeepResearch Bench上进行评估，DeepResearch Bench是全球公认的包含100个博士级研究任务的基准测试平台。我们的架构取得了46.21的总体得分，表现出色，超越了DeepResearch Bench上活跃运行的领先深度研究代理，如Claude Researcher、Nvidia AIQ Research Assistant、Perplexity Research、Kimi Researcher和Grok Deeper Search。这一表现略微优于我们之前的工作Static DRA，并进一步验证了顺序扩展在性能上持续优于并行自一致性范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces a novel Deep Researcher architecture designed to generate detailed research reports on complex PhD level topics by addressing the inherent limitations of the Parallel Scaling paradigm.</div>
</details>
</div>
<div class="card">
<div class="title">BlindSight: Harnessing Sparsity for Efficient Vision-Language Models</div>
<div class="meta-line">Authors: Tharun Adithya Srikrishnan, Deval Shah, Timothy Hein, Ahmed Hasssan, Stephen Youn, Steven K. Reinhardt</div>
<div class="meta-line">First: 2025-07-11T23:15:30+00:00 · Latest: 2026-01-28T18:45:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.09071v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.09071v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (VLMs) enable joint processing of text and images. However, incorporating vision data significantly increases the prompt length, resulting in a longer time to first token (TTFT). This bottleneck can be alleviated by leveraging the inherent sparsity in the attention computation. Analyzing these attention patterns in VLMs when processing a series of images, we observe the absence of inter-image attention in a substantial portion of layers. Based on this, we propose BlindSight: an approach to optimize multi-image VLM inference using an input-template-aware attention sparsity mask with no runtime overhead. We utilize a dataset to derive a prompt-agnostic categorization for attention heads: Dense, Sink, Intra-Image, and Intra-Image+Sink. We develop a Triton-based GPU kernel to leverage this sparsity. BlindSight achieves a 1.8-3.2x speedup in the attention computation (prompt length 36K-300K). BlindSight generalizes across VLMs (Qwen2-VL, Qwen2.5-VL, Gemma 3), with only a 0.78% absolute accuracy degradation on average on multi-image comprehension benchmarks. Finally, we advocate for the design of efficient VLMs that combine BlindSight-inspired sparse and dense layers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BlindSight：利用稀疏性优化高效视觉-语言模型</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（VLMs）能够联合处理文本和图像。然而，引入视觉数据会显著增加提示长度，导致首次令牌生成时间（TTFT）变长。通过利用注意力计算中的固有稀疏性，可以缓解这一瓶颈。在分析VLMs处理一系列图像时的注意力模式时，我们观察到大量层中缺乏跨图像注意力。基于此，我们提出BlindSight：一种利用输入模板感知的注意力稀疏掩码来优化多图像VLM推理的方法，且无运行时开销。我们利用数据集，为注意力头建立一种与提示无关的分类：密集型、汇聚型、图像内型和图像内+汇聚型。我们开发了一个基于Triton的GPU内核，以利用这种稀疏性。BlindSight在注意力计算中实现了1.8-3.2倍的加速（提示长度为36K-300K）。BlindSight在多个VLMs（Qwen2-VL、Qwen2.5-VL、Gemma 3）上具有良好的泛化能力，在多图像理解基准测试中平均仅导致0.78%的绝对准确率下降。最后，我们倡导设计结合BlindSight启发的稀疏和密集层的高效VLMs。</div>
</details>
</div>
<div class="card">
<div class="title">Reward Models Inherit Value Biases from Pretraining</div>
<div class="meta-line">Authors: Brian Christian, Jessica A. F. Thompson, Elle Michelle Yang, Vincent Adam, Hannah Rose Kirk, Christopher Summerfield, Tsvetomira Dumbalska</div>
<div class="meta-line">First: 2026-01-28T18:40:29+00:00 · Latest: 2026-01-28T18:40:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20838v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20838v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reward models (RMs) are central to aligning large language models (LLMs) with human values but have received less attention than pre-trained and post-trained LLMs themselves. Because RMs are initialized from LLMs, they inherit representations that shape their behavior, but the nature and extent of this influence remain understudied. In a comprehensive study of 10 leading open-weight RMs using validated psycholinguistic corpora, we show that RMs exhibit significant differences along multiple dimensions of human value as a function of their base model. Using the &quot;Big Two&quot; psychological axes, we show a robust preference of Llama RMs for &quot;agency&quot; and a corresponding robust preference of Gemma RMs for &quot;communion.&quot; This phenomenon holds even when the preference data and finetuning process are identical, and we trace it back to the logits of the respective instruction-tuned and pre-trained models. These log-probability differences themselves can be formulated as an implicit RM; we derive usable implicit reward scores and show that they exhibit the very same agency/communion difference. We run experiments training RMs with ablations for preference data source and quantity, which demonstrate that this effect is not only repeatable but surprisingly durable. Despite RMs being designed to represent human preferences, our evidence shows that their outputs are influenced by the pretrained LLMs on which they are based. This work underscores the importance of safety and alignment efforts at the pretraining stage, and makes clear that open-source developers&#x27; choice of base model is as much a consideration of values as of performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>奖励模型从预训练中继承价值偏见</div>
<div class="mono" style="margin-top:8px">奖励模型（RMs）在对齐大型语言模型（LLMs）与人类价值观方面起着核心作用，但相较于预训练和微调后的LLMs，它们本身却较少受到关注。由于RMs是从LLMs初始化的，因此它们继承了塑造其行为的表示，但这种影响的性质和程度仍缺乏深入研究。在一项使用经过验证的心理语言学语料库对10个领先的开源奖励模型进行的全面研究中，我们发现RMs在多个人类价值观维度上表现出显著差异，这种差异取决于其基础模型。利用“大二”心理轴，我们展示了Llama奖励模型对“自主性”的偏好，以及Gemma奖励模型对“联结性”的相应偏好。即使在偏好数据和微调过程完全相同的情况下，这一现象依然存在，我们将其归因于相应指令微调和预训练模型的logits。这些log概率差异本身可以被表述为一种隐式的奖励模型；我们推导出可用的隐式奖励分数，并表明它们表现出相同的自主性/联结性差异。我们还进行了实验，通过移除偏好数据来源和数量来训练RMs，结果表明这种效应不仅可重复，而且出人意料地持久。尽管RMs旨在表示人类偏好，但我们的证据表明，它们的输出受到其基础预训练LLMs的影响。这项工作强调了在预训练阶段进行安全性和对齐性努力的重要性，并明确指出开源开发者选择基础模型时，价值观的考量与性能同样重要。</div>
</details>
</div>
<div class="card">
<div class="title">Open-Vocabulary Functional 3D Human-Scene Interaction Generation</div>
<div class="meta-line">Authors: Jie Liu, Yu Sun, Alpar Cseke, Yao Feng, Nicolas Heron, Michael J. Black, Yan Zhang</div>
<div class="meta-line">First: 2026-01-28T18:34:25+00:00 · Latest: 2026-01-28T18:34:25+00:00</div>
<div class="meta-line">Comments: 18 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20835v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20835v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating 3D humans that functionally interact with 3D scenes remains an open problem with applications in embodied AI, robotics, and interactive content creation. The key challenge involves reasoning about both the semantics of functional elements in 3D scenes and the 3D human poses required to achieve functionality-aware interaction. Unfortunately, existing methods typically lack explicit reasoning over object functionality and the corresponding human-scene contact, resulting in implausible or functionally incorrect interactions. In this work, we propose FunHSI, a training-free, functionality-driven framework that enables functionally correct human-scene interactions from open-vocabulary task prompts. Given a task prompt, FunHSI performs functionality-aware contact reasoning to identify functional scene elements, reconstruct their 3D geometry, and model high-level interactions via a contact graph. We then leverage vision-language models to synthesize a human performing the task in the image and estimate proposed 3D body and hand poses. Finally, the proposed 3D body configuration is refined via stage-wise optimization to ensure physical plausibility and functional correctness. In contrast to existing methods, FunHSI not only synthesizes more plausible general 3D interactions, such as &quot;sitting on a sofa&#x27;&#x27;, while supporting fine-grained functional human-scene interactions, e.g., &quot;increasing the room temperature&#x27;&#x27;. Extensive experiments demonstrate that FunHSI consistently generates functionally correct and physically plausible human-scene interactions across diverse indoor and outdoor scenes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>开放词汇功能导向的三维人-场景交互生成</div>
<div class="mono" style="margin-top:8px">生成能够功能性地与三维场景交互的三维人类仍然是一个开放性问题，其应用涵盖具身人工智能、机器人和交互式内容创作等领域。关键挑战在于对三维场景中功能性元素的语义进行推理，以及确定实现功能感知交互所需的三维人体姿态。不幸的是，现有方法通常缺乏对物体功能及其对应的人-场景接触的显式推理，导致生成的交互不合理或功能错误。在本文中，我们提出FunHSI，这是一个无需训练的功能驱动框架，能够从开放词汇的任务提示中生成功能正确的交互。给定一个任务提示，FunHSI执行功能感知的接触推理，以识别功能性场景元素，重建其三维几何结构，并通过接触图建模高层交互。随后，我们利用视觉-语言模型合成执行该任务的人类图像，并估计所提出的三维身体和手部姿态。最后，通过分阶段优化对提出的三维身体配置进行细化，以确保物理合理性和功能正确性。与现有方法相比，FunHSI不仅能够合成更合理的通用三维交互，如&quot;坐在沙发上&quot;，还支持细粒度的功能性人-场景交互，如&quot;提高房间温度&quot;。大量实验表明，FunHSI能够在各种室内和室外场景中持续生成功能正确且物理合理的交互。</div>
</details>
</div>
<div class="card">
<div class="title">Discrete Variational Autoencoding via Policy Search</div>
<div class="meta-line">Authors: Michael Drolet, Firas Al-Hafez, Aditya Bhatt, Jan Peters, Oleg Arenz</div>
<div class="meta-line">First: 2025-09-29T12:44:05+00:00 · Latest: 2026-01-28T18:33:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.24716v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.24716v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Discrete latent bottlenecks in variational autoencoders (VAEs) offer high bit efficiency and can be modeled with autoregressive discrete distributions, enabling parameter-efficient multimodal search with transformers. However, discrete random variables do not allow for exact differentiable parameterization; therefore, discrete VAEs typically rely on approximations, such as Gumbel-Softmax reparameterization or straight-through gradient estimates, or employ high-variance gradient-free methods such as REINFORCE that have had limited success on high-dimensional tasks such as image reconstruction. Inspired by popular techniques in policy search, we propose a training framework for discrete VAEs that leverages the natural gradient of a non-parametric encoder to update the parametric encoder without requiring reparameterization. Our method, combined with automatic step size adaptation and a transformer-based encoder, scales to challenging datasets such as ImageNet and outperforms both approximate reparameterization methods and quantization-based discrete autoencoders in reconstructing high-dimensional data from compact latent spaces.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过策略搜索实现离散变分自编码器</div>
<div class="mono" style="margin-top:8px">变分自编码器（VAEs）中的离散潜在瓶颈提供了高比特效率，并可以通过自回归离散分布进行建模，从而利用变压器实现参数高效的多模态搜索。然而，离散随机变量不允许精确的可微参数化；因此，离散VAEs通常依赖于近似方法，如Gumbel-Softmax重参数化或直通梯度估计，或者采用高方差的无梯度方法，如REINFORCE，在诸如图像重建等高维任务中取得的成果有限。受策略搜索中流行技术的启发，我们提出了一种用于离散VAEs的训练框架，该框架利用非参数编码器的自然梯度来更新参数编码器，而无需重参数化。我们的方法结合自动步长调整和基于变压器的编码器，可扩展到具有挑战性的数据集（如ImageNet），并在从紧凑潜在空间重建高维数据方面优于近似重参数化方法和基于量化离散自编码器。</div>
</details>
</div>
<div class="card">
<div class="title">Linear representations in language models can change dramatically over a conversation</div>
<div class="meta-line">Authors: Andrew Kyle Lampinen, Yuxuan Li, Eghbal Hosseini, Sangnie Bhardwaj, Murray Shanahan</div>
<div class="meta-line">First: 2026-01-28T18:33:17+00:00 · Latest: 2026-01-28T18:33:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20834v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20834v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language model representations often contain linear directions that correspond to high-level concepts. Here, we study the dynamics of these representations: how representations evolve along these dimensions within the context of (simulated) conversations. We find that linear representations can change dramatically over a conversation; for example, information that is represented as factual at the beginning of a conversation can be represented as non-factual at the end and vice versa. These changes are content-dependent; while representations of conversation-relevant information may change, generic information is generally preserved. These changes are robust even for dimensions that disentangle factuality from more superficial response patterns, and occur across different model families and layers of the model. These representation changes do not require on-policy conversations; even replaying a conversation script written by an entirely different model can produce similar changes. However, adaptation is much weaker from simply having a sci-fi story in context that is framed more explicitly as such. We also show that steering along a representational direction can have dramatically different effects at different points in a conversation. These results are consistent with the idea that representations may evolve in response to the model playing a particular role that is cued by a conversation. Our findings may pose challenges for interpretability and steering -- in particular, they imply that it may be misleading to use static interpretations of features or directions, or probes that assume a particular range of features consistently corresponds to a particular ground-truth value. However, these types of representational dynamics also point to exciting new research directions for understanding how models adapt to context.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言模型中的线性表示在对话中可能发生显著变化</div>
<div class="mono" style="margin-top:8px">语言模型的表示通常包含对应于高层次概念的线性方向。本文研究这些表示的动态变化：在（模拟）对话的上下文中，这些表示如何沿着这些维度演变。我们发现，线性表示在对话过程中可能发生显著变化；例如，对话开始时被表示为事实性的信息可能在对话结束时被表示为非事实性的，反之亦然。这些变化是内容依赖的；虽然与对话相关的信息的表示可能发生变化，但通用信息通常被保留。即使对于将事实性与更表面的响应模式解耦的维度，这些变化也是稳健的，并且在不同模型家族和模型层中均会发生。这些表示变化不需要在策略上的对话；即使重放由完全不同的模型编写的对话脚本，也可以产生类似的变化。然而，仅仅在上下文中提供一个被更明确地框定为科幻故事的情节，其适应性会显著减弱。我们还表明，在对话的不同阶段，沿着表示方向的引导可能会产生截然不同的效果。这些结果与表示可能根据模型在对话中扮演的特定角色而演变的观点一致。我们的发现可能对可解释性和引导提出挑战——特别是，它们暗示使用静态的特征或方向解释，或假设特定特征范围始终对应特定真实值的探针可能是误导性的。然而，这些类型的表示动态也指出了理解模型如何适应上下文的新研究方向。</div>
</details>
</div>
<div class="card">
<div class="title">Idea2Story: An Automated Pipeline for Transforming Research Concepts into Complete Scientific Narratives</div>
<div class="meta-line">Authors: Tengyue Xu, Zhuoyang Qian, Gaoge Liu, Li Ling, Zhentao Zhang, Biao Wu, Shuo Zhang, Ke Lu, Wei Shi, Ziqi Wang, Zheng Feng, Yan Luo, Shu Xu, Yongjin Chen, Zhibo Feng, Zhuo Chen, Bruce Yuan, Harry Wang, Kris Chen</div>
<div class="meta-line">First: 2026-01-28T18:31:54+00:00 · Latest: 2026-01-28T18:31:54+00:00</div>
<div class="meta-line">Comments: 11 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20833v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20833v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows. However, existing systems largely rely on runtime-centric execution paradigms, repeatedly reading, summarizing, and reasoning over large volumes of scientific literature online. This on-the-spot computation strategy incurs high computational cost, suffers from context window limitations, and often leads to brittle reasoning and hallucination. We propose Idea2Story, a pre-computation-driven framework for autonomous scientific discovery that shifts literature understanding from online reasoning to offline knowledge construction. Idea2Story continuously collects peer-reviewed papers together with their review feedback, extracts core methodological units, composes reusable research patterns, and organizes them into a structured methodological knowledge graph. At runtime, underspecified user research intents are aligned to established research paradigms, enabling efficient retrieval and reuse of high-quality research patterns instead of open-ended generation and trial-and-error. By grounding research planning and execution in a pre-built knowledge graph, Idea2Story alleviates the context window bottleneck of LLMs and substantially reduces repeated runtime reasoning over literature. We conduct qualitative analyses and preliminary empirical studies demonstrating that Idea2Story can generate coherent, methodologically grounded, and novel research patterns, and can produce several high-quality research demonstrations in an end-to-end setting. These results suggest that offline knowledge construction provides a practical and scalable foundation for reliable autonomous scientific discovery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Idea2Story：一种将研究概念转化为完整科学叙事的自动化流程</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的自主科学发现代理最近取得了显著进展，展示了自动化端到端研究工作流程的能力。然而，现有系统主要依赖于以运行时为中心的执行范式，反复在线阅读、总结和推理大量科学文献。这种即时计算策略带来了高昂的计算成本，受到上下文窗口限制，并且常常导致脆弱的推理和幻觉。我们提出了Idea2Story，这是一个以预计算驱动的自主科学发现框架，将文献理解从在线推理转移到离线知识构建。Idea2Story持续收集同行评审论文及其评审反馈，提取核心方法单元，构建可重用的研究模式，并将它们组织成结构化的知识图谱。在运行时，未明确指定的用户研究意图被对齐到已建立的研究范式，从而实现高效检索和重用高质量研究模式，而不是开放式的生成和试错。通过将研究规划和执行建立在预构建的知识图谱之上，Idea2Story缓解了LLM的上下文窗口瓶颈，并显著减少了对文献的重复运行时推理。我们进行了定性分析和初步实证研究，证明Idea2Story能够生成连贯、方法论基础且新颖的研究模式，并且在端到端设置下能够生成多个高质量的研究演示。这些结果表明，离线知识构建为可靠自主科学发现提供了实用且可扩展的基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Autonomous scientific discovery with large language model (LLM)-based agents has recently made substantial progress, demonstrating the ability to automate end-to-end research workflows.</div>
</details>
</div>
<div class="card">
<div class="title">FLOL: Fast Baselines for Real-World Low-Light Enhancement</div>
<div class="meta-line">Authors: Juan C. Benito, Daniel Feijoo, Alvaro Garcia, Marcos V. Conde</div>
<div class="meta-line">First: 2025-01-16T18:06:09+00:00 · Latest: 2026-01-28T18:31:35+00:00</div>
<div class="meta-line">Comments: Journal Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.09718v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.09718v2">PDF</a> · <a href="https://github.com/cidautai/FLOL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Low-Light Image Enhancement (LLIE) is a key task in computational photography and imaging. The problem of enhancing images captured during night or in dark environments has been well-studied in the computer vision literature. However, current deep learning-based solutions struggle with efficiency and robustness for real-world scenarios (e.g., scenes with noise, saturated pixels). We propose a lightweight neural network that combines image processing in the frequency and spatial domains. Our baseline method, FLOL, is one of the fastest models for this task, achieving results comparable to the state-of-the-art on popular real-world benchmarks such as LOLv2, LSRW, MIT-5K and UHD-LL. Moreover, we are able to process 1080p images in real-time under 12ms. Code and models at https://github.com/cidautai/FLOL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FLOL：面向现实场景的低光照增强快速基线</div>
<div class="mono" style="margin-top:8px">低光照图像增强（LLIE）是计算摄影和成像中的关键任务。在计算机视觉文献中，增强夜间或黑暗环境中捕获的图像问题已被广泛研究。然而，当前基于深度学习的解决方案在现实场景中的效率和鲁棒性仍存在挑战（例如噪声场景和饱和像素）。我们提出了一种轻量级神经网络，结合了频域和空域的图像处理方法。我们的基线方法FLOL是该任务中最快的模型之一，在LOLv2、LSRW、MIT-5K和UHD-LL等流行现实场景基准上取得了与最先进方法相当的成果。此外，我们能够在12ms内实时处理1080p图像。代码和模型请访问https://github.com/cidautai/FLOL</div>
</details>
</div>
<div class="card">
<div class="title">MemCtrl: Using MLLMs as Active Memory Controllers on Embodied Agents</div>
<div class="meta-line">Authors: Vishnu Sashank Dorbala, Dinesh Manocha</div>
<div class="meta-line">First: 2026-01-28T18:31:17+00:00 · Latest: 2026-01-28T18:31:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20831v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20831v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Foundation models rely on in-context learning for personalized decision making. The limited size of this context window necessitates memory compression and retrieval systems like RAG. These systems however often treat memory as large offline storage spaces, which is unfavorable for embodied agents that are expected to operate under strict memory and compute constraints, online. In this work, we propose MemCtrl, a novel framework that uses Multimodal Large Language Models (MLLMs) for pruning memory online. MemCtrl augments MLLMs with a trainable memory head μthat acts as a gate to determine which observations or reflections to retain, update, or discard during exploration. We evaluate with training two types of μ, 1) via an offline expert, and 2) via online RL, and observe significant improvement in overall embodied task completion ability on μ-augmented MLLMs. In particular, on augmenting two low performing MLLMs with MemCtrl on multiple subsets of the EmbodiedBench benchmark, we observe that μ-augmented MLLMs show an improvement of around 16% on average, with over 20% on specific instruction subsets. Finally, we present a qualitative analysis on the memory fragments collected by μ, noting the superior performance of μaugmented MLLMs on long and complex instruction types.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MemCtrl：在具身智能体中使用多模态大语言模型作为主动内存控制器</div>
<div class="mono" style="margin-top:8px">基础模型依赖上下文学习来进行个性化决策。由于上下文窗口的大小有限，需要使用如RAG这样的记忆压缩和检索系统。然而，这些系统通常将记忆视为离线的大存储空间，这对于需要在严格的内存和计算限制下在线运行的具身智能体是不利的。在本工作中，我们提出了MemCtrl，一个新颖的框架，利用多模态大语言模型（MLLMs）在线修剪记忆。MemCtrl通过一个可训练的记忆头μ来增强MLLMs，该头作为门控机制，决定在探索过程中保留、更新或丢弃哪些观察或反思。我们通过训练两种类型的μ进行评估，1）通过离线专家，2）通过在线强化学习，并观察到在μ增强的MLLMs上整体具身任务完成能力有显著提升。特别是，在对EmbodiedBench基准的多个子集上增强两个表现不佳的MLLMs，我们发现μ增强的MLLMs平均提升了约16%，在特定指令子集上甚至超过20%。最后，我们对μ收集的记忆片段进行了定性分析，指出μ增强的MLLMs在长且复杂的指令类型上表现出卓越的性能。</div>
</details>
</div>
<div class="card">
<div class="title">VSCOUT: A Hybrid Variational Autoencoder Approach to Outlier Detection in High-Dimensional Retrospective Monitoring</div>
<div class="meta-line">Authors: Waldyn G. Martinez</div>
<div class="meta-line">First: 2026-01-28T18:30:48+00:00 · Latest: 2026-01-28T18:30:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20830v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20830v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern industrial and service processes generate high-dimensional, non-Gaussian, and contamination-prone data that challenge the foundational assumptions of classical Statistical Process Control (SPC). Heavy tails, multimodality, nonlinear dependencies, and sparse special-cause observations can distort baseline estimation, mask true anomalies, and prevent reliable identification of an in-control (IC) reference set. To address these challenges, we introduce VSCOUT, a distribution-free framework designed specifically for retrospective (Phase I) monitoring in high-dimensional settings. VSCOUT combines an Automatic Relevance Determination Variational Autoencoder (ARD-VAE) architecture with ensemble-based latent outlier filtering and changepoint detection. The ARD prior isolates the most informative latent dimensions, while the ensemble and changepoint filters identify pointwise and structural contamination within the determined latent space. A second-stage retraining step removes flagged observations and re-estimates the latent structure using only the retained inliers, mitigating masking and stabilizing the IC latent manifold. This two-stage refinement produces a clean and reliable IC baseline suitable for subsequent Phase II deployment. Extensive experiments across benchmark datasets demonstrate that VSCOUT achieves superior sensitivity to special-cause structure while maintaining controlled false alarms, outperforming classical SPC procedures, robust estimators, and modern machine-learning baselines. Its scalability, distributional flexibility, and resilience to complex contamination patterns position VSCOUT as a practical and effective method for retrospective modeling and anomaly detection in AI-enabled environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VSCOUT：一种用于高维回顾性监控中异常检测的混合变分自编码器方法</div>
<div class="mono" style="margin-top:8px">现代工业和服务流程生成高维、非高斯分布且易受污染的数据，这些数据挑战了经典统计过程控制（SPC）的基本假设。重尾分布、多模态性、非线性依赖关系以及稀疏的特殊原因观测值可能会扭曲基线估计，掩盖真实的异常，并阻碍可靠地识别处于控制状态（IC）的参考集。为了解决这些挑战，我们引入了VSCOUT，这是一种无分布假设的框架，专门设计用于高维环境下的回顾性（第一阶段）监控。VSCOUT结合了自动相关性确定变分自编码器（ARD-VAE）架构，以及基于集成的潜在空间异常过滤和变化点检测。ARD先验可以隔离最具信息量的潜在维度，而集成和变化点过滤器则在确定的潜在空间中识别点异常和结构污染。第二阶段的再训练步骤通过移除被标记的观测值，并仅使用保留的内点重新估计潜在结构，从而缓解掩码效应并稳定IC潜在流形。这种两阶段的优化方法产生了一个干净且可靠的IC基线，适用于后续的第二阶段部署。在多个基准数据集上的广泛实验表明，VSCOUT在保持受控误报率的同时，对特殊原因结构具有更高的敏感性，优于经典SPC方法、稳健估计器和现代机器学习基线。其可扩展性、分布灵活性以及对复杂污染模式的鲁棒性使VSCOUT成为AI环境下的回顾性建模和异常检测的实用且有效方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modern industrial and service processes generate high-dimensional, non-Gaussian, and contamination-prone data that challenge the foundational assumptions of classical Statistical Process Control (SPC).</div>
</details>
</div>
<div class="card">
<div class="title">Online Conformal Model Selection for Nonstationary Time Series</div>
<div class="meta-line">Authors: Shibo Li, Yao Zheng</div>
<div class="meta-line">First: 2025-06-05T19:45:52+00:00 · Latest: 2026-01-28T18:29:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.05544v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.05544v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper introduces the MPS (Model Prediction Set), a novel framework for online model selection for nonstationary time series. Classical model selection methods, such as information criteria and cross-validation, rely heavily on the stationarity assumption and often fail in dynamic environments which undergo gradual or abrupt changes over time. Yet real-world data are rarely stationary, and model selection under nonstationarity remains a largely open problem. To tackle this challenge, we combine conformal inference with model confidence sets to develop a procedure that adaptively selects models best suited to the evolving dynamics at any given time. Concretely, the MPS updates in real time a confidence set of candidate models that covers the best model for the next time period with a specified long-run probability, while adapting to nonstationarity of unknown forms. Through simulations and real-world data analysis, we demonstrate that MPS reliably and efficiently identifies optimal models under nonstationarity, an essential capability lacking in offline methods. Moreover, MPS frequently produces high-quality sets with small cardinality, whose evolution offers deeper insights into changing dynamics. As a generic framework, MPS accommodates any data-generating process, data structure, model class, training method, and evaluation metric, making it broadly applicable across diverse problem settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非平稳时间序列的在线一致性模型选择</div>
<div class="mono" style="margin-top:8px">本文介绍了MPS（模型预测集），一种用于非平稳时间序列在线模型选择的新框架。经典模型选择方法，如信息准则和交叉验证，严重依赖平稳性假设，往往在经历渐进或突变变化的动态环境中失效。然而，现实世界的数据很少是平稳的，模型选择在非平稳性下仍然是一个主要的开放问题。为了解决这一挑战，我们结合一致性推断与模型置信集，开发了一种能够根据时间演变动态选择最佳模型的程序。具体而言，MPS实时更新候选模型的置信集，以指定的长期概率覆盖下一个时间周期的最佳模型，同时适应未知形式的非平稳性。通过模拟和现实数据的分析，我们证明了MPS在非平稳性下能够可靠且高效地识别最优模型，这是离线方法所缺乏的关键能力。此外，MPS经常产生高质量且小规模的模型集，其演变提供了对变化动态的更深入见解。作为一种通用框架，MPS适用于任何数据生成过程、数据结构、模型类别、训练方法和评估指标，使其在各种问题设置中具有广泛的应用性。</div>
</details>
</div>
<div class="card">
<div class="title">Training Reasoning Models on Saturated Problems via Failure-Prefix Conditioning</div>
<div class="meta-line">Authors: Minwu Kim, Safal Shrestha, Keith Ross</div>
<div class="meta-line">First: 2026-01-28T18:29:21+00:00 · Latest: 2026-01-28T18:29:21+00:00</div>
<div class="meta-line">Comments: 16 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.20829v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.20829v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning with Verifiable Rewards (RLVR) has substantially improved the reasoning abilities of large language models (LLMs), yet training often stalls as problems become saturated. We identify the core challenge as the poor accessibility of informative failures: learning signals exist but are rarely encountered during standard rollouts. To address this, we propose failure-prefix conditioning, a simple and effective method for learning from saturated problems. Rather than starting from the original question, our approach reallocates exploration by conditioning training on prefixes derived from rare incorrect reasoning trajectories, thereby exposing the model to failure-prone states. We observe that failure-prefix conditioning yields performance gains matching those of training on medium-difficulty problems, while preserving token efficiency. Furthermore, we analyze the model&#x27;s robustness, finding that our method reduces performance degradation under misleading failure prefixes, albeit with a mild trade-off in adherence to correct early reasoning. Finally, we demonstrate that an iterative approach, which refreshes failure prefixes during training, unlocks additional gains after performance plateaus. Overall, our results suggest that failure-prefix conditioning offers an effective pathway to extend RLVR training on saturated problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过失败前缀条件化在饱和问题上训练推理模型</div>
<div class="mono" style="margin-top:8px">可验证奖励的强化学习（RLVR）显著提升了大语言模型（LLMs）的推理能力，但训练常常在问题变得饱和时停滞。我们识别出核心挑战是信息性失败的获取困难：学习信号存在，但在标准 rollout 过程中很少遇到。为了解决这一问题，我们提出了失败前缀条件化，这是一种简单且有效的从饱和问题中学习的方法。我们的方法不同于从原始问题开始，而是通过条件化训练，利用罕见的错误推理轨迹生成的前缀来重新分配探索，从而使模型接触到容易失败的状态。我们观察到，失败前缀条件化所带来的性能提升与在中等难度问题上训练相当，同时保持了 token 效率。此外，我们分析了模型的鲁棒性，发现我们的方法在面对误导性失败前缀时减少了性能退化，尽管在遵循正确早期推理方面存在轻微的权衡。最后，我们证明了一种迭代方法，即在训练过程中刷新失败前缀，可以在性能达到平台期后进一步提升效果。总体而言，我们的结果表明，失败前缀条件化为在饱和问题上扩展 RLVR 训练提供了一条有效的途径。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260129_0407.html">20260129_0407</a>
<a href="archive/20260128_0407.html">20260128_0407</a>
<a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
