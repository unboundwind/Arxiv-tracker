<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-22 04:05</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260122_0405</div>
    <div class="row"><div class="card">
<div class="title">Implicit Neural Representation Facilitates Unified Universal Vision Encoding</div>
<div class="meta-line">Authors: Matthew Gwilliam, Xiao Wang, Xuefeng Hu, Zhenheng Yang</div>
<div class="meta-line">First: 2026-01-20T18:59:57+00:00 · Latest: 2026-01-20T18:59:57+00:00</div>
<div class="meta-line">Comments: 18 pages, 16 tables, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14256v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14256v1">PDF</a> · <a href="https://github.com/tiktok/huvr">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Models for image representation learning are typically designed for either recognition or generation. Various forms of contrastive learning help models learn to convert images to embeddings that are useful for classification, detection, and segmentation. On the other hand, models can be trained to reconstruct images with pixel-wise, perceptual, and adversarial losses in order to learn a latent space that is useful for image generation. We seek to unify these two directions with a first-of-its-kind model that learns representations which are simultaneously useful for recognition and generation. We train our model as a hyper-network for implicit neural representation, which learns to map images to model weights for fast, accurate reconstruction. We further integrate our INR hyper-network with knowledge distillation to improve its generalization and performance. Beyond the novel training design, the model also learns an unprecedented compressed embedding space with outstanding performance for various visual tasks. The complete model competes with state-of-the-art results for image representation learning, while also enabling generative capabilities with its high-quality tiny embeddings. The code is available at https://github.com/tiktok/huvr.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>隐式神经表示促进统一的通用视觉编码</div>
<div class="mono" style="margin-top:8px">图像表示学习的模型通常设计用于识别或生成。各种对比学习形式帮助模型学习将图像转换为对分类、检测和分割有用的嵌入。另一方面，模型可以通过像素级、感知级和对抗损失进行训练，以学习对图像生成有用的潜在空间。我们提出了一种开创性的模型，旨在统一这两个方向，使模型能够同时学习对识别和生成都有用的表示。我们将模型训练为一个用于隐式神经表示的超网络，该网络能够快速准确地将图像映射到模型权重。我们进一步将INR超网络与知识蒸馏结合，以提升其泛化能力和性能。除了新颖的训练设计，该模型还学习了一个前所未有的压缩嵌入空间，并在各种视觉任务中表现出色。完整的模型在图像表示学习方面与最先进的结果竞争，同时其高质量的微型嵌入也使模型具备生成能力。代码可在https://github.com/tiktok/huvr获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Models for image representation learning are typically designed for either recognition or generation.</div>
</details>
</div>
<div class="card">
<div class="title">VideoMaMa: Mask-Guided Video Matting via Generative Prior</div>
<div class="meta-line">Authors: Sangbeom Lim, Seoung Wug Oh, Jiahui Huang, Heeji Yoon, Seungryong Kim, Joon-Young Lee</div>
<div class="meta-line">First: 2026-01-20T18:59:56+00:00 · Latest: 2026-01-20T18:59:56+00:00</div>
<div class="meta-line">Comments: Project page: https://cvlab-kaist.github.io/VideoMaMa/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14255v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14255v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://cvlab-kaist.github.io/VideoMaMa/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generalizing video matting models to real-world videos remains a significant challenge due to the scarcity of labeled data. To address this, we present Video Mask-to-Matte Model (VideoMaMa) that converts coarse segmentation masks into pixel accurate alpha mattes, by leveraging pretrained video diffusion models. VideoMaMa demonstrates strong zero-shot generalization to real-world footage, even though it is trained solely on synthetic data. Building on this capability, we develop a scalable pseudo-labeling pipeline for large-scale video matting and construct the Matting Anything in Video (MA-V) dataset, which offers high-quality matting annotations for more than 50K real-world videos spanning diverse scenes and motions. To validate the effectiveness of this dataset, we fine-tune the SAM2 model on MA-V to obtain SAM2-Matte, which outperforms the same model trained on existing matting datasets in terms of robustness on in-the-wild videos. These findings emphasize the importance of large-scale pseudo-labeled video matting and showcase how generative priors and accessible segmentation cues can drive scalable progress in video matting research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoMaMa: 通过生成先验进行掩膜引导的视频抠图</div>
<div class="mono" style="margin-top:8px">由于标注数据稀缺，将视频抠图模型泛化到真实世界视频仍是一个重大挑战。为了解决这一问题，我们提出了Video Mask-to-Matte Model（VideoMaMa），通过利用预训练的视频扩散模型，将粗略的分割掩膜转换为像素精确的alpha抠图。VideoMaMa在真实世界视频上展现出强大的零样本泛化能力，即使其仅在合成数据上进行训练。基于这一能力，我们开发了一个可扩展的伪标注流水线用于大规模视频抠图，并构建了Matting Anything in Video（MA-V）数据集，该数据集为超过50,000个真实世界视频提供了高质量的抠图标注，涵盖多样化的场景和动作。为了验证该数据集的有效性，我们在MA-V上微调SAM2模型，得到SAM2-Matte，其在真实世界视频上的鲁棒性优于在现有抠图数据集上训练的相同模型。这些发现突显了大规模伪标注视频抠图的重要性，并展示了生成先验和可获取的分割线索如何推动视频抠图研究的可扩展进展。</div>
</details>
</div>
<div class="card">
<div class="title">Motion 3-to-4: 3D Motion Reconstruction for 4D Synthesis</div>
<div class="meta-line">Authors: Hongyuan Chen, Xingyu Chen, Youjia Zhang, Zexiang Xu, Anpei Chen</div>
<div class="meta-line">First: 2026-01-20T18:59:48+00:00 · Latest: 2026-01-20T18:59:48+00:00</div>
<div class="meta-line">Comments: Project page: https://motion3-to-4.github.io/. Code: https://github.com/Inception3D/Motion324</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14253v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14253v1">PDF</a> · <a href="https://github.com/Inception3D/Motion324">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://motion3-to-4.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh. While recent advances have significantly improved 2D, video, and 3D content generation, 4D synthesis remains difficult due to limited training data and the inherent ambiguity of recovering geometry and motion from a monocular viewpoint. Motion 3-to-4 addresses these challenges by decomposing 4D synthesis into static 3D shape generation and motion reconstruction. Using a canonical reference mesh, our model learns a compact motion latent representation and predicts per-frame vertex trajectories to recover complete, temporally coherent geometry. A scalable frame-wise transformer further enables robustness to varying sequence lengths. Evaluations on both standard benchmarks and a new dataset with accurate ground-truth geometry show that Motion 3-to-4 delivers superior fidelity and spatial consistency compared to prior work. Project page is available at https://motion3-to-4.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Motion 3-to-4：从单目视频和可选3D参考网格合成高质量4D动态物体</div>
<div class="mono" style="margin-top:8px">我们提出了Motion 3-to-4，这是一个前馈框架，能够从单目视频和可选的3D参考网格中合成高质量的4D动态物体。尽管近期在2D、视频和3D内容生成方面取得了显著进展，但由于训练数据有限以及从单目视角恢复几何和运动的固有歧义性，4D合成仍然具有挑战性。Motion 3-to-4通过将4D合成分解为静态3D形状生成和运动重建来解决这些问题。利用规范化的参考网格，我们的模型学习了一个紧凑的运动潜在表示，并预测每帧顶点轨迹以恢复完整且时间一致的几何结构。一个可扩展的帧级Transformer进一步增强了模型对不同序列长度的鲁棒性。在标准基准数据集和一个具有准确地面真实几何的新数据集上的评估表明，Motion 3-to-4在保真度和空间一致性方面优于以往的工作。项目页面可在https://motion3-to-4.github.io/访问。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present Motion 3-to-4, a feed-forward framework for synthesising high-quality 4D dynamic objects from a single monocular video and an optional 3D reference mesh.</div>
</details>
</div>
<div class="card">
<div class="title">LightOnOCR: A 1B End-to-End Multilingual Vision-Language Model for State-of-the-Art OCR</div>
<div class="meta-line">Authors: Said Taghadouini, Adrien Cavaillès, Baptiste Aubertin</div>
<div class="meta-line">First: 2026-01-20T18:58:32+00:00 · Latest: 2026-01-20T18:58:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14251v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14251v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present \textbf{LightOnOCR-2-1B}, a 1B-parameter end-to-end multilingual vision--language model that converts document images (e.g., PDFs) into clean, naturally ordered text without brittle OCR pipelines. Trained on a large-scale, high-quality distillation mix with strong coverage of scans, French documents, and scientific PDFs, LightOnOCR-2 achieves state-of-the-art results on OlmOCR-Bench while being 9$\times$ smaller and substantially faster than prior best-performing models. We further extend the output format to predict normalized bounding boxes for embedded images, introducing localization during pretraining via a resume strategy and refining it with RLVR using IoU-based rewards. Finally, we improve robustness with checkpoint averaging and task-arithmetic merging. We release model checkpoints under Apache 2.0, and publicly release the dataset and \textbf{LightOnOCR-bbox-bench} evaluation under their respective licenses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LightOnOCR：面向最先进OCR的10亿参数端到端多语言视觉-语言模型</div>
<div class="mono" style="margin-top:8px">我们提出了\textbf{LightOnOCR-2-1B}，这是一个10亿参数的端到端多语言视觉-语言模型，能够将文档图像（如PDF）转换为干净且自然排序的文本，无需脆弱的OCR流水线。该模型在大规模、高质量的蒸馏混合数据集上进行训练，涵盖扫描文档、法语文件和科学PDF，具有广泛的覆盖范围。LightOnOCR-2在OlmOCR-Bench上取得了最先进的结果，同时比之前表现最好的模型小9倍且显著更快。我们进一步扩展了输出格式，以预测嵌入图像的归一化边界框，并通过恢复策略在预训练过程中引入定位功能，再利用基于IoU的奖励进行RLVR优化。最后，我们通过检查点平均和任务算术合并来提升模型的鲁棒性。我们将在Apache 2.0许可下发布模型检查点，并公开发布数据集和\textbf{LightOnOCR-bbox-bench}评估工具。</div>
</details>
</div>
<div class="card">
<div class="title">OmniTransfer: All-in-one Framework for Spatio-temporal Video Transfer</div>
<div class="meta-line">Authors: Pengze Zhang, Yanze Wu, Mengtian Li, Xu Bai, Songtao Zhao, Fulong Ye, Chong Mou, Xinghui Li, Zhuowei Chen, Qian He, Mingyuan Gao</div>
<div class="meta-line">First: 2026-01-20T18:58:11+00:00 · Latest: 2026-01-20T18:58:11+00:00</div>
<div class="meta-line">Comments: Github Page: https://pangzecheung.github.io/OmniTransfer/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14250v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14250v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://pangzecheung.github.io/OmniTransfer/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Videos convey richer information than images or text, capturing both spatial and temporal dynamics. However, most existing video customization methods rely on reference images or task-specific temporal priors, failing to fully exploit the rich spatio-temporal information inherent in videos, thereby limiting flexibility and generalization in video generation. To address these limitations, we propose OmniTransfer, a unified framework for spatio-temporal video transfer. It leverages multi-view information across frames to enhance appearance consistency and exploits temporal cues to enable fine-grained temporal control. To unify various video transfer tasks, OmniTransfer incorporates three key designs: Task-aware Positional Bias that adaptively leverages reference video information to improve temporal alignment or appearance consistency; Reference-decoupled Causal Learning separating reference and target branches to enable precise reference transfer while improving efficiency; and Task-adaptive Multimodal Alignment using multimodal semantic guidance to dynamically distinguish and tackle different tasks. Extensive experiments show that OmniTransfer outperforms existing methods in appearance (ID and style) and temporal transfer (camera movement and video effects), while matching pose-guided methods in motion transfer without using pose, establishing a new paradigm for flexible, high-fidelity video generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OmniTransfer：时空视频迁移的全能框架</div>
<div class="mono" style="margin-top:8px">视频比图像或文本传达更丰富的信息，能够捕捉空间和时间动态。然而，大多数现有的视频定制方法依赖于参考图像或任务特定的时间先验，未能充分利用视频中固有的丰富时空信息，从而限制了视频生成的灵活性和泛化能力。为了解决这些限制，我们提出了OmniTransfer，一个统一的时空视频迁移框架。它利用帧间的多视角信息来增强外观一致性，并通过时间线索实现精细的时间控制。为了统一各种视频迁移任务，OmniTransfer包含三个关键设计：任务感知的位置偏差，通过自适应利用参考视频信息来提升时间对齐或外观一致性；参考解耦的因果学习，将参考和目标分支分离，以实现精确的参考迁移并提高效率；以及任务自适应的多模态对齐，使用多模态语义引导动态区分和处理不同任务。大量实验表明，OmniTransfer在外观（身份和风格）和时间迁移（摄像机运动和视频效果）方面优于现有方法，同时在不使用姿态的情况下匹配姿态引导方法的运动迁移效果，为灵活、高保真视频生成建立了一个新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Videos convey richer information than images or text, capturing both spatial and temporal dynamics.</div>
</details>
</div>
<div class="card">
<div class="title">Soft Tail-dropping for Adaptive Visual Tokenization</div>
<div class="meta-line">Authors: Zeyuan Chen, Kai Zhang, Zhuowen Tu, Yuanjun Xiong</div>
<div class="meta-line">First: 2026-01-20T18:57:19+00:00 · Latest: 2026-01-20T18:57:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14246v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14246v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Soft Tail-dropping Adaptive Tokenizer (STAT), a 1D discrete visual tokenizer that adaptively chooses the number of output tokens per image according to its structural complexity and level of detail. STAT encodes an image into a sequence of discrete codes together with per-token keep probabilities. Beyond standard autoencoder objectives, we regularize these keep probabilities to be monotonically decreasing along the sequence and explicitly align their distribution with an image-level complexity measure. As a result, STAT produces length-adaptive 1D visual tokens that are naturally compatible with causal 1D autoregressive (AR) visual generative models. On ImageNet-1k, equipping vanilla causal AR models with STAT yields competitive or superior visual generation quality compared to other probabilistic model families, while also exhibiting favorable scaling behavior that has been elusive in prior vanilla AR visual generation attempts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Soft Tail-dropping 自适应视觉分词器</div>
<div class="mono" style="margin-top:8px">我们提出了一种 Soft Tail-dropping 自适应分词器（STAT），这是一种 1D 离散视觉分词器，能够根据图像的结构复杂度和细节程度自适应地选择每张图像的输出分词数量。STAT 将图像编码为一系列离散代码，并附带每个分词的保留概率。除了标准的自编码器目标外，我们还对这些保留概率进行正则化，使其在序列中单调递减，并显式地将其分布与图像级别的复杂度度量对齐。因此，STAT 生成的 1D 视觉分词具有长度自适应性，自然兼容因果 1D 自回归（AR）视觉生成模型。在 ImageNet-1k 数据集上，将普通的因果 AR 模型与 STAT 结合使用，能够实现与其他概率模型家族相当或更优的视觉生成质量，同时展现出良好的扩展性，这是之前普通 AR 视觉生成尝试中难以实现的。</div>
</details>
</div>
<div class="card">
<div class="title">Jet-RL: Enabling On-Policy FP8 Reinforcement Learning with Unified Training and Rollout Precision Flow</div>
<div class="meta-line">Authors: Haocheng Xi, Charlie Ruan, Peiyuan Liao, Yujun Lin, Han Cai, Yilong Zhao, Shuo Yang, Kurt Keutzer, Song Han, Ligeng Zhu</div>
<div class="meta-line">First: 2026-01-20T18:54:31+00:00 · Latest: 2026-01-20T18:54:31+00:00</div>
<div class="meta-line">Comments: 11 pages, 6 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14243v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14243v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is essential for enhancing the complex reasoning capabilities of large language models (LLMs). However, existing RL training pipelines are computationally inefficient and resource-intensive, with the rollout phase accounting for over 70% of total training time. Quantized RL training, particularly using FP8 precision, offers a promising approach to mitigating this bottleneck. A commonly adopted strategy applies FP8 precision during rollout while retaining BF16 precision for training. In this work, we present the first comprehensive study of FP8 RL training and demonstrate that the widely used BF16-training + FP8-rollout strategy suffers from severe training instability and catastrophic accuracy collapse under long-horizon rollouts and challenging tasks. Our analysis shows that these failures stem from the off-policy nature of the approach, which introduces substantial numerical mismatch between training and inference. Motivated by these observations, we propose Jet-RL, an FP8 RL training framework that enables robust and stable RL optimization. The key idea is to adopt a unified FP8 precision flow for both training and rollout, thereby minimizing numerical discrepancies and eliminating the need for inefficient inter-step calibration. Extensive experiments validate the effectiveness of Jet-RL: our method achieves up to 33% speedup in the rollout phase, up to 41% speedup in the training phase, and a 16% end-to-end speedup over BF16 training, while maintaining stable convergence across all settings and incurring negligible accuracy degradation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Jet-RL：通过统一训练与 rollout 精度流实现基于策略的 FP8 强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）对于增强大语言模型（LLMs）的复杂推理能力至关重要。然而，现有的 RL 训练流程在计算效率和资源消耗方面存在不足，其中 rollout 阶段占总训练时间的 70% 以上。使用 FP8 精度进行量化 RL 训练提供了一种缓解这一瓶颈的有前景方法。一种常见的策略是在 rollout 阶段使用 FP8 精度，而在训练阶段保留 BF16 精度。在本工作中，我们首次对 FP8 RL 训练进行了全面研究，并表明广泛使用的 BF16 训练 + FP8 rollout 策略在长时间跨度 rollout 和具有挑战性的任务中会遭受严重的训练不稳定性和灾难性精度崩溃。我们的分析表明，这些失败源于该方法的 off-policy 特性，这在训练和推理之间引入了显著的数值不匹配。基于这些观察，我们提出了 Jet-RL，一个支持稳健和稳定 RL 优化的 FP8 训练框架。其核心思想是采用统一的 FP8 精度流，既用于训练也用于 rollout，从而最小化数值差异并消除不必要的 inter-step 校准需求。大量实验验证了 Jet-RL 的有效性：我们的方法在 rollout 阶段实现了最高 33% 的加速，在训练阶段实现了最高 41% 的加速，并在 BF16 训练基础上实现了 16% 的端到端加速，同时在所有设置下保持了稳定的收敛性，并且精度损失可以忽略不计。</div>
</details>
</div>
<div class="card">
<div class="title">APEX-Agents</div>
<div class="meta-line">Authors: Bertie Vidgen, Austin Mann, Abby Fennelly, John Wright Stanly, Lucas Rothman, Marco Burstein, Julien Benchek, David Ostrofsky, Anirudh Ravichandran, Debnil Sur, Neel Venugopal, Alannah Hsia, Isaac Robinson, Calix Huang, Olivia Varones, Daniyal Khan, Michael Haines, Zach Richards, Chirag Mahapatra, Brendan Foody, Osvald Nitski</div>
<div class="meta-line">First: 2026-01-20T18:53:44+00:00 · Latest: 2026-01-20T18:53:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14242v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14242v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce the AI Productivity Index for Agents (APEX-Agents), a benchmark for assessing whether AI agents can execute long-horizon, cross-application tasks created by investment banking analysts, management consultants, and corporate lawyers. APEX-Agents requires agents to navigate realistic work environments with files and tools. We test eight agents for the leaderboard using Pass@1. Gemini 3 Flash (Thinking=High) achieves the highest score of 24.0%, followed by GPT-5.2 (Thinking=High), Claude Opus 4.5 (Thinking=High), and Gemini 3 Pro (Thinking=High). We open source the APEX-Agents benchmark (n=480) with all prompts, rubrics, gold outputs, files, and metadata. We also open-source Archipelago, our infrastructure for agent execution and evaluation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>APEX-Agents</div>
<div class="mono" style="margin-top:8px">我们引入了AI生产力指数（APEX-Agents），这是一个评估AI代理是否能够执行由投资银行分析师、管理咨询顾问和公司律师创建的长周期、跨应用任务的基准。APEX-Agents要求代理在包含文件和工具的真实工作环境中进行操作。我们使用Pass@1对八个代理进行测试以生成排行榜，其中Gemini 3 Flash（Thinking=High）得分最高，为24.0%，其次是GPT-5.2（Thinking=High）、Claude Opus 4.5（Thinking=High）和Gemini 3 Pro（Thinking=High）。我们开源了APEX-Agents基准（n=480），包含所有提示、评分标准、黄金输出、文件和元数据。我们还开源了Archipelago，这是我们的代理执行与评估基础设施。</div>
</details>
</div>
<div class="card">
<div class="title">LRC-DHVC: Towards Local Rate Control in Neural Video Compression</div>
<div class="meta-line">Authors: Marc Windsheimer, Simon Deniffel, André Kaup</div>
<div class="meta-line">First: 2026-01-20T18:51:53+00:00 · Latest: 2026-01-20T18:51:53+00:00</div>
<div class="meta-line">Comments: 5 pages, 5 figures, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14240v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14240v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Local rate control is a key enabler to generalize image and video compression for dedicated challenges, such as video coding for machines. While traditional hybrid video coding can easily adapt the local rate-distortion trade-off by changing the local quantization parameter, no such approach is currently available for learning-based video compression. In this paper, we propose LRC-DHVC, a hierarchical video compression network, which allows continuous local rate control on a pixel level to vary the spatial quality distribution within individual video frames. This is achieved by concatenating a quality map to the input frame and applying a weighted MSE loss which matches the pixelwise trade-off factors in the quality map. During training, the model sees a variety of quality maps due to a constrained-random generation. Our model is the first neural video compression network, which can continuously and spatially adapt to varying quality constraints. Due to the wide quality and bit rate range, a single set of network parameters is sufficient. Compared to single rate point networks, which scale linearly with the number of rate points, the memory requirements for our network parameters remain constant. The code and model are available at link-updated-upon-acceptance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LRC-DHVC：面向神经视频压缩的本地速率控制</div>
<div class="mono" style="margin-top:8px">本地速率控制是实现图像和视频压缩以应对特定挑战（如机器视频编码）的关键。虽然传统混合视频编码可以通过调整局部量化参数轻松适应局部速率-失真权衡，但目前尚无适用于学习型视频压缩的方法。本文提出LRC-DHVC，一种分层视频压缩网络，允许在像素级别进行连续的本地速率控制，从而在单个视频帧内调整空间质量分布。这是通过将质量图与输入帧连接，并应用与质量图中像素级权衡因子匹配的加权均方误差损失实现的。在训练过程中，模型会看到由受限随机生成的各种质量图。我们的模型是首个能够连续且空间适应不同质量约束的神经视频压缩网络。由于其广泛的质量和比特率范围，一套网络参数就足以满足需求。与随速率点数量线性扩展的单速率点网络相比，我们的网络参数的内存需求保持不变。代码和模型可在接受后更新的链接中获取。</div>
</details>
</div>
<div class="card">
<div class="title">Spatiotemporal Wildfire Prediction and Reinforcement Learning for Helitack Suppression</div>
<div class="meta-line">Authors: Shaurya Mathur, Shreyas Bellary Manjunath, Nitin Kulkarni, Alina Vereshchaka</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2026-01-20T18:50:12+00:00 · Latest: 2026-01-20T18:50:12+00:00</div>
<div class="meta-line">Comments: 6 pages, 5 figures (two of them in tables), Conference: IEEE International Conference on Machine Learning and Applications 2025 (ICMLA 2025): https://www.icmla-conference.org/icmla25/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14238v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14238v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sites.google.com/view/firecastrl">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Wildfires are growing in frequency and intensity, devastating ecosystems and communities while causing billions of dollars in suppression costs and economic damage annually in the U.S. Traditional wildfire management is mostly reactive, addressing fires only after they are detected. We introduce \textit{FireCastRL}, a proactive artificial intelligence (AI) framework that combines wildfire forecasting with intelligent suppression strategies. Our framework first uses a deep spatiotemporal model to predict wildfire ignition. For high-risk predictions, we deploy a pre-trained reinforcement learning (RL) agent to execute real-time suppression tactics with helitack units inside a physics-informed 3D simulation. The framework generates a threat assessment report to help emergency responders optimize resource allocation and planning. In addition, we are publicly releasing a large-scale, spatiotemporal dataset containing $\mathbf{9.5}$ million samples of environmental variables for wildfire prediction. Our work demonstrates how deep learning and RL can be combined to support both forecasting and tactical wildfire response. More details can be found at https://sites.google.com/view/firecastrl.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于时空预测与强化学习的直升机灭火预警系统</div>
<div class="mono" style="margin-top:8px">野火的频率和强度正在增加，每年在美国造成数十亿美元的灭火成本和经济损失，同时破坏生态系统和社区。传统的野火管理大多是反应性的，仅在火灾被发现后才进行应对。我们引入了\textit{FireCastRL}，这是一个主动的人工智能（AI）框架，结合了野火预测与智能灭火策略。我们的框架首先使用深度时空模型预测野火点火。对于高风险预测，我们部署一个预训练的强化学习（RL）代理，在物理驱动的3D模拟中实时执行直升机灭火战术。该框架生成威胁评估报告，以帮助应急响应人员优化资源分配和规划。此外，我们还公开发布了一个大规模的时空数据集，包含\mathbf{9.5}百万个环境变量样本，用于野火预测。我们的工作展示了深度学习和强化学习如何结合以支持野火的预测和战术响应。更多详情请访问 https://sites.google.com/view/firecastrl。</div>
</details>
</div>
<div class="card">
<div class="title">Partial Linearity in Categories</div>
<div class="meta-line">Authors: Roy Ferguson, Zurab Janelidze</div>
<div class="meta-line">First: 2026-01-20T18:48:04+00:00 · Latest: 2026-01-20T18:48:04+00:00</div>
<div class="meta-line">Comments: 6 pages, first draft</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14237v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14237v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper we study partial linearity in a category by replacing isomorphism between coproducts and products in a linear category with isomorphism between suitable monoidal structures on a category. The main results a coherence theorem and a generalization of the theory of central morphisms from unital categories to our context of partial linearity</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>范畴中的部分线性性</div>
<div class="mono" style="margin-top:8px">本文通过将线性范畴中积与余积之间的同构替换为范畴上适当monoidal结构之间的同构，研究范畴中的部分线性性。主要结果是一个相容性定理以及将中心态射理论从单元范畴推广到我们部分线性性上下文中的一个推广</div>
</details>
</div>
<div class="card">
<div class="title">Opportunities in AI/ML for the Rubin LSST Dark Energy Science Collaboration</div>
<div class="meta-line">Authors: LSST Dark Energy Science Collaboration, Eric Aubourg, Camille Avestruz, Matthew R. Becker, Biswajit Biswas, Rahul Biswas, Boris Bolliet, Adam S. Bolton, Clecio R. Bom, Raphaël Bonnet-Guerrini, Alexandre Boucaud, Jean-Eric Campagne, Chihway Chang, Aleksandra Ćiprijanović, Johann Cohen-Tanugi, Michael W. Coughlin, John Franklin Crenshaw, Juan C. Cuevas-Tello, Juan de Vicente, Seth W. Digel, Steven Dillmann, Mariano Javier de León Dominguez Romero, Alex Drlica-Wagner, Sydney Erickson, Alexander T. Gagliano, Christos Georgiou, Aritra Ghosh, Matthew Grayling, Kirill A. Grishin, Alan Heavens, Lindsay R. House, Mustapha Ishak, Wassim Kabalan, Arun Kannawadi, François Lanusse, C. Danielle Leonard, Pierre-François Léget, Michelle Lochner, Yao-Yuan Mao, Peter Melchior, Grant Merz, Martin Millon, Anais Möller, Gautham Narayan, Yuuki Omori, Hiranya Peiris, Laurence Perreault-Levasseur, Andrés A. Plazas Malagón, Nesar Ramachandra, Benjamin Remy, Cécile Roucelle, Jaime Ruiz-Zapatero, Stefan Schuldt, Ignacio Sevilla-Noarbe, Ved G. Shah, Tjitske Starkenburg, Stephen Thorp, Laura Toribio San Cipriano, Tilman Tröster, Roberto Trotta, Padma Venkatraman, Amanda Wasserman, Tim White, Justine Zeghal, Tianqing Zhang, Yuanyuan Zhang</div>
<div class="meta-line">First: 2026-01-20T18:46:42+00:00 · Latest: 2026-01-20T18:46:42+00:00</div>
<div class="meta-line">Comments: 84 pages. This is v1.0 of the DESC&#x27;s white paper on AI/ML, a collaboration document that is being made public but which is not planned for submission to a journal</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14235v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14235v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Vera C. Rubin Observatory&#x27;s Legacy Survey of Space and Time (LSST) will produce unprecedented volumes of heterogeneous astronomical data (images, catalogs, and alerts) that challenge traditional analysis pipelines. The LSST Dark Energy Science Collaboration (DESC) aims to derive robust constraints on dark energy and dark matter from these data, requiring methods that are statistically powerful, scalable, and operationally reliable. Artificial intelligence and machine learning (AI/ML) are already embedded across DESC science workflows, from photometric redshifts and transient classification to weak lensing inference and cosmological simulations. Yet their utility for precision cosmology hinges on trustworthy uncertainty quantification, robustness to covariate shift and model misspecification, and reproducible integration within scientific pipelines. This white paper surveys the current landscape of AI/ML across DESC&#x27;s primary cosmological probes and cross-cutting analyses, revealing that the same core methodologies and fundamental challenges recur across disparate science cases. Since progress on these cross-cutting challenges would benefit multiple probes simultaneously, we identify key methodological research priorities, including Bayesian inference at scale, physics-informed methods, validation frameworks, and active learning for discovery. With an eye on emerging techniques, we also explore the potential of the latest foundation model methodologies and LLM-driven agentic AI systems to reshape DESC workflows, provided their deployment is coupled with rigorous evaluation and governance. Finally, we discuss critical software, computing, data infrastructure, and human capital requirements for the successful deployment of these new methodologies, and consider associated risks and opportunities for broader coordination with external actors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>人工智能/机器学习在鲁宾天文台LSST暗能量科学合作中的机遇</div>
<div class="mono" style="margin-top:8px">维拉·鲁宾天文台的时空遗产巡天（LSST）将产生前所未有的异构天文学数据（图像、目录和警报），这挑战了传统的数据分析流程。LSST暗能量科学合作（DESC）旨在从这些数据中得出对暗能量和暗物质的稳健约束，需要统计功效高、可扩展且操作可靠的分析方法。人工智能和机器学习（AI/ML）已广泛嵌入DESC的科学流程中，从光度红移、瞬变分类到弱引力透镜推断和宇宙学模拟。然而，它们在精确宇宙学中的效用依赖于可信赖的不确定性量化、对协变量偏移和模型误设的鲁棒性，以及在科学流程中可重复集成的能力。本文白皮书调查了AI/ML在DESC主要宇宙学探测和跨领域分析中的现状，揭示了在不同科学案例中，相同的核心方法和基本挑战反复出现。由于在这些跨领域挑战上的进展可以同时惠及多个探测手段，我们确定了关键的方法研究优先事项，包括大规模贝叶斯推断、物理引导方法、验证框架以及用于发现的主动学习。同时，我们还探讨了最新基础模型方法和由大语言模型驱动的自主AI系统在重塑DESC流程中的潜力，前提是这些技术的部署必须与严格的评估和治理相结合。最后，我们讨论了成功部署这些新方法所需的关键软件、计算、数据基础设施和人力资源要求，并考虑了与外部机构进行更广泛协调所带来的相关风险和机遇。</div>
</details>
</div>
<div class="card">
<div class="title">Q-learning with Adjoint Matching</div>
<div class="meta-line">Authors: Qiyang Li, Sergey Levine</div>
<div class="meta-line">First: 2026-01-20T18:45:34+00:00 · Latest: 2026-01-20T18:45:34+00:00</div>
<div class="meta-line">Comments: 32 pages, 8 figures, 7 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14234v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14234v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose Q-learning with Adjoint Matching (QAM), a novel TD-based reinforcement learning (RL) algorithm that tackles a long-standing challenge in continuous-action RL: efficient optimization of an expressive diffusion or flow-matching policy with respect to a parameterized Q-function. Effective optimization requires exploiting the first-order information of the critic, but it is challenging to do so for flow or diffusion policies because direct gradient-based optimization via backpropagation through their multi-step denoising process is numerically unstable. Existing methods work around this either by only using the value and discarding the gradient information, or by relying on approximations that sacrifice policy expressivity or bias the learned policy. QAM sidesteps both of these challenges by leveraging adjoint matching, a recently proposed technique in generative modeling, which transforms the critic&#x27;s action gradient to form a step-wise objective function that is free from unstable backpropagation, while providing an unbiased, expressive policy at the optimum. Combined with temporal-difference backup for critic learning, QAM consistently outperforms prior approaches on hard, sparse reward tasks in both offline and offline-to-online RL.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于伴随匹配的Q学习</div>
<div class="mono" style="margin-top:8px">我们提出了一种基于时序差分（TD）的新型强化学习（RL）算法——伴随匹配Q学习（QAM），用于解决连续动作强化学习中的一个长期挑战：相对于参数化的Q函数，高效优化具有表达能力的扩散或流匹配策略。有效的优化需要利用批评者的梯度信息，但对流或扩散策略而言，直接通过其多步去噪过程进行梯度优化会导致数值不稳定。现有方法要么仅使用价值信息而丢弃梯度信息，要么依赖牺牲策略表达能力或导致学习策略偏差的近似方法。QAM通过利用生成建模中最近提出的技术——伴随匹配，将批评者的动作梯度转换为一个无不稳定反向传播的分步目标函数，同时在最优解处提供无偏且具有表达能力的策略。结合批评者学习的时序差分备份，QAM在离线和离线到在线的强化学习任务中，特别是在稀疏奖励的困难任务上，始终优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">KAGE-Bench: Fast Known-Axis Visual Generalization Evaluation for Reinforcement Learning</div>
<div class="meta-line">Authors: Egor Cherepanov, Daniil Zelezetsky, Alexey K. Kovalev, Aleksandr I. Panov</div>
<div class="meta-line">First: 2026-01-20T18:44:28+00:00 · Latest: 2026-01-20T18:44:28+00:00</div>
<div class="meta-line">Comments: 38 pages, 44 figures, 3 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14232v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14232v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://avanturist322.github.io/KAGEBench/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pixel-based reinforcement learning agents often fail under purely visual distribution shift even when latent dynamics and rewards are unchanged, but existing benchmarks entangle multiple sources of shift and hinder systematic analysis. We introduce KAGE-Env, a JAX-native 2D platformer that factorizes the observation process into independently controllable visual axes while keeping the underlying control problem fixed. By construction, varying a visual axis affects performance only through the induced state-conditional action distribution of a pixel policy, providing a clean abstraction for visual generalization. Building on this environment, we define KAGE-Bench, a benchmark of six known-axis suites comprising 34 train-evaluation configuration pairs that isolate individual visual shifts. Using a standard PPO-CNN baseline, we observe strong axis-dependent failures, with background and photometric shifts often collapsing success, while agent-appearance shifts are comparatively benign. Several shifts preserve forward motion while breaking task completion, showing that return alone can obscure generalization failures. Finally, the fully vectorized JAX implementation enables up to 33M environment steps per second on a single GPU, enabling fast and reproducible sweeps over visual factors. Code: https://avanturist322.github.io/KAGEBench/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KAGE-Bench：用于强化学习的快速已知轴视觉泛化评估</div>
<div class="mono" style="margin-top:8px">基于像素的强化学习智能体在纯视觉分布变化下经常失败，即使潜在动态和奖励保持不变，但现有基准测试将多个分布变化源纠缠在一起，阻碍了系统的分析。我们引入了KAGE-Env，这是一个JAX原生的2D平台游戏环境，它将观察过程分解为独立可控的视觉轴，同时保持底层控制问题不变。通过这种设计，改变一个视觉轴仅会影响像素策略所诱导的状态条件动作分布，从而提供一个清晰的视觉泛化抽象。基于此环境，我们定义了KAGE-Bench，这是一个包含六个已知轴套件的基准测试，共34个训练-评估配置对，用于隔离个体视觉变化。使用标准的PPO-CNN基线，我们观察到强烈的轴依赖性失败，背景和光度变化通常导致成功崩溃，而智能体外观变化则相对无害。一些变化保持了前进运动但破坏了任务完成，表明仅凭回报可能掩盖泛化失败。最后，完全向量化的JAX实现使得单个GPU每秒可处理高达3300万环境步骤，从而实现快速且可重复的视觉因素扫描。代码：https://avanturist322.github.io/KAGEBench/</div>
</details>
</div>
<div class="card">
<div class="title">MASCOT: Towards Multi-Agent Socio-Collaborative Companion Systems</div>
<div class="meta-line">Authors: Yiyang Wang, Yiqiao Jin, Alex Cabral, Josiah Hester</div>
<div class="meta-line">First: 2026-01-20T18:44:04+00:00 · Latest: 2026-01-20T18:44:04+00:00</div>
<div class="meta-line">Comments: 15 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14230v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14230v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent systems (MAS) have recently emerged as promising socio-collaborative companions for emotional and cognitive support. However, these systems frequently suffer from persona collapse--where agents revert to generic, homogenized assistant behaviors--and social sycophancy, which produces redundant, non-constructive dialogue. We propose MASCOT, a generalizable framework for multi-perspective socio-collaborative companions. MASCOT introduces a novel bi-level optimization strategy to harmonize individual and collective behaviors: 1) Persona-Aware Behavioral Alignment, an RLAIF-driven pipeline that finetunes individual agents for strict persona fidelity to prevent identity loss; and 2) Collaborative Dialogue Optimization, a meta-policy guided by group-level rewards to ensure diverse and productive discourse. Extensive evaluations across psychological support and workplace domains demonstrate that MASCOT significantly outperforms state-of-the-art baselines, achieving improvements of up to +14.1 in Persona Consistency and +10.6 in Social Contribution. Our framework provides a practical roadmap for engineering the next generation of socially intelligent multi-agent systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MASCOT：面向多智能体社会协作陪伴系统的框架</div>
<div class="mono" style="margin-top:8px">多智能体系统（MAS）最近被提出作为情感和认知支持的社会协作陪伴系统。然而，这些系统经常面临人格崩溃的问题——智能体回归为通用、同质化的助手行为——以及社会谄媚现象，导致冗余且无建设性的对话。我们提出了MASCOT，一个适用于多视角社会协作陪伴的通用框架。MASCOT引入了一种新颖的双层优化策略，以协调个体与集体行为：1）人格感知的行为对齐，这是一种基于强化学习与自适应推理框架（RLAIF）的流程，通过微调个体智能体以确保严格的人格一致性，防止身份丢失；2）协作对话优化，一种由群体级奖励引导的元策略，以确保多样且富有成效的对话。在心理支持和职场领域的广泛评估表明，MASCOT显著优于现有最先进的基线方法，在人格一致性方面提升了+14.1，在社会贡献方面提升了+10.6。我们的框架为构建下一代社会智能多智能体系统提供了实用的工程路线图。</div>
</details>
</div>
<div class="card">
<div class="title">Attention-Based Offline Reinforcement Learning and Clustering for Interpretable Sepsis Treatment</div>
<div class="meta-line">Authors: Punit Kumar, Vaibhav Saran, Divyesh Patel, Nitin Kulkarni, Alina Vereshchaka</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2026-01-20T18:41:44+00:00 · Latest: 2026-01-20T18:41:44+00:00</div>
<div class="meta-line">Comments: 8 pages, 6 figures, Conference: IEEE International Conference on Machine Learning and Applications 2025 (ICMLA 2025): https://www.icmla-conference.org/icmla25/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14228v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14228v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sepsis remains one of the leading causes of mortality in intensive care units, where timely and accurate treatment decisions can significantly impact patient outcomes. In this work, we propose an interpretable decision support framework. Our system integrates four core components: (1) a clustering-based stratification module that categorizes patients into low, intermediate, and high-risk groups upon ICU admission, using clustering with statistical validation; (2) a synthetic data augmentation pipeline leveraging variational autoencoders (VAE) and diffusion models to enrich underrepresented trajectories such as fluid or vasopressor administration; (3) an offline reinforcement learning (RL) agent trained using Advantage Weighted Regression (AWR) with a lightweight attention encoder and supported by an ensemble models for conservative, safety-aware treatment recommendations; and (4) a rationale generation module powered by a multi-modal large language model (LLM), which produces natural-language justifications grounded in clinical context and retrieved expert knowledge. Evaluated on the MIMIC-III and eICU datasets, our approach achieves high treatment accuracy while providing clinicians with interpretable and robust policy recommendations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于注意力机制的离线强化学习与聚类在可解释脓毒症治疗中的应用</div>
<div class="mono" style="margin-top:8px">脓毒症仍然是重症监护病房（ICU）患者死亡的主要原因之一，及时且准确的治疗决策对患者预后有显著影响。本文提出了一种可解释的决策支持框架。我们的系统集成了四个核心模块：(1) 一个基于聚类的分层模块，利用具有统计验证的聚类方法，在ICU入院时将患者分为低、中、高风险组；(2) 一个合成数据增强管道，利用变分自编码器（VAE）和扩散模型来丰富如液体或血管加压药使用等代表性不足的轨迹数据；(3) 一个离线强化学习（RL）智能体，使用轻量级注意力编码器和集成模型进行训练，以提供保守且安全的治疗建议；(4) 一个基于多模态大语言模型（LLM）的推理生成模块，该模块根据临床背景和检索到的专家知识生成自然语言的解释。我们在MIMIC-III和eICU数据集上评估了该方法，实现了高治疗准确性，同时为临床医生提供可解释且稳健的策略建议。</div>
</details>
</div>
<div class="card">
<div class="title">Transformer Architectures for Respiratory Sound Analysis and Multimodal Diagnosis</div>
<div class="meta-line">Authors: Theodore Aptekarev, Vladimir Sokolovsky, Gregory Furman</div>
<div class="meta-line">First: 2026-01-20T18:40:42+00:00 · Latest: 2026-01-20T18:40:42+00:00</div>
<div class="meta-line">Comments: 7 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14227v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14227v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Respiratory sound analysis is a crucial tool for screening asthma and other pulmonary pathologies, yet traditional auscultation remains subjective and experience-dependent. Our prior research established a CNN baseline using DenseNet201, which demonstrated high sensitivity in classifying respiratory sounds. In this work, we (i) adapt the Audio Spectrogram Transformer (AST) for respiratory sound analysis and (ii) evaluate a multimodal Vision-Language Model (VLM) that integrates spectrograms with structured patient metadata.
  AST is initialized from publicly available weights and fine-tuned on a medical dataset containing hundreds of recordings per diagnosis. The VLM experiment uses a compact Moondream-type model that processes spectrogram images alongside a structured text prompt (sex, age, recording site) to output a JSON-formatted diagnosis. Results indicate that AST achieves approximately 97% accuracy with an F1-score around 97% and ROC AUC of 0.98 for asthma detection, significantly outperforming both the internal CNN baseline and typical external benchmarks. The VLM reaches 86-87% accuracy, performing comparably to the CNN baseline while demonstrating the capability to integrate clinical context into the inference process. These results confirm the effectiveness of self-attention for acoustic screening and highlight the potential of multimodal architectures for holistic diagnostic tools.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于呼吸音分析和多模态诊断的Transformer架构</div>
<div class="mono" style="margin-top:8px">呼吸音分析是筛查哮喘和其他肺部病理的重要工具，但传统听诊仍具有主观性和依赖经验的局限。我们之前的研究基于DenseNet201建立了CNN基线模型，该模型在呼吸音分类中表现出高灵敏度。在本工作中，我们（i）将音频频谱图Transformer（AST）适应于呼吸音分析，（ii）评估了一个结合频谱图与结构化患者元数据的多模态视觉-语言模型（VLM）。
AST模型从公开可用的权重初始化，并在包含每种诊断数百条记录的医学数据集上进行微调。VLM实验使用了一种紧凑型Moondream模型，该模型同时处理频谱图图像和结构化文本提示（性别、年龄、记录部位），输出JSON格式的诊断结果。结果表明，AST在哮喘检测中达到约97%的准确率，F1分数约为97%，ROC AUC为0.98，显著优于内部CNN基线模型和常规外部基准。VLM的准确率为86-87%，其表现与CNN基线相当，同时展示了将临床背景整合到推理过程中的能力。这些结果验证了自注意力机制在声学筛查中的有效性，并突显了多模态架构在全面诊断工具中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Learning Approaches to Quantum Error Mitigation</div>
<div class="meta-line">Authors: Leonardo Placidi, Ifan Williams, Enrico Rinaldi, Daniel Mills, Cristina Cîrstoiu, Vanya Eccles, Ross Duncan</div>
<div class="meta-line">First: 2026-01-20T18:40:22+00:00 · Latest: 2026-01-20T18:40:22+00:00</div>
<div class="meta-line">Comments: 48 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14226v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14226v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a systematic investigation of deep learning methods applied to quantum error mitigation of noisy output probability distributions from measured quantum circuits. We compare different architectures, from fully connected neural networks to transformers, and we test different design/training modalities, identifying sequence-to-sequence, attention-based models as the most effective on our datasets. These models consistently produce mitigated distributions that are closer to the ideal outputs when tested on both simulated and real device data obtained from IBM superconducting quantum processing units (QPU) up to five qubits. Across several different circuit depths, our approach outperforms other baseline error mitigation techniques. We perform a series of ablation studies to examine: how different input features (circuit, device properties, noisy output statistics) affect performance; cross-dataset generalization across circuit families; and transfer learning to a different IBM QPU. We observe that generalization performance across similar devices with the same architecture works effectively, without needing to fully retrain models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度学习在量子错误缓解中的应用</div>
<div class="mono" style="margin-top:8px">我们系统地研究了深度学习方法在缓解测量量子电路中噪声输出概率分布的量子错误中的应用。我们比较了从全连接神经网络到Transformer的不同架构，并测试了不同的设计和训练方式，发现序列到序列、基于注意力的模型在我们的数据集上效果最佳。这些模型在测试时，无论是使用模拟数据还是从IBM超导量子处理器（QPU）获取的真实设备数据，都能产生更接近理想输出的缓解分布。在多个不同的电路深度下，我们的方法优于其他基线错误缓解技术。我们进行了一系列消融研究，以探讨：不同的输入特征（电路、设备特性、噪声输出统计）如何影响性能；在不同电路家族间的跨数据集泛化能力；以及向不同IBM QPU的迁移学习效果。我们观察到，相同架构在相似设备间的泛化性能表现良好，无需完全重新训练模型。</div>
</details>
</div>
<div class="card">
<div class="title">Zebra-Llama: Towards Extremely Efficient Hybrid Models</div>
<div class="meta-line">Authors: Mingyu Yang, Mehdi Rezagholizadeh, Guihong Li, Vikram Appia, Emad Barsoum</div>
<div class="meta-line">First: 2025-05-22T20:39:57+00:00 · Latest: 2026-01-20T18:39:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.17272v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.17272v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the growing demand for deploying large language models (LLMs) across diverse applications, improving their inference efficiency is crucial for sustainable and democratized access. However, retraining LLMs to meet new user-specific requirements is prohibitively expensive and environmentally unsustainable. In this work, we propose a practical and scalable alternative: composing efficient hybrid language models from existing pre-trained models. Our approach, Zebra-Llama, introduces a family of 1B, 3B, and 8B hybrid models by combining State Space Models (SSMs) and Multi-head Latent Attention (MLA) layers, using a refined initialization and post-training pipeline to efficiently transfer knowledge from pre-trained Transformers. Zebra-Llama achieves Transformer-level accuracy with near-SSM efficiency using only 7-11B training tokens (compared to trillions of tokens required for pre-training) and an 8B teacher. Moreover, Zebra-Llama dramatically reduces KV cache size -down to 3.9%, 2%, and 2.73% of the original for the 1B, 3B, and 8B variants, respectively-while preserving 100%, 100%, and &gt;97% of average zero-shot performance on LM Harness tasks. Compared to models like MambaInLLaMA, X-EcoMLA, Minitron, and Llamba, Zebra-Llama consistently delivers competitive or superior accuracy while using significantly fewer tokens, smaller teachers, and vastly reduced KV cache memory. Notably, Zebra-Llama-8B surpasses Minitron-8B in few-shot accuracy by 7% while using 8x fewer training tokens, over 12x smaller KV cache, and a smaller teacher (8B vs. 15B). It also achieves 2.6x-3.8x higher throughput (tokens/s) than MambaInLlama up to a 32k context length. We will release code and model checkpoints upon acceptance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>斑马-llama：迈向极高效的混合模型</div>
<div class="mono" style="margin-top:8px">随着在多样化应用中部署大型语言模型（LLMs）的需求不断增长，提高其推理效率对于实现可持续和普及化访问至关重要。然而，为了满足新用户特定需求而重新训练LLMs的成本过高且对环境不可持续。在本工作中，我们提出了一种实用且可扩展的替代方案：通过现有预训练模型构建高效的混合语言模型。我们的方法Zebra-Llama通过结合状态空间模型（SSMs）和多头潜在注意力（MLA）层，引入了一组1B、3B和8B的混合模型，利用精细的初始化和微调流程，高效地将预训练Transformer的知识迁移。Zebra-Llama仅使用7-110亿个训练标记（相比预训练所需的万亿级标记）和一个8B的教师模型，即可达到Transformer级别的精度。此外，Zebra-Llama显著减少了KV缓存的大小，分别降至1B、3B和8B版本的3.9%、2%和2.73%，同时在LM Harness任务中保持了100%、100%和&gt;97%的平均零样本性能。与MambaInLLaMA、X-EcoMLA、Minitron和Llamba等模型相比，Zebra-Llama在使用更少标记、更小的教师模型和大幅减少的KV缓存内存的情况下，持续提供具有竞争力或更优的精度。值得注意的是，Zebra-Llama-8B在少样本精度上比Minitron-8B高出7%，同时使用了8倍更少的训练标记、超过12倍更小的KV缓存和更小的教师模型（8B vs. 15B）。它还实现了比MambaInLLaMA高2.6-3.8倍的吞吐量（标记/秒），在32k上下文长度下表现优异。我们将在接受后发布代码和模型检查点。</div>
</details>
</div>
<div class="card">
<div class="title">AnyTask: an Automated Task and Data Generation Framework for Advancing Sim-to-Real Policy Learning</div>
<div class="meta-line">Authors: Ran Gong, Xiaohan Zhang, Jinghuan Shang, Maria Vittoria Minniti, Jigarkumar Patel, Valerio Pepe, Riedana Yan, Ahmet Gundogdu, Ivan Kapelyukh, Ali Abbas, Xiaoqiang Yan, Harsh Patel, Laura Herlant, Karl Schmeckpeper</div>
<div class="meta-line">First: 2025-12-19T17:55:48+00:00 · Latest: 2026-01-20T18:25:48+00:00</div>
<div class="meta-line">Comments: 28 pages, 25 figures. The first four authors contributed equally</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17853v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.17853v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generalist robot learning remains constrained by data: large-scale, diverse, and high-quality interaction data are expensive to collect in the real world. While simulation has become a promising way for scaling up data collection, the related tasks, including simulation task design, task-aware scene generation, expert demonstration synthesis, and sim-to-real transfer, still demand substantial human effort. We present AnyTask, an automated framework that pairs massively parallel GPU simulation with foundation models to design diverse manipulation tasks and synthesize robot data. We introduce three AnyTask agents for generating expert demonstrations aiming to solve as many tasks as possible: 1) ViPR, a novel task and motion planning agent with VLM-in-the-loop Parallel Refinement; 2) ViPR-Eureka, a reinforcement learning agent with generated dense rewards and LLM-guided contact sampling; 3) ViPR-RL, a hybrid planning and learning approach that jointly produces high-quality demonstrations with only sparse rewards. We train behavior cloning policies on generated data, validate them in simulation, and deploy them directly on real robot hardware. The policies generalize to novel object poses, achieving 44% average success across a suite of real-world pick-and-place, drawer opening, contact-rich pushing, and long-horizon manipulation tasks. Our project website is at https://anytask.rai-inst.com .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnyTask：一个用于推进模拟到现实政策学习的自动化任务与数据生成框架</div>
<div class="mono" style="margin-top:8px">通用型机器人学习仍受限于数据：大规模、多样化且高质量的交互数据在现实世界中收集成本高昂。尽管模拟已成为扩展数据收集的有前景方法，但相关的任务，包括模拟任务设计、任务感知场景生成、专家演示合成以及模拟到现实的迁移，仍然需要大量的人工努力。我们提出了AnyTask，一个结合大规模并行GPU模拟与基础模型的自动化框架，用于设计多样化的操作任务并合成机器人数据。我们引入了三个AnyTask代理，旨在生成能够解决尽可能多任务的专家演示：1）ViPR，一种新型的任务与运动规划代理，具有VLM-in-the-loop并行细化；2）ViPR-Eureka，一种强化学习代理，具有生成的密集奖励和LLM引导的接触采样；3）ViPR-RL，一种混合规划与学习方法，仅使用稀疏奖励即可联合生成高质量的演示。我们在生成的数据上训练行为克隆策略，验证其在模拟中的表现，并直接部署到现实机器人硬件上。这些策略能够泛化到新的物体姿态，在一系列现实世界的抓取与放置、抽屉开启、接触密集的推搡以及长时序操作任务中，平均成功率达到44%。我们的项目网站为https://anytask.rai-inst.com。</div>
</details>
</div>
<div class="card">
<div class="title">Joint Score-Threshold Optimization for Interpretable Risk Assessment</div>
<div class="meta-line">Authors: Fardin Gankhanloo, Emmett Springer, Erik H. Hoyer, Daniel L. Young, Kimia Ghobadi</div>
<div class="meta-line">First: 2025-10-24T18:07:24+00:00 · Latest: 2026-01-20T18:20:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.21934v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.21934v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Risk assessment tools in healthcare commonly employ point-based scoring systems that map patients to ordinal risk categories via thresholds. While electronic health record (EHR) data presents opportunities for data-driven optimization of these tools, two fundamental challenges impede standard supervised learning: (1) labels are often available only for extreme risk categories due to intervention-censored outcomes, and (2) misclassification cost is asymmetric and increases with ordinal distance. We propose a mixed-integer programming (MIP) framework that jointly optimizes scoring weights and category thresholds in the face of these challenges. Our approach prevents label-scarce category collapse via threshold constraints, and utilizes an asymmetric, distance-aware objective. The MIP framework supports governance constraints, including sign restrictions, sparsity, and minimal modifications to incumbent tools, ensuring practical deployability in clinical workflows. We further develop a continuous relaxation of the MIP problem to provide warm-start solutions for more efficient MIP optimization. We apply the proposed score optimization framework to a case study of inpatient falls risk assessment using the Johns Hopkins Fall Risk Assessment Tool.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于可解释风险评估的联合评分-阈值优化</div>
<div class="mono" style="margin-top:8px">医疗保健中的风险评估工具通常采用基于点的评分系统，通过阈值将患者映射到有序风险类别。尽管电子健康记录（EHR）数据为这些工具的数据驱动优化提供了机会，但两个根本性挑战阻碍了标准监督学习的应用：(1) 由于干预性截断结果，标签通常仅在极端风险类别中可用；(2) 分类错误成本是不对称的，并随有序距离增加而增加。我们提出了一种混合整数规划（MIP）框架，以应对这些挑战，联合优化评分权重和类别阈值。我们的方法通过阈值约束防止标签稀缺类别崩溃，并采用不对称、距离感知的目标函数。MIP框架支持治理约束，包括符号限制、稀疏性以及对现有工具的最小修改，确保在临床工作流程中的实际部署可行性。我们进一步开发了MIP问题的连续松弛版本，以提供更高效的MIP优化的初始解。我们将所提出的评分优化框架应用于约翰霍普金斯医院跌倒风险评估工具的住院患者跌倒风险评估案例研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Risk assessment tools in healthcare commonly employ point-based scoring systems that map patients to ordinal risk categories via thresholds.</div>
</details>
</div>
<div class="card">
<div class="title">InT: Self-Proposed Interventions Enable Credit Assignment in LLM Reasoning</div>
<div class="meta-line">Authors: Matthew Y. R. Yang, Hao Bai, Ian Wu, Gene Yang, Amrith Setlur, Aviral Kumar</div>
<div class="meta-line">First: 2026-01-20T18:15:38+00:00 · Latest: 2026-01-20T18:15:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14209v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14209v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Outcome-reward reinforcement learning (RL) has proven effective at improving the reasoning capabilities of large language models (LLMs). However, standard RL assigns credit only at the level of the final answer, penalizing entire reasoning traces when the outcome is incorrect and uniformly reinforcing all steps when it is correct. As a result, correct intermediate steps may be discouraged in failed traces, while spurious steps may be reinforced in successful ones. We refer to this failure mode as the problem of credit assignment. While a natural remedy is to train a process reward model, accurately optimizing such models to identify corrective reasoning steps remains challenging. We introduce Intervention Training (InT), a training paradigm in which the model performs fine-grained credit assignment on its own reasoning traces by proposing short, targeted corrections that steer trajectories toward higher reward. Using reference solutions commonly available in mathematical reasoning datasets and exploiting the fact that verifying a model-generated solution is easier than generating a correct one from scratch, the model identifies the first error in its reasoning and proposes a single-step intervention to redirect the trajectory toward the correct solution. We then apply supervised fine-tuning (SFT) to the on-policy rollout up to the point of error concatenated with the intervention, localizing error to the specific step that caused failure. We show that the resulting model serves as a far better initialization for RL training. After running InT and subsequent fine-tuning with RL, we improve accuracy by nearly 14% over a 4B-parameter base model on IMO-AnswerBench, outperforming larger open-source models such as gpt-oss-20b.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InT：自提出干预使LLM推理中的信用分配成为可能</div>
<div class="mono" style="margin-top:8px">结果奖励强化学习（RL）已被证明能有效提升大型语言模型（LLMs）的推理能力。然而，标准RL仅在最终答案层面进行信用分配，当结果错误时会惩罚整个推理轨迹，当结果正确时则对所有步骤进行统一强化。因此，在失败的轨迹中，正确的中间步骤可能被抑制，而在成功的轨迹中，虚假步骤可能被强化。我们将这种失败模式称为信用分配问题。虽然训练一个过程奖励模型是自然的解决方案，但准确优化此类模型以识别纠正推理步骤仍然具有挑战性。我们引入了干预训练（InT），这是一种训练范式，其中模型通过提出简短且有针对性的修正，对自身的推理轨迹进行细粒度的信用分配，从而引导轨迹向更高奖励的方向发展。利用数学推理数据集中常见的参考解，并利用验证模型生成的解比从头生成正确解更容易这一事实，模型可以识别其推理中的第一个错误，并提出单步干预以引导轨迹走向正确解。随后，我们将监督微调（SFT）应用于包含错误点的在线策略轨迹与干预的组合，将错误定位到具体导致失败的步骤。我们证明，由此得到的模型是强化学习训练的更优初始化。在运行InT并随后使用RL进行微调后，我们在IMO-AnswerBench上将准确率提高了近14%，优于诸如gpt-oss-20b等更大的开源模型。</div>
</details>
</div>
<div class="card">
<div class="title">Rig-Aware 3D Reconstruction of Vehicle Undercarriages using Gaussian Splatting</div>
<div class="meta-line">Authors: Nitin Kulkarni, Akhil Devarashetti, Charlie Cluss, Livio Forte, Dan Buckmaster, Philip Schneider, Chunming Qiao, Alina Vereshchaka</div>
<div class="meta-line">Venue: www</div>
<div class="meta-line">First: 2026-01-20T18:13:03+00:00 · Latest: 2026-01-20T18:13:03+00:00</div>
<div class="meta-line">Comments: 8 pages, 9 figures, Conference: IEEE International Conference on Machine Learning and Applications 2025 (ICMLA 2025): https://www.icmla-conference.org/icmla25/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14208v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14208v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inspecting the undercarriage of used vehicles is a labor-intensive task that requires inspectors to crouch or crawl underneath each vehicle to thoroughly examine it. Additionally, online buyers rarely see undercarriage photos. We present an end-to-end pipeline that utilizes a three-camera rig to capture videos of the undercarriage as the vehicle drives over it, and produces an interactive 3D model of the undercarriage. The 3D model enables inspectors and customers to rotate, zoom, and slice through the undercarriage, allowing them to detect rust, leaks, or impact damage in seconds, thereby improving both workplace safety and buyer confidence. Our primary contribution is a rig-aware Structure-from-Motion (SfM) pipeline specifically designed to overcome the challenges of wide-angle lens distortion and low-parallax scenes. Our method overcomes the challenges of wide-angle lens distortion and low-parallax scenes by integrating precise camera calibration, synchronized video streams, and strong geometric priors from the camera rig. We use a constrained matching strategy with learned components, the DISK feature extractor, and the attention-based LightGlue matcher to generate high-quality sparse point clouds that are often unattainable with standard SfM pipelines. These point clouds seed the Gaussian splatting process to generate photorealistic undercarriage models that render in real-time. Our experiments and ablation studies demonstrate that our design choices are essential to achieve state-of-the-art quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用高斯点云技术进行车辆底盘的设备感知三维重建</div>
<div class="mono" style="margin-top:8px">检查二手车底盘是一项耗时的工作，需要检查人员蹲下或爬行在车辆下方进行详细检查。此外，在线买家很少能看到底盘照片。我们提出了一种端到端的流程，利用三目相机组在车辆驶过时拍摄底盘视频，并生成可交互的三维底盘模型。该三维模型使检查人员和买家能够旋转、缩放和切片查看底盘，从而在几秒钟内检测到锈蚀、泄漏或撞击损伤，从而提高工作场所的安全性和买家的信心。我们的主要贡献是一种设备感知的结构从运动（SfM）流程，专门设计用于克服广角镜头失真和低视差场景的挑战。我们通过集成精确的相机校准、同步视频流以及来自相机组的强几何先验信息，解决了广角镜头失真和低视差场景的问题。我们采用一种受约束的匹配策略，结合学习组件、DISK特征提取器和基于注意力的LightGlue匹配器，生成高质量的稀疏点云，这些点云通常无法通过标准的SfM流程获得。这些点云用于高斯点云生成过程，以创建逼真的底盘模型，并实现实时渲染。我们的实验和消融研究证明，我们的设计选择对于实现最先进的质量至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Inspecting the undercarriage of used vehicles is a labor-intensive task that requires inspectors to crouch or crawl underneath each vehicle to thoroughly examine it.</div>
</details>
</div>
<div class="card">
<div class="title">Copy-Trasform-Paste: Zero-Shot Object-Object Alignment Guided by Vision-Language and Geometric Constraints</div>
<div class="meta-line">Authors: Rotem Gatenyo, Ohad Fried</div>
<div class="meta-line">First: 2026-01-20T18:12:55+00:00 · Latest: 2026-01-20T18:12:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14207v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14207v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study zero-shot 3D alignment of two given meshes, using a text prompt describing their spatial relation -- an essential capability for content creation and scene assembly. Earlier approaches primarily rely on geometric alignment procedures, while recent work leverages pretrained 2D diffusion models to model language-conditioned object-object spatial relationships. In contrast, we directly optimize the relative pose at test time, updating translation, rotation, and isotropic scale with CLIP-driven gradients via a differentiable renderer, without training a new model. Our framework augments language supervision with geometry-aware objectives: a variant of soft-Iterative Closest Point (ICP) term to encourage surface attachment and a penetration loss to discourage interpenetration. A phased schedule strengthens contact constraints over time, and camera control concentrates the optimization on the interaction region. To enable evaluation, we curate a benchmark containing diverse categories and relations, and compare against baselines. Our method outperforms all alternatives, yielding semantically faithful and physically plausible alignments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>复制-变换-粘贴：基于视觉-语言和几何约束的零样本物体-物体对齐</div>
<div class="mono" style="margin-top:8px">我们研究使用文本提示描述两个给定网格的空间关系来进行零样本3D对齐，这是内容创作和场景组装中的关键能力。早期方法主要依赖几何对齐过程，而近期工作则利用预训练的2D扩散模型来建模语言条件下的物体间空间关系。相反，我们在测试时直接优化相对姿态，通过可微分渲染器，利用CLIP驱动的梯度更新平移、旋转和各向同性缩放，而无需训练新模型。我们的框架在语言监督的基础上引入了几何感知的目标：一种软迭代最近点（ICP）项的变体以鼓励表面附着，以及一个穿透损失以防止物体相互穿透。分阶段的训练计划随时间加强接触约束，而相机控制则将优化集中在交互区域。为了便于评估，我们整理了一个包含多样类别和关系的基准数据集，并与基线方法进行比较。我们的方法优于所有其他方法，实现了语义忠实且物理上合理的对齐。</div>
</details>
</div>
<div class="card">
<div class="title">Algebraic Topology Principles behind Topological Quantum Error Correction</div>
<div class="meta-line">Authors: Xiang Zou, Hoi-Kwong Lo</div>
<div class="meta-line">First: 2025-05-09T14:26:37+00:00 · Latest: 2026-01-20T18:10:01+00:00</div>
<div class="meta-line">Comments: 29 pages, 28 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.06082v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.06082v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quantum error correction (QEC) is crucial for realizing scalable quantum technologies, and topological quantum error correction (TQEC) has emerged as the most experimentally advanced paradigm of QEC. Existing homological and topological code constructions, however, are largely confined to orientable two-manifolds with simple boundary conditions. In this work, we develop a unified algebraic-topological framework for TQEC based on homology, cohomology, and intersection theory, which characterizes exactly when an arbitrary-dimensional manifold (with or without boundary) can serve as a quantum memory, thereby extending the standard 2D homological-code picture to arbitrary dimension and to manifolds with boundary via Poincaré-Lefschetz duality. Building on this classification, we introduce concrete code families that exploit nontrivial topology beyond the planar and toric settings. These include ``3-torus code&#x27;&#x27; and higher-dimensional ``volume codes&#x27;&#x27; on compact manifolds with mixed $X$- and $Z$-type boundaries. We further give a topological construction of qudit TQEC codes on general two-dimensional cell complexes using group presentation complexes, which unifies and extends several known quantum LDPC and homological-product-like constructions within a single geometric language. Finally, we combine the theoretical framework with numerical simulations to demonstrate that changing only the global topology can yield improved logical performance at fixed entanglement resources. Taken together, our results provide a systematic set of topological design principles for constructing and analyzing TQEC codes across dimensions and boundaries, and they open new avenues for topology-aware fault-tolerant quantum architectures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>代数拓扑在拓扑量子错误校正中的原理</div>
<div class="mono" style="margin-top:8px">量子错误校正（QEC）对于实现可扩展的量子技术至关重要，而拓扑量子错误校正（TQEC）已成为QEC中最实验先进的范式。然而，现有的同调和拓扑码构造方法大多局限于具有简单边界条件的可定向二维流形。在本文中，我们基于同调、上同调和相交理论，开发了一个统一的代数拓扑框架，用于TQEC，该框架精确地刻画了任意维流形（带或不带边界）何时可以作为量子存储器，从而通过庞加莱-勒夫谢茨对偶性将标准的二维同调码图景扩展到任意维度和带边界的流形。在此分类基础上，我们引入了利用平面和扭面之外的非平凡拓扑的具体系码族，包括在混合X-和Z型边界条件的紧致流形上的『三环码』和更高维的『体积码』。我们进一步利用群表示流形，给出了在一般的二维细胞复形上构造量子d维TQEC码的拓扑方法，这种方法在一个几何语言中统一并扩展了多个已知的量子LDPC和同调乘积类构造。最后，我们将理论框架与数值模拟相结合，证明仅改变全局拓扑即可在固定纠缠资源下提升逻辑性能。综上所述，我们的结果为在不同维度和边界条件下构造和分析TQEC码提供了一套系统的拓扑设计原理，并为具有拓扑意识的容错量子架构开辟了新的途径。</div>
</details>
</div>
<div class="card">
<div class="title">GeLoc3r: Enhancing Relative Camera Pose Regression with Geometric Consistency Regularization</div>
<div class="meta-line">Authors: Jingxing Li, Yongjae Lee, Deliang Fan</div>
<div class="meta-line">First: 2025-09-27T01:21:38+00:00 · Latest: 2026-01-20T18:07:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23038v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.23038v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Prior ReLoc3R achieves breakthrough performance with fast 25ms inference and state-of-the-art regression accuracy, yet our analysis reveals subtle geometric inconsistencies in its internal representations that prevent reaching the precision ceiling of correspondence-based methods like MASt3R (which require 300ms per pair). In this work, we present GeLoc3r, a novel approach to relative camera pose estimation that enhances pose regression methods through Geometric Consistency Regularization (GCR). GeLoc3r overcomes the speed-accuracy dilemma by training regression networks to produce geometrically consistent poses without inference-time geometric computation. During training, GeLoc3r leverages ground-truth depth to generate dense 3D-2D correspondences, weights them using a FusionTransformer that learns correspondence importance, and computes geometrically-consistent poses via weighted RANSAC. This creates a consistency loss that transfers geometric knowledge into the regression network. Unlike FAR method which requires both regression and geometric solving at inference, GeLoc3r only uses the enhanced regression head at test time, maintaining ReLoc3R&#x27;s fast speed and approaching MASt3R&#x27;s high accuracy. On challenging benchmarks, GeLoc3r consistently outperforms ReLoc3R, achieving significant improvements including 40.45% vs. 34.85% AUC@5° on the CO3Dv2 dataset (16% relative improvement), 68.66% vs. 66.70% AUC@5° on RealEstate10K, and 50.45% vs. 49.60% on MegaDepth1500. By teaching geometric consistency during training rather than enforcing it at inference, GeLoc3r represents a paradigm shift in how neural networks learn camera geometry, achieving both the speed of regression and the geometric understanding of correspondence methods.</div></details>
</div>
<div class="card">
<div class="title">Automated Analysis of DFT Output Files for Molecular Descriptor Extraction and Reactivity Modeling</div>
<div class="meta-line">Authors: Yu-Chien Huang, Dennis Chung-Yang Huang, Yun-Cheng Tsai</div>
<div class="meta-line">First: 2026-01-20T18:06:18+00:00 · Latest: 2026-01-20T18:06:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14203v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14203v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding the relationship between molecular structure and chemical reactivity or properties is fundamental to rational molecular design. Linear free energy relationships (LFERs), particularly Hammett analysis, have long served as powerful tools in organic chemistry. Recently, these approaches have been enhanced by incorporating computationally derived parameters, enabling broader applicability across diverse molecules and reactions. To facilitate and scale this process, we present DFTDescriptorPipeline, a fully automated workflow for extracting quantum chemical descriptors from Gaussian log files and constructing structure-property and structure-reactivity relationships using multivariate linear regression (MLR) models. We validate the workflow across four case studies, including photoswitchable molecules and catalytic reactions. In each case, the models provide interpretable results, demonstrating the versatility of this approach and its relevance to a wide range of chemical contexts. We anticipate that this platform will serve as a generalizable framework for integrating quantum chemical calculations into data-driven molecular design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于DFT输出文件的分子描述符提取与反应性建模自动化分析</div>
<div class="mono" style="margin-top:8px">理解分子结构与化学反应性或性质之间的关系是理性分子设计的基础。线性自由能关系（LFERs），尤其是Hammett分析，长期以来一直是有机化学中的有力工具。最近，通过引入计算得出的参数，这些方法得到了增强，从而在多种分子和反应中具有更广泛的应用性。为促进并扩展这一过程，我们提出了DFTDescriptorPipeline，这是一个完全自动化的流程，用于从Gaussian日志文件中提取量子化学描述符，并利用多元线性回归（MLR）模型构建结构-性质和结构-反应性关系。我们在四个案例研究中验证了该流程，包括光开关分子和催化反应。在每个案例中，模型均提供了可解释的结果，展示了该方法的通用性和在多种化学情境中的相关性。我们预计该平台将成为将量子化学计算整合到数据驱动分子设计中的可推广框架。</div>
</details>
</div>
<div class="card">
<div class="title">DiffusionAgent: Navigating Expert Models for Agentic Image Generation</div>
<div class="meta-line">Authors: Jie Qin, Jie Wu, Weifeng Chen, Yueming Lyu</div>
<div class="meta-line">First: 2024-01-18T15:30:58+00:00 · Latest: 2026-01-20T18:02:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2401.10061v2">Abs</a> · <a href="https://arxiv.org/pdf/2401.10061v2">PDF</a> · <a href="https://github.com/DiffusionAgent/DiffusionAgent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the accelerating era of human-instructed visual content creation, diffusion models have demonstrated remarkable generative potential. Yet their deployment is constrained by a dual bottleneck: semantic ambiguity in diverse prompts and the narrow specialization of individual models. A single diffusion architecture struggles to maintain optimal performance across heterogeneous prompts, while conventional &quot;parse-then-call&quot; pipelines artificially separate semantic understanding from generative execution. To bridge this gap, we introduce DiffusionAgent, a unified, language-model-driven agent that casts the entire &quot;prompt comprehension-expert routing-image synthesis&quot; loop into a agentic framework. Our contributions are three-fold: (1) a tree-of-thought-powered expert navigator that performs fine-grained semantic parsing and zero-shot matching to the most suitable diffusion model via an extensible prior-knowledge tree; (2) an advantage database updated with human-in-the-loop feedback, continually aligning model-selection policy with human aesthetic and semantic preferences; and (3) a fully decoupled agent architecture that activates the optimal generative path for open-domain prompts without retraining or fine-tuning any expert. Extensive experiments show that DiffusionAgent retains high generation quality while significantly broadening prompt coverage, establishing a new performance and generality benchmark for multi-domain image synthesis. The code is available at https://github.com/DiffusionAgent/DiffusionAgent</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DiffusionAgent：基于专家模型的代理图像生成</div>
<div class="mono" style="margin-top:8px">在人类指导的视觉内容创作加速发展的时代，扩散模型展现了显著的生成潜力。然而其部署受到双重瓶颈的限制：多样化提示语的语义模糊性以及单个模型的狭窄专业化。单一扩散架构难以在异构提示语上保持最优性能，而传统的&quot;解析-调用&quot;流程人为地将语义理解与生成执行分离。为弥合这一差距，我们引入了DiffusionAgent，这是一个统一的、由语言模型驱动的代理，将整个&quot;提示理解-专家路由-图像合成&quot;循环纳入代理框架。我们的贡献包括三个方面：(1) 一个基于树形思维的专家导航器，通过可扩展的先验知识树进行细粒度语义解析和零样本匹配，找到最适合的扩散模型；(2) 一个通过人类反馈更新的优势数据库，持续将模型选择策略与人类审美和语义偏好对齐；(3) 一个完全解耦的代理架构，无需对任何专家模型进行再训练或微调，即可为开放领域提示语激活最优的生成路径。大量实验表明，DiffusionAgent在保持高质量生成的同时，显著扩展了提示语的覆盖范围，为多领域图像合成建立了新的性能和通用性基准。代码可在https://github.com/DiffusionAgent/DiffusionAgent获取。</div>
</details>
</div>
<div class="card">
<div class="title">Differentiated Pickup Point Offering for Emission Reduction in Last-Mile Delivery</div>
<div class="meta-line">Authors: Albina Galiullina, Wouter van Heeswijk, Tom van Woensel</div>
<div class="meta-line">First: 2026-01-20T18:00:42+00:00 · Latest: 2026-01-20T18:00:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14196v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.14196v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pickup points are widely recognized as a sustainable alternative to home delivery, as consolidating orders at pickup locations can shorten delivery routes and improve first-attempt success rates. However, these benefits may be negated when customers drive to pick up their orders. This study proposes a Differentiated Pickup Point Offering (DPO) policy that aims to jointly reduce emissions from delivery truck routes and customer travel. Under DPO, each arriving customer is offered a single recommended pickup point, rather than an unrestricted choice among all locations, while retaining the option of home delivery. We study this problem in a dynamic and stochastic setting, where the pickup point offered to each customer depends on previously realized customer locations and delivery choices. To design effective DPO policies, we adopt a reinforcement learning-based approach that accounts for spatial relationships between customers and pickup points and their implications for future route consolidation. Computational experiments show that differentiated pickup point offerings can substantially reduce total carbon emissions. The proposed policies reduce total emissions by up to 9% relative to home-only delivery and by 2% on average compared with alternative policies, including unrestricted pickup point choice and nearest pickup point assignment. Differentiated offerings are particularly effective in dense urban settings with many pickup points and short inter-location distances. Moreover, explicitly accounting for the dynamic nature of customer arrivals and choices is especially important when customers are less inclined to choose pickup point delivery over home delivery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>最后一公里配送中差异化取件点提供以减少排放</div>
<div class="mono" style="margin-top:8px">取件点被广泛认为是家庭配送的可持续替代方案，因为将订单集中到取件点可以缩短配送路线并提高首次送达成功率。然而，当顾客驾车前往取件点时，这些优势可能会被抵消。本研究提出了一种差异化取件点提供（DPO）政策，旨在共同减少配送车辆路线和顾客出行的碳排放。在DPO政策下，每位到达的顾客都会被推荐一个单一的取件点，而不是在所有地点中自由选择，同时仍保留家庭配送的选项。我们在一个动态且随机的环境中研究该问题，其中每位顾客的取件点选择依赖于之前已实现的顾客位置和配送选择。为了设计有效的DPO政策，我们采用了一种基于强化学习的方法，考虑了顾客与取件点之间的空间关系及其对未来路线整合的影响。计算实验表明，差异化取件点提供可以显著减少总碳排放。所提出的政策相比仅家庭配送可减少总排放量高达9%，与其它替代政策（包括不限制取件点选择和最近取件点分配）相比平均减少2%。在取件点密集且地点间距离较短的城市环境中，差异化提供尤为有效。此外，当顾客更倾向于选择家庭配送而非取件点配送时，明确考虑顾客到达和选择的动态特性尤为重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Pickup points are widely recognized as a sustainable alternative to home delivery, as consolidating orders at pickup locations can shorten delivery routes and improve first-attempt success rates.</div>
</details>
</div>
<div class="card">
<div class="title">KeyDiff: Key Similarity-Based KV Cache Eviction for Long-Context LLM Inference in Resource-Constrained Environments</div>
<div class="meta-line">Authors: Junyoung Park, Dalton Jones, Matthew J Morse, Raghavv Goel, Mingu Lee, Chris Lott</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-04-21T18:12:46+00:00 · Latest: 2026-01-20T17:55:29+00:00</div>
<div class="meta-line">Comments: 37 pages, 19 figures, NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.15364v4">Abs</a> · <a href="https://arxiv.org/pdf/2504.15364v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We demonstrate that geometrically distinctive keys during LLM inference tend to have high attention scores. Based on the phenomenon we propose KeyDiff, a training-free KV cache eviction method based solely on key similarity. Unlike other KV cache eviction methods, KeyDiff can process arbitrarily long prompts within strict resource constraints and efficiently generate responses. We provide a theoretical basis for KeyDiff by relating key diversity with attention scores. These results imply KeyDiff can efficiently identify the most important tokens to retain. Notably KeyDiff does not rely on attention scores, allowing the use of optimized attention mechanisms like FlashAttention. Under a strict memory allowance, we demonstrate the effectiveness of KeyDiff for the Llama and Qwen model families by observing a performance gap of less than 0.04% with 8K cache budget ($\sim$23% KV cache reduction) from the non-evicting baseline on LongBench for Llama 3.1-8B and Llama 3.2-3B. We also observe near baseline performance for Deepseek-R1-Distill-Llama-8B on the Math500 reasoning benchmark and decrease end-to-end inference latency by up to 30% compared to the other token-eviction methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KeyDiff：基于键相似性的KV缓存淘汰方法用于资源受限环境中的长上下文LLM推理</div>
<div class="mono" style="margin-top:8px">我们证明在LLM推理过程中，几何上具有区别的键往往具有较高的注意力分数。基于这一现象，我们提出了KeyDiff，一种无需训练仅依赖键相似性的KV缓存淘汰方法。与其它KV缓存淘汰方法不同，KeyDiff能够在严格的资源限制下处理任意长度的提示，并高效生成响应。我们通过将键多样性与注意力分数联系起来，为KeyDiff提供了理论依据。这些结果表明KeyDiff能够有效识别需要保留的重要令牌。值得注意的是，KeyDiff不依赖注意力分数，从而允许使用优化的注意力机制，如FlashAttention。在严格的内存限制下，我们在LongBench上对Llama 3.1-8B和Llama 3.2-3B模型家族进行了实验，观察到在8K缓存预算下，KeyDiff的性能差距小于0.04%（相当于约23%的KV缓存减少）。此外，我们在Math500推理基准上观察到Deepseek-R1-Distill-Llama-8B的性能接近基线，并且相比其他令牌淘汰方法，端到端推理延迟降低了最高达30%。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
