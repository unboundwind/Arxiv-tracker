<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-25 04:21</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260225_0421</div>
    <div class="row"><div class="card">
<div class="title">Mobile-O: Unified Multimodal Understanding and Generation on Mobile Device</div>
<div class="meta-line">Authors: Abdelrahman Shaker, Ahmed Heakl, Jaseel Muhammad, Ritesh Thawkar, Omkar Thawakar, Senmao Li, Hisham Cholakkal, Ian Reid, Eric P. Xing, Salman Khan, Fahad Shahbaz Khan</div>
<div class="meta-line">First: 2026-02-23T18:59:58+00:00 · Latest: 2026-02-23T18:59:58+00:00</div>
<div class="meta-line">Comments: Project page: https://amshaker.github.io/Mobile-O/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20161v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20161v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://amshaker.github.io/Mobile-O/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Unified multimodal models can both understand and generate visual content within a single architecture. Existing models, however, remain data-hungry and too heavy for deployment on edge devices. We present Mobile-O, a compact vision-language-diffusion model that brings unified multimodal intelligence to a mobile device. Its core module, the Mobile Conditioning Projector (MCP), fuses vision-language features with a diffusion generator using depthwise-separable convolutions and layerwise alignment. This design enables efficient cross-modal conditioning with minimal computational cost. Trained on only a few million samples and post-trained in a novel quadruplet format (generation prompt, image, question, answer), Mobile-O jointly enhances both visual understanding and generation capabilities. Despite its efficiency, Mobile-O attains competitive or superior performance compared to other unified models, achieving 74% on GenEval and outperforming Show-O and JanusFlow by 5% and 11%, while running 6x and 11x faster, respectively. For visual understanding, Mobile-O surpasses them by 15.3% and 5.1% averaged across seven benchmarks. Running in only ~3s per 512x512 image on an iPhone, Mobile-O establishes the first practical framework for real-time unified multimodal understanding and generation on edge devices. We hope Mobile-O will ease future research in real-time unified multimodal intelligence running entirely on-device with no cloud dependency. Our code, models, datasets, and mobile application are publicly available at https://amshaker.github.io/Mobile-O/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Mobile-O：在移动设备上实现统一的多模态理解和生成</div>
<div class="mono" style="margin-top:8px">统一的多模态模型可以在单一架构中实现对视觉内容的理解和生成。然而，现有模型仍然需要大量数据且过于庞大，难以部署在边缘设备上。我们提出了Mobile-O，这是一个紧凑的视觉-语言-扩散模型，将统一多模态智能带入移动设备。其核心模块Mobile Conditioning Projector（MCP）通过深度可分离卷积和逐层对齐，将视觉-语言特征与扩散生成器融合。这种设计实现了高效的跨模态条件化，且计算成本极低。Mobile-O仅在数百万个样本上进行训练，并采用新颖的四元组格式（生成提示、图像、问题、答案）进行后训练，从而联合提升了视觉理解和生成能力。尽管效率高，Mobile-O在GenEval上达到了74%，在生成速度上分别比Show-O和JanusFlow快6倍和11倍，性能也优于它们5%和11%。在七个基准测试中，其平均视觉理解能力分别超过Show-O和JanusFlow 15.3%和5.1%。在iPhone上，Mobile-O对512x512图像的处理时间仅需约3秒，为边缘设备上的实时统一多模态理解和生成建立了首个实用框架。我们希望Mobile-O能为未来在设备端完全运行、无需依赖云端的实时统一多模态智能研究提供便利。我们的代码、模型、数据集和移动应用可在https://amshaker.github.io/Mobile-O/ 公开获取。</div>
</details>
</div>
<div class="card">
<div class="title">OpenEarthAgent: A Unified Framework for Tool-Augmented Geospatial Agents</div>
<div class="meta-line">Authors: Akashah Shabbir, Muhammad Umer Sheikh, Muhammad Akhtar Munir, Hiyam Debary, Mustansar Fiaz, Muhammad Zaigham Zaheer, Paolo Fraccaro, Fahad Shahbaz Khan, Muhammad Haris Khan, Xiao Xiang Zhu, Salman Khan</div>
<div class="meta-line">First: 2026-02-19T18:59:54+00:00 · Latest: 2026-02-23T18:59:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17665v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.17665v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks. Extending such capabilities to the remote sensing domain remains challenging, as models must reason over spatial scale, geographic structures, and multispectral indices while maintaining coherent multi-step logic. To bridge this gap, OpenEarthAgent introduces a unified framework for developing tool-augmented geospatial agents trained on satellite imagery, natural-language queries, and detailed reasoning traces. The training pipeline relies on supervised fine-tuning over structured reasoning trajectories, aligning the model with verified multistep tool interactions across diverse analytical contexts. The accompanying corpus comprises 14,538 training and 1,169 evaluation instances, with more than 100K reasoning steps in the training split and over 7K reasoning steps in the evaluation split. It spans urban, environmental, disaster, and infrastructure domains, and incorporates GIS-based operations alongside index analyses such as NDVI, NBR, and NDBI. Grounded in explicit reasoning traces, the learned agent demonstrates structured reasoning, stable spatial understanding, and interpretable behaviour through tool-driven geospatial interactions across diverse conditions. We report consistent improvements over a strong baseline and competitive performance relative to recent open and closed-source models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OpenEarthAgent：一种用于工具增强地理空间代理的统一框架</div>
<div class="mono" style="margin-top:8px">多模态推理的最新进展使得代理能够解释图像、将其与语言连接，并执行结构化的分析任务。将这些能力扩展到遥感领域仍具有挑战性，因为模型必须在空间尺度、地理结构和多光谱指数上进行推理，同时保持连贯的多步骤逻辑。为弥合这一差距，OpenEarthAgent 引入了一种统一框架，用于开发基于卫星图像、自然语言查询和详细推理轨迹训练的工具增强型地理空间代理。训练流程依赖于结构化推理轨迹上的监督微调，使模型与多种分析情境下的经过验证的多步骤工具交互对齐。配套语料库包含14,538个训练实例和1,169个评估实例，训练集包含超过100,000个推理步骤，评估集包含超过7,000个推理步骤。该语料库涵盖城市、环境、灾害和基础设施等领域，并结合了基于GIS的操作以及NDVI、NBR和NDBI等指数分析。该代理基于显式的推理轨迹进行学习，通过工具驱动的地理空间交互，在各种条件下展现出结构化的推理、稳定的空间理解以及可解释的行为。我们在一个强大的基线模型上报告了持续的性能提升，并在与近期开源和闭源模型的比较中表现出竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent progress in multimodal reasoning has enabled agents that can interpret imagery, connect it with language, and perform structured analytical tasks.</div>
</details>
</div>
<div class="card">
<div class="title">tttLRM: Test-Time Training for Long Context and Autoregressive 3D Reconstruction</div>
<div class="meta-line">Authors: Chen Wang, Hao Tan, Wang Yifan, Zhiqin Chen, Yuheng Liu, Kalyan Sunkavalli, Sai Bi, Lingjie Liu, Yiwei Hu</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-02-23T18:59:45+00:00 · Latest: 2026-02-23T18:59:45+00:00</div>
<div class="meta-line">Comments: Accepted by CVPR 2026. Project Page: https://cwchenwang.github.io/tttLRM</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20160v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20160v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://cwchenwang.github.io/tttLRM">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose tttLRM, a novel large 3D reconstruction model that leverages a Test-Time Training (TTT) layer to enable long-context, autoregressive 3D reconstruction with linear computational complexity, further scaling the model&#x27;s capability. Our framework efficiently compresses multiple image observations into the fast weights of the TTT layer, forming an implicit 3D representation in the latent space that can be decoded into various explicit formats, such as Gaussian Splats (GS) for downstream applications. The online learning variant of our model supports progressive 3D reconstruction and refinement from streaming observations. We demonstrate that pretraining on novel view synthesis tasks effectively transfers to explicit 3D modeling, resulting in improved reconstruction quality and faster convergence. Extensive experiments show that our method achieves superior performance in feedforward 3D Gaussian reconstruction compared to state-of-the-art approaches on both objects and scenes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>tttLRM：面向长上下文和自回归3D重建的测试时训练</div>
<div class="mono" style="margin-top:8px">我们提出了tttLRM，一种新颖的大型3D重建模型，利用测试时训练（TTT）层实现具有线性计算复杂度的长上下文、自回归3D重建，进一步扩展了模型的能力。我们的框架高效地将多个图像观测压缩到TTT层的快速权重中，在潜在空间中形成隐式3D表示，可解码为多种显式格式，如用于下游任务的高斯点云（GS）。我们模型的在线学习变体支持从流式观测中进行渐进式3D重建与优化。我们证明了在新视角合成任务上进行预训练可以有效迁移到显式3D建模，从而提升重建质量并加快收敛速度。大量实验表明，我们的方法在对象和场景的前馈3D高斯重建任务中，相比最先进的方法表现出更优的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We propose tttLRM, a novel large 3D reconstruction model that leverages a Test-Time Training (TTT) layer to enable long-context, autoregressive 3D reconstruction with linear computational complexity, further scaling the model&#x27;s capability.</div>
</details>
</div>
<div class="card">
<div class="title">A Very Big Video Reasoning Suite</div>
<div class="meta-line">Authors: Maijunxian Wang, Ruisi Wang, Juyi Lin, Ran Ji, Thaddäus Wiedemer, Qingying Gao, Dezhi Luo, Yaoyao Qian, Lianyu Huang, Zelong Hong, Jiahui Ge, Qianli Ma, Hang He, Yifan Zhou, Lingzi Guo, Lantao Mei, Jiachen Li, Hanwen Xing, Tianqi Zhao, Fengyuan Yu, Weihang Xiao, Yizheng Jiao, Jianheng Hou, Danyang Zhang, Pengcheng Xu, Boyang Zhong, Zehong Zhao, Gaoyun Fang, John Kitaoka, Yile Xu, Hua Xu, Kenton Blacutt, Tin Nguyen, Siyuan Song, Haoran Sun, Shaoyue Wen, Linyang He, Runming Wang, Yanzhi Wang, Mengyue Yang, Ziqiao Ma, Raphaël Millière, Freda Shi, Nuno Vasconcelos, Daniel Khashabi, Alan Yuille, Yilun Du, Ziming Liu, Bo Li, Dahua Lin, Ziwei Liu, Vikash Kumar, Yijiang Li, Lei Yang, Zhongang Cai, Hokin Deng</div>
<div class="meta-line">First: 2026-02-23T18:59:41+00:00 · Latest: 2026-02-23T18:59:41+00:00</div>
<div class="meta-line">Comments: Homepage: https://video-reason.com/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20159v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20159v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Rapid progress in video models has largely focused on visual quality, leaving their reasoning capabilities underexplored. Video reasoning grounds intelligence in spatiotemporally consistent visual environments that go beyond what text can naturally capture, enabling intuitive reasoning over spatiotemporal structure such as continuity, interaction, and causality. However, systematically studying video reasoning and its scaling behavior is hindered by the lack of large-scale training data. To address this gap, we introduce the Very Big Video Reasoning (VBVR) Dataset, an unprecedentedly large-scale resource spanning 200 curated reasoning tasks following a principled taxonomy and over one million video clips, approximately three orders of magnitude larger than existing datasets. We further present VBVR-Bench, a verifiable evaluation framework that moves beyond model-based judging by incorporating rule-based, human-aligned scorers, enabling reproducible and interpretable diagnosis of video reasoning capabilities. Leveraging the VBVR suite, we conduct one of the first large-scale scaling studies of video reasoning and observe early signs of emergent generalization to unseen reasoning tasks. Together, VBVR lays a foundation for the next stage of research in generalizable video reasoning. The data, benchmark toolkit, and models are publicly available at https://video-reason.com/ .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一个非常庞大的视频推理套件</div>
<div class="mono" style="margin-top:8px">视频模型的快速发展主要集中在视觉质量上，其推理能力尚未被充分探索。视频推理将智能建立在时空一致的视觉环境中，这种环境超越了文本自然捕捉的范围，使对连续性、交互性和因果性等时空结构的直观推理成为可能。然而，由于缺乏大规模训练数据，系统地研究视频推理及其扩展行为受到阻碍。为了解决这一问题，我们引入了Very Big Video Reasoning（VBVR）数据集，这是一个前所未有的大规模资源，包含200个经过整理的推理任务，遵循原则性的分类体系，以及超过一百万的视频片段，规模大约是现有数据集的三倍。我们进一步提出了VBVR-Bench，一个可验证的评估框架，通过引入基于规则、与人类对齐的评分系统，超越了基于模型的判断方式，从而实现视频推理能力的可重复和可解释诊断。借助VBVR套件，我们进行了视频推理领域首次大规模扩展研究，并观察到了对未见过推理任务的早期泛化迹象。总体而言，VBVR为视频推理的下一阶段研究奠定了基础。数据、基准工具包和模型可在https://video-reason.com/ 公开获取。</div>
</details>
</div>
<div class="card">
<div class="title">Flow3r: Factored Flow Prediction for Scalable Visual Geometry Learning</div>
<div class="meta-line">Authors: Zhongxiao Cong, Qitao Zhao, Minsik Jeon, Shubham Tulsiani</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-02-23T18:59:30+00:00 · Latest: 2026-02-23T18:59:30+00:00</div>
<div class="meta-line">Comments: CVPR 2026. Project website: https://flow3r-project.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20157v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20157v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://flow3r-project.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current feed-forward 3D/4D reconstruction systems rely on dense geometry and pose supervision -- expensive to obtain at scale and particularly scarce for dynamic real-world scenes. We present Flow3r, a framework that augments visual geometry learning with dense 2D correspondences (`flow&#x27;) as supervision, enabling scalable training from unlabeled monocular videos. Our key insight is that the flow prediction module should be factored: predicting flow between two images using geometry latents from one and pose latents from the other. This factorization directly guides the learning of both scene geometry and camera motion, and naturally extends to dynamic scenes. In controlled experiments, we show that factored flow prediction outperforms alternative designs and that performance scales consistently with unlabeled data. Integrating factored flow into existing visual geometry architectures and training with ${\sim}800$K unlabeled videos, Flow3r achieves state-of-the-art results across eight benchmarks spanning static and dynamic scenes, with its largest gains on in-the-wild dynamic videos where labeled data is most scarce.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Flow3r: 用于可扩展视觉几何学习的因子化流预测</div>
<div class="mono" style="margin-top:8px">当前的前馈式3D/4D重建系统依赖于密集的几何和姿态监督——在大规模获取上成本高昂，尤其在动态真实场景中尤为稀缺。我们提出了Flow3r框架，通过密集的2D对应关系（`流&#x27;）作为监督，增强视觉几何学习，从而实现从无标签单目视频中进行可扩展的训练。我们的关键洞察是流预测模块应当被因子化：使用一张图像的几何潜变量和另一张图像的姿态潜变量来预测两帧之间的流。这种因子化直接引导场景几何和相机运动的学习，并自然地扩展到动态场景。在受控实验中，我们展示了因子化流预测优于其他设计，并且性能随着无标签数据量的增加而稳定提升。将因子化流整合到现有的视觉几何架构中，并使用约80万帧无标签视频进行训练，Flow3r在涵盖静态和动态场景的八个基准测试中均取得了最先进的结果，尤其在野外动态视频中表现最佳，这些场景的标注数据最为稀缺。</div>
</details>
</div>
<div class="card">
<div class="title">Skill-Inject: Measuring Agent Vulnerability to Skill File Attacks</div>
<div class="meta-line">Authors: David Schmotz, Luca Beurer-Kellner, Sahar Abdelnabi, Maksym Andriushchenko</div>
<div class="meta-line">First: 2026-02-23T18:59:27+00:00 · Latest: 2026-02-23T18:59:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20156v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20156v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM agents are evolving rapidly, powered by code execution, tools, and the recently introduced agent skills feature. Skills allow users to extend LLM applications with specialized third-party code, knowledge, and instructions. Although this can extend agent capabilities to new domains, it creates an increasingly complex agent supply chain, offering new surfaces for prompt injection attacks. We identify skill-based prompt injection as a significant threat and introduce SkillInject, a benchmark evaluating the susceptibility of widely-used LLM agents to injections through skill files. SkillInject contains 202 injection-task pairs with attacks ranging from obviously malicious injections to subtle, context-dependent attacks hidden in otherwise legitimate instructions. We evaluate frontier LLMs on SkillInject, measuring both security in terms of harmful instruction avoidance and utility in terms of legitimate instruction compliance. Our results show that today&#x27;s agents are highly vulnerable with up to 80% attack success rate with frontier models, often executing extremely harmful instructions including data exfiltration, destructive action, and ransomware-like behavior. They furthermore suggest that this problem will not be solved through model scaling or simple input filtering, but that robust agent security will require context-aware authorization frameworks. Our benchmark is available at https://www.skill-inject.com/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Skill-Inject：衡量代理对技能文件攻击的脆弱性</div>
<div class="mono" style="margin-top:8px">LLM代理正迅速发展，依赖于代码执行、工具和最近引入的代理技能功能。技能允许用户通过专用的第三方代码、知识和指令扩展LLM应用。尽管这可以将代理能力扩展到新领域，但它也创建了日益复杂的代理供应链，为提示注入攻击提供了新的攻击面。我们识别出基于技能的提示注入是一个重大威胁，并引入SkillInject，这是一个评估广泛使用的LLM代理对通过技能文件注入的易受攻击性的基准。SkillInject包含202个注入任务对，攻击类型从明显恶意的注入到隐藏在看似合法指令中的微妙、依赖上下文的攻击。我们在SkillInject上评估前沿LLM模型，衡量其在避免有害指令方面的安全性以及在遵循合法指令方面的实用性。我们的结果显示，当前的代理对攻击高度脆弱，使用前沿模型时攻击成功率高达80%，经常执行极端有害的指令，包括数据外泄、破坏性操作和勒索软件行为。此外，这些结果还表明，仅通过模型扩展或简单的输入过滤无法解决此问题，而需要基于上下文的授权框架来实现稳健的代理安全。我们的基准测试可在https://www.skill-inject.com/获取。</div>
</details>
</div>
<div class="card">
<div class="title">JUCAL: Jointly Calibrating Aleatoric and Epistemic Uncertainty in Classification Tasks</div>
<div class="meta-line">Authors: Jakob Heiss, Sören Lambrecht, Jakob Weissteiner, Hanna Wutte, Žan Žurič, Josef Teichmann, Bin Yu</div>
<div class="meta-line">First: 2026-02-23T18:59:10+00:00 · Latest: 2026-02-23T18:59:10+00:00</div>
<div class="meta-line">Comments: 11 pages + appendix. Preliminary version of an ongoing project that will be expanded with furhter evaluations</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20153v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20153v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study post-calibration uncertainty for trained ensembles of classifiers. Specifically, we consider both aleatoric (label noise) and epistemic (model) uncertainty. Among the most popular and widely used calibration methods in classification are temperature scaling (i.e., pool-then-calibrate) and conformal methods. However, the main shortcoming of these calibration methods is that they do not balance the proportion of aleatoric and epistemic uncertainty. Not balancing these uncertainties can severely misrepresent predictive uncertainty, leading to overconfident predictions in some input regions while being underconfident in others. To address this shortcoming, we present a simple but powerful calibration algorithm Joint Uncertainty Calibration (JUCAL) that jointly calibrates aleatoric and epistemic uncertainty. JUCAL jointly calibrates two constants to weight and scale epistemic and aleatoric uncertainties by optimizing the negative log-likelihood (NLL) on the validation/calibration dataset. JUCAL can be applied to any trained ensemble of classifiers (e.g., transformers, CNNs, or tree-based methods), with minimal computational overhead, without requiring access to the models&#x27; internal parameters. We experimentally evaluate JUCAL on various text classification tasks, for ensembles of varying sizes and with different ensembling strategies. Our experiments show that JUCAL significantly outperforms SOTA calibration methods across all considered classification tasks, reducing NLL and predictive set size by up to 15% and 20%, respectively. Interestingly, even applying JUCAL to an ensemble of size 5 can outperform temperature-scaled ensembles of size up to 50 in terms of NLL and predictive set size, resulting in up to 10 times smaller inference costs. Thus, we propose JUCAL as a new go-to method for calibrating ensembles in classification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>JUCAL：在分类任务中联合校准偶然不确定性和认知不确定性</div>
<div class="mono" style="margin-top:8px">我们研究训练后的分类器集合的后校准不确定性。具体而言，我们考虑偶然不确定性（标签噪声）和认知不确定性（模型不确定性）。在分类任务中，温度缩放（即先聚合后校准）和符合性方法是最流行和广泛使用的校准方法。然而，这些校准方法的主要缺点是它们未能平衡偶然和认知不确定性的比例。不均衡这些不确定性可能导致预测不确定性被严重误表，从而在某些输入区域产生过于自信的预测，而在其他区域则过于不自信。为了解决这一问题，我们提出了一种简单但强大的校准算法——联合不确定性校准（JUCAL），该算法联合校准偶然和认知不确定性。JUCAL通过在验证/校准数据集上优化负对数似然（NLL），联合校准两个常数以加权和缩放认知和偶然不确定性。JUCAL可以应用于任何训练好的分类器集合（例如Transformer、CNN或基于树的方法），且计算开销极小，无需访问模型的内部参数。我们在各种文本分类任务上对JUCAL进行了实验评估，适用于不同规模的集合和不同的集成策略。实验结果表明，JUCAL在所有考虑的分类任务中显著优于当前最先进的校准方法，分别将NLL和预测集合大小降低了最多15%和20%。有趣的是，即使将JUCAL应用于一个大小为5的集合，其在NLL和预测集合大小上的表现也优于温度缩放后的大小达50的集合，从而使得推理成本减少高达10倍。因此，我们提出JUCAL作为分类任务中校准集合的新标准方法。</div>
</details>
</div>
<div class="card">
<div class="title">Behavior Learning (BL): Learning Hierarchical Optimization Structures from Data</div>
<div class="meta-line">Authors: Zhenyao Ma, Yue Liang, Dongxu Li</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-23T18:59:04+00:00 · Latest: 2026-02-23T18:59:04+00:00</div>
<div class="meta-line">Comments: ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20152v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20152v1">PDF</a> · <a href="https://github.com/MoonYLiang/Behavior-Learning">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Inspired by behavioral science, we propose Behavior Learning (BL), a novel general-purpose machine learning framework that learns interpretable and identifiable optimization structures from data, ranging from single optimization problems to hierarchical compositions. It unifies predictive performance, intrinsic interpretability, and identifiability, with broad applicability to scientific domains involving optimization. BL parameterizes a compositional utility function built from intrinsically interpretable modular blocks, which induces a data distribution for prediction and generation. Each block represents and can be written in symbolic form as a utility maximization problem (UMP), a foundational paradigm in behavioral science and a universal framework of optimization. BL supports architectures ranging from a single UMP to hierarchical compositions, the latter modeling hierarchical optimization structures. Its smooth and monotone variant (IBL) guarantees identifiability. Theoretically, we establish the universal approximation property of BL, and analyze the M-estimation properties of IBL. Empirically, BL demonstrates strong predictive performance, intrinsic interpretability and scalability to high-dimensional data. Code: https://github.com/MoonYLiang/Behavior-Learning ; install via pip install blnetwork.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>行为学习（BL）：从数据中学习分层优化结构</div>
<div class="mono" style="margin-top:8px">受行为科学启发，我们提出了行为学习（BL），一种新颖的通用机器学习框架，能够从数据中学习可解释且可识别的优化结构，涵盖从单一优化问题到分层组合的多种结构。它统一了预测性能、内在可解释性和可识别性，广泛适用于涉及优化的科学领域。BL通过内在可解释的模块化块构建组合效用函数，从而诱导出用于预测和生成的数据分布。每个模块都可以表示为效用最大化问题（UMP），这是行为科学中的基础范式，也是优化的通用框架。BL支持从单一UMP到分层组合的多种架构，后者用于建模分层优化结构。其平滑且单调的变体（IBL）保证了可识别性。理论上，我们建立了BL的通用逼近性质，并分析了IBL的M-估计性质。实证上，BL表现出强大的预测性能、内在可解释性和对高维数据的可扩展性。代码：https://github.com/MoonYLiang/Behavior-Learning；可通过 pip install blnetwork 安装。</div>
</details>
</div>
<div class="card">
<div class="title">Conformal Risk Control for Non-Monotonic Losses</div>
<div class="meta-line">Authors: Anastasios N. Angelopoulos</div>
<div class="meta-line">First: 2026-02-23T18:58:54+00:00 · Latest: 2026-02-23T18:58:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20151v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20151v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Conformal risk control is an extension of conformal prediction for controlling risk functions beyond miscoverage. The original algorithm controls the expected value of a loss that is monotonic in a one-dimensional parameter. Here, we present risk control guarantees for generic algorithms applied to possibly non-monotonic losses with multidimensional parameters. The guarantees depend on the stability of the algorithm -- unstable algorithms have looser guarantees. We give applications of this technique to selective image classification, FDR and IOU control of tumor segmentations, and multigroup debiasing of recidivism predictions across overlapping race and sex groups using empirical risk minimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>非单调损失的共形风险控制</div>
<div class="mono" style="margin-top:8px">共形风险控制是共形预测的扩展，用于控制超出误覆盖范围的风险函数。原始算法控制的是一个在一维参数上单调的损失的期望值。本文针对可能具有多维参数的非单调损失，提出了适用于通用算法的风险控制保证。这些保证依赖于算法的稳定性——不稳定的算法具有较宽松的保证。我们展示了该技术在选择性图像分类、肿瘤分割的FDR和IOU控制，以及使用经验风险最小化在重叠种族和性别群体中进行再犯预测的多组去偏中的应用。</div>
</details>
</div>
<div class="card">
<div class="title">Simulation-Ready Cluttered Scene Estimation via Physics-aware Joint Shape and Pose Optimization</div>
<div class="meta-line">Authors: Wei-Cheng Huang, Jiaheng Han, Xiaohan Ye, Zherong Pan, Kris Hauser</div>
<div class="meta-line">First: 2026-02-23T18:58:24+00:00 · Latest: 2026-02-23T18:58:24+00:00</div>
<div class="meta-line">Comments: 15 pages, 13 figures, in submission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20150v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20150v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Estimating simulation-ready scenes from real-world observations is crucial for downstream planning and policy learning tasks. Regretfully, existing methods struggle in cluttered environments, often exhibiting prohibitive computational cost, poor robustness, and restricted generality when scaling to multiple interacting objects. We propose a unified optimization-based formulation for real-to-sim scene estimation that jointly recovers the shapes and poses of multiple rigid objects under physical constraints. Our method is built on two key technical innovations. First, we leverage the recently introduced shape-differentiable contact model, whose global differentiability permits joint optimization over object geometry and pose while modeling inter-object contacts. Second, we exploit the structured sparsity of the augmented Lagrangian Hessian to derive an efficient linear system solver whose computational cost scales favorably with scene complexity. Building on this formulation, we develop an end-to-end real-to-sim scene estimation pipeline that integrates learning-based object initialization, physics-constrained joint shape-pose optimization, and differentiable texture refinement. Experiments on cluttered scenes with up to 5 objects and 22 convex hulls demonstrate that our approach robustly reconstructs physically valid, simulation-ready object shapes and poses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于物理感知的联合形状与姿态优化的仿真就绪杂乱场景估计</div>
<div class="mono" style="margin-top:8px">从现实世界观测中估计仿真就绪场景对于下游的规划和策略学习任务至关重要。然而，现有方法在杂乱环境中表现不佳，通常在处理多个相互作用物体时表现出计算成本过高、鲁棒性差和泛化能力受限的问题。我们提出了一种统一的基于优化的现实到仿真的场景估计框架，该框架在物理约束下联合恢复多个刚体物体的形状和姿态。我们的方法基于两个关键技术创新。首先，我们利用了最近引入的可微形状接触模型，其全局可微性允许在建模物体间接触的同时，对物体几何形状和姿态进行联合优化。其次，我们利用增强拉格朗日海森矩阵的结构稀疏性，推导出一种计算成本随场景复杂度呈有利增长的高效线性系统求解器。基于此框架，我们开发了一个端到端的现实到仿真场景估计流程，集成了基于学习的物体初始化、物理约束下的联合形状-姿态优化以及可微纹理细化。在包含最多5个物体和22个凸包的杂乱场景上的实验表明，我们的方法能够稳健地重建符合物理规律的仿真就绪物体形状和姿态。</div>
</details>
</div>
<div class="card">
<div class="title">Quantum Information Approach to Bosonization of Supersymmetric Yang-Mills Fields</div>
<div class="meta-line">Authors: Radhakrishnan Balu, S. James Gates</div>
<div class="meta-line">First: 2026-02-23T18:57:21+00:00 · Latest: 2026-02-23T18:57:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20149v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20149v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We consider bosonization of supersymmetry in the context of Wess-Zumino quantum mechanics. Our motivation for this investigation is the flexibility the bosonic fock space affords as any classical probability distribution can be realized on it making it a versatile framework to work with for quantum processes. We proceed by constructing a minimal bosonization of a system with one bosonic and two fermionic degrees of freedom. We iterate this process to construct a tower of SUSY systems that is akin to unfolded Adinkras. We then identify an osp(2|2) symmetry of the system constructed. To build an irreducible representation of the system we induce representations across the sectors, a first to our knowledge, as the previous work have focused on induction only within the bosonic sector. First, we start with a fermionic representation using Clifford algebras and then induce a representation to gl(2|2) and restrict it to osp(2|2). In the second method, we induce a representation from that of the bosonic sector. In both cases, our representations are in terms of qubit operators that provide a way to solve SUSY problems using quantum information based approaches. Depending upon the direction of induction the representations are suitable for implementation on a hybrid qubit and fermionic or bosonic quantum computers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>量子信息方法在超对称杨-米尔斯场玻色化中的应用</div>
<div class="mono" style="margin-top:8px">我们在韦斯-祖明诺量子力学的框架下研究超对称性的玻色化。我们进行这项研究的动机在于玻色子福克空间的灵活性，因为任何经典概率分布都可以在该空间上实现，使其成为处理量子过程的多功能框架。我们首先构造了一个具有一个玻色子和两个费米子自由度系统的最小玻色化，然后通过迭代这一过程，构建出类似于展开的阿迪克拉（Adinkra）的超对称系统塔。接着，我们识别了该系统所具有的osp(2|2)对称性。为了构建系统的不可约表示，我们首次跨不同扇区诱导表示，而以往的研究主要集中在玻色子扇区内。首先，我们从克利福德代数出发构造一个费米子表示，然后将其诱导到gl(2|2)并限制在osp(2|2)上。在第二种方法中，我们从玻色子扇区的表示诱导出表示。在两种情况下，我们的表示都使用了量子比特算符，这为利用基于量子信息的方法解决超对称问题提供了途径。根据诱导的方向，这些表示适用于在混合量子比特与费米子或仅玻色子量子计算机上实现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We consider bosonization of supersymmetry in the context of Wess-Zumino quantum mechanics.</div>
</details>
</div>
<div class="card">
<div class="title">Agentic AI for Scalable and Robust Optical Systems Control</div>
<div class="meta-line">Authors: Zehao Wang, Mingzhe Han, Wei Cheng, Yue-Kai Huang, Philip Ji, Denton Wu, Mahdi Safari, Flemming Holtorf, Kenaish AlQubaisi, Norbert M. Linke, Danyang Zhuo, Yiran Chen, Ting Wang, Dirk Englund, Tingjun Chen</div>
<div class="meta-line">First: 2026-02-23T18:54:32+00:00 · Latest: 2026-02-23T18:54:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20144v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20144v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present AgentOptics, an agentic AI framework for high-fidelity, autonomous optical system control built on the Model Context Protocol (MCP). AgentOptics interprets natural language tasks and executes protocol-compliant actions on heterogeneous optical devices through a structured tool abstraction layer. We implement 64 standardized MCP tools across 8 representative optical devices and construct a 410-task benchmark to evaluate request understanding, role-aware responses, multi-step coordination, robustness to linguistic variation, and error handling. We assess two deployment configurations--commercial online LLMs and locally hosted open-source LLMs--and compare them with LLM-based code generation baselines. AgentOptics achieves 87.7%--99.0% average task success rates, significantly outperforming code-generation approaches, which reach up to 50% success. We further demonstrate broader applicability through five case studies extending beyond device-level control to system orchestration, monitoring, and closed-loop optimization. These include DWDM link provisioning and coordinated monitoring of coherent 400 GbE and analog radio-over-fiber (ARoF) channels; autonomous characterization and bias optimization of a wideband ARoF link carrying 5G fronthaul traffic; multi-span channel provisioning with launch power optimization; closed-loop fiber polarization stabilization; and distributed acoustic sensing (DAS)-based fiber monitoring with LLM-assisted event detection. These results establish AgentOptics as a scalable, robust paradigm for autonomous control and orchestration of heterogeneous optical systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于可扩展和鲁棒光学系统控制的代理型人工智能</div>
<div class="mono" style="margin-top:8px">我们提出了AgentOptics，这是一个基于模型上下文协议（MCP）的代理型AI框架，用于高保真度、自主的光学系统控制。AgentOptics通过结构化的工具抽象层，解释自然语言任务并在异构光学设备上执行符合协议的操作。我们在8种代表性光学设备上实现了64个标准化的MCP工具，并构建了一个包含410个任务的基准测试，以评估请求理解、角色感知响应、多步骤协调、对语言变化的鲁棒性以及错误处理能力。我们评估了两种部署配置——商业在线大语言模型（LLM）和本地部署的开源LLM，并与基于LLM的代码生成基线方法进行了比较。AgentOptics实现了87.7%至99.0%的平均任务成功率，显著优于代码生成方法，后者最高仅达到50%的成功率。我们进一步通过五个案例研究展示了其更广泛的应用性，这些案例超出了设备级控制，涵盖了系统编排、监控和闭环优化。这些案例包括：DWDM链路配置、相干400 GbE和模拟射频光纤（ARoF）通道的协调监控；承载5G前传流量的宽频ARoF链路的自主表征与偏置优化；多跨段通道配置与发射功率优化；光纤偏振的闭环稳定；以及基于分布式声学传感（DAS）的光纤监控与LLM辅助的事件检测。这些结果确立了AgentOptics作为一种可扩展、鲁棒的自主控制和编排异构光学系统的范式。</div>
</details>
</div>
<div class="card">
<div class="title">TROLL: Trust Regions improve Reinforcement Learning for Large Language Models</div>
<div class="meta-line">Authors: Philipp Becker, Niklas Freymuth, Serge Thilges, Fabian Otto, Gerhard Neumann</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-04T14:14:20+00:00 · Latest: 2026-02-23T18:54:13+00:00</div>
<div class="meta-line">Comments: Published as a conference paper at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.03817v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.03817v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning (RL) with PPO-like clip objectives has become the standard choice for reward-based fine-tuning of large language models (LLMs). Although recent work has explored improved estimators of advantages and normalization, the clipping mechanism itself has remained untouched. Originally introduced as a proxy for principled KL-based trust regions, clipping is a crude approximation that often causes unstable updates and suboptimal performance. We replace the clip objective with a novel discrete differentiable trust region projection, which provides principled token-level KL constraints. The projection operates on a sparse subset of the model&#x27;s most important token logits to balance computational cost and projection effectiveness. Our approach, Trust Region Optimization for Large Language models (TROLL), serves as a direct replacement for PPO-like clipping during training and does not alter the model&#x27;s inference behavior. Across mathematical reasoning and code generation tasks, model families, as well as advantage-estimation methods, TROLL consistently outperforms PPO-like clipping in terms of training speed, stability, and final success rates.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TROLL: 通过信任区域优化提升大语言模型的强化学习</div>
<div class="mono" style="margin-top:8px">使用类似PPO的clip目标的强化学习（RL）已成为基于奖励的大语言模型（LLMs）微调的标准选择。尽管近期的研究探索了优势估计和归一化的改进方法，但clip机制本身仍未被修改。最初被引入作为基于KL的原理化信任区域的代理，clip是一种粗糙的近似方法，常常导致不稳定的更新和次优性能。我们用一种新颖的离散可微信任区域投影替代clip目标，该投影在token级别提供原理化的KL约束。该投影作用于模型中最重要的token logit的稀疏子集，以在计算成本和投影效果之间取得平衡。我们的方法，信任区域优化用于大语言模型（TROLL），在训练过程中直接替代PPO类似的clip机制，且不改变模型的推理行为。在数学推理、代码生成任务以及不同模型家族和优势估计方法中，TROLL在训练速度、稳定性以及最终成功率方面均优于PPO类似的clip方法。</div>
</details>
</div>
<div class="card">
<div class="title">Recurrent Structural Policy Gradient for Partially Observable Mean Field Games</div>
<div class="meta-line">Authors: Clarisse Wibault, Johannes Forkel, Sebastian Towers, Tiphaine Wibault, Juan Duque, George Whittle, Andreas Schaab, Yucheng Yang, Chiyuan Wang, Michael Osborne, Benjamin Moll, Jakob Foerster</div>
<div class="meta-line">First: 2026-02-23T18:53:09+00:00 · Latest: 2026-02-23T18:53:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20141v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20141v1">PDF</a> · <a href="https://github.com/CWibault/mfax">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mean Field Games (MFGs) provide a principled framework for modeling interactions in large population models: at scale, population dynamics become deterministic, with uncertainty entering only through aggregate shocks, or common noise. However, algorithmic progress has been limited since model-free methods are too high variance and exact methods scale poorly. Recent Hybrid Structural Methods (HSMs) use Monte Carlo rollouts for the common noise in combination with exact estimation of the expected return, conditioned on those samples. However, HSMs have not been scaled to Partially Observable settings. We propose Recurrent Structural Policy Gradient (RSPG), the first history-aware HSM for settings involving public information. We also introduce MFAX, our JAX-based framework for MFGs. By leveraging known transition dynamics, RSPG achieves state-of-the-art performance as well as an order-of-magnitude faster convergence and solves, for the first time, a macroeconomics MFG with heterogeneous agents, common noise and history-aware policies. MFAX is publicly available at: https://github.com/CWibault/mfax.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于部分可观测均场博弈的递归结构策略梯度</div>
<div class="mono" style="margin-top:8px">均场博弈（MFGs）提供了一个建模大规模群体互动的原理性框架：在大规模情况下，群体动态变为确定性，不确定性仅通过聚合冲击或公共噪声进入。然而，由于无模型方法方差过高，而精确方法扩展性差，算法进展受到限制。最近的混合结构方法（HSMs）结合了蒙特卡洛 rollout 用于公共噪声，以及基于这些样本的精确预期回报估计。然而，HSMs 仍未扩展到部分可观测的场景。我们提出了一种基于历史信息的混合结构策略梯度方法（RSPG），这是首个适用于涉及公共信息场景的 HSM。我们还引入了基于 JAX 的 MFG 框架 MFAX。通过利用已知的转移动态，RSPG 实现了最先进的性能，并且收敛速度提高了数量级，首次解决了涉及异质代理人、公共噪声和基于历史的策略的宏观经济学 MFG 问题。MFAX 可在以下链接获取：https://github.com/CWibault/mfax。</div>
</details>
</div>
<div class="card">
<div class="title">Towards a Science of AI Agent Reliability</div>
<div class="meta-line">Authors: Stephan Rabanser, Sayash Kapoor, Peter Kirgis, Kangheng Liu, Saiteja Utpala, Arvind Narayanan</div>
<div class="meta-line">First: 2026-02-18T18:05:44+00:00 · Latest: 2026-02-23T18:49:07+00:00</div>
<div class="meta-line">Comments: Interactive dashboard available at: https://hal.cs.princeton.edu/reliability</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16666v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.16666v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hal.cs.princeton.edu/reliability">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向AI代理可靠性科学</div>
<div class="mono" style="margin-top:8px">AI代理正越来越多地被部署以执行重要任务。尽管标准基准上的准确率得分持续上升表明进展迅速，但许多代理在实际应用中仍会失败。这种差异凸显了当前评估方法的基本局限性：将代理行为压缩为单一的成功指标掩盖了关键的操作缺陷。值得注意的是，它忽略了代理是否在多次运行中行为一致、是否能抵御扰动、是否能可预测地失败，以及错误的严重程度是否有限。基于安全关键型工程，我们通过提出十二项具体指标，从四个关键维度（一致性、鲁棒性、可预测性和安全性）全面分解代理的可靠性。在两个互补基准上评估14个模型后，我们发现近期能力的提升仅带来了可靠性的小幅改进。通过揭示这些持续存在的局限性，我们的指标不仅补充了传统评估方法，还提供了分析代理如何执行、退化和失败的工具。</div>
</details>
</div>
<div class="card">
<div class="title">Do Large Language Models Understand Data Visualization Rules?</div>
<div class="meta-line">Authors: Martin Sinnona, Valentin Bonas, Emmanuel Iarussi, Viviana Siless</div>
<div class="meta-line">First: 2026-02-23T18:47:51+00:00 · Latest: 2026-02-23T18:47:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20137v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20137v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data visualization rules-derived from decades of research in design and perception-ensure trustworthy chart communication. While prior work has shown that large language models (LLMs) can generate charts or flag misleading figures, it remains unclear whether they can reason about and enforce visualization rules directly. Constraint-based systems such as Draco encode these rules as logical constraints for precise automated checks, but maintaining symbolic encodings requires expert effort, motivating the use of LLMs as flexible rule validators. In this paper, we present the first systematic evaluation of LLMs against visualization rules using hard-verification ground truth derived from Answer Set Programming (ASP). We translated a subset of Draco&#x27;s constraints into natural-language statements and generated a controlled dataset of 2,000 Vega-Lite specifications annotated with explicit rule violations. LLMs were evaluated on both accuracy in detecting violations and prompt adherence, which measures whether outputs follow the required structured format. Results show that frontier models achieve high adherence (Gemma 3 4B / 27B: 100%, GPT-oss 20B: 98%) and reliably detect common violations (F1 up to 0.82),yet performance drops for subtler perceptual rules (F1 &lt; 0.15 for some categories) and for outputs generated from technical ASP formulations.Translating constraints into natural language improved performance by up to 150% for smaller models. These findings demonstrate the potential of LLMs as flexible, language-driven validators while highlighting their current limitations compared to symbolic solvers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型是否理解数据可视化规则？</div>
<div class="mono" style="margin-top:8px">数据可视化规则源于设计和感知领域的数十年研究，确保图表交流的可信度。尽管已有研究表明大语言模型（LLMs）可以生成图表或识别误导性图表，但尚不清楚它们是否能够直接推理并执行可视化规则。基于约束的系统如Draco将这些规则编码为逻辑约束以实现精确的自动化检查，但维护符号编码需要专家努力，这促使人们使用LLMs作为灵活的规则验证器。在本文中，我们首次系统地评估了LLMs在数据可视化规则上的表现，使用来自Answer Set Programming（ASP）的硬验证真实数据集进行验证。我们将Draco的部分约束翻译成自然语言陈述，并生成了一个包含2000个Vega-Lite规范的受控数据集，这些规范明确标注了规则违规情况。评估结果显示，前沿模型在检测违规和遵循提示格式方面表现出色（Gemma 3 4B / 27B: 100%，GPT-oss 20B: 98%），但在更微妙的感知规则上表现下降（某些类别的F1 &lt; 0.15）。对于较小的模型，将约束翻译成自然语言可使其性能提升高达150%。这些发现展示了LLMs作为灵活、语言驱动的验证器的潜力，同时也突显了它们与符号求解器相比的当前局限性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Data visualization rules-derived from decades of research in design and perception-ensure trustworthy chart communication.</div>
</details>
</div>
<div class="card">
<div class="title">A Benchmark of Causal vs. Correlation AI for Predictive Maintenance</div>
<div class="meta-line">Authors: Shaunak Dhande, Chutian Ma, Giacinto Paolo Saggese, Paul Smith, Krishna Taduri</div>
<div class="meta-line">First: 2025-11-30T23:59:37+00:00 · Latest: 2026-02-23T18:46:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.01149v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.01149v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Predictive maintenance in manufacturing environments presents a challenging optimization problem characterized by extreme cost asymmetry, where missed failures incur costs roughly fifty times higher than false alarms. Conventional machine learning approaches typically optimize statistical accuracy metrics that do not reflect this operational reality and cannot reliably distinguish causal relationships from spurious correlations. This study benchmarks eight predictive models, ranging from baseline statistical approaches to Bayesian structural causal methods, on a dataset of 10,000 CNC machines with a 3.3 percent failure prevalence. While ensemble correlation-based models such as Random Forest (L4) achieve the highest raw cost savings (70.8 percent reduction), the Bayesian Structural Causal Model (L7) delivers competitive financial performance (66.4 percent cost reduction) with an inherent ability of failure attribution, which correlation-based models do not readily provide. The model achieves perfect attribution for HDF, PWF, and OSF failure types. These results suggest that causal methods, when combined with domain knowledge and Bayesian inference, offer a potentially favorable trade-off between predictive performance and operational interpretability in predictive maintenance applications.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>因果AI与相关性AI在预测性维护中的基准测试</div>
<div class="mono" style="margin-top:8px">在制造环境中进行预测性维护是一个具有极端成本不对称性的优化难题，其中漏检故障的成本大约是误报成本的五十倍。传统机器学习方法通常优化统计准确性指标，这些指标无法反映实际运营情况，也无法可靠地区分因果关系与虚假相关性。本研究在一个包含10,000台CNC机床的数据集上，对八种预测模型进行了基准测试，这些模型从基础统计方法到贝叶斯结构因果方法不等。尽管基于相关性的集成模型如随机森林（L4）实现了最高的原始成本节约（70.8%的减少），但贝叶斯结构因果模型（L7）在故障归因方面具有固有优势，其财务表现也具有竞争力（66.4%的成本减少）。该模型在HDF、PWF和OSF故障类型上实现了完美的归因。这些结果表明，当结合领域知识和贝叶斯推断时，因果方法在预测性维护应用中可能在预测性能和操作可解释性之间提供更有利的权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Find the Fruit: Zero-Shot Sim2Real RL for Occlusion-Aware Plant Manipulation</div>
<div class="meta-line">Authors: Nitesh Subedi, Hsin-Jung Yang, Devesh K. Jha, Soumik Sarkar</div>
<div class="meta-line">First: 2025-05-22T11:37:39+00:00 · Latest: 2026-02-23T18:46:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.16547v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.16547v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous harvesting in the open presents a complex manipulation problem. In most scenarios, an autonomous system has to deal with significant occlusion and require interaction in the presence of large structural uncertainties (every plant is different). Perceptual and modeling uncertainty make design of reliable manipulation controllers for harvesting challenging, resulting in poor performance during deployment. We present a sim2real reinforcement learning (RL) framework for occlusion-aware plant manipulation, where a policy is learned entirely in simulation to reposition stems and leaves to reveal target fruit(s). In our proposed approach, we decouple high-level kinematic planning from low-level compliant control which simplifies the sim2real transfer. This decomposition allows the learned policy to generalize across multiple plants with different stiffness and morphology. In experiments with multiple real-world plant setups, our system achieves up to 86.7% success in exposing target fruits, demonstrating robustness to occlusion variation and structural uncertainty.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>寻找果实：面向遮挡感知的植物操作零样本Sim2Real强化学习</div>
<div class="mono" style="margin-top:8px">在开放环境中进行自主采摘面临复杂的操作问题。在大多数情况下，自主系统需要处理显著的遮挡，并在存在大量结构不确定性（每株植物都不同）的情况下进行交互。感知和建模的不确定性使得设计可靠的采摘操作控制器变得困难，导致部署时性能不佳。我们提出了一种面向遮挡感知的植物操作Sim2Real强化学习框架，其中策略完全在模拟环境中学习，以重新定位茎和叶片，从而暴露目标果实。在我们提出的方法中，我们将高层运动规划与底层柔顺控制解耦，从而简化Sim2Real的迁移。这种分解使学习到的策略能够在具有不同刚度和形态的多种植物上进行泛化。在多个真实植物设置的实验中，我们的系统在暴露目标果实方面实现了高达86.7%的成功率，展示了其对遮挡变化和结构不确定性的鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">KNIGHT: Knowledge Graph-Driven Multiple-Choice Question Generation with Adaptive Hardness Calibration</div>
<div class="meta-line">Authors: Mohammad Amanlou, Erfan Shafiee Moghaddam, Yasaman Amou Jafari, Mahdi Noori, Farhan Farsi, Behnam Bahrak</div>
<div class="meta-line">First: 2026-02-23T18:46:27+00:00 · Latest: 2026-02-23T18:46:27+00:00</div>
<div class="meta-line">Comments: Accepted at the Third Conference on Parsimony and Learning (CPAL 2026). 36 pages, 12 figures. (Equal contribution: Yasaman Amou Jafari and Mahdi Noori.)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20135v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20135v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG). Yet evaluating these systems remains bottlenecked by the time and cost of building specialized assessment datasets. We introduce KNIGHT, an LLM-based, knowledge-graph-driven framework for generating multiple-choice question (MCQ) datasets from external sources. KNIGHT constructs a topic-specific knowledge graph, a structured and parsimonious summary of entities and relations, that can be reused to generate instructor-controlled difficulty levels, including multi-hop questions, without repeatedly re-feeding the full source text. This knowledge graph acts as a compressed, reusable state, making question generation a cheap read over the graph. We instantiate KNIGHT on Wikipedia/Wikidata while keeping the framework domain- and ontology-agnostic. As a case study, KNIGHT produces six MCQ datasets in History, Biology, and Mathematics. We evaluate quality on five criteria: fluency, unambiguity (single correct answer), topic relevance, option uniqueness, and answerability given the provided sources (as a proxy for hallucination). Results show that KNIGHT enables token- and cost-efficient generation from a reusable graph representation, achieves high quality across these criteria, and yields model rankings aligned with MMLU-style benchmarks, while supporting topic-specific and difficulty-controlled evaluation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>KNIGHT：基于知识图谱的多选题生成与自适应难度校准</div>
<div class="mono" style="margin-top:8px">随着大规模语言模型（LLMs）的兴起，它们在诸如检索增强生成（RAG）等应用中变得至关重要。然而，评估这些系统仍然受到构建专用评估数据集的时间和成本的限制。我们引入了KNIGHT，这是一个基于LLM、知识图谱驱动的框架，用于从外部来源生成多选题（MCQ）数据集。KNIGHT构建了一个特定主题的知识图谱，这是一个结构化且简洁的实体和关系摘要，可以重复使用以生成教师可控难度级别的问题，包括多跳问题，而无需反复重新输入完整的源文本。该知识图谱充当了一个压缩且可重复使用的状态，使问题生成成为对图谱的低成本读取。我们在维基百科/维基数据上实例化KNIGHT，同时保持框架的领域和本体无关性。作为案例研究，KNIGHT在历史、生物和数学领域生成了六个MCQ数据集。我们从五个方面评估质量：流畅性、无歧义性（单个正确答案）、主题相关性、选项唯一性和根据提供的来源可回答性（作为幻觉的代理指标）。结果表明，KNIGHT能够从可重复使用的图表示中实现令牌和成本高效的生成，在这些标准下实现了高质量，且生成的模型排名与MMLU风格基准一致，同时支持主题特定和难度控制的评估。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">With the rise of large language models (LLMs), they have become instrumental in applications such as Retrieval-Augmented Generation (RAG).</div>
</details>
</div>
<div class="card">
<div class="title">Modeling Epidemiological Dynamics Under Adversarial Data and User Deception</div>
<div class="meta-line">Authors: Yiqi Su, Christo Kurisummoottil Thomas, Walid Saad, Bud Mishra, Naren Ramakrishnan</div>
<div class="meta-line">First: 2026-02-23T18:45:55+00:00 · Latest: 2026-02-23T18:45:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20134v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20134v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Epidemiological models increasingly rely on self-reported behavioral data such as vaccination status, mask usage, and social distancing adherence to forecast disease transmission and assess the impact of non-pharmaceutical interventions (NPIs). While such data provide valuable real-time insights, they are often subject to strategic misreporting, driven by individual incentives to avoid penalties, access benefits, or express distrust in public health authorities. To account for such human behavior, in this paper, we introduce a game-theoretic framework that models the interaction between the population and a public health authority as a signaling game. Individuals (senders) choose how to report their behaviors, while the public health authority (receiver) updates their epidemiological model(s) based on potentially distorted signals. Focusing on deception around masking and vaccination, we characterize analytically game equilibrium outcomes and evaluate the degree to which deception can be tolerated while maintaining epidemic control through policy interventions. Our results show that separating equilibria-with minimal deception-drive infections to near zero over time. Remarkably, even under pervasive dishonesty in pooling equilibria, well-designed sender and receiver strategies can still maintain effective epidemic control. This work advances the understanding of adversarial data in epidemiology and offers tools for designing more robust public health models in the presence of strategic user behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在对抗性数据和用户欺骗下的流行病学动态建模</div>
<div class="mono" style="margin-top:8px">流行病学模型越来越多地依赖于自我报告的行为数据，如疫苗接种状态、口罩使用和社交距离遵守情况，以预测疾病传播并评估非药物干预措施（NPIs）的影响。尽管这些数据提供了有价值的实时洞察，但它们常常受到策略性误报的影响，这种误报源于个人为了避免惩罚、获取利益或表达对公共卫生机构的不信任而采取的行为。为考虑这种人类行为，本文引入了一个博弈论框架，将人口与公共卫生机构之间的互动建模为一个信号博弈。个体（发送者）选择如何报告其行为，而公共卫生机构（接收者）则根据可能被扭曲的信号更新其流行病学模型。我们重点研究与口罩和疫苗相关的欺骗行为，通过分析博弈均衡结果，评估在政策干预下欺骗行为可容忍的程度。我们的研究结果表明，分离均衡（最小欺骗）能够使感染人数随时间趋于接近零。值得注意的是，即使在聚合均衡中存在普遍的不诚实行为，精心设计的发送者和接收者策略仍能维持有效的疫情控制。本工作推进了流行病学中对抗性数据的理解，并为在存在策略性用户行为的情况下设计更稳健的公共卫生模型提供了工具。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Epidemiological models increasingly rely on self-reported behavioral data such as vaccination status, mask usage, and social distancing adherence to forecast disease transmission and assess the impact of non-pharmaceutical interventions (NPIs).</div>
</details>
</div>
<div class="card">
<div class="title">AdaEvolve: Adaptive LLM Driven Zeroth-Order Optimization</div>
<div class="meta-line">Authors: Mert Cemri, Shubham Agrawal, Akshat Gupta, Shu Liu, Audrey Cheng, Qiuyang Mang, Ashwin Naren, Lutfi Eren Erdogan, Koushik Sen, Matei Zaharia, Alex Dimakis, Ion Stoica</div>
<div class="meta-line">First: 2026-02-23T18:45:31+00:00 · Latest: 2026-02-23T18:45:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20133v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20133v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The paradigm of automated program generation is shifting from one-shot generation to inference-time search, where Large Language Models (LLMs) function as semantic mutation operators within evolutionary loops. While effective, these systems are currently governed by static schedules that fail to account for the non-stationary dynamics of the search process. This rigidity results in substantial computational waste, as resources are indiscriminately allocated to stagnating populations while promising frontiers remain under-exploited. We introduce AdaEvolve, a framework that reformulates LLM-driven evolution as a hierarchical adaptive optimization problem. AdaEvolve uses an &quot;accumulated improvement signal&quot; to unify decisions across three levels: Local Adaptation, which dynamically modulates the exploration intensity within a population of solution candidates; Global Adaptation, which routes the global resource budget via bandit-based scheduling across different solution candidate populations; and Meta-Guidance which generates novel solution tactics based on the previously generated solutions and their corresponding improvements when the progress stalls. We demonstrate that AdaEvolve consistently outperforms the open-sourced baselines across 185 different open-ended optimization problems including combinatorial, systems optimization and algorithm design problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AdaEvolve：基于自适应大语言模型的零阶优化</div>
<div class="mono" style="margin-top:8px">自动化程序生成范式正从一次性生成转向推理时搜索，其中大语言模型（LLMs）在进化循环中作为语义突变算子。尽管这些系统有效，但目前仍由静态调度控制，无法适应搜索过程的非平稳动态。这种僵硬性导致了大量计算资源的浪费，因为资源被无差别地分配给停滞的种群，而有潜力的前沿区域则未被充分探索。我们引入AdaEvolve框架，将LLM驱动的进化重新表述为一个分层自适应优化问题。AdaEvolve利用&quot;累积改进信号&quot;在三个层级上统一决策：局部自适应，动态调节解候选种群的探索强度；全局自适应，通过基于多臂老虎机的调度机制在不同解候选种群间分配全局资源预算；以及元指导，在进度停滞时基于之前生成的解及其改进生成新的解策略。我们在包括组合优化、系统优化和算法设计在内的185个不同开放性优化问题上展示了AdaEvolve持续优于开源基线的表现。</div>
</details>
</div>
<div class="card">
<div class="title">Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges</div>
<div class="meta-line">Authors: Minh Dinh, Stéphane Deny</div>
<div class="meta-line">First: 2026-02-20T18:14:05+00:00 · Latest: 2026-02-23T18:44:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18406v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.18406v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training$\unicode{x2013}$for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to learn equivariant operators in a latent space, from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于鲁棒物体识别的潜在等变算子：前景与挑战</div>
<div class="mono" style="margin-top:8px">尽管深度学习在计算机视觉领域取得了成功，但在识别训练中很少见的群对称变换下的物体（例如以不寻常姿态、尺度、位置或其组合出现的物体）仍存在困难。等变神经网络是解决在对称变换下泛化问题的一种方法，但需要事先了解变换。另一种架构家族则提出在潜在空间中从对称变换的示例学习等变算子。本文使用旋转和位移的噪声MNIST简单数据集，展示了如何成功利用此类架构进行分布外分类，从而克服传统和等变网络的局限性。虽然这一概念具有吸引力，但我们讨论了在将这些架构扩展到更复杂数据集的过程中面临的挑战。</div>
</details>
</div>
<div class="card">
<div class="title">LAD: Learning Advantage Distribution for Reasoning</div>
<div class="meta-line">Authors: Wendi Li, Sharon Li</div>
<div class="meta-line">First: 2026-02-23T18:44:10+00:00 · Latest: 2026-02-23T18:44:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20132v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20132v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards. This paradigm can lead to overfitting to dominant reward signals, while neglecting alternative yet valid reasoning trajectories, thereby limiting diversity and exploration. To address this issue, we introduce Learning Advantage Distributions (LAD), a distribution-matching framework that replaces advantage maximization with learning the advantage-induced distribution. By establishing the equivalence between the optimal policy update and an advantage-based target distribution, we derive a practical LAD objective formulated as minimizing an $f$-divergence between the policy-induced and advantage-induced distributions. This yields a gradient update that increases likelihood for high-advantage responses while suppressing over-confident probability growth, preventing collapse without requiring auxiliary entropy regularization. LAD incurs no extra training cost compared to GRPO and scales naturally to LLM post-training. In a controlled bandit setting, LAD faithfully recovers the multimodal advantage distribution, validating the theoretical formulation. Experiments on math and code reasoning tasks across several LLM backbones show that LAD reliably improves both accuracy and generative diversity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LAD：用于推理的学习优势分布</div>
<div class="mono" style="margin-top:8px">当前大型模型推理的强化学习目标主要集中在最大化预期奖励上。这种范式可能导致对主导奖励信号的过拟合，同时忽略其他有效但不同的推理路径，从而限制了多样性和探索能力。为了解决这一问题，我们引入了学习优势分布（LAD），这是一种分布匹配框架，用学习优势诱导分布替代优势最大化。通过建立最优策略更新与基于优势的目标分布之间的等价关系，我们推导出一个实用的LAD目标，其形式为最小化策略诱导分布与优势诱导分布之间的$f$-散度。这产生了一种梯度更新，既增加了高优势响应的似然性，又抑制了过度自信的概率增长，从而防止崩溃，而无需额外的熵正则化。与GRPO相比，LAD不增加额外的训练成本，并且可以自然地扩展到LLM的后训练阶段。在受控的多臂老虎机环境中，LAD忠实恢复了多模态的优势分布，验证了理论公式。在多个LLM主干模型上的数学和代码推理任务实验表明，LAD能够可靠地提升准确性和生成多样性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Current reinforcement learning objectives for large-model reasoning primarily focus on maximizing expected rewards.</div>
</details>
</div>
<div class="card">
<div class="title">To Reason or Not to: Selective Chain-of-Thought in Medical Question Answering</div>
<div class="meta-line">Authors: Zaifu Zhan, Min Zeng, Shuang Zhou, Yiran Song, Xiaoyi Chen, Yu Hou, Yifan Wu, Yang Ruan, Rui Zhang</div>
<div class="meta-line">First: 2026-02-23T18:42:50+00:00 · Latest: 2026-02-23T18:42:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20130v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20130v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Objective: To improve the efficiency of medical question answering (MedQA) with large language models (LLMs) by avoiding unnecessary reasoning while maintaining accuracy.
  Methods: We propose Selective Chain-of-Thought (Selective CoT), an inference-time strategy that first predicts whether a question requires reasoning and generates a rationale only when needed. Two open-source LLMs (Llama-3.1-8B and Qwen-2.5-7B) were evaluated on four biomedical QA benchmarks-HeadQA, MedQA-USMLE, MedMCQA, and PubMedQA. Metrics included accuracy, total generated tokens, and inference time.
  Results: Selective CoT reduced inference time by 13-45% and token usage by 8-47% with minimal accuracy loss ($\leq$4\%). In some model-task pairs, it achieved both higher accuracy and greater efficiency than standard CoT. Compared with fixed-length CoT, Selective CoT reached similar or superior accuracy at substantially lower computational cost.
  Discussion: Selective CoT dynamically balances reasoning depth and efficiency by invoking explicit reasoning only when beneficial, reducing redundancy on recall-type questions while preserving interpretability.
  Conclusion: Selective CoT provides a simple, model-agnostic, and cost-effective approach for medical QA, aligning reasoning effort with question complexity to enhance real-world deployability of LLM-based clinical systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>是否需要推理：医学问答中的选择性链式推理</div>
<div class="mono" style="margin-top:8px">目标：通过避免不必要的推理同时保持准确性，提高大型语言模型（LLMs）在医学问答（MedQA）任务中的效率。
方法：我们提出了一种选择性链式推理（Selective CoT）策略，在推理过程中首先预测问题是否需要推理，仅在需要时生成推理过程。我们在四个生物医学问答基准数据集（HeadQA、MedQA-USMLE、MedMCQA 和 PubMedQA）上评估了两个开源 LLM（Llama-3.1-8B 和 Qwen-2.5-7B）。评估指标包括准确率、生成的总 token 数和推理时间。
结果：选择性链式推理在保持准确率（≤4%）的情况下，将推理时间减少了 13-45%，token 使用量减少了 8-47%。在某些模型-任务对中，它在准确率和效率上均优于标准链式推理。与固定长度链式推理相比，选择性链式推理在显著更低的计算成本下达到了相似或更高的准确率。
讨论：选择性链式推理通过仅在有益时调用显式推理，动态平衡推理深度与效率，减少回忆类问题的冗余，同时保持可解释性。
结论：选择性链式推理为医学问答提供了一种简单、模型无关且成本效益高的方法，通过与问题复杂度匹配推理努力，提高基于 LLM 的临床系统的实际部署能力。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptation to Intrinsic Dependence in Diffusion Language Models</div>
<div class="meta-line">Authors: Yunxiao Zhao, Changxiao Cai</div>
<div class="meta-line">First: 2026-02-23T18:41:34+00:00 · Latest: 2026-02-23T18:41:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20126v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20126v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) approaches, enabling parallel token generation beyond a rigid left-to-right order. Despite growing empirical success, the theoretical understanding of how unmasking schedules -- which specify the order and size of unmasked tokens during sampling -- affect generation quality remains limited. In this work, we introduce a distribution-agnostic unmasking schedule for DLMs that adapts to the (unknown) dependence structure of the target data distribution, without requiring any prior knowledge or hyperparameter tuning. In contrast to prior deterministic procedures that fix unmasking sizes, our method randomizes the number of tokens revealed at each iteration. We show that, for two specific parameter choices, the sampling convergence guarantees -- measured by Kullback-Leibler (KL) divergence -- scale as $\widetilde O(\mathsf{TC}/K)$ and $\widetilde O(\mathsf{DTC}/K)$ respectively. Here, $K$ is the number of iterations, and $\mathsf{TC}$ and $\mathsf{DTC}$ are the total correlation and dual total correlation of the target distribution, capturing the intrinsic dependence structure underlying the data. Importantly, our guarantees hold in the practically relevant parallel-sampling regime $K&lt;L$ where $L$ is the token sequence length. These results significantly improve upon prior convergence theories and yield substantial sampling acceleration for low-complexity distributions. Overall, our findings unveil the adaptivity of DLMs to intrinsic data structures and shed light on the benefit of randomized unmasking sizes in inference schedule design.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散语言模型对内在依赖性的适应</div>
<div class="mono" style="margin-top:8px">扩散语言模型（DLMs）最近作为一种有前景的替代自回归（AR）方法出现，使得在严格的从左到右顺序之外实现并行生成标记成为可能。尽管在实证上取得了显著成功，但关于解掩码调度（即在采样过程中指定解掩码标记的顺序和数量）如何影响生成质量的理论理解仍有限。在本文中，我们引入了一种与分布无关的解掩码调度方法，该方法能够适应目标数据分布的（未知）依赖结构，而无需任何先验知识或超参数调优。与之前确定性的固定解掩码大小的方法不同，我们的方法在每次迭代中随机化揭示的标记数量。我们证明，对于两种特定的参数选择，采样收敛保证（以Kullback-Leibler（KL）散度衡量）分别按 $\widetilde O(\mathsf{TC}/K)$ 和 $\widetilde O(\mathsf{DTC}/K)$ 的速率缩放。其中，$K$ 是迭代次数，$\mathsf{TC}$ 和 $\mathsf{DTC}$ 分别是目标分布的总相关性和双重总相关性，捕捉了数据背后的内在依赖结构。重要的是，我们的保证适用于实际相关的并行采样范式 $K &lt; L$，其中 $L$ 是标记序列长度。这些结果显著改进了之前的收敛理论，并为低复杂度分布提供了显著的采样加速。总体而言，我们的发现揭示了DLMs对内在数据结构的适应性，并阐明了在推理调度设计中随机解掩码大小的优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Diffusion language models (DLMs) have recently emerged as a promising alternative to autoregressive (AR) approaches, enabling parallel token generation beyond a rigid left-to-right order.</div>
</details>
</div>
<div class="card">
<div class="title">NanoKnow: How to Know What Your Language Model Knows</div>
<div class="meta-line">Authors: Lingwei Gu, Nour Jedidi, Jimmy Lin</div>
<div class="meta-line">First: 2026-02-23T18:37:49+00:00 · Latest: 2026-02-23T18:37:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20122v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20122v1">PDF</a> · <a href="https://github.com/castorini/NanoKnow">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How do large language models (LLMs) know what they know? Answering this question has been difficult because pre-training data is often a &quot;black box&quot; -- unknown or inaccessible. The recent release of nanochat -- a family of small LLMs with fully open pre-training data -- addresses this as it provides a transparent view into where a model&#x27;s parametric knowledge comes from. Towards the goal of understanding how knowledge is encoded by LLMs, we release NanoKnow, a benchmark dataset that partitions questions from Natural Questions and SQuAD into splits based on whether their answers are present in nanochat&#x27;s pre-training corpus. Using these splits, we can now properly disentangle the sources of knowledge that LLMs rely on when producing an output. To demonstrate NanoKnow&#x27;s utility, we conduct experiments using eight nanochat checkpoints. Our findings show: (1) closed-book accuracy is strongly influenced by answer frequency in the pre-training data, (2) providing external evidence can mitigate this frequency dependence, (3) even with external evidence, models are more accurate when answers were seen during pre-training, demonstrating that parametric and external knowledge are complementary, and (4) non-relevant information is harmful, with accuracy decreasing based on both the position and the number of non-relevant contexts. We release all NanoKnow artifacts at https://github.com/castorini/NanoKnow.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NanoKnow：如何了解你的语言模型知道什么</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）是如何知道它们所知道的内容的？回答这个问题一直很困难，因为预训练数据通常是“黑箱”——未知或无法访问。最近发布的 nanochat——一个具有完全开放预训练数据的小型 LLM 家族——解决了这一问题，因为它提供了透明的视角，揭示模型参数知识的来源。为了理解 LLMs 如何编码知识，我们发布了 NanoKnow，一个基准数据集，它根据问题的答案是否存在于 nanochat 的预训练语料库中，将自然问题（Natural Questions）和 SQuAD 的问题划分为不同的子集。通过这些子集，我们现在可以正确地分离 LLMs 在生成输出时所依赖的知识来源。为了展示 NanoKnow 的实用性，我们使用八个 nanochat 检查点进行了实验。我们的发现表明：（1）闭卷准确率强烈受到预训练数据中答案频率的影响；（2）提供外部证据可以缓解这种频率依赖性；（3）即使有外部证据，当答案在预训练期间出现过时，模型的准确性更高，这表明参数知识和外部知识是互补的；（4）非相关信息是有害的，准确率会随着非相关上下文的位置和数量而下降。我们将在 https://github.com/castorini/NanoKnow 发布所有 NanoKnow 相关资源。</div>
</details>
</div>
<div class="card">
<div class="title">Skykatana: a scalable framework to construct sky masks for the Vera Rubin Observatory and large astronomical surveys</div>
<div class="meta-line">Authors: Claudio Lopez, Emilio Donoso, Mariano Javier de L. Dominguez Romero</div>
<div class="meta-line">Venue: Lopez et al., 2026, Astronomy and Computing, 55, 101083</div>
<div class="meta-line">First: 2025-12-16T19:10:22+00:00 · Latest: 2026-02-23T18:37:09+00:00</div>
<div class="meta-line">Comments: 21 pages, 18 figures, accepted for publication in Astronomy and Computing Journal</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14848v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.14848v2">PDF</a> · <a href="https://github.com/samotracio/skykatana">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://osf.io/r5vw6">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern wide-field surveys require robust spatial masks to excise bright-star halos, bleed trails, poor-quality regions, and user-defined geometry at scale. We present Skykatana, an open source pipeline that builds and combines boolean HEALPix/HEALSparse maps into science-ready masks and engineered for low-memory operation. Skykatana can efficiently construct, visualize multi-order coverage maps and generate random points in high-resolution masks over half of the celestial sphere with very limited resources and leveraging the hierarchical partition of data the HATS/LSDB framework. We demonstrate two end-to-end applications: (1) a Subaru HSC-WISE composite mask; and (2) Rubin star masks generated on demand in the Rubin Science Platform by querying HATS/LSDB Gaia data and assigning radii from empirical fits to Rubin DP1 data. We release full bright-star masks for various regions of the Rubin footprint and describe performance and scaling. The code, documentation, and examples are publicly available at https://github.com/samotracio/skykatana, and the LSST masks can be obtained from https://osf.io/r5vw6</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Skykatana：为Vera Rubin天文台和大型天文调查构建天空掩膜的可扩展框架</div>
<div class="mono" style="margin-top:8px">现代宽视场调查需要稳健的空间掩膜来去除亮星晕、拖尾、质量差区域和用户定义的几何形状。我们提出了Skykatana，这是一个开源流水线，用于构建和组合布尔型HEALPix/HEALSparse地图，生成科学可用的掩膜，并专为低内存操作设计。Skykatana可以高效地构建、可视化多阶覆盖地图，并利用HATS/LSDB框架的数据分层分区，在有限资源下生成高分辨率掩膜中半球的随机点。我们展示了两个端到端的应用：(1) Subaru HSC-WISE合成掩膜；(2) 通过查询HATS/LSDB的Gaia数据并根据对Rubin DP1数据的经验拟合分配半径，按需生成Rubin星掩膜。我们发布了Rubin覆盖区域的各种亮星掩膜，并描述了其性能和扩展性。代码、文档和示例可在https://github.com/samotracio/skykatana上公开获取，LSST掩膜可在https://osf.io/r5vw6获取。</div>
</details>
</div>
<div class="card">
<div class="title">Towards Unifying Perceptual Reasoning and Logical Reasoning</div>
<div class="meta-line">Authors: Hiroyuki Kido</div>
<div class="meta-line">First: 2022-06-27T10:32:47+00:00 · Latest: 2026-02-23T18:36:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2206.13174v2">Abs</a> · <a href="https://arxiv.org/pdf/2206.13174v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">An increasing number of scientific experiments support the view of perception as Bayesian inference, which is rooted in Helmholtz&#x27;s view of perception as unconscious inference. Recent study of logic presents a view of logical reasoning as Bayesian inference. In this paper, we give a simple probabilistic model that is applicable to both perceptual reasoning and logical reasoning. We show that the model unifies the two essential processes common in perceptual and logical systems: on the one hand, the process by which perceptual and logical knowledge is derived from another knowledge, and on the other hand, the process by which such knowledge is derived from data. We fully characterise the model in terms of logical consequence relations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向感知推理与逻辑推理的统一</div>
<div class="mono" style="margin-top:8px">越来越多的科学实验支持将感知视为贝叶斯推断的观点，这一观点源于赫尔姆霍茨提出的无意识推断的感知观。最近对逻辑的研究提出将逻辑推理视为贝叶斯推断的观点。本文提出一个适用于感知推理和逻辑推理的简单概率模型。我们展示了该模型如何统一感知系统和逻辑系统中两种基本过程：一方面，感知和逻辑知识如何从其他知识中推导出来；另一方面，这些知识如何从数据中推导出来。我们从逻辑蕴含关系的角度对模型进行了完整刻画。</div>
</details>
</div>
<div class="card">
<div class="title">NovaPlan: Zero-Shot Long-Horizon Manipulation via Closed-Loop Video Language Planning</div>
<div class="meta-line">Authors: Jiahui Fu, Junyu Nan, Lingfeng Sun, Hongyu Li, Jianing Qian, Jennifer L. Barry, Kris Kitani, George Konidaris</div>
<div class="meta-line">First: 2026-02-23T18:35:18+00:00 · Latest: 2026-02-23T18:35:18+00:00</div>
<div class="meta-line">Comments: 25 pages, 15 figures. Project webpage: https://nova-plan.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20119v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20119v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://nova-plan.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Solving long-horizon tasks requires robots to integrate high-level semantic reasoning with low-level physical interaction. While vision-language models (VLMs) and video generation models can decompose tasks and imagine outcomes, they often lack the physical grounding necessary for real-world execution. We introduce NovaPlan, a hierarchical framework that unifies closed-loop VLM and video planning with geometrically grounded robot execution for zero-shot long-horizon manipulation. At the high level, a VLM planner decomposes tasks into sub-goals and monitors robot execution in a closed loop, enabling the system to recover from single-step failures through autonomous re-planning. To compute low-level robot actions, we extract and utilize both task-relevant object keypoints and human hand poses as kinematic priors from the generated videos, and employ a switching mechanism to choose the better one as a reference for robot actions, maintaining stable execution even under heavy occlusion or depth inaccuracy. We demonstrate the effectiveness of NovaPlan on three long-horizon tasks and the Functional Manipulation Benchmark (FMB). Our results show that NovaPlan can perform complex assembly tasks and exhibit dexterous error recovery behaviors without any prior demonstrations or training. Project page: https://nova-plan.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>NovaPlan: 通过闭环视频语言规划实现零样本长时序操作</div>
<div class="mono" style="margin-top:8px">解决长时序任务需要机器人将高级语义推理与低级物理交互相结合。虽然视觉-语言模型（VLMs）和视频生成模型可以分解任务并想象结果，但它们通常缺乏现实世界执行所需的物理基础。我们提出了NovaPlan，一个将闭环视觉-语言模型与视频规划统一起来的分层框架，结合几何基础的机器人执行，实现零样本长时序操作。在高层，视觉-语言规划器将任务分解为子目标，并通过闭环监控机器人执行，使系统能够通过自主重新规划从单步失败中恢复。为了计算低层机器人动作，我们从生成的视频中提取并利用任务相关的物体关键点和人类手部姿态作为运动学先验，并采用切换机制选择更优的作为机器人动作的参考，即使在严重遮挡或深度不准确的情况下也能保持稳定执行。我们在三个长时序任务和功能操作基准（FMB）上展示了NovaPlan的有效性。我们的结果表明，NovaPlan可以在没有任何先验演示或训练的情况下执行复杂的装配任务，并表现出灵活的错误恢复行为。项目页面: https://nova-plan.github.io/</div>
</details>
</div>
<div class="card">
<div class="title">ReSyn: Autonomously Scaling Synthetic Environments for Reasoning Models</div>
<div class="meta-line">Authors: Andre He, Nathaniel Weir, Kaj Bostrom, Allen Nie, Darion Cassel, Sam Bayless, Huzefa Rangwala</div>
<div class="meta-line">First: 2026-02-23T18:34:29+00:00 · Latest: 2026-02-23T18:34:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.20117v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.20117v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning with verifiable rewards (RLVR) has emerged as a promising approach for training reasoning language models (RLMs) by leveraging supervision from verifiers. Although verifier implementation is easier than solution annotation for many tasks, existing synthetic data generation methods remain largely solution-centric, while verifier-based methods rely on a few hand-crafted procedural environments. In this work, we scale RLVR by introducing ReSyn, a pipeline that generates diverse reasoning environments equipped with instance generators and verifiers, covering tasks such as constraint satisfaction, algorithmic puzzles, and spatial reasoning. A Qwen2.5-7B-Instruct model trained with RL on ReSyn data achieves consistent gains across reasoning benchmarks and out-of-domain math benchmarks, including a 27\% relative improvement on the challenging BBEH benchmark. Ablations show that verifier-based supervision and increased task diversity both contribute significantly, providing empirical evidence that generating reasoning environments at scale can enhance reasoning abilities in RLMs</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReSyn：为推理模型自主扩展合成环境</div>
<div class="mono" style="margin-top:8px">可验证奖励的强化学习（RLVR）作为一种利用验证器监督来训练推理语言模型（RLMs）的方法，已展现出巨大潜力。尽管对于许多任务来说，验证器的实现比解决方案标注更容易，但现有的合成数据生成方法仍主要以解决方案为中心，而基于验证器的方法则依赖于少数手工构建的程序化环境。在本工作中，我们通过引入ReSyn，一个能够生成多样推理环境的流水线，包括实例生成器和验证器，覆盖约束满足、算法谜题和空间推理等任务。在ReSyn数据上使用RL训练的Qwen2.5-7B-Instruct模型在多个推理基准和领域外数学基准上均取得了一致的提升，包括在具有挑战性的BBEH基准上实现了27%的相对提升。消融实验表明，基于验证器的监督和任务多样性增加都显著促进了模型性能，提供了实证证据表明大规模生成推理环境可以增强RLMs的推理能力</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260224_0435.html">20260224_0435</a>
<a href="archive/20260223_0401.html">20260223_0401</a>
<a href="archive/20260222_0402.html">20260222_0402</a>
<a href="archive/20260221_0415.html">20260221_0415</a>
<a href="archive/20260220_0410.html">20260220_0410</a>
<a href="archive/20260219_0419.html">20260219_0419</a>
<a href="archive/20260218_0416.html">20260218_0416</a>
<a href="archive/20260217_0410.html">20260217_0410</a>
<a href="archive/20260216_0403.html">20260216_0403</a>
<a href="archive/20260215_0401.html">20260215_0401</a>
<a href="archive/20260213_0421.html">20260213_0421</a>
<a href="archive/20260212_0425.html">20260212_0425</a>
<a href="archive/20260210_0435.html">20260210_0435</a>
<a href="archive/20260209_0404.html">20260209_0404</a>
<a href="archive/20260208_0353.html">20260208_0353</a>
<a href="archive/20260207_0410.html">20260207_0410</a>
<a href="archive/20260206_0412.html">20260206_0412</a>
<a href="archive/20260205_0417.html">20260205_0417</a>
<a href="archive/20260204_0421.html">20260204_0421</a>
<a href="archive/20260202_0402.html">20260202_0402</a>
<a href="archive/20260201_0354.html">20260201_0354</a>
<a href="archive/20260131_0408.html">20260131_0408</a>
<a href="archive/20260130_0406.html">20260130_0406</a>
<a href="archive/20260129_0407.html">20260129_0407</a>
<a href="archive/20260128_0407.html">20260128_0407</a>
<a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
