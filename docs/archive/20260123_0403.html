<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-23 04:03</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260123_0403</div>
    <div class="row"><div class="card">
<div class="title">APPLE: Attribute-Preserving Pseudo-Labeling for Diffusion-Based Face Swapping</div>
<div class="meta-line">Authors: Jiwon Kang, Yeji Choi, JoungBin Lee, Wooseok Jang, Jinhyeok Choi, Taekeun Kang, Yongjae Park, Myungin Kim, Seungryong Kim</div>
<div class="meta-line">First: 2026-01-21T18:59:55+00:00 · Latest: 2026-01-21T18:59:55+00:00</div>
<div class="meta-line">Comments: Project Page: https://cvlab-kaist.github.io/APPLE/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15288v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15288v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://cvlab-kaist.github.io/APPLE/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Face swapping aims to transfer the identity of a source face onto a target face while preserving target-specific attributes such as pose, expression, lighting, skin tone, and makeup. However, since real ground truth for face swapping is unavailable, achieving both accurate identity transfer and high-quality attribute preservation remains challenging. In addition, recent diffusion-based approaches attempt to improve visual fidelity through conditional inpainting on masked target images, but the masked condition removes crucial appearance cues of target, resulting in plausible yet misaligned attributes. To address these limitations, we propose APPLE (Attribute-Preserving Pseudo-Labeling), a diffusion-based teacher-student framework that enhances attribute fidelity through attribute-aware pseudo-label supervision. We reformulate face swapping as a conditional deblurring task to more faithfully preserve target-specific attributes such as lighting, skin tone, and makeup. In addition, we introduce an attribute-aware inversion scheme to further improve detailed attribute preservation. Through an elaborate attribute-preserving design for teacher learning, APPLE produces high-quality pseudo triplets that explicitly provide the student with direct face-swapping supervision. Overall, APPLE achieves state-of-the-art performance in terms of attribute preservation and identity transfer, producing more photorealistic and target-faithful results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>APPLE: 基于属性保持的伪标签生成用于基于扩散模型的面部替换</div>
<div class="mono" style="margin-top:8px">面部替换旨在将源面部的身份转移到目标面部，同时保持目标特定的属性，如姿态、表情、光照、肤色和妆容。然而，由于面部替换的真实标签不可用，实现准确的身份转移和高质量的属性保持仍然具有挑战性。此外，最近的基于扩散模型的方法尝试通过在遮罩目标图像上进行条件修复来提高视觉保真度，但遮罩条件去除了目标图像中关键的外观线索，导致属性保持合理但不准确。为了解决这些限制，我们提出了APPLE（Attribute-Preserving Pseudo-Labeling），这是一种基于扩散模型的教师-学生框架，通过属性感知的伪标签监督来增强属性保真度。我们将面部替换重新表述为一个条件去模糊任务，以更忠实地保持目标特定的属性，如光照、肤色和妆容。此外，我们引入了一种属性感知的反向方案，以进一步提高详细属性的保持效果。通过在教师学习中精心设计的属性保持机制，APPLE生成高质量的伪三元组，为学生提供直接的面部替换监督。总体而言，APPLE在属性保持和身份转移方面达到了最先进的性能，生成了更逼真且更符合目标的面部替换结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Face swapping aims to transfer the identity of a source face onto a target face while preserving target-specific attributes such as pose, expression, lighting, skin tone, and makeup.</div>
</details>
</div>
<div class="card">
<div class="title">Towards Understanding Best Practices for Quantization of Vision-Language Models</div>
<div class="meta-line">Authors: Gautom Das, Vincent La, Ethan Lau, Abhinav Shrivastava, Matthew Gwilliam</div>
<div class="meta-line">First: 2026-01-21T18:59:51+00:00 · Latest: 2026-01-21T18:59:51+00:00</div>
<div class="meta-line">Comments: 15 pages, 12 figures, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15287v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15287v1">PDF</a> · <a href="https://github.com/gautomdas/mmq">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) deliver impressive results for a variety of tasks, but state-of-the-art systems require fast GPUs with large amounts of memory. To reduce both the memory and latency of these systems, practitioners quantize their learned parameters, typically at half precision. A growing body of research focuses on preserving the model performance with more aggressive bit widths, and some work has been done to apply these strategies to other models, like vision transformers. In our study we investigate how a variety of quantization methods, including state-of-the-art GPTQ and AWQ, can be applied effectively to multimodal pipelines comprised of vision models, language models, and their connectors. We address how performance on captioning, retrieval, and question answering can be affected by bit width, quantization method, and which portion of the pipeline the quantization is used for. Results reveal that ViT and LLM exhibit comparable importance in model performance, despite significant differences in parameter size, and that lower-bit quantization of the LLM achieves high accuracy at reduced bits per weight (bpw). These findings provide practical insights for efficient deployment of MLLMs and highlight the value of exploration for understanding component sensitivities in multimodal models. Our code is available at https://github.com/gautomdas/mmq.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>理解视觉-语言模型量化最佳实践</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在多种任务中表现出色，但最先进的系统需要配备高速GPU和大量内存。为了减少这些系统的内存和延迟，从业者通常将学习到的参数进行量化，通常采用半精度。越来越多的研究致力于在更激进的位宽下保持模型性能，一些工作也尝试将这些策略应用于其他模型，如视觉变换器（ViT）。在我们的研究中，我们探讨了多种量化方法，包括最先进的GPTQ和AWQ，如何有效地应用于由视觉模型、语言模型及其连接器组成的多模态流水线。我们分析了位宽、量化方法以及量化应用于流水线的哪个部分对图像描述、检索和问答任务性能的影响。结果表明，尽管参数规模存在显著差异，ViT和LLM在模型性能中表现出相当的重要性，而对LLM进行低位宽量化可以在降低每权重位数（bpw）的同时实现高精度。这些发现为多模态大语言模型（MLLMs）的高效部署提供了实用见解，并突显了探索在理解多模态模型组件敏感性方面的价值。我们的代码可在https://github.com/gautomdas/mmq获取。</div>
</details>
</div>
<div class="card">
<div class="title">Iterative Refinement Improves Compositional Image Generation</div>
<div class="meta-line">Authors: Shantanu Jaiswal, Mihir Prabhudesai, Nikash Bhardwaj, Zheyang Qin, Amir Zadeh, Chuan Li, Katerina Fragkiadaki, Deepak Pathak</div>
<div class="meta-line">First: 2026-01-21T18:59:40+00:00 · Latest: 2026-01-21T18:59:40+00:00</div>
<div class="meta-line">Comments: Project webpage: https://iterative-img-gen.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15286v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15286v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://iterative-img-gen.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes. Existing inference-time strategies, such as parallel sampling with verifiers or simply increasing denoising steps, can improve prompt alignment but remain inadequate for richly compositional settings where many constraints must be satisfied. Inspired by the success of chain-of-thought reasoning in large language models, we propose an iterative test-time strategy in which a T2I model progressively refines its generations across multiple steps, guided by feedback from a vision-language model as the critic in the loop. Our approach is simple, requires no external tools or priors, and can be flexibly applied to a wide range of image generators and vision-language models. Empirically, we demonstrate consistent gains on image generation across benchmarks: a 16.9% improvement in all-correct rate on ConceptMix (k=7), a 13.8% improvement on T2I-CompBench (3D-Spatial category) and a 12.5% improvement on Visual Jenga scene decomposition compared to compute-matched parallel sampling. Beyond quantitative gains, iterative refinement produces more faithful generations by decomposing complex prompts into sequential corrections, with human evaluators preferring our method 58.7% of the time over 41.3% for the parallel baseline. Together, these findings highlight iterative self-correction as a broadly applicable principle for compositional image generation. Results and visualizations are available at https://iterative-img-gen.github.io/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迭代精炼提升组合图像生成</div>
<div class="mono" style="margin-top:8px">文本到图像（T2I）模型取得了显著进展，但在处理需要同时处理多个对象、关系和属性的复杂提示时仍存在困难。现有的推理时间策略，如使用验证器进行并行采样或简单地增加去噪步骤，虽然可以改善提示对齐，但在需要满足多个约束条件的丰富组合场景中仍显不足。受大型语言模型中链式思维推理成功的影响，我们提出了一种迭代的测试时间策略，其中T2I模型在多个步骤中逐步精炼其生成，由视觉-语言模型作为循环中的批评者提供反馈。我们的方法简单，不需要外部工具或先验知识，并且可以灵活应用于各种图像生成器和视觉-语言模型。在基准测试中，我们实验证明了图像生成的一致性提升：在ConceptMix（k=7）上，所有正确率提高了16.9%；在T2I-CompBench（3D-空间类别）上提高了13.8%；在Visual Jenga场景分解上提高了12.5%，相比计算匹配的并行采样。除了定量提升，迭代精炼通过将复杂提示分解为顺序修正，生成更忠实的图像，人类评估者在58.7%的情况下更偏好我们的方法，而并行基线仅为41.3%。这些发现共同突显了迭代自我校正作为组合图像生成广泛适用原则的重要性。结果和可视化内容可在 https://iterative-img-gen.github.io/ 查看。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Text-to-image (T2I) models have achieved remarkable progress, yet they continue to struggle with complex prompts that require simultaneously handling multiple objects, relations, and attributes.</div>
</details>
</div>
<div class="card">
<div class="title">Walk through Paintings: Egocentric World Models from Internet Priors</div>
<div class="meta-line">Authors: Anurag Bagchi, Zhipeng Bao, Homanga Bharadhwaj, Yu-Xiong Wang, Pavel Tokmakov, Martial Hebert</div>
<div class="meta-line">First: 2026-01-21T18:59:32+00:00 · Latest: 2026-01-21T18:59:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15284v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15284v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">What if a video generation model could not only imagine a plausible future, but the correct one, accurately reflecting how the world changes with each action? We address this question by presenting the Egocentric World Model (EgoWM), a simple, architecture-agnostic method that transforms any pretrained video diffusion model into an action-conditioned world model, enabling controllable future prediction. Rather than training from scratch, we repurpose the rich world priors of Internet-scale video models and inject motor commands through lightweight conditioning layers. This allows the model to follow actions faithfully while preserving realism and strong generalization. Our approach scales naturally across embodiments and action spaces, ranging from 3-DoF mobile robots to 25-DoF humanoids, where predicting egocentric joint-angle-driven dynamics is substantially more challenging. The model produces coherent rollouts for both navigation and manipulation tasks, requiring only modest fine-tuning. To evaluate physical correctness independently of visual appearance, we introduce the Structural Consistency Score (SCS), which measures whether stable scene elements evolve consistently with the provided actions. EgoWM improves SCS by up to 80 percent over prior state-of-the-art navigation world models, while achieving up to six times lower inference latency and robust generalization to unseen environments, including navigation inside paintings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>穿越绘画：从互联网先验知识中构建以自我为中心的世界模型</div>
<div class="mono" style="margin-top:8px">如果一个视频生成模型不仅能想象一个合理的未来，还能准确预测正确的未来，真实反映世界随每个动作的变化，那会怎样？我们通过提出以自我为中心的世界模型（EgoWM）来解决这个问题，这是一种简单且与架构无关的方法，能够将任何预训练的视频扩散模型转化为基于动作的世界模型，从而实现可控的未来预测。我们没有从头开始训练，而是利用互联网规模视频模型中丰富的世界先验知识，并通过轻量级的条件层注入运动指令。这使得模型能够忠实地跟随动作，同时保持现实感和强大的泛化能力。我们的方法在不同实体和动作空间中具有自然的可扩展性，从3自由度的移动机器人到25自由度的人形机器人，其中预测以自我为中心的关节角度驱动的动力学要困难得多。该模型能够为导航和操作任务生成连贯的轨迹，仅需适度微调即可实现。为了独立评估物理正确性，我们引入了结构一致性分数（SCS），用于衡量稳定场景元素是否与提供的动作一致演化。EgoWM在先前最先进的导航世界模型基础上，将SCS提升了最高80%，同时实现了最高六倍的推理延迟降低，并且对未见过的环境具有更强的泛化能力，包括在绘画内部的导航任务。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">What if a video generation model could not only imagine a plausible future, but the correct one, accurately reflecting how the world changes with each action?</div>
</details>
</div>
<div class="card">
<div class="title">LuxRemix: Lighting Decomposition and Remixing for Indoor Scenes</div>
<div class="meta-line">Authors: Ruofan Liang, Norman Müller, Ethan Weber, Duncan Zauss, Nandita Vijaykumar, Peter Kontschieder, Christian Richardt</div>
<div class="meta-line">First: 2026-01-21T18:59:22+00:00 · Latest: 2026-01-21T18:59:22+00:00</div>
<div class="meta-line">Comments: Project page: https://luxremix.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15283v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15283v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://luxremix.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a novel approach for interactive light editing in indoor scenes from a single multi-view scene capture. Our method leverages a generative image-based light decomposition model that factorizes complex indoor scene illumination into its constituent light sources. This factorization enables independent manipulation of individual light sources, specifically allowing control over their state (on/off), chromaticity, and intensity. We further introduce multi-view lighting harmonization to ensure consistent propagation of the lighting decomposition across all scene views. This is integrated into a relightable 3D Gaussian splatting representation, providing real-time interactive control over the individual light sources. Our results demonstrate highly photorealistic lighting decomposition and relighting outcomes across diverse indoor scenes. We evaluate our method on both synthetic and real-world datasets and provide a quantitative and qualitative comparison to state-of-the-art techniques. For video results and interactive demos, see https://luxremix.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LuxRemix：室内场景的光照分解与重混</div>
<div class="mono" style="margin-top:8px">我们提出了一种新颖的方法，用于从单个多视角场景捕捉中实现室内场景的交互式光照编辑。我们的方法利用了一个基于图像的生成式光照分解模型，将复杂的室内场景光照分解为各个光源成分。这种分解使得可以独立操控每个光源，特别是其开关状态、色度和强度。我们进一步引入了多视角光照和谐化技术，以确保光照分解在所有场景视角中的一致传播。该技术被集成到可重新照明的3D高斯点云表示中，从而实现对各个光源的实时交互控制。我们的实验结果展示了在多种室内场景中高度逼真的光照分解和重新照明效果。我们在合成和真实世界数据集上评估了我们的方法，并与最先进的技术进行了定量和定性比较。视频结果和交互式演示请参见 https://luxremix.github.io。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking Video Generation Model for the Embodied World</div>
<div class="meta-line">Authors: Yufan Deng, Zilin Pan, Hongyu Zhang, Xiaojie Li, Ruoqing Hu, Yufei Ding, Yiming Zou, Yan Zeng, Daquan Zhou</div>
<div class="meta-line">First: 2026-01-21T18:59:18+00:00 · Latest: 2026-01-21T18:59:18+00:00</div>
<div class="meta-line">Comments: Github: https://github.com/DAGroup-PKU/ReVidgen/ Project website: https://dagroup-pku.github.io/ReVidgen.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15282v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15282v1">PDF</a> · <a href="https://github.com/DAGroup-PKU/ReVidgen/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://dagroup-pku.github.io/ReVidgen.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video generation models have significantly advanced embodied intelligence, unlocking new possibilities for generating diverse robot data that capture perception, reasoning, and action in the physical world. However, synthesizing high-quality videos that accurately reflect real-world robotic interactions remains challenging, and the lack of a standardized benchmark limits fair comparisons and progress. To address this gap, we introduce a comprehensive robotics benchmark, RBench, designed to evaluate robot-oriented video generation across five task domains and four distinct embodiments. It assesses both task-level correctness and visual fidelity through reproducible sub-metrics, including structural consistency, physical plausibility, and action completeness. Evaluation of 25 representative models highlights significant deficiencies in generating physically realistic robot behaviors. Furthermore, the benchmark achieves a Spearman correlation coefficient of 0.96 with human evaluations, validating its effectiveness. While RBench provides the necessary lens to identify these deficiencies, achieving physical realism requires moving beyond evaluation to address the critical shortage of high-quality training data. Driven by these insights, we introduce a refined four-stage data pipeline, resulting in RoVid-X, the largest open-source robotic dataset for video generation with 4 million annotated video clips, covering thousands of tasks and enriched with comprehensive physical property annotations. Collectively, this synergistic ecosystem of evaluation and data establishes a robust foundation for rigorous assessment and scalable training of video models, accelerating the evolution of embodied AI toward general intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考具身世界的视频生成模型</div>
<div class="mono" style="margin-top:8px">视频生成模型在推动具身智能方面取得了显著进展，为生成多样化的机器人数据打开了新可能，这些数据能够捕捉物理世界中的感知、推理和行动。然而，合成高质量的视频以准确反映现实中的机器人交互仍然具有挑战性，缺乏标准化基准也限制了公平比较和进展。为了解决这一问题，我们引入了一个全面的机器人基准测试集RBench，旨在评估面向机器人的视频生成模型，涵盖五个任务领域和四种不同的具身形式。该基准通过可重复的子指标，包括结构一致性、物理合理性以及动作完整性，评估任务层面的正确性和视觉保真度。对25个代表性模型的评估突显了生成物理现实的机器人行为存在显著不足。此外，该基准与人类评估的斯皮尔曼相关系数达到0.96，验证了其有效性。尽管RBench提供了识别这些不足的必要视角，但实现物理现实性需要超越评估，解决高质量训练数据的严重短缺问题。基于这些见解，我们提出了一种改进的四阶段数据管道，从而创建了RoVid-X，这是目前最大的开源机器人视频生成数据集，包含400万条标注视频片段，涵盖数千个任务，并附有全面的物理属性标注。总体而言，这一评估与数据的协同生态系统为视频模型的严格评估和可扩展训练奠定了坚实基础，加速了具身AI向通用智能演进的过程。</div>
</details>
</div>
<div class="card">
<div class="title">StableWorld: Towards Stable and Consistent Long Interactive Video Generation</div>
<div class="meta-line">Authors: Ying Yang, Zhengyao Lv, Tianlin Pan, Haofan Wang, Binxin Yang, Hubery Yin, Chen Li, Ziwei Liu, Chenyang Si</div>
<div class="meta-line">First: 2026-01-21T18:59:02+00:00 · Latest: 2026-01-21T18:59:02+00:00</div>
<div class="meta-line">Comments: 17 pages, 21 figures,</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15281v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15281v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we explore the overlooked challenge of stability and temporal consistency in interactive video generation, which synthesizes dynamic and controllable video worlds through interactive behaviors such as camera movements and text prompts. Despite remarkable progress in world modeling, current methods still suffer from severe instability and temporal degradation, often leading to spatial drift and scene collapse during long-horizon interactions. To better understand this issue, we initially investigate the underlying causes of instability and identify that the major source of error accumulation originates from the same scene, where generated frames gradually deviate from the initial clean state and propagate errors to subsequent frames. Building upon this observation, we propose a simple yet effective method, \textbf{StableWorld}, a Dynamic Frame Eviction Mechanism. By continuously filtering out degraded frames while retaining geometrically consistent ones, StableWorld effectively prevents cumulative drift at its source, leading to more stable and temporal consistency of interactive generation. Promising results on multiple interactive video models, \eg, Matrix-Game, Open-Oasis, and Hunyuan-GameCraft, demonstrate that StableWorld is model-agnostic and can be applied to different interactive video generation frameworks to substantially improve stability, temporal consistency, and generalization across diverse interactive scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>StableWorld：迈向稳定且一致的长时交互视频生成</div>
<div class="mono" style="margin-top:8px">本文探讨了交互视频生成中被忽视的稳定性与时间一致性挑战，该方法通过诸如摄像机运动和文本提示等交互行为合成动态且可控的视频世界。尽管在世界建模方面取得了显著进展，当前方法仍存在严重的不稳定性与时间退化问题，常导致长时间交互过程中出现空间漂移和场景崩溃。为更好地理解这一问题，我们首先研究了不稳定性的根本原因，并发现主要的误差累积来源是同一场景，其中生成的帧逐渐偏离初始干净状态，并将误差传播到后续帧中。基于这一观察，我们提出了一种简单而有效的方法 StableWorld，即动态帧淘汰机制。通过持续过滤退化的帧并保留几何一致的帧，StableWorld 有效从源头防止累积漂移，从而实现更稳定且时间一致的交互生成。在多个交互视频模型（例如 Matrix-Game、Open-Oasis 和 Hunyuan-GameCraft）上的有前景结果表明，StableWorld 是模型无关的，可以应用于不同的交互视频生成框架，显著提升稳定性、时间一致性和在多样化交互场景中的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">LLM-based Multimodal Feedback Produces Equivalent Learning and Better Student Perceptions than Educator Feedback</div>
<div class="meta-line">Authors: Chloe Qianhui Zhao, Jie Cao, Jionghao Lin, Kenneth R. Koedinger</div>
<div class="meta-line">First: 2026-01-21T18:58:08+00:00 · Latest: 2026-01-21T18:58:08+00:00</div>
<div class="meta-line">Comments: 11 pages, to be published at the 16th International Learning Analytics &amp; Knowledge Conference (LAK &#x27;26)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15280v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15280v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Providing timely, targeted, and multimodal feedback helps students quickly correct errors, build deep understanding and stay motivated, yet making it at scale remains a challenge. This study introduces a real-time AI-facilitated multimodal feedback system that integrates structured textual explanations with dynamic multimedia resources, including the retrieved most relevant slide page references and streaming AI audio narration. In an online crowdsourcing experiment, we compared this system against fixed business-as-usual feedback by educators across three dimensions: (1) learning effectiveness, (2) learner engagement, (3) perceived feedback quality and value. Results showed that AI multimodal feedback achieved learning gains equivalent to original educator feedback while significantly outperforming it on perceived clarity, specificity, conciseness, motivation, satisfaction, and reducing cognitive load, with comparable correctness, trust, and acceptance. Process logs revealed distinct engagement patterns: for multiple-choice questions, educator feedback encouraged more submissions; for open-ended questions, AI-facilitated targeted suggestions lowered revision barriers and promoted iterative improvement. These findings highlight the potential of AI multimodal feedback to provide scalable, real-time, and context-aware support that both reduces instructor workload and enhances student experience.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的多模态反馈在学习效果和学生感知方面与教师反馈相当且更优</div>
<div class="mono" style="margin-top:8px">及时、有针对性的多模态反馈有助于学生快速纠正错误、建立深入理解并保持动力，但在大规模应用中仍面临挑战。本研究引入了一种实时的人工智能辅助多模态反馈系统，该系统整合了结构化的文本解释与动态多媒体资源，包括检索到的最相关幻灯片页面引用和流媒体人工智能语音解说。在一项在线众包实验中，我们从三个维度将该系统与教师固定的常规反馈进行了比较：(1) 学习效果，(2) 学习者参与度，(3) 反馈质量和价值的感知。结果显示，人工智能多模态反馈在学习成效方面与原始教师反馈相当，同时在清晰度、具体性、简洁性、动机、满意度以及降低认知负荷方面显著优于教师反馈，其正确性、信任度和接受度也相当。过程日志揭示了不同的参与模式：对于多项选择题，教师反馈鼓励了更多的提交；而对于开放性问题，人工智能辅助的针对性建议降低了修改障碍，促进了迭代改进。这些发现突显了人工智能多模态反馈在提供可扩展、实时且情境感知支持方面的潜力，既能减轻教师的工作负担，又能提升学生的学习体验。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Providing timely, targeted, and multimodal feedback helps students quickly correct errors, build deep understanding and stay motivated, yet making it at scale remains a challenge.</div>
</details>
</div>
<div class="card">
<div class="title">MolecularIQ: Characterizing Chemical Reasoning Capabilities Through Symbolic Verification on Molecular Graphs</div>
<div class="meta-line">Authors: Christoph Bartmann, Johannes Schimunek, Mykyta Ielanskyi, Philipp Seidl, Günter Klambauer, Sohvi Luukkonen</div>
<div class="meta-line">First: 2026-01-21T18:58:01+00:00 · Latest: 2026-01-21T18:58:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15279v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15279v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A molecule&#x27;s properties are fundamentally determined by its composition and structure encoded in its molecular graph. Thus, reasoning about molecular properties requires the ability to parse and understand the molecular graph. Large Language Models (LLMs) are increasingly applied to chemistry, tackling tasks such as molecular name conversion, captioning, text-guided generation, and property or reaction prediction. Most existing benchmarks emphasize general chemical knowledge, rely on literature or surrogate labels that risk leakage or bias, or reduce evaluation to multiple-choice questions. We introduce MolecularIQ, a molecular structure reasoning benchmark focused exclusively on symbolically verifiable tasks. MolecularIQ enables fine-grained evaluation of reasoning over molecular graphs and reveals capability patterns that localize model failures to specific tasks and molecular structures. This provides actionable insights into the strengths and limitations of current chemistry LLMs and guides the development of models that reason faithfully over molecular structure.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MolecularIQ：通过分子图的符号验证表征化学推理能力</div>
<div class="mono" style="margin-top:8px">一种分子的性质从根本上由其在分子图中的组成和结构决定。因此，对分子性质的推理需要能够解析和理解分子图。大型语言模型（LLMs）正越来越多地应用于化学领域，处理诸如分子名称转换、描述生成、文本引导生成以及性质或反应预测等任务。大多数现有的基准测试侧重于一般的化学知识，依赖文献或替代标签，这些标签可能带来信息泄露或偏差，或者将评估简化为多项选择题。我们引入了MolecularIQ，这是一个专注于符号可验证任务的分子结构推理基准测试。MolecularIQ能够对分子图上的推理进行细致评估，并揭示模型在特定任务和分子结构上的失败模式。这为当前化学领域LLMs的优势和局限性提供了可操作的见解，并指导开发能够忠实推理分子结构的模型。</div>
</details>
</div>
<div class="card">
<div class="title">RayRoPE: Projective Ray Positional Encoding for Multi-view Attention</div>
<div class="meta-line">Authors: Yu Wu, Minsik Jeon, Jen-Hao Rick Chang, Oncel Tuzel, Shubham Tulsiani</div>
<div class="meta-line">First: 2026-01-21T18:55:51+00:00 · Latest: 2026-01-21T18:55:51+00:00</div>
<div class="meta-line">Comments: Project page: https://rayrope.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15275v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15275v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://rayrope.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study positional encodings for multi-view transformers that process tokens from a set of posed input images, and seek a mechanism that encodes patches uniquely, allows SE(3)-invariant attention with multi-frequency similarity, and can be adaptive to the geometry of the underlying scene. We find that prior (absolute or relative) encoding schemes for multi-view attention do not meet the above desiderata, and present RayRoPE to address this gap. RayRoPE represents patch positions based on associated rays but leverages a predicted point along the ray instead of the direction for a geometry-aware encoding. To achieve SE(3) invariance, RayRoPE computes query-frame projective coordinates for computing multi-frequency similarity. Lastly, as the &#x27;predicted&#x27; 3D point along a ray may not be precise, RayRoPE presents a mechanism to analytically compute the expected position encoding under uncertainty. We validate RayRoPE on the tasks of novel-view synthesis and stereo depth estimation and show that it consistently improves over alternate position encoding schemes (e.g. 15% relative improvement on LPIPS in CO3D). We also show that RayRoPE can seamlessly incorporate RGB-D input, resulting in even larger gains over alternatives that cannot positionally encode this information.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RayRoPE：用于多视角注意力的射线位置编码</div>
<div class="mono" style="margin-top:8px">我们研究处理一组带姿态输入图像的多视角变压器的位置编码，并寻求一种机制，能够唯一编码补丁，实现具有多频率相似性的SE(3)不变注意力，并能适应底层场景的几何结构。我们发现，先前的多视角注意力位置编码方案（绝对或相对）无法满足上述需求，因此提出了RayRoPE来填补这一空白。RayRoPE基于与射线相关的位置来表示补丁，但利用射线上预测的点而非方向进行几何感知的编码。为了实现SE(3)不变性，RayRoPE计算查询帧的射影坐标以实现多频率相似性。最后，由于射线上预测的3D点可能不够精确，RayRoPE提出了一种在不确定性下分析计算预期位置编码的机制。我们在新视角合成和立体深度估计任务中验证了RayRoPE，并表明其在多种替代位置编码方案上表现更优（例如在CO3D中LPIPS相对提升15%）。我们还展示了RayRoPE能够无缝整合RGB-D输入，从而在无法编码此类信息的替代方案上实现更大的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">Bottom-up approach to texture zeros in the neutrino mass matrix</div>
<div class="meta-line">Authors: Iffat Ara Mazumder, Rupak Dutta</div>
<div class="meta-line">First: 2024-09-07T08:05:13+00:00 · Latest: 2026-01-21T18:52:24+00:00</div>
<div class="meta-line">Comments: 24 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2409.04756v2">Abs</a> · <a href="https://arxiv.org/pdf/2409.04756v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate one and two texture zeros in the neutrino mass matrix using the latest oscillation data through a bottom-up approach. In this context, we begin by estimating the detailed features of each matrix element by varying the CP violating phases within $(0,\,2π)$, the lowest neutrino mass within $(0,\,1)\,{\rm eV}$ and the neutrino oscillation parameters such as three mixing angles and the two mass squared differences within $3σ$ of their central values. We find that for normal ordering, only $ee$, $eμ$ and $eτ$ elements of the mass matrix can vanish, whereas, for inverted ordering, five elements -- $eμ$, $eτ$, $μμ$, $μτ$ and $ττ$ -- can vanish. For two texture zeros, only $(ee,\, eμ= 0)$ and $(ee,\, eτ=0)$ are allowed in case of normal ordering. For a particular vanishing element, we also estimate the range of the lowest neutrino mass and the CP violating phases. In particular, very interesting correlation among the CP violating phases and the lowest neutrino mass is obtained for each vanishing cases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>中微子质量矩阵中纹理零的自底向上方法</div>
<div class="mono" style="margin-top:8px">我们采用自底向上的方法，利用最新的振荡数据研究中微子质量矩阵中一个和两个纹理零的情况。在此过程中，我们通过在(0, 2π)范围内变化CP破坏相位、在(0, 1) eV范围内估计最低中微子质量，以及在3σ范围内调整三个混合角和两个质量平方差等振荡参数，来估算每个矩阵元的详细特征。我们发现，在正常序情况下，只有ee、eμ和eτ元可以为零；而在反常序情况下，五个元——eμ、eτ、μμ、μτ和ττ——可以为零。对于两个纹理零的情况，在正常序下仅允许(ee, eμ=0)和(ee, eτ=0)。对于特定的消失元，我们还估算了最低中微子质量的范围和CP破坏相位。特别地，每种消失情况都得到了CP破坏相位与最低中微子质量之间非常有趣的关联。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We investigate one and two texture zeros in the neutrino mass matrix using the latest oscillation data through a bottom-up approach.</div>
</details>
</div>
<div class="card">
<div class="title">A binary-related origin mediated by environmental conditions for blue straggler stars</div>
<div class="meta-line">Authors: Francesco R. Ferraro, Barbara Lanzoni, Enrico Vesperini, Emanuele Dalessandro, Mario Cadelano, Cristina Pallanca, Giacomo Beccari, Domenico Nardiello, Mattia Libralato, Giampaolo Piotto</div>
<div class="meta-line">Venue: Nature Communications (2026) 17,768</div>
<div class="meta-line">First: 2025-06-09T12:24:07+00:00 · Latest: 2026-01-21T18:51:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.07692v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.07692v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Blue stragglers are anomalously massive core hydrogen-burning stars that, according to the theory of single star evolution, should not exist. They are suspected to form in mass-enhancement processes, involving binary evolution or stellar collisions. In dynamically active systems like globular clusters, the number of blue stragglers originated by collisions is expected to increase with the local density and the rate of stellar encounters. Here we analyse more than 3000 blue stragglers in 48 Galactic globular clusters with different structures, finding that their number normalized to the sampled luminosity anti-correlates (instead of correlating) with the central density, collision rate, and dynamical age of the parent cluster. Similar trends are also found for the cluster binary fraction. Once inserted in the context of the current knowledge of the BSS phenomenon, these correlations indicate that low-density regions (possibly because of a higher binary production/survival rate) are the natural habitat of both BSSs and binary systems, and the observed BSSs mostly have a binary-related origin mediated by the environmental conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>由环境条件介导的与二进制相关的蓝离散星起源</div>
<div class="mono" style="margin-top:8px">蓝离散星是异常质量大的核心氢燃烧星，根据单星演化理论，它们不应该存在。人们怀疑它们是在质量增强过程中形成的，涉及二进制演化或恒星碰撞。在动态活跃的系统如球状星团中，碰撞产生的蓝离散星数量预计会随着局部密度和恒星相遇率的增加而增加。在这里，我们分析了48个结构不同的银河球状星团中超过3000个蓝离散星，发现它们的数量在采样光度下与母星团的中心密度、碰撞率和动力学年龄呈反相关（而非正相关）。类似的趋势也出现在星团二进制星比例中。一旦将这些相关性置于当前对蓝离散星现象的理解框架中，这些结果表明低密度区域（可能是由于更高的二进制生成/存活率）是蓝离散星和二进制系统的自然栖息地，观测到的蓝离散星大部分具有由环境条件介导的与二进制相关的起源。</div>
</details>
</div>
<div class="card">
<div class="title">Evaluation of Large Language Models in Legal Applications: Challenges, Methods, and Future Directions</div>
<div class="meta-line">Authors: Yiran Hu, Huanghai Liu, Chong Wang, Kunran Li, Tien-Hsuan Wu, Haitao Li, Xinran Xu, Siqing Huo, Weihang Su, Ning Zheng, Siyuan Zheng, Qingyao Ai, Yun Liu, Renjun Bian, Yiqun Liu, Charles L. A. Clarke, Weixing Shen, Ben Kao</div>
<div class="meta-line">First: 2026-01-21T18:51:37+00:00 · Latest: 2026-01-21T18:51:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15267v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15267v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are being increasingly integrated into legal applications, including judicial decision support, legal practice assistance, and public-facing legal services. While LLMs show strong potential in handling legal knowledge and tasks, their deployment in real-world legal settings raises critical concerns beyond surface-level accuracy, involving the soundness of legal reasoning processes and trustworthy issues such as fairness and reliability. Systematic evaluation of LLM performance in legal tasks has therefore become essential for their responsible adoption. This survey identifies key challenges in evaluating LLMs for legal tasks grounded in real-world legal practice. We analyze the major difficulties involved in assessing LLM performance in the legal domain, including outcome correctness, reasoning reliability, and trustworthiness. Building on these challenges, we review and categorize existing evaluation methods and benchmarks according to their task design, datasets, and evaluation metrics. We further discuss the extent to which current approaches address these challenges, highlight their limitations, and outline future research directions toward more realistic, reliable, and legally grounded evaluation frameworks for LLMs in legal domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>法律领域中大型语言模型的评估：挑战、方法与未来方向</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）正越来越多地被整合到法律应用中，包括司法决策支持、法律实务辅助和面向公众的法律服务。尽管LLMs在处理法律知识和任务方面展现出强大潜力，但其在现实法律环境中的部署引发了超越表面准确性的关键问题，涉及法律推理过程的合理性以及公平性、可靠性等可信赖性问题。因此，系统评估LLMs在法律任务中的表现对于其负责任的采用变得至关重要。本文调查了基于现实法律实践的法律任务中评估LLMs的关键挑战，分析了评估LLMs在法律领域表现的主要困难，包括结果正确性、推理可靠性以及可信赖性。在此基础上，我们根据任务设计、数据集和评估指标对现有的评估方法和基准进行了回顾和分类。我们进一步讨论了当前方法在应对这些挑战方面的程度，指出了其局限性，并概述了未来研究方向，以构建更加现实、可靠且具有法律基础的LLMs评估框架。</div>
</details>
</div>
<div class="card">
<div class="title">Scalable Stewardship of an LLM-Assisted Clinical Benchmark with Physician Oversight</div>
<div class="meta-line">Authors: Junze Ye, Daniel Tawfik, Alex J. Goodell, Nikhil V. Kotha, Mark K. Buyyounouski, Mohsen Bayati</div>
<div class="meta-line">First: 2025-12-22T18:59:34+00:00 · Latest: 2026-01-21T18:48:54+00:00</div>
<div class="meta-line">Comments: Project codebase: https://github.com/junzeye/validate-medcalc-labels</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19691v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.19691v2">PDF</a> · <a href="https://github.com/junzeye/validate-medcalc-labels">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We examine the reliability of a widely used clinical AI benchmark whose reference labels were partially generated by LLMs, and find that a substantial fraction are clinically misaligned. We introduce a phased stewardship procedure to amplify the positive impact of physician experts&#x27; feedback and then demonstrate, via a controlled RL experiment, how uncaught label bias can materially affect downstream LLM evaluation and alignment. Our results demonstrate that partially LLM-generated labels can embed systemic errors that distort not only evaluation but also downstream model alignment. By adopting a hybrid oversight system, we can prioritize scarce expert feedback to maintain benchmarks as living, clinically-grounded documents. Ensuring this alignment is a prerequisite for the safe deployment of LLMs in high-stakes medical decision support.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM辅助临床基准的可扩展管理与医师监督</div>
<div class="mono" style="margin-top:8px">我们研究了一个广泛使用的临床AI基准的可靠性，其参考标签部分由LLMs生成，发现其中相当一部分存在临床不一致。我们引入了一个分阶段的管理流程，以增强医师专家反馈的积极影响，并通过一个受控的强化学习实验展示未被发现的标签偏差如何实质性影响下游LLM的评估与对齐。我们的结果表明，部分由LLM生成的标签可能嵌入系统性错误，不仅扭曲评估，还影响下游模型的对齐。通过采用混合监督系统，我们可以优先使用稀缺的专家反馈，以保持基准作为活生生的、以临床为基础的文档。确保这种对齐是LLM在高风险医疗决策支持中安全部署的前提。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamics of self-maps in their primal topologies</div>
<div class="meta-line">Authors: Jose C. Martin</div>
<div class="meta-line">First: 2026-01-21T18:48:47+00:00 · Latest: 2026-01-21T18:48:47+00:00</div>
<div class="meta-line">Comments: 11 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15264v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15264v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study a series of dynamical concepts for self-maps in the primal topology induced by them. Among the concepts studied are non-wandering points, limit points, recurrent points, minimal sets, transitive points and self-maps, topologically ergodic self-maps, weakly mixing self-maps, strongly mixing self-maps, Lyapunov stable self-maps, chaotic self-maps in the sense of Auslander-Yorke, chaotic self-maps in the sense of Devaney, asymptotic pairs, proximal pairs, and syndetically proximal pairs.
  Some results are given in the more general context of continuous self-maps in an Alexandroff topological space. We prove that a continuous self-map of an Alexandroff space is always Lyapunov stable.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自映射在其原始拓扑中的动力学</div>
<div class="mono" style="margin-top:8px">我们研究自映射在由其诱导的原始拓扑中的若干动力学概念。所研究的概念包括非游荡点、极限点、递归点、极小集、传递点和自映射，拓扑遍历自映射，弱混合自映射，强混合自映射，李雅普诺夫稳定自映射，Auslander-Yorke意义下的混沌自映射，Devaney意义下的混沌自映射，渐近对，邻近对，以及稠密邻近对。
一些结果是在更一般的连续自映射的亚历山大罗夫拓扑空间背景下给出的。我们证明了亚历山大罗夫空间上的连续自映射总是李雅普诺夫稳定的。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Automation: Rethinking Work, Creativity, and Governance in the Age of Generative AI</div>
<div class="meta-line">Authors: Haocheng Lin</div>
<div class="meta-line">First: 2025-12-09T20:25:24+00:00 · Latest: 2026-01-21T18:42:26+00:00</div>
<div class="meta-line">Comments: Improved structure and clarity of the introduction and literature review; explicit articulation of the paper&#x27;s contributions; refined the integration of AI across labour, UBI, and governance</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11893v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.11893v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid expansion of generative artificial intelligence (AI) is transforming work, creativity, and economic security in ways that extend beyond automation and productivity. This paper examines four interconnected dimensions of contemporary AI deployment: (1) transformations in employment and task composition (2) unequal diffusion of AI across sectors and socio-demographic groups (3) the role of universal basic income (UBI) as a stabilising response to AI-induced volatility (4) the effects of model alignment and content governance on human creativity, autonomy, and decision-making
  Using a hybrid approach that integrates labour market task exposure modelling, sectoral diffusion analysis, policy review, and qualitative discourse critique, the study develops an Inclusive AI Governance Framework. It introduces Level 1.5 autonomy as a human centred design principle that preserves evaluative authority while enabling partial automation, and highlights evidence of creative regression and emergent sycophancy in newer model generations. The paper argues that UBI should be embedded within a broader socio-technical governance ecosystem encompassing skills development, proportional regulation, and creativity preservation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越自动化：在生成式人工智能时代重新思考工作、创造力与治理</div>
<div class="mono" style="margin-top:8px">生成式人工智能（AI）的迅速扩展正在以超越自动化和生产力的方式改变工作、创造力和经济安全。本文探讨了当代AI部署的四个相互关联的维度：（1）就业和任务结构的转变；（2）AI在不同行业和社会人口群体中的不平等扩散；（3）基本收入（UBI）作为应对AI引发的波动的稳定化回应的作用；（4）模型对齐与内容治理对人类创造力、自主性和决策的影响。研究采用结合劳动力市场任务暴露建模、行业扩散分析、政策审查和定性话语批判的混合方法，构建了一个包容性AI治理框架。它引入了Level 1.5自主性作为以人类为中心的设计原则，该原则在保持评估权威的同时允许部分自动化，并指出了新一代模型中出现的创造性倒退和新兴谄媚现象。论文认为，基本收入应嵌入一个更广泛的社会治理技术生态系统中，包括技能发展、比例化监管和创造力保护。</div>
</details>
</div>
<div class="card">
<div class="title">DrivIng: A Large-Scale Multimodal Driving Dataset with Full Digital Twin Integration</div>
<div class="meta-line">Authors: Dominik Rößle, Xujun Xie, Adithya Mohan, Venkatesh Thirugnana Sambandham, Daniel Cremers, Torsten Schön</div>
<div class="meta-line">First: 2026-01-21T18:41:05+00:00 · Latest: 2026-01-21T18:41:05+00:00</div>
<div class="meta-line">Comments: Accepted to the IEEE Intelligent Vehicles Symposium 2026. For code and dataset, see https://github.com/cvims/DrivIng</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15260v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15260v1">PDF</a> · <a href="https://github.com/cvims/DrivIng">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Perception is a cornerstone of autonomous driving, enabling vehicles to understand their surroundings and make safe, reliable decisions. Developing robust perception algorithms requires large-scale, high-quality datasets that cover diverse driving conditions and support thorough evaluation. Existing datasets often lack a high-fidelity digital twin, limiting systematic testing, edge-case simulation, sensor modification, and sim-to-real evaluations. To address this gap, we present DrivIng, a large-scale multimodal dataset with a complete geo-referenced digital twin of a ~18 km route spanning urban, suburban, and highway segments. Our dataset provides continuous recordings from six RGB cameras, one LiDAR, and high-precision ADMA-based localization, captured across day, dusk, and night. All sequences are annotated at 10 Hz with 3D bounding boxes and track IDs across 12 classes, yielding ~1.2 million annotated instances. Alongside the benefits of a digital twin, DrivIng enables a 1-to-1 transfer of real traffic into simulation, preserving agent interactions while enabling realistic and flexible scenario testing. To support reproducible research and robust validation, we benchmark DrivIng with state-of-the-art perception models and publicly release the dataset, digital twin, HD map, and codebase.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DrivIng：一个包含完整数字孪生集成的大型多模态驾驶数据集</div>
<div class="mono" style="margin-top:8px">感知是自动驾驶的核心，使车辆能够理解周围环境并做出安全、可靠的决策。开发稳健的感知算法需要覆盖多样驾驶条件、高质量的大规模数据集，以支持全面的评估。现有数据集通常缺乏高保真度的数字孪生，限制了系统性测试、边缘情况模拟、传感器修改和仿真到现实的评估。为解决这一问题，我们提出了DrivIng，这是一个大型多模态数据集，包含约18公里路线的完整地理参考数字孪生，涵盖城市、郊区和高速公路段。我们的数据集提供了六台RGB摄像头、一台激光雷达和基于高精度ADMA的定位的连续记录，覆盖白天、黄昏和夜晚。所有序列均以10Hz频率标注，包含12类的3D边界框和跟踪ID，总计约120万个标注实例。借助数字孪生的优势，DrivIng能够实现真实交通与仿真的1:1转换，保留代理交互的同时，支持现实且灵活的场景测试。为支持可重复研究和稳健验证，我们使用最先进的感知模型对DrivIng进行了基准测试，并公开发布了数据集、数字孪生、高精地图和代码库。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Perception is a cornerstone of autonomous driving, enabling vehicles to understand their surroundings and make safe, reliable decisions.</div>
</details>
</div>
<div class="card">
<div class="title">Many Experiments, Few Repetitions, Unpaired Data, and Sparse Effects: Is Causal Inference Possible?</div>
<div class="meta-line">Authors: Felix Schur, Niklas Pfister, Peng Ding, Sach Mukherjee, Jonas Peters</div>
<div class="meta-line">First: 2026-01-21T18:36:34+00:00 · Latest: 2026-01-21T18:36:34+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15254v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15254v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the problem of estimating causal effects under hidden confounding in the following unpaired data setting: we observe some covariates $X$ and an outcome $Y$ under different experimental conditions (environments) but do not observe them jointly; we either observe $X$ or $Y$. Under appropriate regularity conditions, the problem can be cast as an instrumental variable (IV) regression with the environment acting as a (possibly high-dimensional) instrument. When there are many environments but only a few observations per environment, standard two-sample IV estimators fail to be consistent. We propose a GMM-type estimator based on cross-fold sample splitting of the instrument-covariate sample and prove that it is consistent as the number of environments grows but the sample size per environment remains constant. We further extend the method to sparse causal effects via $\ell_1$-regularized estimation and post-selection refitting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大量实验、极少重复、非配对数据和稀疏效应：因果推断是否可能？</div>
<div class="mono" style="margin-top:8px">我们研究在非配对数据设置下估计因果效应的问题：在不同的实验条件下（环境），我们观察到一些协变量 $X$ 和结果 $Y$，但并未同时观察它们；我们只观察到 $X$ 或 $Y$。在适当的正则条件下，该问题可以转化为以环境作为（可能为高维）工具变量的工具变量（IV）回归问题。当存在大量环境但每个环境的观测样本量较少时，标准的两样本IV估计量无法保持一致性。我们提出了一种基于工具变量-协变量样本交叉折叠分割的GMM型估计量，并证明了当环境数量增加而每个环境的样本量保持不变时，该估计量具有一致性。我们进一步将该方法扩展到稀疏因果效应，通过 $\ell_1$ 正则化估计和后选择重拟合实现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We study the problem of estimating causal effects under hidden confounding in the following unpaired data setting: we observe some covariates $X$ and an outcome $Y$ under different experimental conditions (environments) but do not observe them jointly; we either observe $X$ or $Y$.</div>
</details>
</div>
<div class="card">
<div class="title">FlowSSC: Universal Generative Monocular Semantic Scene Completion via One-Step Latent Diffusion</div>
<div class="meta-line">Authors: Zichen Xi, Hao-Xiang Chen, Nan Xue, Hongyu Yan, Qi-Yuan Feng, Levent Burak Kara, Joaquim Jorge, Qun-Ce Xu</div>
<div class="meta-line">First: 2026-01-21T18:32:27+00:00 · Latest: 2026-01-21T18:32:27+00:00</div>
<div class="meta-line">Comments: Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15250v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15250v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Semantic Scene Completion (SSC) from monocular RGB images is a fundamental yet challenging task due to the inherent ambiguity of inferring occluded 3D geometry from a single view. While feed-forward methods have made progress, they often struggle to generate plausible details in occluded regions and preserve the fundamental spatial relationships of objects. Such accurate generative reasoning capability for the entire 3D space is critical in real-world applications. In this paper, we present FlowSSC, the first generative framework applied directly to monocular semantic scene completion. FlowSSC treats the SSC task as a conditional generation problem and can seamlessly integrate with existing feed-forward SSC methods to significantly boost their performance. To achieve real-time inference without compromising quality, we introduce Shortcut Flow-matching that operates in a compact triplane latent space. Unlike standard diffusion models that require hundreds of steps, our method utilizes a shortcut mechanism to achieve high-fidelity generation in a single step, enabling practical deployment in autonomous systems. Extensive experiments on SemanticKITTI demonstrate that FlowSSC achieves state-of-the-art performance, significantly outperforming existing baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlowSSC：通过一步潜在扩散实现单目语义场景补全的通用生成模型</div>
<div class="mono" style="margin-top:8px">从单目RGB图像进行语义场景补全（SSC）是一项基础但具有挑战性的任务，由于从单一视角推断被遮挡的3D几何结构固有的模糊性。尽管前馈方法已取得进展，但它们通常难以在被遮挡区域生成合理的细节，并保持物体的基本空间关系。在完整3D空间中实现这种精确的生成推理能力对于现实世界应用至关重要。本文中，我们提出了FlowSSC，这是首个直接应用于单目语义场景补全的生成框架。FlowSSC将SSC任务视为一个条件生成问题，并能够无缝集成到现有的前馈SSC方法中，显著提升其性能。为了实现实时推理而不牺牲质量，我们引入了Shortcut Flow-matching机制，它在紧凑的三平面潜在空间中运行。与需要数百步的标准扩散模型不同，我们的方法利用了快捷机制，在单步内实现高保真生成，从而在自主系统中实现实际部署。在SemanticKITTI数据集上的大量实验表明，FlowSSC达到了最先进的性能，显著优于现有基线。</div>
</details>
</div>
<div class="card">
<div class="title">Recommending Best Paper Awards for ML/AI Conferences via the Isotonic Mechanism</div>
<div class="meta-line">Authors: Garrett G. Wen, Buxin Su, Natalie Collina, Zhun Deng, Weijie Su</div>
<div class="meta-line">First: 2026-01-21T18:30:42+00:00 · Latest: 2026-01-21T18:30:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15249v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15249v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning and artificial intelligence conferences such as NeurIPS and ICML now regularly receive tens of thousands of submissions, posing significant challenges to maintaining the quality and consistency of the peer review process. This challenge is particularly acute for best paper awards, which are an important part of the peer review process, yet whose selection has increasingly become a subject of debate in recent years. In this paper, we introduce an author-assisted mechanism to facilitate the selection of best paper awards. Our method employs the Isotonic Mechanism for eliciting authors&#x27; assessments of their own submissions in the form of a ranking, which is subsequently utilized to adjust the raw review scores for optimal estimation of the submissions&#x27; ground-truth quality. We demonstrate that authors are incentivized to report truthfully when their utility is a convex additive function of the adjusted scores, and we validate this convexity assumption for best paper awards using publicly accessible review data of ICLR from 2019 to 2023 and NeurIPS from 2021 to 2023. Crucially, in the special case where an author has a single quota -- that is, may nominate only one paper -- we prove that truthfulness holds even when the utility function is merely nondecreasing and additive. This finding represents a substantial relaxation of the assumptions required in prior work. For practical implementation, we extend our mechanism to accommodate the common scenario of overlapping authorship. Finally, simulation results demonstrate that our mechanism significantly improves the quality of papers selected for awards.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过等吨机制推荐机器学习/人工智能会议的最佳论文奖</div>
<div class="mono" style="margin-top:8px">诸如NeurIPS和ICML等机器学习和人工智能会议现在经常收到数万份投稿，这对保持同行评审过程的质量和一致性提出了重大挑战。这一挑战在最佳论文奖的评选中尤为突出，因为最佳论文奖是同行评审过程的重要组成部分，但近年来其评选标准却越来越成为争议的焦点。本文介绍了一种作者辅助机制，以促进最佳论文奖的评选。我们的方法利用等吨机制，以排名的形式获取作者对其投稿的评估，随后用于调整原始评审分数，以更准确地估计投稿的真实质量。我们证明，当作者的效用是调整分数的凸加法函数时，他们有动机诚实地报告自己的评估。我们使用2019年至2023年ICLR和2021年至2023年NeurIPS的公开评审数据验证了这一凸性假设。关键的是，在作者只有一个提名配额（即只能提名一篇论文）的特殊情况下，我们证明即使效用函数仅为非递减且加法的，诚实性仍然成立。这一发现是对先前工作所需假设的实质性放松。在实际应用中，我们将该机制扩展以适应常见的作者重叠情况。最后，模拟结果表明，我们的机制显著提高了获奖论文的质量。</div>
</details>
</div>
<div class="card">
<div class="title">On the Reliability and Stability of Selective Methods in Malware Classification Tasks</div>
<div class="meta-line">Authors: Alexander Herzog, Aliai Eusebi, Lorenzo Cavallaro</div>
<div class="meta-line">First: 2025-05-28T20:22:43+00:00 · Latest: 2026-01-21T18:26:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.22843v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.22843v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The performance figures of modern drift-adaptive malware classifiers appear promising, but does this translate to genuine operational reliability? The standard evaluation paradigm primarily focuses on baseline performance metrics, neglecting confidence-error alignment and operational stability. While prior works established the importance of temporal evaluation and introduced selective classification in malware classification tasks, we take a complementary direction by investigating whether malware classifiers maintain reliable and stable confidence estimates under distribution shifts and exploring the tensions between scientific advancement and practical impacts when they do not. We propose Aurora, a framework to evaluate malware classifiers based on their confidence quality and operational resilience. Aurora subjects the confidence profile of a given model to verification to assess the reliability of its estimates. Unreliable confidence estimates erode operational trust, waste valuable annotation budgets on non-informative samples for active learning, and leave error-prone instances undetected in selective classification. Aurora is further complemented by a set of metrics designed to go beyond point-in-time performance, striving towards a more holistic assessment of operational stability throughout temporal evaluation periods. The fragility we observe in SOTA frameworks across datasets of varying drift severity suggests it may be time to revisit the underlying assumptions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于在恶意软件分类任务中选择性方法的可靠性与稳定性</div>
<div class="mono" style="margin-top:8px">现代漂移自适应恶意软件分类器的性能指标看似有前景，但这是否意味着实际操作中的可靠性？标准评估范式主要关注基线性能指标，忽略了置信度与错误之间的对齐以及操作稳定性。尽管已有研究强调了时间维度评估的重要性，并在恶意软件分类任务中引入了选择性分类，但我们采取了互补的方法，探讨恶意软件分类器在分布变化下是否能保持可靠的置信度估计，并研究当这些估计不可靠时，科学进步与实际影响之间的张力。我们提出了Aurora框架，用于基于置信度质量和操作韧性评估恶意软件分类器。Aurora通过验证给定模型的置信度分布来评估其估计的可靠性。不可靠的置信度估计会削弱操作信任，浪费主动学习中对非信息性样本的宝贵标注预算，并在选择性分类中遗漏错误易发的实例。此外，Aurora还结合了一组指标，超越了单一时间点的性能评估，旨在实现对时间维度评估期间操作稳定性的更全面评估。我们在不同漂移严重程度的数据集上观察到的SOTA框架的脆弱性表明，可能需要重新审视其底层假设。</div>
</details>
</div>
<div class="card">
<div class="title">Feasibility Preservation under Monotone Retrieval Truncation</div>
<div class="meta-line">Authors: Sean Plummer</div>
<div class="meta-line">First: 2026-01-21T18:25:16+00:00 · Latest: 2026-01-21T18:25:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15241v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15241v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Retrieval-based systems approximate access to a corpus by exposing only a truncated subset of available evidence. Even when relevant information exists in the corpus, truncation can prevent compatible evidence from co-occurring, leading to failures that are not captured by relevance-based evaluation. This paper studies retrieval from a structural perspective, modeling query answering as a feasibility problem under truncation.
  We formalize retrieval as a sequence of candidate evidence sets and characterize conditions under which feasibility in the limit implies feasibility at finite retrieval depth. We show that monotone truncation suffices to guarantee finite witnessability for individual queries. For classes of queries, we identify finite generation of witness certificates as the additional condition required to obtain a uniform retrieval bound, and we show that this condition is necessary. We further exhibit sharp counterexamples demonstrating failure under non-monotone truncation, non-finitely-generated query classes, and purely slotwise coverage.
  Together, these results isolate feasibility preservation as a correctness criterion for retrieval independent of relevance scoring or optimization, and clarify structural limitations inherent to truncation-based retrieval.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>单调检索截断下的可行性保持</div>
<div class="mono" style="margin-top:8px">基于检索的系统通过仅暴露可用证据的截断子集来近似访问语料库。即使语料库中存在相关信息，截断可能导致兼容证据无法共现，从而产生无法通过基于相关性的评估捕捉的失败。本文从结构角度研究检索，将查询回答建模为截断下的可行性问题。
我们形式化检索为候选证据集序列，并刻画在极限下可行性蕴含有限检索深度下可行性的条件。我们证明单调截断足以保证单个查询的有限可见证性。对于查询类别，我们识别出有限生成的见证证书作为获得统一检索界限的额外条件，并证明该条件是必要的。我们进一步展示了尖锐的反例，说明在非单调截断、非有限生成查询类别以及纯粹按槽位覆盖的情况下会出现失败。
这些结果共同将可行性保持隔离为一种与相关性评分或优化无关的检索正确性标准，并澄清了基于截断的检索固有的结构性限制。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-context principal component analysis</div>
<div class="meta-line">Authors: Kexin Wang, Salil Bhate, João M. Pereira, Joe Kileel, Matylda Figlerowicz, Anna Seigal</div>
<div class="meta-line">First: 2026-01-21T18:24:32+00:00 · Latest: 2026-01-21T18:24:32+00:00</div>
<div class="meta-line">Comments: 47 pages, 8 figures. Supplementary tables are provided as downloadable file</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15239v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15239v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Principal component analysis (PCA) is a tool to capture factors that explain variation in data. Across domains, data are now collected across multiple contexts (for example, individuals with different diseases, cells of different types, or words across texts). While the factors explaining variation in data are undoubtedly shared across subsets of contexts, no tools currently exist to systematically recover such factors. We develop multi-context principal component analysis (MCPCA), a theoretical and algorithmic framework that decomposes data into factors shared across subsets of contexts. Applied to gene expression, MCPCA reveals axes of variation shared across subsets of cancer types and an axis whose variability in tumor cells, but not mean, is associated with lung cancer progression. Applied to contextualized word embeddings from language models, MCPCA maps stages of a debate on human nature, revealing a discussion between science and fiction over decades. These axes are not found by combining data across contexts or by restricting to individual contexts. MCPCA is a principled generalization of PCA to address the challenge of understanding factors underlying data across contexts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多上下文主成分分析</div>
<div class="mono" style="margin-top:8px">主成分分析（PCA）是一种用于捕捉解释数据变异因素的工具。如今，数据在多个上下文中被收集（例如，不同疾病的个体、不同类型的细胞，或不同文本中的词语）。虽然解释数据变异的因素无疑在某些上下文子集之间是共享的，但目前尚无系统性工具能够恢复这些因素。我们开发了多上下文主成分分析（MCPCA），这是一种理论和算法框架，能够将数据分解为在上下文子集之间共享的因素。应用于基因表达数据时，MCPCA揭示了在不同癌症类型子集中共享的变异轴，以及在肿瘤细胞中变异而非均值相关的轴，与肺癌进展相关。应用于语言模型的上下文化词嵌入时，MCPCA映射了关于人性的辩论阶段，揭示了科学与虚构之间数十年的讨论。这些轴无法通过跨上下文合并数据或仅限于单个上下文来发现。MCPCA是对PCA的一种原理性推广，旨在应对理解跨上下文数据潜在因素的挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Generating Compilers for Qubit Mapping and Routing</div>
<div class="meta-line">Authors: Abtin Molavi, Amanda Xu, Ethan Cecchetti, Swamit Tannu, Aws Albarghouthi</div>
<div class="meta-line">First: 2025-08-14T16:07:07+00:00 · Latest: 2026-01-21T18:23:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.10781v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.10781v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To evaluate a quantum circuit on a quantum processor, one must find a mapping from circuit qubits to processor qubits and plan the instruction execution while satisfying the processor&#x27;s constraints. This is known as the qubit mapping and routing (QMR) problem. High-quality QMR solutions are key to maximizing the utility of scarce quantum resources and minimizing the probability of logical errors affecting computation. The challenge is that the landscape of quantum processors is incredibly diverse and fast-evolving. Given this diversity, dozens of papers have addressed the QMR problem for different qubit hardware, connectivity constraints, and quantum error correction schemes by a developing a new algorithm for a particular context. We present an alternative approach: automatically generating qubit mapping and routing compilers for arbitrary quantum processors. Though each QMR problem is different, we identify a common core structure-device state machine-that we use to formulate an abstract QMR problem. Our formulation naturally leads to a compact domain-specific language for specifying QMR problems and a powerful parametric algorithm that can be instantiated for any QMR specification. Our thorough evaluation on case studies of important QMR problems shows that generated compilers are competitive with handwritten, specialized compilers in terms of runtime and solution quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>为量子比特映射和路由生成编译器</div>
<div class="mono" style="margin-top:8px">要在量子处理器上评估一个量子电路，必须找到从电路量子比特到处理器量子比特的映射，并在满足处理器约束的条件下规划指令执行。这一过程被称为量子比特映射和路由（QMR）问题。高质量的QMR解决方案对于最大化稀缺量子资源的利用率和最小化逻辑错误影响计算的概率至关重要。挑战在于量子处理器的多样性及其快速演进的特性。鉴于这种多样性，已有数十篇论文针对不同的量子比特硬件、连接性约束和量子错误校正方案，通过为特定情境开发新算法来解决QMR问题。我们提出了一种替代方法：为任意量子处理器自动生成量子比特映射和路由编译器。尽管每个QMR问题各不相同，但我们识别出一个共同的核心结构——设备状态机，并据此构建了一个抽象的QMR问题。我们的方法自然地引导出一种紧凑的领域特定语言来指定QMR问题，并开发出一种强大的参数化算法，可以适用于任何QMR规范。我们在重要QMR问题的案例研究上的全面评估表明，生成的编译器在运行时间和解决方案质量方面与人工编写的专用编译器具有竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion In Diffusion: Reclaiming Global Coherence in Semi-Autoregressive Diffusion</div>
<div class="meta-line">Authors: Linrui Ma, Yufei Cui, Kai Han, Yunhe Wang</div>
<div class="meta-line">First: 2026-01-20T05:00:26+00:00 · Latest: 2026-01-21T18:21:39+00:00</div>
<div class="meta-line">Comments: Work In Progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.13599v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.13599v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One of the most compelling features of global discrete diffusion language models is their global bidirectional contextual capability. However, existing block-based diffusion studies tend to introduce autoregressive priors, which, while offering benefits, can cause models to lose this global coherence at the macro level. To regain global contextual understanding while preserving the advantages of the semi-autoregressive paradigm, we propose Diffusion in Diffusion, a &#x27;draft-then-refine&#x27; framework designed to overcome the irreversibility and myopia problems inherent in block diffusion models. Our approach first employs block diffusion to generate rapid drafts using small blocks, then refines these drafts through global bidirectional diffusion with a larger bidirectional receptive field. We utilize snapshot confidence remasking to identify the most critical tokens that require modification, and apply mix-scale training to expand the block diffusion model&#x27;s global capabilities. Empirical results demonstrate that our approach sets a new benchmark for discrete diffusion models on the OpenWebText dataset. Using only 26% of the fine-tuning budget of baseline models, we reduce generative perplexity from 25.7 to 21.9, significantly narrowing the performance gap with autoregressive models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散中的扩散：在半自回归扩散模型中恢复全局一致性</div>
<div class="mono" style="margin-top:8px">全球离散扩散语言模型最具吸引力的特性之一是其全局双向上下文理解能力。然而，现有的基于块的扩散研究往往引入自回归先验，虽然带来了优势，但可能导致模型在宏观层面上失去这种全局一致性。为了在保留半自回归范式优势的同时恢复全局上下文理解，我们提出了一种名为&#x27;先草稿后润色&#x27;的框架，即Diffusion in Diffusion，旨在克服块扩散模型固有的不可逆性和短视问题。我们的方法首先使用块扩散以小块快速生成草稿，然后通过具有更大双向感受野的全局双向扩散对这些草稿进行润色。我们利用快照置信度重掩码技术来识别需要修改的关键标记，并采用多尺度训练来扩展块扩散模型的全局能力。实验证明，我们的方法在OpenWebText数据集上为离散扩散模型设立了新的基准。仅使用基线模型26%的微调预算，我们将生成性困惑度从25.7降低至21.9，显著缩小了与自回归模型的性能差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">One of the most compelling features of global discrete diffusion language models is their global bidirectional contextual capability.</div>
</details>
</div>
<div class="card">
<div class="title">New Perspectives on the Polyak Stepsize: Surrogate Functions and Negative Results</div>
<div class="meta-line">Authors: Francesco Orabona, Ryan D&#x27;Orazio</div>
<div class="meta-line">First: 2025-05-26T17:00:27+00:00 · Latest: 2026-01-21T18:20:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.20219v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.20219v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Polyak stepsize has been proven to be a fundamental stepsize in convex optimization, giving near optimal gradient descent rates across a wide range of assumptions. The universality of the Polyak stepsize has also inspired many stochastic variants, with theoretical guarantees and strong empirical performance. Despite the many theoretical results, our understanding of the convergence properties and shortcomings of the Polyak stepsize or its variants is both incomplete and fractured across different analyses. We propose a new, unified, and simple perspective for the Polyak stepsize and its variants as gradient descent on a surrogate loss. We show that each variant is equivalent to minimize a surrogate function with stepsizes that adapt to a guaranteed local curvature. Our general surrogate loss perspective is then used to provide a unified analysis of existing variants across different assumptions. Moreover, we show a number of negative results proving that the non-convergence results in some of the upper bounds is indeed real.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于Polyak步长的新视角：替代函数与负结果</div>
<div class="mono" style="margin-top:8px">Polyak步长已被证明是在凸优化中一种基本的步长，能够在多种假设条件下提供接近最优的梯度下降速率。Polyak步长的普适性也启发了许多随机变体，这些变体具有理论保证和强大的实证表现。尽管已有许多理论结果，我们对Polyak步长及其变体的收敛性质和不足之处的理解仍不完整，并且在不同的分析中存在分歧。我们提出了一种新的、统一的、简单的视角，将Polyak步长及其变体视为在替代损失函数上的梯度下降。我们展示了每个变体等价于使用能够适应保证局部曲率的步长来最小化替代函数。随后，我们利用这一通用的替代损失视角，对在不同假设条件下的现有变体进行了统一分析。此外，我们还展示了一些负结果，证明某些上界中的非收敛性确实是真实存在的。</div>
</details>
</div>
<div class="card">
<div class="title">Tracing 3D Anatomy in 2D Strokes: A Multi-Stage Projection Driven Approach to Cervical Spine Fracture Identification</div>
<div class="meta-line">Authors: Fabi Nahian Madhurja, Rusab Sarmun, Muhammad E. H. Chowdhury, Adam Mushtak, Israa Al-Hashimi, Sohaib Bassam Zoghoul</div>
<div class="meta-line">First: 2026-01-21T18:15:47+00:00 · Latest: 2026-01-21T18:15:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15235v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15235v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cervical spine fractures are critical medical conditions requiring precise and efficient detection for effective clinical management. This study explores the viability of 2D projection-based vertebra segmentation for vertebra-level fracture detection in 3D CT volumes, presenting an end-to-end pipeline for automated analysis of cervical vertebrae (C1-C7). By approximating a 3D volume through optimized 2D axial, sagittal, and coronal projections, regions of interest are identified using the YOLOv8 model from all views and combined to approximate the 3D cervical spine area, achieving a 3D mIoU of 94.45 percent. This projection-based localization strategy reduces computational complexity compared to traditional 3D segmentation methods while maintaining high performance. It is followed by a DenseNet121-Unet-based multi-label segmentation leveraging variance- and energy-based projections, achieving a Dice score of 87.86 percent. Strategic approximation of 3D vertebral masks from these 2D segmentation masks enables the extraction of individual vertebra volumes. The volumes are analyzed for fractures using an ensemble of 2.5D Spatio-Sequential models incorporating both raw slices and projections per vertebra for complementary evaluation. This ensemble achieves vertebra-level and patient-level F1 scores of 68.15 and 82.26, and ROC-AUC scores of 91.62 and 83.04, respectively. We further validate our approach through an explainability study that provides saliency map visualizations highlighting anatomical regions relevant for diagnosis, and an interobserver variability analysis comparing our model&#x27;s performance with expert radiologists, demonstrating competitive results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在二维笔触中追踪三维解剖结构：一种基于多阶段投影的颈椎骨折识别方法</div>
<div class="mono" style="margin-top:8px">颈椎骨折是需要精确且高效检测的关键医疗状况，以实现有效的临床管理。本研究探讨了基于二维投影的椎体分割在三维CT体积中进行椎体级骨折检测的可行性，提出了一种用于自动分析颈椎椎体（C1-C7）的端到端流程。通过优化的二维轴位、矢状位和冠状位投影近似三维体积，利用YOLOv8模型从所有视角识别感兴趣区域并进行组合，以近似三维颈椎区域，实现了94.45%的3D mIoU。这种基于投影的定位策略相比传统的三维分割方法降低了计算复杂度，同时保持了高性能。随后，采用基于DenseNet121和Unet的多标签分割方法，利用基于方差和能量的投影，实现了87.86%的Dice分数。通过战略性地从这些二维分割掩膜中近似三维椎体掩膜，可以提取出单个椎体的体积。这些体积通过集成的2.5D空间-时序模型进行骨折分析，该模型结合了每个椎体的原始切片和投影进行互补评估。该集成模型在椎体级和患者级分别实现了68.15和82.26的F1分数，以及91.62和83.04的ROC-AUC分数。我们进一步通过可解释性研究验证了该方法，提供了突出诊断相关解剖区域的显著性图可视化，并通过与专家放射科医生的观察者间变异性分析比较模型性能，展示了具有竞争力的结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Cervical spine fractures are critical medical conditions requiring precise and efficient detection for effective clinical management.</div>
</details>
</div>
<div class="card">
<div class="title">PROGRESSLM: Towards Progress Reasoning in Vision-Language Models</div>
<div class="meta-line">Authors: Jianshu Zhang, Chengxuan Qian, Haosen Sun, Haoran Lu, Dingcheng Wang, Letian Xue, Han Liu</div>
<div class="meta-line">First: 2026-01-21T17:56:59+00:00 · Latest: 2026-01-21T17:56:59+00:00</div>
<div class="meta-line">Comments: Website: https://progresslm.github.io/ProgressLM/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15224v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15224v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://progresslm.github.io/ProgressLM/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Estimating task progress requires reasoning over long-horizon dynamics rather than recognizing static visual content. While modern Vision-Language Models (VLMs) excel at describing what is visible, it remains unclear whether they can infer how far a task has progressed from partial observations. To this end, we introduce Progress-Bench, a benchmark for systematically evaluating progress reasoning in VLMs. Beyond benchmarking, we further explore a human-inspired two-stage progress reasoning paradigm through both training-free prompting and training-based approach based on curated dataset ProgressLM-45K. Experiments on 14 VLMs show that most models are not yet ready for task progress estimation, exhibiting sensitivity to demonstration modality and viewpoint changes, as well as poor handling of unanswerable cases. While training-free prompting that enforces structured progress reasoning yields limited and model-dependent gains, the training-based ProgressLM-3B achieves consistent improvements even at a small model scale, despite being trained on a task set fully disjoint from the evaluation tasks. Further analyses reveal characteristic error patterns and clarify when and why progress reasoning succeeds or fails.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PROGRESSLM：面向视觉语言模型的任务进展推理</div>
<div class="mono" style="margin-top:8px">估计任务进展需要对长时序动态进行推理，而非仅识别静态视觉内容。尽管现代视觉语言模型（VLMs）擅长描述可见内容，但尚不清楚它们是否能从部分观察中推断任务的进展程度。为此，我们引入Progress-Bench，一个用于系统评估VLMs进展推理能力的基准测试。除了基准测试，我们还通过无训练提示和基于精心整理数据集ProgressLM-45K的训练方法，进一步探索一种受人类启发的两阶段进展推理范式。在14个VLMs上的实验表明，大多数模型尚未准备好进行任务进展估计，表现出对演示模态和视角变化的敏感性，以及对无法回答情况的处理能力较差。尽管无训练提示方法通过强制结构化进展推理带来有限且依赖模型的提升，但基于训练的ProgressLM-3B即使在小型模型规模下也能实现一致的改进，尽管其训练数据集与评估任务完全不重叠。进一步分析揭示了典型的错误模式，并阐明了进展推理成功或失败的时机与原因。</div>
</details>
</div>
<div class="card">
<div class="title">From Construction to Injection: Edit-Based Fingerprints for Large Language Models</div>
<div class="meta-line">Authors: Yue Li, Xin Yi, Dongsheng Shi, Yongyi Cui, Gerard de Melo, Linlin Wang</div>
<div class="meta-line">First: 2025-09-03T08:22:04+00:00 · Latest: 2026-01-21T17:56:42+00:00</div>
<div class="meta-line">Comments: preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.03122v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.03122v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Establishing reliable and verifiable fingerprinting mechanisms is fundamental to controlling the unauthorized redistribution of large language models (LLMs). However, existing approaches face two major challenges: (a) ensuring imperceptibility, including resistance to statistical identification and avoidance of accidental activation during fingerprint construction, and (b) preserving both model utility and fingerprint detectability under subsequent model modifications. To address these challenges, we propose an end-to-end fingerprinting framework with two components. First, we design a rule-based code-mixing fingerprint (CF) that maps natural-query-like prompts to multi-candidate targets, reducing accidental triggering via high-complexity code-mixing formulations. Second, we introduce Multi-Candidate Editing (MCEdit), which jointly optimizes multi-candidate targets and enforces margins between target and non-target outputs to improve post-modification detectability. Extensive experiments demonstrate that our framework provides a robust and practical solution for fingerprinting LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从构建到注入：基于编辑的大型语言模型指纹</div>
<div class="mono" style="margin-top:8px">建立可靠且可验证的指纹机制是控制大型语言模型（LLMs）未经授权再分发的基础。然而，现有方法面临两个主要挑战：(a) 确保不可察觉性，包括抵抗统计识别和在指纹构建过程中避免意外激活；(b) 在后续模型修改中保持模型的实用性与指纹的可检测性。为了解决这些问题，我们提出了一种端到端的指纹框架，包含两个组成部分。首先，我们设计了一种基于规则的代码混合指纹（CF），将类似自然查询的提示映射到多候选目标，通过高复杂度的代码混合公式减少意外触发。其次，我们引入了多候选编辑（MCEdit），联合优化多候选目标并强制在目标输出与非目标输出之间建立边界，以提高修改后的可检测性。大量实验表明，我们的框架为大型语言模型的指纹提供了稳健且实用的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Establishing reliable and verifiable fingerprinting mechanisms is fundamental to controlling the unauthorized redistribution of large language models (LLMs).</div>
</details>
</div>
<div class="card">
<div class="title">ScenDi: 3D-to-2D Scene Diffusion Cascades for Urban Generation</div>
<div class="meta-line">Authors: Hanlei Guo, Jiahao Shao, Xinya Chen, Xiyang Tan, Sheng Miao, Yujun Shen, Yiyi Liao</div>
<div class="meta-line">First: 2026-01-21T17:53:21+00:00 · Latest: 2026-01-21T17:53:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15221v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.15221v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in 3D object generation using diffusion models have achieved remarkable success, but generating realistic 3D urban scenes remains challenging. Existing methods relying solely on 3D diffusion models tend to suffer a degradation in appearance details, while those utilizing only 2D diffusion models typically compromise camera controllability. To overcome this limitation, we propose ScenDi, a method for urban scene generation that integrates both 3D and 2D diffusion models. We first train a 3D latent diffusion model to generate 3D Gaussians, enabling the rendering of images at a relatively low resolution. To enable controllable synthesis, this 3DGS generation process can be optionally conditioned by specifying inputs such as 3d bounding boxes, road maps, or text prompts. Then, we train a 2D video diffusion model to enhance appearance details conditioned on rendered images from the 3D Gaussians. By leveraging the coarse 3D scene as guidance for 2D video diffusion, ScenDi generates desired scenes based on input conditions and successfully adheres to accurate camera trajectories. Experiments on two challenging real-world datasets, Waymo and KITTI-360, demonstrate the effectiveness of our approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ScenDi：用于城市生成的3D到2D场景扩散级联</div>
<div class="mono" style="margin-top:8px">近年来，使用扩散模型生成3D物体取得了显著成功，但生成逼真的3D城市场景仍然具有挑战性。现有方法仅依赖3D扩散模型往往在外观细节上出现退化，而仅使用2D扩散模型的方法通常会牺牲相机控制能力。为克服这一局限，我们提出了ScenDi，一种结合3D和2D扩散模型的城市场景生成方法。我们首先训练一个3D潜在扩散模型以生成3D高斯，从而能够在相对较低的分辨率下渲染图像。为了实现可控合成，该3DGS生成过程可以选择性地通过指定输入（如3D边界框、道路地图或文本提示）进行条件化。随后，我们训练一个2D视频扩散模型，以在3D高斯渲染图像的基础上增强外观细节。通过利用粗略的3D场景作为2D视频扩散的指导，ScenDi能够根据输入条件生成所需的场景，并成功遵循精确的相机轨迹。在两个具有挑战性的现实世界数据集Waymo和KITTI-360上的实验验证了我们方法的有效性。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
