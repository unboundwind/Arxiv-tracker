<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-18 03:45</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251218_0345</div>
    <div class="row"><div class="card">
<div class="title">MemFlow: Flowing Adaptive Memory for Consistent and Efficient Long Video Narratives</div>
<div class="meta-line">Authors: Sihui Ji, Xi Chen, Shuai Yang, Xin Tao, Pengfei Wan, Hengshuang Zhao</div>
<div class="meta-line">First: 2025-12-16T18:59:59+00:00 · Latest: 2025-12-16T18:59:59+00:00</div>
<div class="meta-line">Comments: Project Page: https://sihuiji.github.io/MemFlow.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14699v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14699v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sihuiji.github.io/MemFlow.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The core challenge for streaming video generation is maintaining the content consistency in long context, which poses high requirement for the memory design. Most existing solutions maintain the memory by compressing historical frames with predefined strategies. However, different to-generate video chunks should refer to different historical cues, which is hard to satisfy with fixed strategies. In this work, we propose MemFlow to address this problem. Specifically, before generating the coming chunk, we dynamically update the memory bank by retrieving the most relevant historical frames with the text prompt of this chunk. This design enables narrative coherence even if new event happens or scenario switches in future frames. In addition, during generation, we only activate the most relevant tokens in the memory bank for each query in the attention layers, which effectively guarantees the generation efficiency. In this way, MemFlow achieves outstanding long-context consistency with negligible computation burden (7.9% speed reduction compared with the memory-free baseline) and keeps the compatibility with any streaming video generation model with KV cache.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MemFlow: 流动的自适应记忆用于一致且高效的长视频叙事</div>
<div class="mono" style="margin-top:8px">流媒体视频生成的核心挑战在于保持长上下文中的内容一致性，这对记忆设计提出了高要求。大多数现有方案通过预定义策略压缩历史帧来维护记忆。然而，不同的待生成视频片段应参考不同的历史线索，这很难通过固定策略满足。在本工作中，我们提出MemFlow来解决这一问题。具体而言，在生成下一个片段之前，我们通过该片段的文本提示动态更新记忆库，检索最相关的历史帧。这种设计即使在后续帧中发生新事件或场景切换时，也能保持叙事连贯性。此外，在生成过程中，我们仅在注意力层中激活每个查询在记忆库中最相关的token，从而有效保证生成效率。通过这种方式，MemFlow实现了出色的长上下文一致性，计算负担微乎其微（相比无记忆基线仅速度降低7.9%），并且保持与任何具有KV缓存的流媒体视频生成模型的兼容性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of MemFlow is to improve the consistency and efficiency of long video generation by addressing the limitations of fixed memory compression strategies. The method involves dynamically updating the memory bank by retrieving relevant historical frames based on the text prompt of the upcoming video chunk before generation. This approach ensures narrative coherence when new events or scenarios occur. The key experimental results show that MemFlow maintains high content consistency with minimal computational overhead, achieving only a 7.9% speed reduction compared to a memory-free baseline while remaining compatible with any streaming video generation model that uses KV cache.</div>
<div class="mono" style="margin-top:8px">MemFlow的动机是通过改进固定记忆压缩策略的局限性，提升长视频生成的一致性和效率。其方法是在生成下一个视频块之前，根据该块的文本提示动态更新记忆库，检索最相关的过往帧。这种设计确保了即使在新事件或场景出现时也能保持叙事连贯性。实验结果表明，MemFlow在保持内容一致性的同时，计算负担极小，相比无记忆基线仅降低7.9%的速度，并且与任何使用KV缓存的流视频生成模型兼容。</div>
</details>
</div>
<div class="card">
<div class="title">TimeLens: Rethinking Video Temporal Grounding with Multimodal LLMs</div>
<div class="meta-line">Authors: Jun Zhang, Teng Wang, Yuying Ge, Yixiao Ge, Xinhao Li, Ying Shan, Limin Wang</div>
<div class="meta-line">First: 2025-12-16T18:59:58+00:00 · Latest: 2025-12-16T18:59:58+00:00</div>
<div class="meta-line">Comments: Project Page: https://timelens-arc-lab.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14698v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14698v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://timelens-arc-lab.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper does not introduce a novel method but instead establishes a straightforward, incremental, yet essential baseline for video temporal grounding (VTG), a core capability in video understanding. While multimodal large language models (MLLMs) excel at various video understanding tasks, the recipes for optimizing them for VTG remain under-explored. In this paper, we present TimeLens, a systematic investigation into building MLLMs with strong VTG ability, along two primary dimensions: data quality and algorithmic design. We first expose critical quality issues in existing VTG benchmarks and introduce TimeLens-Bench, comprising meticulously re-annotated versions of three popular benchmarks with strict quality criteria. Our analysis reveals dramatic model re-rankings compared to legacy benchmarks, confirming the unreliability of prior evaluation standards. We also address noisy training data through an automated re-annotation pipeline, yielding TimeLens-100K, a large-scale, high-quality training dataset. Building on our data foundation, we conduct in-depth explorations of algorithmic design principles, yielding a series of meaningful insights and effective yet efficient practices. These include interleaved textual encoding for time representation, a thinking-free reinforcement learning with verifiable rewards (RLVR) approach as the training paradigm, and carefully designed recipes for RLVR training. These efforts culminate in TimeLens models, a family of MLLMs with state-of-the-art VTG performance among open-source models and even surpass proprietary models such as GPT-5 and Gemini-2.5-Flash. All codes, data, and models will be released to facilitate future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TimeLens: 用多模态大语言模型重新思考视频时间定位</div>
<div class="mono" style="margin-top:8px">本文并未引入新方法，而是为视频时间定位（VTG）这一视频理解的核心能力建立了一个简单、渐进但至关重要的基准。尽管多模态大语言模型（MLLMs）在各种视频理解任务中表现出色，但如何优化它们以适应VTG仍缺乏系统研究。本文提出了TimeLens，系统地探讨了构建具有强大VTG能力的MLLMs的两个主要维度：数据质量和算法设计。我们首先揭示了现有VTG基准中的关键质量问题，并引入了TimeLens-Bench，它包含三个流行基准的精心重新标注版本，并严格遵循质量标准。我们的分析表明，与传统基准相比，模型排名发生了显著变化，证实了先前评估标准的不可靠性。我们还通过自动化重新标注流程处理了训练数据中的噪声，生成了TimeLens-100K，一个大规模、高质量的训练数据集。基于我们的数据基础，我们深入探讨了算法设计原则，获得了一系列有意义的见解和高效有效的实践方法。这些包括用于时间表示的交错文本编码、一种可验证奖励的无思考强化学习（RLVR）方法作为训练范式，以及精心设计的RLVR训练方案。这些努力最终促成了TimeLens模型，这是一系列具有最先进VTG性能的MLLMs，在开源模型中表现突出，甚至超越了专有模型如GPT-5和Gemini-2.5-Flash。所有代码、数据和模型都将发布，以促进未来的研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper aims to establish a reliable baseline for video temporal grounding (VTG) by addressing critical issues in existing benchmarks and training data. The authors introduce TimeLens-Bench, a re-annotated dataset with strict quality criteria, and develop TimeLens-100K, a large-scale, high-quality training set through an automated re-annotation pipeline. Their analysis shows significant changes in model rankings compared to previous benchmarks, highlighting the need for improved evaluation standards. They also propose algorithmic design principles, including interleaved textual encoding and a thinking-free reinforcement learning with verifiable rewards (RLVR) approach, which contribute to achieving state-of-the-art VTG performance in open-source MLLMs, outperforming proprietary models like GPT-5 and Gemini-2.5-Flash.</div>
<div class="mono" style="margin-top:8px">本文旨在通过使用多模态大语言模型（MLLMs）建立可靠的基线，解决现有视频时间定位（VTG）基准和训练方法的不足。作者提出了TimeLens-Bench，这是三个流行基准的重新标注版本，采用严格的质量标准，揭示了模型性能与以往基准的显著差异。他们还通过自动化重新标注流程构建了TimeLens-100K，这是一个大规模、高质量的训练数据集。基于这些数据改进，他们探讨了算法设计原则，提出了交错文本编码和一种无需推理的可验证奖励强化学习（RLVR）方法。这些努力最终产生了TimeLens模型，该模型在开源模型中实现了最先进的VTG性能，并超越了如GPT-5和Gemini-2.5-Flash等专有模型。</div>
</details>
</div>
<div class="card">
<div class="title">Spherical Leech Quantization for Visual Tokenization and Generation</div>
<div class="meta-line">Authors: Yue Zhao, Hanwen Jiang, Zhenlin Xu, Chutong Yang, Ehsan Adeli, Philipp Krähenbühl</div>
<div class="meta-line">First: 2025-12-16T18:59:57+00:00 · Latest: 2025-12-16T18:59:57+00:00</div>
<div class="meta-line">Comments: Tech report; project page: https://zhaoyue-zephyrus.github.io/npq/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14697v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14697v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://zhaoyue-zephyrus.github.io/npq/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Non-parametric quantization has received much attention due to its efficiency on parameters and scalability to a large codebook. In this paper, we present a unified formulation of different non-parametric quantization methods through the lens of lattice coding. The geometry of lattice codes explains the necessity of auxiliary loss terms when training auto-encoders with certain existing lookup-free quantization variants such as BSQ. As a step forward, we explore a few possible candidates, including random lattices, generalized Fibonacci lattices, and densest sphere packing lattices. Among all, we find the Leech lattice-based quantization method, which is dubbed as Spherical Leech Quantization ($Λ_{24}$-SQ), leads to both a simplified training recipe and an improved reconstruction-compression tradeoff thanks to its high symmetry and even distribution on the hypersphere. In image tokenization and compression tasks, this quantization approach achieves better reconstruction quality across all metrics than BSQ, the best prior art, while consuming slightly fewer bits. The improvement also extends to state-of-the-art auto-regressive image generation frameworks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于视觉标记化和生成的球面Leech量化</div>
<div class="mono" style="margin-top:8px">由于其对参数的高效性和对大码本的可扩展性，非参数量化受到了广泛关注。本文通过格码的视角，提出了一种统一的非参数量化方法的表述框架。格码的几何特性解释了在使用某些现有的无查找表量化变体（如BSQ）训练自编码器时引入辅助损失项的必要性。在此基础上，我们探索了几种可能的候选方法，包括随机格、广义斐波那契格和最密集球面填充格。其中，我们发现基于Leech格的量化方法（称为球面Leech量化（$Λ_{24}$-SQ））由于其高对称性和在超球面上的均匀分布，能够带来更简化的训练方案和更优的重建与压缩权衡。在图像标记化和压缩任务中，该量化方法在所有指标上均优于BSQ（当前最佳方法），同时消耗的比特数略少。这种改进也扩展到了当前最先进的自回归图像生成框架。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper is motivated by the need for efficient and scalable non-parametric quantization methods in visual tasks. The authors propose a unified framework based on lattice coding to analyze and improve existing quantization techniques. They evaluate several lattice-based approaches, including random lattices, generalized Fibonacci lattices, and densest sphere packing lattices, and find that the Leech lattice-based method, termed Spherical Leech Quantization ($Λ_{24}$-SQ), offers a simplified training process and better reconstruction-compression tradeoff. Experimental results show that $Λ_{24}$-SQ outperforms BSQ, the current state-of-the-art, in image tokenization and compression tasks, achieving higher reconstruction quality with slightly reduced bit consumption.</div>
<div class="mono" style="margin-top:8px">本文旨在探索更高效且可扩展的非参数量化方法，用于视觉任务。作者通过格码的视角提出了一种统一的框架，以分析和改进现有的无查找表量化技术。他们评估了多种格结构，包括随机格、广义斐波那契格和最密集球面填充格，并发现基于Leech格的量化方法——球面Leech量化（$Λ_{24}$-SQ）——具有更简化的训练流程和更优的重建-压缩权衡。实验结果表明，在图像分词和压缩任务中，$Λ_{24}$-SQ在所有指标上均优于BSQ，当前最先进的方法，同时消耗更少的比特。</div>
</details>
</div>
<div class="card">
<div class="title">CRISP: Contact-Guided Real2Sim from Monocular Video with Planar Scene Primitives</div>
<div class="meta-line">Authors: Zihan Wang, Jiashun Wang, Jeff Tan, Yiwen Zhao, Jessica Hodgins, Shubham Tulsiani, Deva Ramanan</div>
<div class="meta-line">First: 2025-12-16T18:59:50+00:00 · Latest: 2025-12-16T18:59:50+00:00</div>
<div class="meta-line">Comments: Project page: https://crisp-real2sim.github.io/CRISP-Real2Sim/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14696v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14696v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://crisp-real2sim.github.io/CRISP-Real2Sim/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce CRISP, a method that recovers simulatable human motion and scene geometry from monocular video. Prior work on joint human-scene reconstruction relies on data-driven priors and joint optimization with no physics in the loop, or recovers noisy geometry with artifacts that cause motion tracking policies with scene interactions to fail. In contrast, our key insight is to recover convex, clean, and simulation-ready geometry by fitting planar primitives to a point cloud reconstruction of the scene, via a simple clustering pipeline over depth, normals, and flow. To reconstruct scene geometry that might be occluded during interactions, we make use of human-scene contact modeling (e.g., we use human posture to reconstruct the occluded seat of a chair). Finally, we ensure that human and scene reconstructions are physically-plausible by using them to drive a humanoid controller via reinforcement learning. Our approach reduces motion tracking failure rates from 55.2\% to 6.9\% on human-centric video benchmarks (EMDB, PROX), while delivering a 43\% faster RL simulation throughput. We further validate it on in-the-wild videos including casually-captured videos, Internet videos, and even Sora-generated videos. This demonstrates CRISP&#x27;s ability to generate physically-valid human motion and interaction environments at scale, greatly advancing real-to-sim applications for robotics and AR/VR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CRISP：基于单目视频和平面场景基元的接触引导真实到模拟</div>
<div class="mono" style="margin-top:8px">我们引入了CRISP，一种从单目视频中恢复可模拟的人体运动和场景几何的方法。以往的人体-场景联合重建工作依赖于数据驱动的先验知识和联合优化，没有物理约束，或者恢复出带有噪声和伪影的几何结构，导致与场景交互的运动跟踪策略失效。相反，我们的关键洞察是通过在深度、法线和光流上进行简单的聚类流程，将平面基元拟合到场景点云重建中，从而恢复出凸面、干净且适合模拟的几何结构。为了重建在交互过程中可能被遮挡的场景几何，我们利用了人体-场景接触建模（例如，我们通过人体姿态重建椅子被遮挡的座位部分）。最后，我们通过强化学习使用这些重建结果来驱动类人控制器，以确保人体和场景的重建在物理上是合理的。我们的方法在以人体为中心的视频基准数据集（EMDB、PROX）上将运动跟踪失败率从55.2%降低至6.9%，同时将RL模拟吞吐量提高了43%。我们还在野外视频上进一步验证了该方法，包括随意拍摄的视频、互联网视频，甚至是Sora生成的视频。这表明CRISP能够大规模生成物理合理的人体运动和交互环境，极大地推动了真实到模拟在机器人和AR/VR中的应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">CRISP aims to improve the accuracy and reliability of human motion and scene geometry reconstruction from monocular video by incorporating physical plausibility. The method uses planar scene primitives fitted to a point cloud reconstruction through a clustering pipeline based on depth, normals, and optical flow. It leverages human-scene contact modeling to recover occluded parts of the environment, such as the seat of a chair, using human posture information. Experimental results show that CRISP reduces motion tracking failure rates from 55.2% to 6.9% on benchmarks like EMDB and PROX, and increases RL simulation throughput by 43%. It is also validated on diverse real-world videos, including casually captured and Sora-generated content, demonstrating its effectiveness in generating physically valid human motion and interaction environments.</div>
<div class="mono" style="margin-top:8px">CRISP 的动机是解决从单目视频中生成物理上合理的可模拟人类运动和场景几何结构的挑战，这对于机器人和增强现实/虚拟现实应用至关重要。该方法通过在深度、法线和光流数据上应用聚类管道，拟合平面场景基本体，从而恢复出凸面、干净且适合模拟的几何结构。实验结果表明，CRISP 将 EMDB 和 PROX 等基准测试中的运动跟踪失败率从 55.2% 降低至 6.9%，并实现了 43% 的强化学习模拟吞吐量提升。此外，它在各种真实世界视频上进行了验证，包括随意拍摄、互联网视频和 Sora 生成的视频，展示了其在大规模生成物理合理的人机交互环境方面的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Love First, Know Later: Persona-Based Romantic Compatibility Through LLM Text World Engines</div>
<div class="meta-line">Authors: Haoyang Shang, Zhengyang Yan, Xuan Liu</div>
<div class="meta-line">Venue: NeurIPS 2025 Oral</div>
<div class="meta-line">First: 2025-12-04T02:07:05+00:00 · Latest: 2025-12-16T18:59:14+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 Workshop: First Workshop on LLM Persona Modeling (Oral)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11844v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.11844v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose Love First, Know Later: a paradigm shift in computational matching that simulates interactions first, then assesses compatibility. Instead of comparing static profiles, our framework leverages LLMs as text world engines that operate in dual capacity-as persona-driven agents following behavioral policies and as the environment modeling interaction dynamics. We formalize compatibility assessment as a reward-modeling problem: given observed matching outcomes, we learn to extract signals from simulations that predict human preferences. Our key insight is that relationships hinge on responses to critical moments-we translate this observation from relationship psychology into mathematical hypotheses, enabling effective simulation. Theoretically, we prove that as LLM policies better approximate human behavior, the induced matching converges to optimal stable matching. Empirically, we validate on speed dating data for initial chemistry and divorce prediction for long-term stability. This paradigm enables interactive, personalized matching systems where users iteratively refine their agents, unlocking future possibilities for transparent and interactive compatibility assessment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>先爱后知：基于LLM文本世界引擎的个性导向浪漫匹配</div>
<div class="mono" style="margin-top:8px">我们提出“先爱后知”：一种计算匹配范式的转变，先模拟互动，再评估匹配兼容性。我们的框架利用LLM作为文本世界引擎，以双重角色运行——作为遵循行为策略的个性驱动代理，以及作为环境建模互动动态。我们将兼容性评估形式化为一个奖励建模问题：根据观察到的匹配结果，我们学习从模拟中提取信号以预测人类偏好。我们的关键洞察是，关系依赖于对关键时刻的反应——我们将这一关系心理学观察转化为数学假设，从而实现有效的模拟。理论上，我们证明了随着LLM策略更接近人类行为，所诱导的匹配将收敛于最优稳定匹配。实证上，我们在速配数据上验证了初始化学反应，并在离婚预测上验证了长期稳定性。这种范式使交互式、个性化的匹配系统成为可能，用户可以迭代优化其代理，从而开启透明和交互式兼容性评估的新可能性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for more realistic and interactive methods of assessing romantic compatibility. The proposed method, Love First, Know Later, uses large language models (LLMs) as text world engines to simulate interactions between individuals based on their personas, then evaluates compatibility through reward modeling derived from observed matching outcomes. Experimental results on speed dating data and divorce prediction data show that the framework effectively captures initial chemistry and long-term stability, with compatibility assessments converging to optimal stable matching as LLM policies better approximate human behavior.</div>
<div class="mono" style="margin-top:8px">该研究旨在开发更真实和互动的浪漫兼容性评估方法。所提出的方法利用大语言模型（LLMs）作为文本世界引擎，根据个人的人设模拟互动，再通过从观察到的匹配结果中提取信号进行奖励建模来评估兼容性。实验结果表明，该框架在速配数据和离婚预测数据上有效捕捉了初始吸引力和长期稳定性，随着LLM策略更接近人类行为，兼容性评估逐渐收敛于最优稳定匹配。</div>
</details>
</div>
<div class="card">
<div class="title">Universal Reasoning Model</div>
<div class="meta-line">Authors: Zitian Gao, Lynx Chen, Yihao Xiao, He Xing, Ran Tao, Haoming Luo, Joey Zhou, Bryan Dai</div>
<div class="meta-line">First: 2025-12-16T18:58:45+00:00 · Latest: 2025-12-16T18:58:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14693v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14693v1">PDF</a> · <a href="https://github.com/zitian-gao/URM">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Universal transformers (UTs) have been widely used for complex reasoning tasks such as ARC-AGI and Sudoku, yet the specific sources of their performance gains remain underexplored. In this work, we systematically analyze UTs variants and show that improvements on ARC-AGI primarily arise from the recurrent inductive bias and strong nonlinear components of Transformer, rather than from elaborate architectural designs. Motivated by this finding, we propose the Universal Reasoning Model (URM), which enhances the UT with short convolution and truncated backpropagation. Our approach substantially improves reasoning performance, achieving state-of-the-art 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2. Our code is avaliable at https://github.com/zitian-gao/URM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通用推理模型</div>
<div class="mono" style="margin-top:8px">通用转换器（UTs）已被广泛用于复杂的推理任务，如ARC-AGI和数独，但其性能提升的具体来源仍缺乏深入研究。在本工作中，我们系统地分析了UTs的变体，并表明在ARC-AGI上的改进主要来自于Transformer的循环归纳偏置和强大的非线性组件，而非复杂的架构设计。受此发现启发，我们提出了通用推理模型（URM），通过引入短卷积和截断反向传播来增强UT。我们的方法显著提升了推理性能，在ARC-AGI 1和ARC-AGI 2上分别达到了53.8%和16.0%的pass@1最佳成绩。我们的代码可在https://github.com/zitian-gao/URM获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research stems from the need to understand the sources of performance gains in universal transformers (UTs) for complex reasoning tasks. The study investigates UT variants and identifies that improvements in tasks like ARC-AGI are mainly due to the recurrent inductive bias and strong nonlinear components of the Transformer, rather than complex architectures. To address this, the authors propose the Universal Reasoning Model (URM), which integrates short convolution and truncated backpropagation into UTs. The experimental results show that URM achieves state-of-the-art performance with 53.8% pass@1 on ARC-AGI 1 and 16.0% pass@1 on ARC-AGI 2.</div>
<div class="mono" style="margin-top:8px">本研究的动机是理解通用变换器（UTs）在复杂推理任务中性能提升的具体来源。通过分析不同UT变体，研究发现Transformer的循环归纳偏置和强大的非线性组件是其在ARC-AGI等任务中表现优异的关键因素。基于这一发现，作者提出了通用推理模型（URM），该模型在UT中引入了短卷积和截断反向传播。实验结果表明，URM在ARC-AGI 1和ARC-AGI 2上分别达到了53.8%和16.0%的pass@1性能，优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Native and Compact Structured Latents for 3D Generation</div>
<div class="meta-line">Authors: Jianfeng Xiang, Xiaoxue Chen, Sicheng Xu, Ruicheng Wang, Zelong Lv, Yu Deng, Hongyuan Zhu, Yue Dong, Hao Zhao, Nicholas Jing Yuan, Jiaolong Yang</div>
<div class="meta-line">First: 2025-12-16T18:58:28+00:00 · Latest: 2025-12-16T18:58:28+00:00</div>
<div class="meta-line">Comments: Project Page: https://microsoft.github.io/TRELLIS.2/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14692v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14692v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://microsoft.github.io/TRELLIS.2/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in 3D generative modeling have significantly improved the generation realism, yet the field is still hampered by existing representations, which struggle to capture assets with complex topologies and detailed appearance. This paper present an approach for learning a structured latent representation from native 3D data to address this challenge. At its core is a new sparse voxel structure called O-Voxel, an omni-voxel representation that encodes both geometry and appearance. O-Voxel can robustly model arbitrary topology, including open, non-manifold, and fully-enclosed surfaces, while capturing comprehensive surface attributes beyond texture color, such as physically-based rendering parameters. Based on O-Voxel, we design a Sparse Compression VAE which provides a high spatial compression rate and a compact latent space. We train large-scale flow-matching models comprising 4B parameters for 3D generation using diverse public 3D asset datasets. Despite their scale, inference remains highly efficient. Meanwhile, the geometry and material quality of our generated assets far exceed those of existing models. We believe our approach offers a significant advancement in 3D generative modeling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>原生与紧凑结构化隐空间用于3D生成</div>
<div class="mono" style="margin-top:8px">近年来，3D生成模型在生成真实感方面取得了显著进展，但该领域仍受到现有表示方法的限制，难以捕捉具有复杂拓扑结构和精细外观的资产。本文提出了一种从原生3D数据中学习结构化隐空间的方法，以解决这一挑战。其核心是一种新的稀疏体素结构O-Voxel，这是一种全向体素表示，能够同时编码几何和外观信息。O-Voxel可以稳健地建模任意拓扑结构，包括开放、非流形和完全封闭的表面，同时捕捉超出纹理颜色的全面表面属性，如基于物理的渲染参数。基于O-Voxel，我们设计了一种稀疏压缩变分自编码器（VAE），提供了高空间压缩率和紧凑的隐空间。我们使用多种公开的3D资产数据集训练了包含40亿参数的大规模流匹配模型，用于3D生成。尽管模型规模庞大，推理过程仍保持高效。同时，我们生成的资产在几何和材质质量方面远超现有模型。我们认为，我们的方法为3D生成模型带来了显著的进展。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of current 3D generative models in capturing complex topologies and detailed appearances by introducing a new structured latent representation called O-Voxel. O-Voxel is a sparse, omni-voxel structure that encodes both geometry and appearance, enabling robust modeling of arbitrary surfaces. The authors develop a Sparse Compression VAE based on O-Voxel, achieving high spatial compression and a compact latent space. They train large-scale flow-matching models with 4B parameters on diverse 3D asset datasets, resulting in generated assets with superior geometry and material quality compared to existing models.</div>
<div class="mono" style="margin-top:8px">本文旨在解决现有3D生成模型在捕捉复杂拓扑结构和详细外观方面的不足。作者提出了一种新的稀疏体素结构O-Voxel，能够同时编码几何和外观信息，从而实现对任意拓扑结构的稳健建模。基于O-Voxel，他们设计了Sparse Compression VAE，实现了高空间压缩率和紧凑的潜在空间。在多种3D资产数据集上训练包含40亿参数的大规模流匹配模型，生成的资产在几何和材质质量上显著优于现有模型。</div>
</details>
</div>
<div class="card">
<div class="title">MMGR: Multi-Modal Generative Reasoning</div>
<div class="meta-line">Authors: Zefan Cai, Haoyi Qiu, Tianyi Ma, Haozhe Zhao, Gengze Zhou, Kung-Hsiang Huang, Parisa Kordjamshidi, Minjia Zhang, Xiao Wen, Jiuxiang Gu, Nanyun Peng, Junjie Hu</div>
<div class="meta-line">First: 2025-12-16T18:58:04+00:00 · Latest: 2025-12-16T18:58:04+00:00</div>
<div class="meta-line">Comments: work in progress</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14691v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14691v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video foundation models generate visually realistic and temporally coherent content, but their reliability as world simulators depends on whether they capture physical, logical, and spatial constraints. Existing metrics such as Frechet Video Distance (FVD) emphasize perceptual quality and overlook reasoning failures, including violations of causality, physics, and global consistency. We introduce MMGR (Multi-Modal Generative Reasoning Evaluation and Benchmark), a principled evaluation framework based on five reasoning abilities: Physical, Logical, 3D Spatial, 2D Spatial, and Temporal. MMGR evaluates generative reasoning across three domains: Abstract Reasoning (ARC-AGI, Sudoku), Embodied Navigation (real-world 3D navigation and localization), and Physical Commonsense (sports and compositional interactions). MMGR applies fine-grained metrics that require holistic correctness across both video and image generation. We benchmark leading video models (Veo-3, Sora-2, Wan-2.2) and image models (Nano-banana, Nano-banana Pro, GPT-4o-image, Qwen-image), revealing strong performance gaps across domains. Models show moderate success on Physical Commonsense tasks but perform poorly on Abstract Reasoning (below 10 percent accuracy on ARC-AGI) and struggle with long-horizon spatial planning in embodied settings. Our analysis highlights key limitations in current models, including overreliance on perceptual data, weak global state consistency, and objectives that reward visual plausibility over causal correctness. MMGR offers a unified diagnostic benchmark and a path toward reasoning-aware generative world models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MMGR：多模态生成推理</div>
<div class="mono" style="margin-top:8px">视频基础模型可以生成视觉上逼真且时间上连贯的内容，但其作为世界模拟器的可靠性取决于是否捕捉了物理、逻辑和空间约束。现有的度量标准如Frechet Video Distance（FVD）强调感知质量，却忽略了推理失败，包括因果关系、物理规律和全局一致性方面的违反。我们引入MMGR（多模态生成推理评估与基准），这是一个基于五种推理能力的评估框架：物理推理、逻辑推理、三维空间推理、二维空间推理和时间推理。MMGR在三个领域评估生成推理：抽象推理（ARC-AGI、数独）、具身导航（现实世界中的三维导航与定位）和物理常识（体育和组合交互）。MMGR应用细粒度度量标准，要求视频和图像生成的全面正确性。我们对领先的视频模型（Veo-3、Sora-2、Wan-2.2）和图像模型（Nano-banana、Nano-banana Pro、GPT-4o-image、Qwen-image）进行了基准测试，揭示了不同领域之间显著的性能差距。模型在物理常识任务上表现出中等水平的成功，但在抽象推理（在ARC-AGI上准确率低于10%）和具身环境中的长视野空间规划方面表现不佳。我们的分析指出了当前模型的关键局限性，包括过度依赖感知数据、全局状态一致性较弱以及目标奖励视觉合理性而非因果正确性。MMGR提供了一个统一的诊断基准，并为构建具有推理能力的生成式世界模型指明了方向。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research introduces MMGR, a framework for evaluating multi-modal generative models by assessing their reasoning abilities across physical, logical, 3D spatial, 2D spatial, and temporal domains. It benchmarks leading video and image models, revealing significant performance gaps, particularly in abstract reasoning tasks like ARC-AGI where accuracy remains below 10 percent. The framework highlights limitations such as reliance on perceptual data and weak global consistency, emphasizing the need for models that prioritize causal correctness over visual plausibility.</div>
<div class="mono" style="margin-top:8px">本研究的动机是评估视频基础模型作为世界模拟器的可靠性，通过检查其捕捉物理、逻辑和空间约束的能力。MMGR是一个多模态生成推理框架，用于评估五个推理能力：物理、逻辑、三维空间、二维空间和时间推理，涵盖三个领域：抽象推理、具身导航和物理常识。主要实验结果表明，尽管模型在物理常识任务上表现中等，但在抽象推理任务中表现较差，ARC-AGI上的准确率低于10%，并且在具身环境中的长视野空间规划方面存在困难。研究指出了当前模型的关键局限性，包括过度依赖感知数据和全局状态一致性较弱。</div>
</details>
</div>
<div class="card">
<div class="title">CHIP: Adaptive Compliance for Humanoid Control through Hindsight Perturbation</div>
<div class="meta-line">Authors: Sirui Chen, Zi-ang Cao, Zhengyi Luo, Fernando Castañeda, Chenran Li, Tingwu Wang, Ye Yuan, Linxi &quot;Jim&quot; Fan, C. Karen Liu, Yuke Zhu</div>
<div class="meta-line">First: 2025-12-16T18:56:04+00:00 · Latest: 2025-12-16T18:56:04+00:00</div>
<div class="meta-line">Comments: The first two authors contributed equally. Project page: https://nvlabs.github.io/CHIP/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14689v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14689v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://nvlabs.github.io/CHIP/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in humanoid robots has unlocked agile locomotion skills, including backflipping, running, and crawling. Yet it remains challenging for a humanoid robot to perform forceful manipulation tasks such as moving objects, wiping, and pushing a cart. We propose adaptive Compliance Humanoid control through hIsight Perturbation (CHIP), a plug-and-play module that enables controllable end-effector stiffness while preserving agile tracking of dynamic reference motions. CHIP is easy to implement and requires neither data augmentation nor additional reward tuning. We show that a generalist motion-tracking controller trained with CHIP can perform a diverse set of forceful manipulation tasks that require different end-effector compliance, such as multi-robot collaboration, wiping, box delivery, and door opening.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CHIP：通过回顾扰动实现人形机器人自适应合规控制</div>
<div class="mono" style="margin-top:8px">近年来，人形机器人的进展解锁了敏捷的运动技能，包括后空翻、奔跑和爬行。然而，对于人形机器人执行需要较大力的操控任务（如移动物体、擦拭和推车）仍然具有挑战性。我们提出了一种名为 CHIP（通过回顾扰动实现自适应合规人形控制）的即插即用模块，它能够在保持对动态参考运动的敏捷跟踪的同时，实现可控的末端执行器刚度。CHIP易于实现，不需要数据增强或额外的奖励调优。我们展示了通过 CHIP 训练的通用运动跟踪控制器可以执行多种需要不同末端执行器合规性的力控任务，例如多机器人协作、擦拭、箱子传递和开门。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind this research is to address the challenge of enabling humanoid robots to perform forceful manipulation tasks while maintaining agile motion tracking. The proposed method, CHIP, introduces an adaptive compliance module that allows for controllable end-effector stiffness without requiring data augmentation or reward tuning. Experimental results demonstrate that a generalist motion-tracking controller using CHIP can effectively handle a variety of manipulation tasks, including multi-robot collaboration, wiping, box delivery, and door opening, showcasing its versatility and performance.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决人形机器人执行需要可控末端执行器刚度的强力操作任务的挑战。所提出的方法CHIP是一种自适应合规控制模块，能够在保持敏捷运动跟踪的同时动态调整刚度。实验结果表明，使用CHIP训练的通用运动跟踪控制器可以成功执行多种强力操作任务，如多机器人协作、擦拭、箱子传递和门开启，而无需数据增强或奖励调整。</div>
</details>
</div>
<div class="card">
<div class="title">Spoken DialogSum: An Emotion-Rich Conversational Dataset for Spoken Dialogue Summarization</div>
<div class="meta-line">Authors: Yen-Ju Lu, Kunxiao Gao, Mingrui Liang, Helin Wang, Thomas Thebaud, Laureano Moro-Velazquez, Najim Dehak, Jesus Villalba</div>
<div class="meta-line">First: 2025-12-16T18:54:20+00:00 · Latest: 2025-12-16T18:54:20+00:00</div>
<div class="meta-line">Comments: 12 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14687v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14687v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent audio language models can follow long conversations. However, research on emotion-aware or spoken dialogue summarization is constrained by the lack of data that links speech, summaries, and paralinguistic cues. We introduce Spoken DialogSum, the first corpus aligning raw conversational audio with factual summaries, emotion-rich summaries, and utterance-level labels for speaker age, gender, and emotion. The dataset is built in two stages: first, an LLM rewrites DialogSum scripts with Switchboard-style fillers and back-channels, then tags each utterance with emotion, pitch, and speaking rate. Second, an expressive TTS engine synthesizes speech from the tagged scripts, aligned with paralinguistic labels. Spoken DialogSum comprises 13,460 emotion-diverse dialogues, each paired with both a factual and an emotion-focused summary. The dataset is available online at https://fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/. Baselines show that an Audio-LLM raises emotional-summary ROUGE-L by 28% relative to a cascaded ASR-LLM system, confirming the value of end-to-end speech modeling.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Spoken DialogSum：用于语音对话摘要的富含情感的对话语料库</div>
<div class="mono" style="margin-top:8px">最近的音频语言模型可以处理长对话。然而，情感感知或语音对话摘要的研究受限于缺乏将语音、摘要和副语言线索联系起来的数据。我们引入了Spoken DialogSum，这是首个将原始对话音频与事实性摘要、情感丰富的摘要以及每句话的说话人年龄、性别和情感标签对齐的语料库。该数据集分为两个阶段构建：首先，一个大语言模型（LLM）以Switchboard风格的填充词和反馈通道重写DialogSum脚本，然后为每句话标注情感、音调和语速。其次，一个表达性TTS引擎从这些标注的脚本中合成语音，与副语言标签对齐。Spoken DialogSum包含13,460个情感多样的对话，每条对话都配有事实性摘要和情感聚焦摘要。该数据集可在https://fatfat-emosum.github.io/EmoDialog-Sum-Audio-Samples/在线获取。基线实验表明，与级联的ASR-LLM系统相比，音频语言模型（Audio-LLM）在情感摘要上的ROUGE-L指标提升了28%，证实了端到端语音建模的价值。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for better dialogue summarization systems that can capture emotional nuances in spoken conversations. The Spoken DialogSum dataset is created through a two-stage process: first, using an LLM to generate scripts with realistic conversational elements, then employing an expressive TTS engine to synthesize speech aligned with paralinguistic features. The dataset includes 13,460 dialogues with factual and emotion-focused summaries, along with speaker-level labels for age, gender, and emotion. Experimental results indicate that an Audio-LLM achieves a 28% higher ROUGE-L score for emotional summaries compared to a cascaded ASR-LLM system, highlighting the benefits of end-to-end speech modeling in capturing emotional content.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决情感感知口语对话摘要缺乏数据的问题。Spoken DialogSum数据集通过两个阶段构建：首先使用大型语言模型（LLM）生成包含Switchboard风格填充词和反馈语的对话脚本，然后利用表达性TTS引擎合成与旁白特征对齐的语音。数据集包含13,460个情感丰富的对话，每个对话配有事实性摘要和情感导向摘要，以及说话人的年龄、性别和情感标签。实验结果表明，与级联的ASR-LLM系统相比，音频语言模型在情感摘要ROUGE-L得分上相对提高了28%，证实了端到端语音建模的优势。</div>
</details>
</div>
<div class="card">
<div class="title">Bias-Variance Trade-off for Clipped Stochastic First-Order Methods: From Bounded Variance to Infinite Mean</div>
<div class="meta-line">Authors: Chuan He</div>
<div class="meta-line">First: 2025-12-16T18:52:15+00:00 · Latest: 2025-12-16T18:52:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14686v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14686v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Stochastic optimization is fundamental to modern machine learning. Recent research has extended the study of stochastic first-order methods (SFOMs) from light-tailed to heavy-tailed noise, which frequently arises in practice, with clipping emerging as a key technique for controlling heavy-tailed gradients. Extensive theoretical advances have further shown that the oracle complexity of SFOMs depends on the tail index $α$ of the noise. Nonetheless, existing complexity results often cover only the case $α\in (1,2]$, that is, the regime where the noise has a finite mean, while the complexity bounds tend to infinity as $α$ approaches $1$. This paper tackles the general case of noise with tail index $α\in(0,2]$, covering regimes ranging from noise with bounded variance to noise with an infinite mean, where the latter case has been scarcely studied. Through a novel analysis of the bias-variance trade-off in gradient clipping, we show that when a symmetry measure of the noise tail is controlled, clipped SFOMs achieve improved complexity guarantees in the presence of heavy-tailed noise for any tail index $α\in (0,2]$. Our analysis of the bias-variance trade-off not only yields new unified complexity guarantees for clipped SFOMs across this full range of tail indices, but is also straightforward to apply and can be combined with classical analyses under light-tailed noise to establish oracle complexity guarantees under heavy-tailed noise. Finally, numerical experiments validate our theoretical findings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>截断随机一阶方法中的偏差-方差权衡：从有界方差到无限均值</div>
<div class="mono" style="margin-top:8px">随机优化是现代机器学习的基础。近期的研究将随机一阶方法（SFOMs）的研究从轻尾噪声扩展到重尾噪声，后者在实践中经常出现，截断成为控制重尾梯度的关键技术。理论上的大量进展进一步表明，SFOMs的Oracle复杂度依赖于噪声的尾指数$α$。然而，现有的复杂度结果通常仅涵盖$α\in (1,2]$的情况，即噪声具有有限均值的范围，而当$α$趋近于1时，复杂度界限趋于无穷大。本文处理了尾指数$α\in(0,2]$的噪声的一般情况，涵盖了从有界方差噪声到无限均值噪声的多种情形，其中后者的研究较为有限。通过梯度截断中偏差-方差权衡的新分析，我们证明当噪声尾部的对称性度量被控制时，截断的SFOMs在任何尾指数$α\in (0,2]$的重尾噪声下都能获得改进的复杂度保证。我们对偏差-方差权衡的分析不仅为截断的SFOMs在这一完整的尾指数范围内提供了新的统一复杂度保证，而且易于应用，还可以与轻尾噪声下的经典分析结合，以建立重尾噪声下的Oracle复杂度保证。最后，数值实验验证了我们的理论发现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of analyzing stochastic first-order methods (SFOMs) under heavy-tailed noise, which is common in practical machine learning applications. The authors introduce a novel approach to the bias-variance trade-off by incorporating gradient clipping, a technique used to manage heavy-tailed gradients. Their analysis covers the full range of tail indices α ∈ (0,2], including cases with infinite mean, which have been underexplored. The study demonstrates that by controlling a symmetry measure of the noise tail, clipped SFOMs can achieve improved complexity guarantees regardless of the tail index. These results provide a unified theoretical framework that extends classical analyses under light-tailed noise to heavy-tailed scenarios, offering new insights into the performance of SFOMs in such settings.</div>
<div class="mono" style="margin-top:8px">本文旨在解决在重尾噪声环境下分析随机一阶方法（SFOMs）的挑战，这类噪声在实际机器学习应用中很常见。作者通过引入梯度裁剪并分析噪声尾部的对称性度量，提出了一种新的偏差-方差权衡方法。他们的分析为裁剪后的SFOMs在尾指数$α\in (0,2]$的整个范围内提供了改进的复杂度保证，包括具有无限均值的噪声情况，这在以往研究中较少涉及。研究结果表明，控制噪声尾部的对称性度量可以提升重尾环境下的性能，且该方法可与轻尾噪声的经典分析结合，以推导出重尾噪声下的Oracle复杂度界限。</div>
</details>
</div>
<div class="card">
<div class="title">Misspecification-robust amortised simulation-based inference using variational methods</div>
<div class="meta-line">Authors: Matthew O&#x27;Callaghan, Kaisey S. Mandel, Gerry Gilmore</div>
<div class="meta-line">First: 2025-09-06T14:10:49+00:00 · Latest: 2025-12-16T18:48:04+00:00</div>
<div class="meta-line">Comments: Latex edits, fixed typos</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.05724v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.05724v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in neural density estimation have enabled powerful simulation-based inference (SBI) methods that can flexibly approximate Bayesian inference for intractable stochastic models. Although these methods have demonstrated reliable posterior estimation when the simulator accurately represents the underlying data generative process (DGP), recent work has shown that they perform poorly in the presence of model misspecification. This poses a significant issue for their use in real-world problems, due to simulators always misrepresenting the true DGP to a certain degree. In this paper, we introduce robust variational neural posterior estimation (RVNP), a method which addresses the problem of misspecification in amortised SBI by bridging the simulation-to-reality gap using variational inference and error modelling. We test RVNP on multiple benchmark tasks, including using real data from astronomy, and show that it can recover robust posterior inference in a data-driven manner without adopting hyperparameters or priors governing the misspecification influence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用变分方法的鲁棒性模拟基础推断</div>
<div class="mono" style="margin-top:8px">最近在神经密度估计方面的进展使得强大的基于模拟的推断（SBI）方法成为可能，这些方法可以灵活地近似不可行随机模型的贝叶斯推断。尽管这些方法在模拟器准确反映底层数据生成过程（DGP）时已显示出可靠的后验估计能力，但近期研究表明，它们在模型误设的情况下表现不佳。这给它们在现实问题中的应用带来了重大挑战，因为模拟器总是以某种程度歪曲真实的DGP。在本文中，我们引入了鲁棒变分神经后验估计（RVNP），该方法通过结合变分推断和误差建模来弥合模拟与现实之间的差距，从而解决基于模拟的SBI中的误设问题。我们在多个基准任务上测试了RVNP，包括使用来自天文学的真实数据，并展示了它如何以数据驱动的方式恢复鲁棒的后验推断，而无需采用控制误设影响的超参数或先验。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of model misspecification in simulation-based inference (SBI) methods, which can fail when the simulator does not accurately represent the true data generative process. The authors propose Robust Variational Neural Posterior Estimation (RVNP), a method that integrates variational inference with error modeling to bridge the simulation-to-reality gap. Experimental results on various benchmark tasks, including real astronomical data, demonstrate that RVNP achieves robust posterior inference without requiring hyperparameters or priors that control the impact of misspecification.</div>
<div class="mono" style="margin-top:8px">本文针对模拟化推理（SBI）方法在模型误设情况下的性能问题，提出了一种稳健变分神经后验估计（RVNP）方法，通过结合变分推理与误差建模来弥合模拟与真实数据生成过程之间的差距。实验结果在多个基准任务上，包括真实的天文学数据，表明RVNP能够在不依赖控制误设影响的超参数或先验的情况下，实现可靠的后验估计。</div>
</details>
</div>
<div class="card">
<div class="title">Early Warning Index for Patient Deteriorations in Hospitals</div>
<div class="meta-line">Authors: Dimitris Bertsimas, Yu Ma, Kimberly Villalobos Carballo, Gagan Singh, Michal Laskowski, Jeff Mather, Dan Kombert, Howard Haronian</div>
<div class="meta-line">First: 2025-12-16T18:47:27+00:00 · Latest: 2025-12-16T18:47:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14683v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14683v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hospitals lack automated systems to harness the growing volume of heterogeneous clinical and operational data to effectively forecast critical events. Early identification of patients at risk for deterioration is essential not only for patient care quality monitoring but also for physician care management. However, translating varied data streams into accurate and interpretable risk assessments poses significant challenges due to inconsistent data formats. We develop a multimodal machine learning framework, the Early Warning Index (EWI), to predict the aggregate risk of ICU admission, emergency response team dispatch, and mortality. Key to EWI&#x27;s design is a human-in-the-loop process: clinicians help determine alert thresholds and interpret model outputs, which are enhanced by explainable outputs using Shapley Additive exPlanations (SHAP) to highlight clinical and operational factors (e.g., scheduled surgeries, ward census) driving each patient&#x27;s risk. We deploy EWI in a hospital dashboard that stratifies patients into three risk tiers. Using a dataset of 18,633 unique patients at a large U.S. hospital, our approach automatically extracts features from both structured and unstructured electronic health record (EHR) data and achieves C-statistics of 0.796. It is currently used as a triage tool for proactively managing at-risk patients. The proposed approach saves physicians valuable time by automatically sorting patients of varying risk levels, allowing them to concentrate on patient care rather than sifting through complex EHR data. By further pinpointing specific risk drivers, the proposed model provides data-informed adjustments to caregiver scheduling and allocation of critical resources. As a result, clinicians and administrators can avert downstream complications, including costly procedures or high readmission rates and improve overall patient flow.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>医院患者恶化的早期预警指数</div>
<div class="mono" style="margin-top:8px">医院缺乏自动化的系统来处理日益增长的异构临床和运营数据，以有效预测关键事件。早期识别有恶化风险的患者不仅对患者护理质量监控至关重要，也对医生的护理管理具有重要意义。然而，由于数据格式不一致，将各种数据流转化为准确且可解释的风险评估面临重大挑战。我们开发了一种多模态机器学习框架，即早期预警指数（EWI），用于预测ICU入院、紧急响应团队派遣和死亡率的综合风险。EWI设计的关键在于人机协同过程：临床医生帮助确定警报阈值并解释模型输出，通过可解释的输出（使用Shapley Additive exPlanations，SHAP）突出临床和运营因素（如计划手术、病房床位数）对每位患者风险的影响。我们将在医院仪表板中部署EWI，将患者分为三个风险等级。使用美国一家大型医院的18,633名独特患者数据集，我们的方法能够自动从结构化和非结构化的电子健康记录（EHR）数据中提取特征，并达到0.796的C统计量。目前，该方法被用作主动管理高风险患者的分诊工具。所提出的方案通过自动对不同风险等级的患者进行分类，节省了医生宝贵的时间，使他们能够专注于患者护理，而不是处理复杂的EHR数据。此外，通过进一步明确具体的风险驱动因素，该模型还为护理人员排班和关键资源分配提供了数据驱动的调整建议。因此，临床医生和管理人员可以避免后续的并发症，包括昂贵的程序或高再入院率，从而改善整体患者流动。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research stems from the need for hospitals to better utilize heterogeneous clinical and operational data for early identification of patients at risk of deterioration. The Early Warning Index (EWI) is a multimodal machine learning framework designed to predict ICU admission, emergency response team dispatch, and mortality risk. The model incorporates a human-in-the-loop process, allowing clinicians to set alert thresholds and interpret results, with SHAP values providing explainable insights into clinical and operational factors influencing risk. Deployed in a hospital dashboard, EWI stratifies patients into three risk tiers using a dataset of 18,633 patients, achieving a C-statistic of 0.796. It effectively supports proactive patient management by automating risk sorting and highlighting key risk drivers, thereby improving resource allocation and reducing downstream complications.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于医院需要有效管理日益复杂的临床和运营数据，以预测关键患者事件。本文提出了一种多模态机器学习框架——早期预警指数（EWI），该框架整合结构化和非结构化的电子健康记录（EHR）数据，以预测ICU入院、紧急响应团队派遣和死亡风险。EWI系统采用人机协同机制，并利用SHAP值提供可解释的输出，使临床医生能够理解和调整风险阈值。实验结果基于18,633名患者的数据库，显示该模型达到0.796的C统计量，证明其在风险分层方面的有效性，并支持对高风险患者的主动管理。</div>
</details>
</div>
<div class="card">
<div class="title">TomoGraphView: 3D Medical Image Classification with Omnidirectional Slice Representations and Graph Neural Networks</div>
<div class="meta-line">Authors: Johannes Kiechle, Stefan M. Fischer, Daniel M. Lang, Cosmin I. Bercea, Matthew J. Nyflot, Lina Felsner, Julia A. Schnabel, Jan C. Peeken</div>
<div class="meta-line">First: 2025-11-12T16:30:34+00:00 · Latest: 2025-12-16T18:46:50+00:00</div>
<div class="meta-line">Comments: Preprint submitted to Medical Image Analysis (MedIA)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.09605v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.09605v3">PDF</a> · <a href="http://github.com/compai-lab/2025-MedIA-kiechle">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://pypi.org/project/OmniSlicer">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The sharp rise in medical tomography examinations has created a demand for automated systems that can reliably extract informative features for downstream tasks such as tumor characterization. Although 3D volumes contain richer information than individual slices, effective 3D classification remains difficult: volumetric data encode complex spatial dependencies, and the scarcity of large-scale 3D datasets has constrained progress toward 3D foundation models. As a result, many recent approaches rely on 2D vision foundation models trained on natural images, repurposing them as feature extractors for medical scans with surprisingly strong performance. Despite their practical success, current methods that apply 2D foundation models to 3D scans via slice-based decomposition remain fundamentally limited. Standard slicing along axial, sagittal, and coronal planes often fails to capture the true spatial extent of a structure when its orientation does not align with these canonical views. More critically, most approaches aggregate slice features independently, ignoring the underlying 3D geometry and losing spatial coherence across slices. To overcome these limitations, we propose TomoGraphView, a novel framework that integrates omnidirectional volume slicing with spherical graph-based feature aggregation. Instead of restricting the model to axial, sagittal, or coronal planes, our method samples both canonical and non-canonical cross-sections generated from uniformly distributed points on a sphere enclosing the volume. We publicly share our accessible code base at http://github.com/compai-lab/2025-MedIA-kiechle and provide a user-friendly library for omnidirectional volume slicing at https://pypi.org/project/OmniSlicer.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TomoGraphView：基于全方位切片表示和图神经网络的3D医学图像分类</div>
<div class="mono" style="margin-top:8px">医学断层扫描检查的急剧增加催生了对自动化系统的迫切需求，这些系统能够可靠地提取用于下游任务（如肿瘤表征）的有信息特征。尽管3D体数据包含比单个切片更丰富的信息，但有效的3D分类仍然困难重重：体数据编码了复杂的空间依赖关系，而大规模3D数据集的缺乏限制了3D基础模型的发展。因此，许多近期方法依赖于在自然图像上训练的2D视觉基础模型，将其重新用于医学扫描的特征提取，并取得了令人惊讶的强性能。尽管这些方法在实践中取得了成功，但当前通过基于切片的分解将2D基础模型应用于3D扫描的方法仍然存在根本性限制。标准的轴向、矢状和冠状面切片往往无法捕捉结构的真实空间范围，当其方向与这些经典视图不一致时。更关键的是，大多数方法独立地聚合切片特征，忽略了底层的3D几何结构，并在切片之间丢失了空间一致性。为克服这些限制，我们提出了TomoGraphView，一个新颖的框架，结合了全方位体积切片和基于球面图的特征聚合。我们的方法不仅采样了围绕体积均匀分布的点生成的典型和非常规横截面，还突破了将模型限制在轴向、矢状或冠状面的传统方式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing use of medical tomography has driven the need for automated systems to extract meaningful features for tasks like tumor analysis. While 3D volumes offer more information than 2D slices, effective 3D classification is challenging due to complex spatial dependencies and limited large-scale datasets. Current methods using 2D foundation models for 3D scans via slice decomposition are constrained by their inability to capture structures not aligned with standard planes and by ignoring spatial coherence. To address these issues, TomoGraphView introduces a framework that combines omnidirectional volume slicing with spherical graph-based feature aggregation, allowing for more comprehensive and coherent representation of 3D medical images.</div>
<div class="mono" style="margin-top:8px">随着医学断层扫描的广泛应用，对自动提取有意义特征以用于肿瘤等下游任务的需求日益增长。尽管3D体积数据比单个切片包含更多信息，但复杂的空间依赖关系和有限的3D数据集限制了其有效分类。目前使用2D视觉模型对3D扫描进行切片分解的方法存在局限，无法捕捉与标准平面不一致的结构，并且在切片间丢失了空间连贯性。为了解决这些问题，TomoGraphView提出了一种结合全方位体积切片和球面图特征聚合的新框架，从而实现更全面和连贯的3D医学图像表示。</div>
</details>
</div>
<div class="card">
<div class="title">GraphBench: Next-generation graph learning benchmarking</div>
<div class="meta-line">Authors: Timo Stoll, Chendi Qian, Ben Finkelshtein, Ali Parviz, Darius Weber, Fabrizio Frasca, Hadar Shavit, Antoine Siraudin, Arman Mielke, Marie Anastacio, Erik Müller, Maya Bechler-Speicher, Michael Bronstein, Mikhail Galkin, Holger Hoos, Mathias Niepert, Bryan Perozzi, Jan Tönshoff, Christopher Morris</div>
<div class="meta-line">First: 2025-12-04T05:30:31+00:00 · Latest: 2025-12-16T18:45:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.04475v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.04475v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning on graphs has recently achieved impressive progress in various domains, including molecular property prediction and chip design. However, benchmarking practices remain fragmented, often relying on narrow, task-specific datasets and inconsistent evaluation protocols, which hampers reproducibility and broader progress. To address this, we introduce GraphBench, a comprehensive benchmarking suite that spans diverse domains and prediction tasks, including node-level, edge-level, graph-level, and generative settings. GraphBench provides standardized evaluation protocols -- with consistent dataset splits and performance metrics that account for out-of-distribution generalization -- as well as a unified hyperparameter tuning framework. Additionally, we benchmark GraphBench using message-passing neural networks and graph transformer models, providing principled baselines and establishing a reference performance. See www.graphbench.io for further details.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GraphBench：下一代图学习基准测试</div>
<div class="mono" style="margin-top:8px">近年来，图上的机器学习在多个领域取得了显著进展，包括分子属性预测和芯片设计。然而，基准测试实践仍然分散，常常依赖狭窄、任务特定的数据集和不一致的评估协议，这阻碍了可重复性和更广泛的发展。为了解决这一问题，我们引入了GraphBench，这是一个涵盖多种领域和预测任务的全面基准测试套件，包括节点级、边级、图级和生成性设置。GraphBench 提供标准化的评估协议——包含一致的数据集划分和性能度量，以考虑分布外泛化能力——以及统一的超参数调优框架。此外，我们使用消息传递神经网络和图变换模型对GraphBench进行基准测试，提供原理性的基线并建立参考性能。如需进一步信息，请访问 www.graphbench.io。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for GraphBench stems from the fragmented and inconsistent benchmarking practices in graph learning, which hinder reproducibility and progress across domains. The main method involves creating a comprehensive benchmarking suite that covers diverse graph tasks and domains, with standardized evaluation protocols and consistent dataset splits. The key experimental findings show that GraphBench provides a unified framework for hyperparameter tuning and establishes reference performance using message-passing neural networks and graph transformer models.</div>
<div class="mono" style="margin-top:8px">GraphBench的动机源于图机器学习中碎片化且不一致的基准测试实践，这些实践阻碍了跨领域的可重复性和进展。其主要方法是构建一个涵盖多种图预测任务和领域的全面基准测试套件，并采用标准化的评估协议和一致的数据集划分。实验结果表明，GraphBench提供了一个统一的超参数调优框架，并通过消息传递神经网络和图变换模型建立了参考性能。</div>
</details>
</div>
<div class="card">
<div class="title">VASA-3D: Lifelike Audio-Driven Gaussian Head Avatars from a Single Image</div>
<div class="meta-line">Authors: Sicheng Xu, Guojun Chen, Jiaolong Yang, Yizhong Zhang, Yu Deng, Steve Lin, Baining Guo</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-12-16T18:44:00+00:00 · Latest: 2025-12-16T18:44:00+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025 paper. Project webpage: https://www.microsoft.com/en-us/research/project/vasa-3d/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14677v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14677v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://www.microsoft.com/en-us/research/project/vasa-3d/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose VASA-3D, an audio-driven, single-shot 3D head avatar generator. This research tackles two major challenges: capturing the subtle expression details present in real human faces, and reconstructing an intricate 3D head avatar from a single portrait image. To accurately model expression details, VASA-3D leverages the motion latent of VASA-1, a method that yields exceptional realism and vividness in 2D talking heads. A critical element of our work is translating this motion latent to 3D, which is accomplished by devising a 3D head model that is conditioned on the motion latent. Customization of this model to a single image is achieved through an optimization framework that employs numerous video frames of the reference head synthesized from the input image. The optimization takes various training losses robust to artifacts and limited pose coverage in the generated training data. Our experiment shows that VASA-3D produces realistic 3D talking heads that cannot be achieved by prior art, and it supports the online generation of 512x512 free-viewpoint videos at up to 75 FPS, facilitating more immersive engagements with lifelike 3D avatars.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VASA-3D：从单张图像生成逼真的音频驱动高斯头像</div>
<div class="mono" style="margin-top:8px">我们提出了VASA-3D，一种音频驱动的单次拍摄3D头部虚拟形象生成方法。本研究解决了两个主要挑战：捕捉真实人脸中的细微表情细节，以及从单张肖像图像重建复杂的3D头部虚拟形象。为了准确建模表情细节，VASA-3D利用了VASA-1的运动潜在变量，该方法在2D说话头像中实现了卓越的逼真度和生动性。我们工作的一个关键要素是将该运动潜在变量转换为3D，这通过设计一个基于运动潜在变量的3D头部模型来实现。该模型的定制化通过一个优化框架完成，该框架使用从输入图像合成的参考头部的多个视频帧。优化过程采用多种鲁棒的训练损失，以应对生成训练数据中的伪影和有限姿态覆盖问题。实验表明，VASA-3D能够生成现实感极强的3D说话头像，这是以往方法无法实现的，并且支持在线生成最高可达75 FPS的512x512自由视角视频，从而促进了与逼真3D虚拟形象更沉浸的互动。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">VASA-3D aims to address the challenges of capturing subtle facial expressions and reconstructing detailed 3D head avatars from a single image. The method utilizes a 3D head model conditioned on motion latents derived from VASA-1, which is optimized using synthesized video frames from the input image. Experimental results demonstrate that VASA-3D generates highly realistic 3D talking heads with free-viewpoint video capabilities at up to 75 FPS, surpassing previous methods in realism and expressiveness.</div>
<div class="mono" style="margin-top:8px">VASA-3D旨在解决从单张图像生成逼真3D头部虚拟形象以及捕捉真实人脸微妙表情的挑战。该方法利用VASA-1的运动潜变量，通过将这些潜变量作为条件来构建3D头部模型。为了根据输入图像定制模型，采用了一种优化框架，使用从图像合成的视频帧进行训练。实验结果表明，VASA-3D能够生成高度逼真的3D对话头像，并支持在线生成512x512自由视角视频，最高可达75帧每秒，超越了现有方法的能力。</div>
</details>
</div>
<div class="card">
<div class="title">VIBE: Can a VLM Read the Room?</div>
<div class="meta-line">Authors: Tania Chakraborty, Eylon Caplan, Dan Goldwasser</div>
<div class="meta-line">Venue: EMNLP</div>
<div class="meta-line">First: 2025-06-11T19:07:35+00:00 · Latest: 2025-12-16T18:42:51+00:00</div>
<div class="meta-line">Comments: Findings of EMNLP, 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.11162v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.11162v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Understanding human social behavior such as recognizing emotions and the social dynamics causing them is an important and challenging problem. While LLMs have made remarkable advances, they are limited to the textual domain and cannot account for the major role that non-verbal cues play in understanding social situations. Vision Language Models (VLMs) can potentially account for this gap, however their ability to make correct inferences over such social cues has received little attention. In this paper, we explore the capabilities of VLMs at social reasoning. We identify a previously overlooked limitation in VLMs: the Visual Social-Pragmatic Inference gap. To target this gap, we propose a new task for VLMs: Visual Social-Pragmatic Inference. We construct a high quality dataset to test the abilities of a VLM for this task and benchmark the performance of several VLMs on it.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VIBE：视觉语言模型能否读懂房间？</div>
<div class="mono" style="margin-top:8px">理解人类社交行为，如识别情绪及其背后的社会动态，是一个重要且具有挑战性的问题。尽管大型语言模型（LLMs）取得了显著进展，但它们局限于文本领域，无法充分考虑非语言线索在理解社交情境中的重要作用。视觉语言模型（VLMs）有可能弥补这一差距，但它们在处理此类社交线索时进行正确推理的能力却很少受到关注。本文探讨了VLMs在社交推理方面的能力，我们发现了一个此前被忽视的局限性：视觉社会-语用推理差距。为了解决这一问题，我们提出了一项新的任务：视觉社会-语用推理，并构建了一个高质量的数据集来测试VLMs在该任务中的能力，并对几种VLMs在该任务上的表现进行了基准测试。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of understanding human social behavior through the lens of Vision Language Models (VLMs), which are capable of processing both visual and textual information. The motivation stems from the observation that while Large Language Models (LLMs) excel in textual reasoning, they lack the ability to interpret non-verbal cues essential for social understanding. The authors introduce a new task called Visual Social-Pragmatic Inference to evaluate VLMs&#x27; capacity for social reasoning and highlight a previously unnoticed limitation in these models. They construct a high-quality dataset to assess this capability and benchmark several VLMs, revealing significant shortcomings in their ability to infer social dynamics from visual inputs.</div>
<div class="mono" style="margin-top:8px">本文探讨了通过视觉语言模型（VLMs）理解人类社会行为的挑战，特别是针对大型语言模型（LLMs）在文本领域取得显著进展但忽视非语言线索的局限性。作者提出了一个新的任务——视觉社会-语用推理，以评估VLMs在利用视觉数据进行社会情境推理方面的能力。他们构建了一个高质量的数据集，并对多个VLMs进行了基准测试，揭示了这些模型在社会推理方面存在一个被忽视的不足。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Lipschitz Continuity and Monotonicity: Fractal and Chaotic Activation Functions in Echo State Networks</div>
<div class="meta-line">Authors: Rae Chipera, Jenny Du, Irene Tsapara</div>
<div class="meta-line">First: 2025-12-16T18:41:01+00:00 · Latest: 2025-12-16T18:41:01+00:00</div>
<div class="meta-line">Comments: 50 pages, 21 figures. Extended version with full proofs, parameter sweeps, and appendices</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14675v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14675v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contemporary reservoir computing relies heavily on smooth, globally Lipschitz continuous activation functions, limiting applications in defense, disaster response, and pharmaceutical modeling where robust operation under extreme conditions is critical. We systematically investigate non-smooth activation functions, including chaotic, stochastic, and fractal variants, in echo state networks. Through comprehensive parameter sweeps across 36,610 reservoir configurations, we demonstrate that several non-smooth functions not only maintain the Echo State Property (ESP) but outperform traditional smooth activations in convergence speed and spectral radius tolerance. Notably, the Cantor function (continuous everywhere and flat almost everywhere) maintains ESP-consistent behavior up to spectral radii of rho ~ 10, an order of magnitude beyond typical bounds for smooth functions, while achieving 2.6x faster convergence than tanh and ReLU. We introduce a theoretical framework for quantized activation functions, defining a Degenerate Echo State Property (d-ESP) that captures stability for discrete-output functions and proving that d-ESP implies traditional ESP. We identify a critical crowding ratio Q=N/k (reservoir size / quantization levels) that predicts failure thresholds for discrete activations. Our analysis reveals that preprocessing topology, rather than continuity per se, determines stability: monotone, compressive preprocessing maintains ESP across scales, while dispersive or discontinuous preprocessing triggers sharp failures. While our findings challenge assumptions about activation function design in reservoir computing, the mechanism underlying the exceptional performance of certain fractal functions remains unexplained, suggesting fundamental gaps in our understanding of how geometric properties of activation functions influence reservoir dynamics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越利普希茨连续性和单调性：在回声状态网络中使用分形和混沌激活函数</div>
<div class="mono" style="margin-top:8px">现代的回声状态网络计算严重依赖于平滑且全局利普希茨连续的激活函数，这限制了其在国防、灾害响应和制药建模等需要在极端条件下保持鲁棒性的应用。我们系统地研究了回声状态网络中的非平滑激活函数，包括混沌、随机和分形变体。通过对36,610种网络配置进行全面参数扫描，我们证明了某些非平滑函数不仅保持回声状态属性（ESP），而且在收敛速度和谱半径容忍度方面优于传统的平滑激活函数。值得注意的是，康托函数（在所有点连续但在几乎所有点处平坦）在谱半径达到rho ~ 10时仍能保持与ESP一致的行为，这比传统平滑函数的典型界限高出一个数量级，同时比tanh和ReLU快2.6倍。我们提出了一种量化激活函数的理论框架，定义了退化的回声状态属性（d-ESP），该属性捕捉了离散输出函数的稳定性，并证明了d-ESP蕴含传统的ESP。我们识别出一个关键的拥挤比Q=N/k（网络规模/量化等级），用于预测离散激活函数的失效阈值。我们的分析表明，预处理拓扑结构，而非连续性本身，决定了稳定性：单调、压缩的预处理可以保持ESP在不同尺度上，而分散或不连续的预处理则会引发急剧的失效。尽管我们的发现挑战了回声状态网络中激活函数设计的假设，但某些分形函数的卓越性能背后的机制仍未被解释，这表明我们对激活函数几何特性如何影响网络动态的理解仍存在根本性差距。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research explores the use of non-smooth activation functions in echo state networks (ESNs), challenging the conventional reliance on smooth, globally Lipschitz continuous functions. By conducting extensive parameter sweeps across 36,610 reservoir configurations, the study identifies several non-smooth functions, such as the Cantor function, that maintain the Echo State Property (ESP) and exhibit superior performance in terms of convergence speed and tolerance to spectral radius variations. The Cantor function, which is continuous everywhere but flat almost everywhere, achieves 2.6 times faster convergence than tanh and ReLU while sustaining ESP up to spectral radii of rho ~ 10, significantly higher than typical smooth function limits.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决传统平滑激活函数在水库计算中的局限性，特别是在需要在极端条件下保持鲁棒性的应用中。研究探讨了非平滑激活函数，包括混沌、随机和分形变体，在回声状态网络中的应用。通过对36,610种配置的参数扫描，发现某些非平滑函数不仅保持回声状态属性（ESP），还在收敛速度和谱半径容忍度方面表现出色。例如，康托函数在谱半径达到rho ~ 10时仍能保持ESP一致性，其收敛速度比tanh和ReLU快2.6倍。</div>
</details>
</div>
<div class="card">
<div class="title">COMMA: A Communicative Multimodal Multi-Agent Benchmark</div>
<div class="meta-line">Authors: Timothy Ossowski, Danyal Maqbool, Jixuan Chen, Zefan Cai, Tyler Bradshaw, Junjie Hu</div>
<div class="meta-line">Venue: Transactions on Machine Learning Research, 2025</div>
<div class="meta-line">First: 2024-10-10T02:49:47+00:00 · Latest: 2025-12-16T18:36:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.07553v5">Abs</a> · <a href="https://arxiv.org/pdf/2410.07553v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid advances of multimodal agents built on large foundation models have largely overlooked their potential for language-based communication between agents in collaborative tasks. This oversight presents a critical gap in understanding their effectiveness in real-world deployments, particularly when communicating with humans. Existing agentic benchmarks fail to address key aspects of inter-agent communication and collaboration, particularly in scenarios where agents have unequal access to information and must work together to achieve tasks beyond the scope of individual capabilities. To fill this gap, we introduce COMMA: a novel puzzle benchmark designed to evaluate the collaborative performance of multimodal multi-agent systems through language communication. Our benchmark features a variety of multimodal puzzles, providing a comprehensive evaluation across four key categories of agentic capability in a communicative collaboration setting. Our findings reveal surprising weaknesses in state-of-the-art models, including strong proprietary models like GPT-4o and reasoning models like o4-mini. Many chain of thought reasoning models such as R1-Onevision and LLaVA-CoT struggle to outperform even a random baseline in agent-agent collaboration, indicating a potential growth area in their communication abilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>COMMA：一种用于评估多智能体协作性能的沟通型多模态基准</div>
<div class="mono" style="margin-top:8px">基于大基础模型的多模态智能体的快速发展在很大程度上忽视了它们在协作任务中通过语言进行智能体间沟通的潜力。这种忽视导致了对它们在现实部署中有效性的理解存在关键空白，尤其是在与人类沟通时。现有的智能体基准测试未能涵盖智能体间沟通与协作的关键方面，特别是在智能体信息获取不平等且需要协同完成超出单个智能体能力范围的任务的场景中。为填补这一空白，我们引入了COMMA：一个新颖的谜题基准，旨在通过语言沟通评估多模态多智能体系统的协作性能。我们的基准包含多种多模态谜题，为沟通协作环境下的四种关键智能体能力提供了全面的评估。我们的研究结果揭示了当前最先进的模型（包括强大的专有模型如GPT-4o和推理模型如o4-mini）在这些方面的令人惊讶的弱点。许多链式思维推理模型，如R1-Onevision和LLaVA-CoT，在智能体间的协作中甚至难以超越随机基线，这表明它们的沟通能力仍有较大的提升空间。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the lack of benchmarks that evaluate the effectiveness of language-based communication in collaborative tasks involving multimodal multi-agent systems. To address this, the authors introduce COMMA, a novel puzzle benchmark that assesses agents&#x27; ability to work together through language. The benchmark includes diverse multimodal puzzles and evaluates four key categories of agentic capability. Experimental results show that state-of-the-art models, including GPT-4o and o4-mini, exhibit significant weaknesses in collaborative performance, with some chain-of-thought models underperforming even a random baseline in agent-agent communication.</div>
<div class="mono" style="margin-top:8px">本研究的动机是现有基准测试未能充分评估多模态多智能体系统在协作任务中的沟通与合作能力，尤其是在信息获取不均的情境下。为此，提出了COMMA这一新型谜题基准，用于通过语言交流评估多智能体系统的协作表现。实验结果表明，包括GPT-4o和o4-mini在内的先进模型在协作任务中存在明显不足，某些链式思维模型在智能体间的交流中甚至表现不如随机基线。</div>
</details>
</div>
<div class="card">
<div class="title">Temporal Tokenization Strategies for Event Sequence Modeling with Large Language Models</div>
<div class="meta-line">Authors: Zefang Liu, Nam H. Nguyen, Yinzhu Quan, Shi-Xiong Zhang</div>
<div class="meta-line">First: 2025-12-15T18:10:51+00:00 · Latest: 2025-12-16T18:35:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.13618v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.13618v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Representing continuous time is a critical and under-explored challenge in modeling temporal event sequences with large language models (LLMs). Various strategies like byte-level representations or calendar tokens have been proposed. However, the optimal approach remains unclear, especially given the diverse statistical distributions of real-world event data, which range from smooth log-normal to discrete, spiky patterns. This paper presents the first empirical study of temporal tokenization for event sequences, comparing distinct encoding strategies: naive numeric strings, high-precision byte-level representations, human-semantic calendar tokens, classic uniform binning, and adaptive residual scalar quantization. We evaluate these strategies by fine-tuning LLMs on real-world datasets that exemplify these diverse distributions. Our analysis reveals that no single strategy is universally superior; instead, prediction performance depends heavily on aligning the tokenizer with the data&#x27;s statistical properties, with log-based strategies excelling on skewed distributions and human-centric formats proving robust for mixed modalities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于时间的标记化策略在事件序列建模中的应用</div>
<div class="mono" style="margin-top:8px">在使用大语言模型（LLMs）建模时间事件序列时，表示连续时间是一个关键且尚未充分探索的挑战。已有多种策略被提出，如字节级表示或日历标记。然而，最优策略仍不明确，尤其是在现实世界事件数据具有从平滑对数正态分布到离散尖峰模式等多种统计分布的情况下。本文首次对事件序列的时间标记化策略进行了实证研究，比较了不同的编码方法：简单的数值字符串、高精度字节级表示、人类语义日历标记、经典均匀分箱以及自适应残差标量量化。我们通过在代表这些多样分布的现实数据集上微调LLMs来评估这些策略。分析表明，没有一种策略在所有情况下都最优；预测性能高度依赖于标记器与数据统计特性的一致性，其中基于对数的策略在偏态分布中表现优异，而以人类为中心的格式在混合模态中表现出更强的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the challenge of representing continuous time in event sequence modeling using large language models (LLMs). It evaluates several temporal tokenization strategies, including naive numeric strings, byte-level representations, calendar tokens, uniform binning, and adaptive residual scalar quantization, by fine-tuning LLMs on real-world datasets with varying statistical distributions. The results show that no single strategy is universally optimal, and performance depends on matching the tokenizer to the data&#x27;s distribution characteristics, with log-based methods performing well on skewed data and human-centric formats showing robustness in mixed modalities.</div>
<div class="mono" style="margin-top:8px">本文探讨了在使用大语言模型（LLMs）进行事件序列建模时，如何表示连续时间这一关键且未被充分研究的问题。研究评估了多种时间标记策略，包括原始数值字符串、字节级表示、日历标记、均匀分箱和自适应残差标量量化，并通过在具有不同统计分布的真实数据集上微调LLMs进行比较。实验结果表明，没有一种方法在所有情况下都最优，模型性能高度依赖于标记器与数据特征的匹配程度，其中基于对数的方法在偏态分布上表现优异，而以人类为中心的格式在混合模态数据中更具鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">ART: Articulated Reconstruction Transformer</div>
<div class="meta-line">Authors: Zizhang Li, Cheng Zhang, Zhengqin Li, Henry Howard-Jenkins, Zhaoyang Lv, Chen Geng, Jiajun Wu, Richard Newcombe, Jakob Engel, Zhao Dong</div>
<div class="meta-line">First: 2025-12-16T18:35:23+00:00 · Latest: 2025-12-16T18:35:23+00:00</div>
<div class="meta-line">Comments: Project Page: https://kyleleey.github.io/ART/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14671v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14671v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://kyleleey.github.io/ART/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce ART, Articulated Reconstruction Transformer -- a category-agnostic, feed-forward model that reconstructs complete 3D articulated objects from only sparse, multi-state RGB images. Previous methods for articulated object reconstruction either rely on slow optimization with fragile cross-state correspondences or use feed-forward models limited to specific object categories. In contrast, ART treats articulated objects as assemblies of rigid parts, formulating reconstruction as part-based prediction. Our newly designed transformer architecture maps sparse image inputs to a set of learnable part slots, from which ART jointly decodes unified representations for individual parts, including their 3D geometry, texture, and explicit articulation parameters. The resulting reconstructions are physically interpretable and readily exportable for simulation. Trained on a large-scale, diverse dataset with per-part supervision, and evaluated across diverse benchmarks, ART achieves significant improvements over existing baselines and establishes a new state of the art for articulated object reconstruction from image inputs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ART：关节重建变换器</div>
<div class="mono" style="margin-top:8px">我们引入了ART（Articulated Reconstruction Transformer），这是一种类别无关的前馈模型，能够仅从稀疏的、多状态的RGB图像中重建完整的3D关节物体。以往的关节物体重建方法要么依赖于缓慢优化和脆弱的跨状态对应关系，要么使用仅限于特定物体类别的前馈模型。相比之下，ART将关节物体视为刚性部件的组合，将重建问题建模为基于部件的预测。我们新设计的变换器架构将稀疏图像输入映射到一组可学习的部件槽，ART由此联合解码统一的部件表示，包括其3D几何形状、纹理和显式关节参数。所得到的重建结果具有物理可解释性，并且易于导出用于模拟。ART在大规模、多样化的带有部件级监督的数据集上进行训练，并在多个多样化基准上进行评估，显著优于现有基线方法，并在从图像输入重建关节物体方面建立了新的技术标杆。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research is to develop a more efficient and general approach for reconstructing 3D articulated objects from sparse RGB images. ART, a feed-forward transformer model, addresses limitations of previous methods that either require slow optimization or are restricted to specific object categories. The model treats articulated objects as collections of rigid parts and uses a novel transformer architecture to map sparse image inputs to learnable part slots, enabling joint decoding of 3D geometry, texture, and articulation parameters. Experimental results show that ART outperforms existing baselines and sets a new state of the art in articulated object reconstruction from images.</div>
<div class="mono" style="margin-top:8px">ART的动机是解决现有方法在 articulated object 重建中的局限性，这些方法要么需要缓慢优化，要么仅限于特定物体类别。ART采用了一种基于变压器的前馈模型，将 articulated objects 视为刚性部件的集合，从而从稀疏的多状态RGB图像中进行基于部件的预测。该模型将图像输入映射到可学习的部件槽，并联合解码每个部件的3D几何形状、纹理和显式关节参数。实验结果表明，ART在多个基准测试中显著优于现有方法，实现了从图像输入重建完整3D articulated objects 的最先进性能。</div>
</details>
</div>
<div class="card">
<div class="title">EVOLVE-VLA: Test-Time Training from Environment Feedback for Vision-Language-Action Models</div>
<div class="meta-line">Authors: Zechen Bai, Chen Gao, Mike Zheng Shou</div>
<div class="meta-line">First: 2025-12-16T18:26:38+00:00 · Latest: 2025-12-16T18:26:38+00:00</div>
<div class="meta-line">Comments: 15 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14666v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14666v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Achieving truly adaptive embodied intelligence requires agents that learn not just by imitating static demonstrations, but by continuously improving through environmental interaction, which is akin to how humans master skills through practice. Vision-Language-Action (VLA) models have advanced robotic manipulation by leveraging large language models, yet remain fundamentally limited by Supervised Finetuning (SFT): requiring hundreds of demonstrations per task, rigidly memorizing trajectories, and failing to adapt when deployment conditions deviate from training. We introduce EVOLVE-VLA, a test-time training framework enabling VLAs to continuously adapt through environment interaction with minimal or zero task-specific demonstrations. The key technical challenge is replacing oracle reward signals (unavailable at test time) with autonomous feedback. We address this through a learned progress estimator providing dense feedback, and critically, we design our framework to ``tame&#x27;&#x27; this inherently noisy signal via two mechanisms: (1) an accumulative progress estimation mechanism smoothing noisy point-wise estimates, and (2) a progressive horizon extension strategy enabling gradual policy evolution. EVOLVE-VLA achieves substantial gains: +8.6\% on long-horizon tasks, +22.0\% in 1-shot learning, and enables cross-task generalization -- achieving 20.8\% success on unseen tasks without task-specific demonstrations training (vs. 0\% for pure SFT). Qualitative analysis reveals emergent capabilities absent in demonstrations, including error recovery and novel strategies. This work represents a critical step toward VLAs that truly learn and adapt, moving beyond static imitation toward continuous self-improvements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EVOLVE-VLA：基于环境反馈的测试时训练框架用于视觉-语言-动作模型</div>
<div class="mono" style="margin-top:8px">实现真正适应性的具身智能需要代理不仅通过静态演示进行模仿学习，还要通过与环境的持续交互不断改进，这类似于人类通过练习掌握技能的方式。视觉-语言-动作（VLA）模型通过利用大规模语言模型在机器人操作方面取得了进展，但其根本性限制在于监督微调（SFT）：每个任务需要数百个演示，机械记忆轨迹，并且在部署条件偏离训练条件时无法适应。我们引入了EVOLVE-VLA，一个测试时训练框架，使VLA模型能够通过与环境的交互以最小或零任务特定演示实现持续适应。关键的技术挑战是用自主反馈替代不可用的oracle奖励信号。我们通过一个学习到的进度估计器提供密集反馈来解决这一问题，并且关键的是，我们设计了框架以通过两种机制「驯服」这一固有的噪声信号：(1) 一个累积进度估计机制，用于平滑噪声点估计；(2) 一种渐进式时间范围扩展策略，使策略能够逐步进化。EVOLVE-VLA取得了显著提升：在长时间范围任务中提升+8.6%，在单次学习中提升+22.0%，并实现了跨任务泛化——在没有任务特定演示训练的情况下，对未见过的任务成功率达到20.8%（相比之下纯SFT为0%）。定性分析揭示了演示中缺失的新兴能力，包括错误恢复和新颖策略。这项工作是向真正能够学习和适应的VLA模型迈出的关键一步，超越了静态模仿，迈向持续的自我提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind EVOLVE-VLA is to enable vision-language-action models to adaptively learn and improve through interaction with the environment rather than relying on static demonstrations. The method involves a test-time training framework that replaces oracle reward signals with autonomous feedback generated by a learned progress estimator. This estimator is designed with two mechanisms: an accumulative progress estimation to smooth noisy feedback and a progressive horizon extension to facilitate gradual policy evolution. The key experimental results show significant improvements, including an 8.6% increase in performance on long-horizon tasks, a 22% improvement in 1-shot learning, and the ability to generalize to unseen tasks with 20.8% success without task-specific demonstrations.</div>
<div class="mono" style="margin-top:8px">EVOLVE-VLA 的研究动机是通过环境反馈实现 Vision-Language-Action 模型的持续学习，从而超越静态演示的局限。该方法采用了一种测试时训练框架，用学习到的进度估计器替代 oracle 奖励信号，提供密集反馈。为处理该反馈的噪声，框架引入了累积进度估计机制和渐进式时间范围扩展策略。实验结果表明，该方法在长时间任务中提升了 8.6%，在单次学习中提升了 22.0%，并在无需任务特定演示的情况下，在未见过的任务中达到了 20.8% 的成功率，远超纯监督微调的 0%。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Visual Sentiment Analysis via Semiotic Isotopy-Guided Dataset Construction</div>
<div class="meta-line">Authors: Marco Blanchini, Giovanna Maria Dimitri, Benedetta Tondi, Tarcisio Lancioni, Mauro Barni</div>
<div class="meta-line">First: 2025-12-16T18:26:22+00:00 · Latest: 2025-12-16T18:26:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14665v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14665v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual Sentiment Analysis (VSA) is a challenging task due to the vast diversity of emotionally salient images and the inherent difficulty of acquiring sufficient data to capture this variability comprehensively. Key obstacles include building large-scale VSA datasets and developing effective methodologies that enable algorithms to identify emotionally significant elements within an image. These challenges are reflected in the limited generalization performance of VSA algorithms and models when trained and tested across different datasets. Starting from a pool of existing data collections, our approach enables the creation of a new larger dataset that not only contains a wider variety of images than the original ones, but also permits training new models with improved capability to focus on emotionally relevant combinations of image elements. This is achieved through the integration of the semiotic isotopy concept within the dataset creation process, providing deeper insights into the emotional content of images. Empirical evaluations show that models trained on a dataset generated with our method consistently outperform those trained on the original data collections, achieving superior generalization across major VSA benchmarks</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过语义同构引导的数据集构建增强视觉情感分析</div>
<div class="mono" style="margin-top:8px">由于情感显著图像的广泛多样性以及获取足够数据以全面捕捉这种变化的固有困难，视觉情感分析（VSA）是一项具有挑战性的任务。主要障碍包括构建大规模的VSA数据集以及开发有效的 methodology，使算法能够识别图像中的情感相关元素。这些挑战体现在VSA算法和模型在不同数据集上的训练和测试时泛化性能有限。我们的方法从现有的数据集合出发，创建了一个更大且包含更广泛图像种类的新数据集，同时允许训练具有更强情感相关元素识别能力的新模型。这是通过在数据集构建过程中整合语义同构概念实现的，从而提供对图像情感内容更深入的理解。实证评估表明，使用我们方法生成的数据集训练的模型在主要的VSA基准测试中表现优于基于原始数据集合训练的模型，实现了更优的泛化能力</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research stems from the challenges in Visual Sentiment Analysis (VSA) due to the diversity of emotionally salient images and the lack of comprehensive datasets. The proposed method introduces semiotic isotopy-guided dataset construction, which enhances the variety and emotional relevance of images in the dataset. Experimental results demonstrate that models trained on the newly constructed dataset exhibit better generalization performance compared to those trained on existing collections, achieving superior results across major VSA benchmarks.</div>
<div class="mono" style="margin-top:8px">视觉情感分析（VSA）面临挑战，主要由于情感表达丰富的图像多样性以及获取全面数据集的困难。本文提出了一种方法，通过整合语义同构概念来构建更大且更具多样性的数据集，从而帮助模型更有效地识别图像中的情感相关元素。实验结果表明，使用该方法构建的数据集训练的模型在主要的VSA基准测试中表现出更优的泛化性能。</div>
</details>
</div>
<div class="card">
<div class="title">Focus: A Streaming Concentration Architecture for Efficient Vision-Language Models</div>
<div class="meta-line">Authors: Chiyue Wei, Cong Guo, Junyao Zhang, Haoxuan Shan, Yifan Xu, Ziyue Zhang, Yudong Liu, Qinsi Wang, Changchun Zhou, Hai &quot;Helen&quot; Li, Yiran Chen</div>
<div class="meta-line">First: 2025-12-16T18:21:18+00:00 · Latest: 2025-12-16T18:21:18+00:00</div>
<div class="meta-line">Comments: HPCA 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14661v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14661v1">PDF</a> · <a href="https://github.com/dubcyfor3/Focus">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have demonstrated strong performance on tasks such as video captioning and visual question answering. However, their growing scale and video-level inputs lead to significant computational and memory overhead, posing challenges for real-time deployment on hardware accelerators. While prior work attempts to reduce redundancy via token pruning or merging, these methods typically operate at coarse granularity and incur high runtime overhead due to global token-level operations. In this study, we propose Focus, a Streaming Concentration Architecture that efficiently accelerates VLM inference through progressive, fine-grained redundancy elimination. Focus introduces a multilevel concentration paradigm that hierarchically compresses vision-language inputs at three levels: (1) semantic-guided token pruning based on textual prompts, (2) spatial-temporal block-level concentration using localized comparisons, and (3) vector-level redundancy removal via motion-aware matching. All concentration steps are tightly co-designed with the architecture to support streaming-friendly, on-chip execution. Focus leverages GEMM tiling, convolution-style layout, and cross-modal attention to minimize off-chip access while enabling high throughput. Implemented as a modular unit within a systolic-array accelerator, Focus achieves a 2.4x speedup and 3.3x reduction in energy, significantly outperforming state-of-the-art accelerators in both performance and energy efficiency. Full-stack implementation of Focus is open-sourced at https://github.com/dubcyfor3/Focus.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>聚焦：一种用于高效视觉-语言模型的流式集中架构</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）在视频字幕生成和视觉问答等任务中表现出色。然而，其规模的扩大和视频级输入导致了显著的计算和内存开销，给在硬件加速器上的实时部署带来了挑战。尽管已有工作尝试通过令牌修剪或合并来减少冗余，但这些方法通常在粗粒度层面操作，由于全局令牌级操作而带来较高的运行时开销。在本研究中，我们提出了Focus，一种流式集中架构，通过渐进式、细粒度的冗余消除来高效加速VLM推理。Focus引入了多级集中范式，通过三个层级对视觉-语言输入进行分层压缩：（1）基于文本提示的语义引导令牌修剪，（2）利用局部比较进行时空块级集中，（3）通过运动感知匹配进行向量级冗余消除。所有集中步骤均与架构紧密协同设计，以支持流式友好的片上执行。Focus利用GEMM分块、卷积式布局和跨模态注意力机制，以减少片外访问并实现高吞吐量。Focus作为流体阵列加速器中的一个模块单元实现，实现了2.4倍的速度提升和3.3倍的能耗降低，在性能和能效方面显著优于现有最先进的加速器。Focus的全栈实现已开源，地址为https://github.com/dubcyfor3/Focus。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The increasing scale of Vision-Language Models (VLMs) and their video-level inputs create significant computational and memory challenges for real-time deployment. To address this, the study introduces Focus, a Streaming Concentration Architecture that reduces redundancy through three hierarchical levels: semantic-guided token pruning, spatial-temporal block-level concentration, and vector-level redundancy removal. Experimental results show that Focus achieves a 2.4x speedup and 3.3x energy reduction compared to state-of-the-art accelerators, demonstrating improved performance and efficiency in VLM inference.</div>
<div class="mono" style="margin-top:8px">随着视觉语言模型（VLMs）规模的扩大及其视频级输入的增加，实时部署面临显著的计算和内存挑战。为了解决这一问题，本研究提出了Focus，一种流式集中架构，通过在三个层级上进行渐进式、细粒度的冗余消除来提升效率：基于文本提示的语义引导式标记修剪、利用局部比较的空间时间块级集中，以及通过运动感知匹配的向量级冗余去除。实验结果表明，Focus相比最先进的加速器实现了2.4倍的速度提升和3.3倍的能耗降低，展示了其在VLM推理中的卓越性能和能效。</div>
</details>
</div>
<div class="card">
<div class="title">gridfm-datakit-v1: A Python Library for Scalable and Realistic Power Flow and Optimal Power Flow Data Generation</div>
<div class="meta-line">Authors: Alban Puech, Matteo Mazzonelli, Celia Cintas, Tamara R. Govindasamy, Mangaliso Mngomezulu, Jonas Weiss, Matteo Baù, Anna Varbella, François Mirallès, Kibaek Kim, Le Xie, Hendrik F. Hamann, Etienne Vos, Thomas Brunschwiler</div>
<div class="meta-line">First: 2025-12-16T18:17:50+00:00 · Latest: 2025-12-16T18:17:50+00:00</div>
<div class="meta-line">Comments: Main equal contributors: Alban Puech, Matteo Mazzonelli. Other equal contributors: Celia Cintas, Tamara R. Govindasamy, Mangaliso Mngomezulu, Jonas Weiss</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14658v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14658v1">PDF</a> · <a href="https://github.com/gridfm/gridfm-datakit">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce gridfm-datakit-v1, a Python library for generating realistic and diverse Power Flow (PF) and Optimal Power Flow (OPF) datasets for training Machine Learning (ML) solvers. Existing datasets and libraries face three main challenges: (1) lack of realistic stochastic load and topology perturbations, limiting scenario diversity; (2) PF datasets are restricted to OPF-feasible points, hindering generalization of ML solvers to cases that violate operating limits (e.g., branch overloads or voltage violations); and (3) OPF datasets use fixed generator cost functions, limiting generalization across varying costs. gridfm-datakit addresses these challenges by: (1) combining global load scaling from real-world profiles with localized noise and supporting arbitrary N-k topology perturbations to create diverse yet realistic datasets; (2) generating PF samples beyond operating limits; and (3) producing OPF data with varying generator costs. It also scales efficiently to large grids (up to 10,000 buses). Comparisons with OPFData, OPF-Learn, PGLearn, and PF$Δ$ are provided. Available on GitHub at https://github.com/gridfm/gridfm-datakit under Apache 2.0 and via `pip install gridfm-datakit`.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>gridfm-datakit-v1: 一种用于可扩展且现实的潮流和最优潮流数据生成的Python库</div>
<div class="mono" style="margin-top:8px">我们介绍了gridfm-datakit-v1，这是一个用于生成真实且多样化的潮流（PF）和最优潮流（OPF）数据集的Python库，用于训练机器学习（ML）求解器。现有数据集和库面临三个主要挑战：(1) 缺乏真实的随机负荷和拓扑扰动，限制了场景的多样性；(2) 潮流数据集仅限于OPF可行点，阻碍了机器学习求解器在违反运行限制情况下的泛化能力（例如支路过载或电压越限）；(3) 最优潮流数据集使用固定的发电机成本函数，限制了在不同成本情况下的泛化能力。gridfm-datakit通过以下方式解决这些问题：(1) 将真实世界负荷曲线的全局缩放与局部噪声结合，并支持任意N-k拓扑扰动，以创建多样化且真实的数据集；(2) 生成超出运行限制的潮流样本；(3) 生成具有不同发电机成本的最优潮流数据。它还能够高效扩展到大规模电网（最多10,000个节点）。与OPFData、OPF-Learn、PGLearn和PF$Δ$进行了比较。可在GitHub上获取，地址为https://github.com/gridfm/gridfm-datakit，采用Apache 2.0许可证，也可通过`pip install gridfm-datakit`安装。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for gridfm-datakit-v1 is to overcome the limitations of existing datasets by providing more realistic and diverse Power Flow (PF) and Optimal Power Flow (OPF) data for training Machine Learning solvers. The library employs a method that combines real-world load profiles with localized noise and arbitrary N-k topology perturbations to generate varied yet realistic scenarios. It also creates PF samples that exceed operating limits and OPF data with different generator cost functions, enhancing the generalization of ML models. Experimental results show that gridfm-datakit-v1 effectively addresses these challenges and scales efficiently to large power systems with up to 10,000 buses, outperforming other tools in generating comprehensive and realistic data.</div>
<div class="mono" style="margin-top:8px">开发gridfm-datakit-v1的动机源于现有数据集和库在生成真实且多样化的电力潮流（PF）和最优潮流（OPF）数据方面存在局限。该库通过结合真实负载剖面与局部噪声，并支持任意N-k拓扑扰动，增强了场景的多样性。它还能够生成超出运行限制的PF样本，并创建具有不同发电机成本函数的OPF数据，从而提升机器学习模型的泛化能力。该工具可高效扩展至拥有最多10,000个节点的大型电网系统，并与OPFData、OPF-Learn、PGLearn和PF$Δ$等现有工具进行了比较。</div>
</details>
</div>
<div class="card">
<div class="title">WaveSim: A Wavelet-based Multi-scale Similarity Metric for Weather and Climate Fields</div>
<div class="meta-line">Authors: Gabriele Accarino, Viviana Acquaviva, Sara Shamekh, Duncan Watson-Parris, David Lawrence</div>
<div class="meta-line">First: 2025-12-16T18:15:53+00:00 · Latest: 2025-12-16T18:15:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14656v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14656v1">PDF</a> · <a href="https://github.com/gabrieleaccarino/wavesim">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce WaveSim, a multi-scale similarity metric for the evaluation of spatial fields in weather and climate applications. WaveSim exploits wavelet transforms to decompose input fields into scale-specific wavelet coefficients. The metric is built by multiplying three orthogonal components derived from these coefficients: Magnitude, which quantifies similarities in the energy distribution of the coefficients, i.e., the intensity of the field; Displacement, which captures spatial shift by comparing the centers of mass of normalized energy distributions; and Structure, which assesses pattern organization independent of location and amplitude. Each component yields a scale-specific similarity score ranging from 0 (no similarity) to 1 (perfect similarity), which are then combined across scales to produce an overall similarity measure. We first evaluate WaveSim using synthetic test cases, applying controlled spatial and temporal perturbations to systematically assess its sensitivity and expected behavior. We then demonstrate its applicability to physically relevant case studies of key modes of climate variability in Earth System Models. Traditional point-wise metrics lack a mechanism for attributing errors to physical scales or modes of dissimilarity. By operating in the wavelet domain and decomposing the signal along independent axes, WaveSim bypasses these limitations and provides an interpretable and diagnostically rich framework for assessing similarity in complex fields. Additionally, the WaveSim framework allows users to place emphasis on a specific scale or component, and lends itself to user-specific model intercomparison, model evaluation, and calibration and training of forecasting systems. We provide a PyTorch-ready implementation of WaveSim, along with all evaluation scripts, at: https://github.com/gabrieleaccarino/wavesim.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WaveSim：一种基于小波的多尺度相似性度量方法用于天气和气候场</div>
<div class="mono" style="margin-top:8px">我们引入了WaveSim，这是一种用于评估天气和气候应用中空间场的多尺度相似性度量方法。WaveSim利用小波变换将输入场分解为特定尺度的小波系数。该度量方法由这三个正交分量相乘构建而成：幅度分量，量化系数能量分布的相似性，即场的强度；位移分量，通过比较归一化能量分布的质心来捕捉空间位移；结构分量，评估模式组织，与位置和振幅无关。每个分量产生一个特定尺度的相似性得分，范围从0（无相似性）到1（完全相似性），然后在不同尺度上进行组合以生成总体相似性度量。我们首先使用合成测试案例评估WaveSim，通过施加受控的空间和时间扰动，系统地评估其敏感性和预期行为。随后，我们展示了其在地球系统模型中关键气候变率模式的物理相关案例研究中的适用性。传统的逐点度量方法缺乏将误差归因于物理尺度或不相似模式的机制。通过在小波域中操作并沿独立轴分解信号，WaveSim克服了这些限制，提供了一个可解释且诊断信息丰富的框架，用于评估复杂场中的相似性。此外，WaveSim框架允许用户强调特定尺度或分量，并适用于用户特定的模型比较、模型评估以及预测系统的校准和训练。我们提供了WaveSim的PyTorch可用实现，以及所有评估脚本，网址为：https://github.com/gabrieleaccarino/wavesim。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">WaveSim is introduced as a multi-scale similarity metric for evaluating spatial fields in weather and climate applications. It utilizes wavelet transforms to decompose input fields into scale-specific coefficients and combines three orthogonal components—Magnitude, Displacement, and Structure—to compute a similarity score. Magnitude assesses energy distribution, Displacement captures spatial shifts, and Structure evaluates pattern organization. The metric is tested on synthetic cases with controlled perturbations and applied to climate variability case studies in Earth System Models. WaveSim overcomes limitations of traditional point-wise metrics by providing scale-specific insights and enabling user-defined emphasis on specific components or scales, supporting model intercomparison and forecasting system calibration.</div>
<div class="mono" style="margin-top:8px">WaveSim 是一种用于评估天气和气候空间场的多尺度相似性度量方法。该方法利用小波变换将输入场分解为不同尺度的小波系数，并结合三个正交的组成部分——幅度、位移和结构——来计算整体相似性得分。幅度评估能量分布，位移捕捉空间位移，结构评估模式组织。该方法首先在具有受控扰动的合成案例上进行了测试，随后应用于地球系统模型中的气候变率相关案例研究。WaveSim 通过提供一种尺度感知且可解释的框架，克服了传统点对点度量方法的局限性，能够诊断复杂场中的相似性和差异性。</div>
</details>
</div>
<div class="card">
<div class="title">ViRC: Enhancing Visual Interleaved Mathematical CoT with Reason Chunking</div>
<div class="meta-line">Authors: Lihong Wang, Liangqi Li, Weiwei Feng, Jiamin Wu, Changtao Miao, Tieru Wu, Rui Ma, Bo Zhang, Zhe Li</div>
<div class="meta-line">First: 2025-12-16T18:13:54+00:00 · Latest: 2025-12-16T18:13:54+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/Leon-LihongWang/ViRC</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14654v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14654v1">PDF</a> · <a href="https://github.com/Leon-LihongWang/ViRC">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">CoT has significantly enhanced the reasoning ability of LLMs while it faces challenges when extended to multimodal domains, particularly in mathematical tasks. Existing MLLMs typically perform textual reasoning solely from a single static mathematical image, overlooking dynamic visual acquisition during reasoning. In contrast, humans repeatedly examine visual image and employ step-by-step reasoning to prove intermediate propositions. This strategy of decomposing the problem-solving process into key logical nodes adheres to Miller&#x27;s Law in cognitive science. Inspired by this insight, we propose a ViRC framework for multimodal mathematical tasks, introducing a Reason Chunking mechanism that structures multimodal mathematical CoT into consecutive Critical Reasoning Units (CRUs) to simulate human expert problem-solving patterns. CRUs ensure intra-unit textual coherence for intermediate proposition verification while integrating visual information across units to generate subsequent propositions and support structured reasoning. To this end, we present CRUX dataset by using three visual tools and four reasoning patterns to provide explicitly annotated CRUs across multiple reasoning paths for each mathematical problem. Leveraging the CRUX dataset, we propose a progressive training strategy inspired by human cognitive learning, which includes Instructional SFT, Practice SFT, and Strategic RL, aimed at further strengthening the Reason Chunking ability of the model.The resulting ViRC-7B model achieves a 18.8\% average improvement over baselines across multiple mathematical benchmarks. Code is available at https://github.com/Leon-LihongWang/ViRC.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ViRC：通过推理分块增强视觉交错数学推理</div>
<div class="mono" style="margin-top:8px">虽然CoT显著提升了LLMs的推理能力，但在扩展到多模态领域时，尤其是在数学任务中，仍面临挑战。现有的多模态大语言模型通常仅从单个静态数学图像中进行文本推理，忽略了推理过程中动态的视觉获取。相反，人类会反复查看图像，并采用逐步推理来证明中间命题。这种将问题解决过程分解为关键逻辑节点的策略遵循了认知科学中的Miller定律。受此启发，我们提出了一个用于多模态数学任务的ViRC框架，引入了推理分块机制，将多模态数学CoT结构化为连续的推理单元（CRUs），以模拟人类专家的问题解决模式。CRUs确保单元内文本的一致性，用于验证中间命题，同时在单元间整合视觉信息以生成后续命题并支持结构化推理。为此，我们使用三种视觉工具和四种推理模式构建了CRUX数据集，为每个数学问题提供跨多个推理路径的显式标注的CRUs。借助CRUX数据集，我们提出了一种受人类认知学习启发的渐进式训练策略，包括指导型SFT、练习型SFT和策略型强化学习，旨在进一步增强模型的推理分块能力。最终的ViRC-7B模型在多个数学基准测试中平均优于基线模型18.8\%。代码可在https://github.com/Leon-LihongWang/ViRC获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to enhance the reasoning capabilities of large language models (LLMs) in mathematical tasks by addressing the limitations of existing multimodal large language models (MLLMs) that rely on static image analysis. The proposed ViRC framework introduces a Reason Chunking mechanism that breaks down the problem-solving process into Critical Reasoning Units (CRUs), mimicking human step-by-step reasoning with visual input. This approach ensures textual coherence within each CRU for verifying intermediate propositions and integrates visual information across units to support structured reasoning. The CRUX dataset, annotated with multiple reasoning paths, is used to train the model using a progressive strategy involving Instructional SFT, Practice SFT, and Strategic RL, leading to a 18.8\% average improvement over baselines on various mathematical benchmarks.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过引入动态视觉分析机制提升大语言模型在数学任务中的推理能力，解决现有多模态模型仅依赖静态图像分析的局限性。ViRC框架提出了一种Reason Chunking方法，将问题解决过程分解为关键推理单元（CRUs），模拟人类逐步推理的模式。该方法在每个CRU内确保文本连贯性以验证中间命题，并在不同单元间整合视觉信息以支持结构化推理。通过CRUX数据集，该模型采用渐进式训练策略，包括指导型监督微调、练习型监督微调和策略型强化学习，从而在多个数学基准测试中实现了比基线模型平均高18.8\%的性能提升。</div>
</details>
</div>
<div class="card">
<div class="title">AI-GenBench: A New Ongoing Benchmark for AI-Generated Image Detection</div>
<div class="meta-line">Authors: Lorenzo Pellegrini, Davide Cozzolino, Serafino Pandolfini, Davide Maltoni, Matteo Ferrara, Luisa Verdoliva, Marco Prati, Marco Ramilli</div>
<div class="meta-line">First: 2025-04-29T15:41:13+00:00 · Latest: 2025-12-16T18:11:21+00:00</div>
<div class="meta-line">Comments: Accepted at Verimedia workshop, IJCNN 2025. 9 pages, 6 figures, 4 tables, code available: https://github.com/MI-BioLab/AI-GenBench</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.20865v3">Abs</a> · <a href="https://arxiv.org/pdf/2504.20865v3">PDF</a> · <a href="https://github.com/MI-BioLab/AI-GenBench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid advancement of generative AI has revolutionized image creation, enabling high-quality synthesis from text prompts while raising critical challenges for media authenticity. We present Ai-GenBench, a novel benchmark designed to address the urgent need for robust detection of AI-generated images in real-world scenarios. Unlike existing solutions that evaluate models on static datasets, Ai-GenBench introduces a temporal evaluation framework where detection methods are incrementally trained on synthetic images, historically ordered by their generative models, to test their ability to generalize to new generative models, such as the transition from GANs to diffusion models. Our benchmark focuses on high-quality, diverse visual content and overcomes key limitations of current approaches, including arbitrary dataset splits, unfair comparisons, and excessive computational demands. Ai-GenBench provides a comprehensive dataset, a standardized evaluation protocol, and accessible tools for both researchers and non-experts (e.g., journalists, fact-checkers), ensuring reproducibility while maintaining practical training requirements. By establishing clear evaluation rules and controlled augmentation strategies, Ai-GenBench enables meaningful comparison of detection methods and scalable solutions. Code and data are publicly available to ensure reproducibility and to support the development of robust forensic detectors to keep pace with the rise of new synthetic generators.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI-GenBench：一种用于AI生成图像检测的新持续基准</div>
<div class="mono" style="margin-top:8px">生成式AI的快速发展革新了图像创作，使高质量的图像合成成为可能，同时也对媒体真实性提出了关键挑战。我们提出了AI-GenBench，一个全新的基准，旨在应对现实场景中对AI生成图像进行稳健检测的迫切需求。与现有基于静态数据集评估模型的解决方案不同，AI-GenBench引入了一个时间评估框架，其中检测方法在按生成模型历史顺序排列的合成图像上逐步训练，以测试其对新生成模型（如从GANs过渡到扩散模型）的泛化能力。我们的基准专注于高质量、多样化的视觉内容，并克服了当前方法中的关键限制，包括任意的数据集划分、不公平的比较以及过高的计算需求。AI-GenBench提供了全面的数据集、标准化的评估协议以及对研究者和非专家（如记者、事实核查员）都易于使用的工具，确保可重复性的同时保持实际的训练需求。通过建立明确的评估规则和受控的增强策略，AI-GenBench使检测方法的有意义比较和可扩展解决方案成为可能。代码和数据已公开，以确保可重复性，并支持开发稳健的数字取证检测器，以应对新型合成生成器的兴起。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the increasing prevalence of AI-generated images and the need for reliable detection methods in real-world applications. Ai-GenBench introduces a temporal evaluation framework where detection models are incrementally trained on synthetic images generated by historically ordered models to assess their ability to generalize to new ones, such as from GANs to diffusion models. The key experimental findings demonstrate that the benchmark provides a diverse and high-quality dataset, addresses limitations like arbitrary splits and computational inefficiency, and supports standardized evaluation and reproducibility for both experts and non-experts.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于AI生成图像的迅速增长以及在现实应用中对可靠检测方法的需求。Ai-GenBench引入了一个时间评估框架，通过按历史顺序排列的生成模型合成图像，逐步训练检测模型以评估其对新模型（如从GAN过渡到扩散模型）的泛化能力。该基准提供了多样且高质量的数据集、标准化评估协议和易于使用的工具，以确保可重复性和实用性。实验结果表明，当前的检测方法在适应新生成模型方面存在困难，突显了开发更强大和可扩展解决方案的必要性。</div>
</details>
</div>
<div class="card">
<div class="title">Understanding Sampler Stochasticity in Training Diffusion Models for RLHF</div>
<div class="meta-line">Authors: Jiayuan Sheng, Hanyang Zhao, Haoxian Chen, David D. Yao, Wenpin Tang</div>
<div class="meta-line">First: 2025-10-12T19:08:38+00:00 · Latest: 2025-12-16T18:10:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.10767v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.10767v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement Learning from Human Feedback (RLHF) is increasingly used to fine-tune diffusion models, but a key challenge arises from the mismatch between stochastic samplers used during training and deterministic samplers used during inference. In practice, models are fine-tuned using stochastic SDE samplers to encourage exploration, while inference typically relies on deterministic ODE samplers for efficiency and stability. This discrepancy induces a reward gap, raising concerns about whether high-quality outputs can be expected during inference. In this paper, we theoretically characterize this reward gap and provide non-vacuous bounds for general diffusion models, along with sharper convergence rates for Variance Exploding (VE) and Variance Preserving (VP) Gaussian models. Methodologically, we adopt the generalized denoising diffusion implicit models (gDDIM) framework to support arbitrarily high levels of stochasticity, preserving data marginals throughout. Empirically, our findings through large-scale experiments on text-to-image models using denoising diffusion policy optimization (DDPO) and mixed group relative policy optimization (MixGRPO) validate that reward gaps consistently narrow over training, and ODE sampling quality improves when models are updated using higher-stochasticity SDE training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>理解扩散模型在RLHF训练中的采样随机性</div>
<div class="mono" style="margin-top:8px">强化学习从人类反馈（RLHF）正被越来越多地用于微调扩散模型，但一个关键挑战来自于训练过程中使用的随机采样器与推理过程中使用的确定性采样器之间的不匹配。在实践中，模型通常使用随机SDE采样器进行微调以鼓励探索，而推理则通常依赖于确定性的ODE采样器以提高效率和稳定性。这种差异导致了奖励差距，引发了对推理过程中是否能获得高质量输出的担忧。在本文中，我们从理论上刻画了这种奖励差距，并为一般的扩散模型提供了非空的界限，同时为方差爆炸（VE）和方差保持（VP）高斯模型提供了更精确的收敛速率。方法上，我们采用广义去噪扩散隐式模型（gDDIM）框架，以支持任意高水平的随机性，同时在整个过程中保持数据边缘分布。实证上，我们通过在文本到图像模型上使用去噪扩散策略优化（DDPO）和混合组相对策略优化（MixGRPO）进行的大规模实验验证了我们的发现：奖励差距在训练过程中持续缩小，当模型使用更高随机性的SDE训练进行更新时，ODE采样质量也得到提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of reward gap caused by the discrepancy between stochastic samplers used during training and deterministic samplers used during inference in diffusion models fine-tuned for RLHF. It theoretically analyzes the reward gap and provides non-vacuous bounds for general diffusion models, along with improved convergence rates for VE and VP Gaussian models. The authors employ the generalized denoising diffusion implicit models (gDDIM) framework to enable high levels of stochasticity while preserving data marginals. Experimental results on text-to-image models using DDPO and MixGRPO show that reward gaps decrease over training and that ODE sampling quality improves when models are trained with higher-stochasticity SDE methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于在RLHF中训练和推理阶段使用不同采样器导致的奖励差距问题。论文采用广义去噪扩散隐式模型（gDDIM）框架，在训练过程中支持任意高水平的随机性，同时保持数据边缘分布。在文本到图像模型上使用DDPO和MixGRPO进行的大规模实验结果表明，奖励差距随着训练过程逐渐缩小，且当模型使用更高随机性的SDE训练时，ODE采样质量得到提升。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptable Segmentation Pipeline for Diverse Brain Tumors with Radiomic-guided Subtyping and Lesion-Wise Model Ensemble</div>
<div class="meta-line">Authors: Daniel Capellán-Martín, Abhijeet Parida, Zhifan Jiang, Nishad Kulkarni, Krithika Iyer, Austin Tapp, Syed Muhammad Anwar, María J. Ledesma-Carbayo, Marius George Linguraru</div>
<div class="meta-line">Venue: MICCAI</div>
<div class="meta-line">First: 2025-12-16T18:09:48+00:00 · Latest: 2025-12-16T18:09:48+00:00</div>
<div class="meta-line">Comments: 12 pages, 5 figures, 3 tables. Algorithm presented at MICCAI BraTS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.14648v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.14648v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robust and generalizable segmentation of brain tumors on multi-parametric magnetic resonance imaging (MRI) remains difficult because tumor types differ widely. The BraTS 2025 Lighthouse Challenge benchmarks segmentation methods on diverse high-quality datasets of adult and pediatric tumors: multi-consortium international pediatric brain tumor segmentation (PED), preoperative meningioma tumor segmentation (MEN), meningioma radiotherapy segmentation (MEN-RT), and segmentation of pre- and post-treatment brain metastases (MET). We present a flexible, modular, and adaptable pipeline that improves segmentation performance by selecting and combining state-of-the-art models and applying tumor- and lesion-specific processing before and after training. Radiomic features extracted from MRI help detect tumor subtype, ensuring a more balanced training. Custom lesion-level performance metrics determine the influence of each model in the ensemble and optimize post-processing that further refines the predictions, enabling the workflow to tailor every step to each case. On the BraTS testing sets, our pipeline achieved performance comparable to top-ranked algorithms across multiple challenges. These findings confirm that custom lesion-aware processing and model selection yield robust segmentations yet without locking the method to a specific network architecture. Our method has the potential for quantitative tumor measurement in clinical practice, supporting diagnosis and prognosis.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于放射组学引导的亚型识别和病灶级模型集成的可适应脑肿瘤分割流程</div>
<div class="mono" style="margin-top:8px">由于肿瘤类型差异较大，对多参数磁共振成像（MRI）中的脑肿瘤进行稳健且可推广的分割仍然具有挑战性。BraTS 2025 Lighthouse 挑战赛在成人和儿童肿瘤的多样化高质量数据集上评估分割方法：多机构国际儿童脑肿瘤分割（PED）、术前脑膜瘤分割（MEN）、脑膜瘤放疗分割（MEN-RT）以及治疗前后的脑转移瘤分割（MET）。我们提出了一种灵活、模块化且可适应的流程，通过选择和组合最先进的模型，并在训练前后应用针对肿瘤和病灶特异的处理方法，从而提升分割性能。从MRI中提取的放射组学特征有助于检测肿瘤亚型，确保训练更加均衡。自定义的病灶级性能指标决定了集成中每个模型的影响，并优化了进一步细化预测的后处理步骤，使工作流程能够针对每个病例进行定制。在BraTS测试集上，我们的流程在多个挑战中实现了与排名靠前算法相当的性能。这些结果证实了自定义的病灶感知处理和模型选择能够产生稳健的分割结果，而无需将方法绑定到特定的网络架构。我们的方法在临床实践中具有定量肿瘤测量的潜力，支持诊断和预后。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the challenge of robust and generalizable brain tumor segmentation across diverse MRI data due to varying tumor types. The proposed method employs a flexible, modular pipeline that integrates radiomic-guided subtyping and lesion-wise model ensembles to enhance segmentation accuracy. By leveraging radiomic features for tumor subtyping and using custom lesion-level metrics to optimize model contributions and post-processing, the pipeline adapts to different tumor characteristics. Experimental results on BraTS 2025 testing sets show that the method achieves performance comparable to top-ranked algorithms in multiple segmentation challenges.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决由于肿瘤类型差异而导致的多参数磁共振成像（MRI）中脑肿瘤分割的鲁棒性和泛化性问题。本文提出了一种灵活、模块化且可适应的分割流程，结合了最先进的模型、基于放射组学的亚型识别以及基于病灶的模型集成。该方法利用MRI中的放射组学特征来检测肿瘤亚型，并通过自定义病灶级性能指标优化模型贡献和后处理步骤。在BraTS 2025测试集上，该流程实现了与多个挑战中排名靠前的算法相当的性能。这些结果证实了基于病灶的处理和模型选择在提升分割准确性方面的有效性，同时不将方法限制在特定的网络架构上。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
