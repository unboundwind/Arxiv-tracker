<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-01 03:52</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260101_0352</div>
    <div class="row"><div class="card">
<div class="title">Stream-DiffVSR: Low-Latency Streamable Video Super-Resolution via Auto-Regressive Diffusion</div>
<div class="meta-line">Authors: Hau-Shiang Shiu, Chin-Yang Lin, Zhixiang Wang, Chi-Wei Hsiao, Po-Fan Yu, Yu-Chih Chen, Yu-Lun Liu</div>
<div class="meta-line">First: 2025-12-29T18:59:57+00:00 · Latest: 2025-12-29T18:59:57+00:00</div>
<div class="meta-line">Comments: Project page: https://jamichss.github.io/stream-diffvsr-project-page/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23709v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23709v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://jamichss.github.io/stream-diffvsr-project-page/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion-based video super-resolution (VSR) methods achieve strong perceptual quality but remain impractical for latency-sensitive settings due to reliance on future frames and expensive multi-step denoising. We propose Stream-DiffVSR, a causally conditioned diffusion framework for efficient online VSR. Operating strictly on past frames, it combines a four-step distilled denoiser for fast inference, an Auto-regressive Temporal Guidance (ARTG) module that injects motion-aligned cues during latent denoising, and a lightweight temporal-aware decoder with a Temporal Processor Module (TPM) that enhances detail and temporal coherence. Stream-DiffVSR processes 720p frames in 0.328 seconds on an RTX4090 GPU and significantly outperforms prior diffusion-based methods. Compared with the online SOTA TMP, it boosts perceptual quality (LPIPS +0.095) while reducing latency by over 130x. Stream-DiffVSR achieves the lowest latency reported for diffusion-based VSR, reducing initial delay from over 4600 seconds to 0.328 seconds, thereby making it the first diffusion VSR method suitable for low-latency online deployment. Project page: https://jamichss.github.io/stream-diffvsr-project-page/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Stream-DiffVSR：通过自回归扩散实现低延迟可流式视频超分辨率</div>
<div class="mono" style="margin-top:8px">基于扩散的视频超分辨率（VSR）方法虽然能够实现强大的感知质量，但由于依赖未来帧和昂贵的多步去噪过程，仍难以在延迟敏感的场景中实际应用。我们提出Stream-DiffVSR，这是一种因果条件扩散框架，用于高效的在线VSR。该框架仅基于过去帧进行操作，结合了四步蒸馏去噪器以实现快速推理，一个在潜在空间去噪过程中注入运动对齐提示的自回归时间引导（ARTG）模块，以及一个轻量级的时间感知解码器，配备时间处理器模块（TPM）以增强细节和时间一致性。在RTX4090 GPU上，Stream-DiffVSR处理720p帧仅需0.328秒，并显著优于以往的基于扩散的方法。与在线最先进的TMP相比，它提升了感知质量（LPIPS +0.095），同时将延迟降低了超过130倍。Stream-DiffVSR实现了目前基于扩散的VSR方法中最低的延迟，将初始延迟从超过4600秒降低至0.328秒，从而成为首个适用于低延迟在线部署的扩散VSR方法。项目页面：https://jamichss.github.io/stream-diffvsr-project-page/</div>
</details>
</div>
<div class="card">
<div class="title">Training AI Co-Scientists Using Rubric Rewards</div>
<div class="meta-line">Authors: Shashwat Goel, Rishi Hazra, Dulhan Jayalath, Timon Willi, Parag Jain, William F. Shen, Ilias Leontiadis, Francesco Barbieri, Yoram Bachrach, Jonas Geiping, Chenxi Whitehouse</div>
<div class="meta-line">First: 2025-12-29T18:59:33+00:00 · Latest: 2025-12-29T18:59:33+00:00</div>
<div class="meta-line">Comments: 11 pages in the main paper, total 119 including sample outputs in the Appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23707v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23707v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI co-scientists are emerging as a tool to assist human researchers in achieving their research goals. A crucial feature of these AI co-scientists is the ability to generate a research plan given a set of aims and constraints. The plan may be used by researchers for brainstorming, or may even be implemented after further refinement. However, language models currently struggle to generate research plans that follow all constraints and implicit requirements. In this work, we study how to leverage the vast corpus of existing research papers to train language models that generate better research plans. We build a scalable, diverse training corpus by automatically extracting research goals and goal-specific grading rubrics from papers across several domains. We then train models for research plan generation via reinforcement learning with self-grading. A frozen copy of the initial policy acts as the grader during training, with the rubrics creating a generator-verifier gap that enables improvements without external human supervision. To validate this approach, we conduct a study with human experts for machine learning research goals, spanning 225 hours. The experts prefer plans generated by our finetuned Qwen3-30B-A3B model over the initial model for 70% of research goals, and approve 84% of the automatically extracted goal-specific grading rubrics. To assess generality, we also extend our approach to research goals from medical papers, and new arXiv preprints, evaluating with a jury of frontier models. Our finetuning yields 12-22% relative improvements and significant cross-domain generalization, proving effective even in problem settings like medical research where execution feedback is infeasible. Together, these findings demonstrate the potential of a scalable, automated training recipe as a step towards improving general AI co-scientists.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用评分标准奖励训练AI合作者</div>
<div class="mono" style="margin-top:8px">AI合作者正逐渐成为帮助人类研究人员实现研究目标的工具。这些AI合作者的一个关键特征是能够根据给定的研究目标和约束生成研究计划。该计划可用于研究人员的头脑风暴，甚至在进一步优化后实施。然而，当前的语言模型在生成符合所有约束和隐含要求的研究计划方面仍存在困难。在本研究中，我们探讨如何利用大量现有的研究论文来训练生成更优研究计划的语言模型。我们通过自动提取多个领域论文中的研究目标和目标特定的评分标准，构建了一个可扩展且多样化的训练语料库。随后，我们通过带有自我评分的强化学习训练研究计划生成模型。在训练过程中，初始策略的冻结副本作为评分者，评分标准创造了生成者与验证者之间的差距，使得模型能够在没有外部人类监督的情况下进行改进。为了验证这一方法，我们进行了225小时的人类专家研究，结果显示专家在70%的研究目标上更倾向于使用我们微调后的Qwen3-30B-A3B模型生成的计划，并批准了84%的自动提取的目标特定评分标准。为了评估方法的通用性，我们还将该方法扩展到医学论文和新的arXiv预印本中的研究目标，并通过前沿模型组成的评审团进行评估。我们的微调方法在不同领域中实现了12-22%的相对提升，并展示了显著的跨领域泛化能力，即使在执行反馈不可行的问题设置（如医学研究）中也证明了其有效性。这些发现共同表明，一种可扩展的自动化训练方法在提升通用AI合作者方面具有潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Knows Transparency: Repurposing Video Diffusion for Transparent Object Depth and Normal Estimation</div>
<div class="meta-line">Authors: Shaocong Xu, Songlin Wei, Qizhe Wei, Zheng Geng, Hong Li, Licheng Shen, Qianpu Sun, Shu Han, Bin Ma, Bohan Li, Chongjie Ye, Yuhang Zheng, Nan Wang, Saining Zhang, Hao Zhao</div>
<div class="meta-line">First: 2025-12-29T18:59:24+00:00 · Latest: 2025-12-29T18:59:24+00:00</div>
<div class="meta-line">Comments: Project Page: https://daniellli.github.io/projects/DKT/; Code: https://github.com/Daniellli/DKT; Dataset: https://huggingface.co/datasets/Daniellesry/TransPhy3D</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23705v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23705v1">PDF</a> · <a href="https://github.com/Daniellli/DKT">Code1</a> · <a href="https://huggingface.co/datasets/Daniellesry/TransPhy3D">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a> · <a href="https://daniellli.github.io/projects/DKT/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates. Our key observation is that modern video diffusion models already synthesize convincing transparent phenomena, suggesting they have internalized the optical rules. We build TransPhy3D, a synthetic video corpus of transparent/reflective scenes: 11k sequences rendered with Blender/Cycles. Scenes are assembled from a curated bank of category-rich static assets and shape-rich procedural assets paired with glass/plastic/metal materials. We render RGB + depth + normals with physically based ray tracing and OptiX denoising. Starting from a large video diffusion model, we learn a video-to-video translator for depth (and normals) via lightweight LoRA adapters. During training we concatenate RGB and (noisy) depth latents in the DiT backbone and co-train on TransPhy3D and existing frame-wise synthetic datasets, yielding temporally consistent predictions for arbitrary-length input videos. The resulting model, DKT, achieves zero-shot SOTA on real and synthetic video benchmarks involving transparency: ClearPose, DREDS (CatKnown/CatNovel), and TransPhy3D-Test. It improves accuracy and temporal consistency over strong image/video baselines, and a normal variant sets the best video normal estimation results on ClearPose. A compact 1.3B version runs at ~0.17 s/frame. Integrated into a grasping stack, DKT&#x27;s depth boosts success rates across translucent, reflective and diffuse surfaces, outperforming prior estimators. Together, these results support a broader claim: &quot;Diffusion knows transparency.&quot; Generative video priors can be repurposed, efficiently and label-free, into robust, temporally coherent perception for challenging real-world manipulation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散已知透明性：将视频扩散模型用于透明物体深度与法线估计</div>
<div class="mono" style="margin-top:8px">透明物体对感知系统来说仍然极具挑战性：折射、反射和透射破坏了立体视觉、飞行时间（ToF）和纯判别单目深度估计的假设，导致出现空洞和时间上不稳定的估计。我们的主要观察是，现代视频扩散模型已经能够合成具有说服力的透明现象，这表明它们已经内化了光学规则。我们构建了TransPhy3D，这是一个包含透明/反射场景的合成视频语料库：11k个序列通过Blender/Cycles渲染。场景由精心挑选的类别丰富的静态资产和形状丰富的程序化资产组成，并与玻璃/塑料/金属材质配对。我们使用基于物理的光线追踪和OptiX降噪技术渲染RGB + 深度 + 法线。从一个大型视频扩散模型出发，我们通过轻量级LoRA适配器学习了一个视频到视频的翻译器，用于深度（和法线）估计。在训练过程中，我们将RGB和（噪声）深度潜在特征连接在一起，并在TransPhy3D和现有的帧级合成数据集上进行联合训练，从而得到任意长度输入视频的时间一致预测。所得到的模型DKT在涉及透明性的现实和合成视频基准测试中实现了零样本SOTA表现，包括ClearPose、DREDS（CatKnown/CatNovel）和TransPhy3D-Test。它在准确性和时间一致性方面优于强大的图像/视频基线模型，其法线变体在ClearPose上实现了最佳的视频法线估计结果。一个紧凑的1.3B版本模型每帧运行时间约为0.17秒。将DKT集成到抓取堆栈中，其深度估计在半透明、反射和漫反射表面上显著提升了成功率，优于以往的估计器。这些结果共同支持一个更广泛的主张：&quot;扩散已知透明性。&quot; 生成视频先验可以被高效且无需标签地重新利用，以实现具有时间一致性的鲁棒感知，用于具有挑战性的现实世界操作。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Transparent objects remain notoriously hard for perception systems: refraction, reflection and transmission break the assumptions behind stereo, ToF and purely discriminative monocular depth, causing holes and temporally unstable estimates.</div>
</details>
</div>
<div class="card">
<div class="title">Memory preservation and cooperative shielding in complex quantum networks</div>
<div class="meta-line">Authors: Simone Ausilio, Fausto Borgonovi, Giuseppe Luca Celardo, Jorge Yago Malo, Maria Luisa Chiofalo</div>
<div class="meta-line">First: 2025-03-07T18:16:08+00:00 · Latest: 2025-12-29T18:59:13+00:00</div>
<div class="meta-line">Comments: 33 pages, 6 figures; peer-reviewed version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.05655v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.05655v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Complex quantum networks are powerful tools in the modeling of transport phenomena, particularly for biological systems, and enable the study of emergent phenomena in many-body quantum systems. High connectivity and long-range interactions induce strong constraints on the system dynamics. Here, we study the transport properties of a quantum network described by the paradigmatic XXZ Hamiltonian, with non-trivial graph connectivity and topology, and long-range interactions. We show how long-range interactions induce memory preserving effects and strongly affect the spreading of the excitations due to cooperative shielding. We describe the memory-preserving effect in all-to-all connected regular networks with distance-independent couplings. Indeed, the memory of the number of initially injected excitations is preserved over long times, encoded in the number of frequencies present in the dynamics. Interestingly, we find that memory-preserving effects occur also in less regular graphs, such as quantum networks with either power-law node connectivity or complex, small-world type, architectures. We discuss the implications of these properties in biology-related problems, such as an application to Weber&#x27;s law in neuroscience, and their implementation in specific quantum technologies via biomimicry. We also show how the presence of long-range interaction strongly affects the dynamics of the excitations in small-world networks and power law all-to-all coupled networks. Indeed, because of cooperative shielding blue, as the connectivity or the range of interaction increases, the initial excitation spreads more slowly among the network and becomes strongly dependent on the initial conditions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>复杂量子网络中的记忆保持与合作屏蔽</div>
<div class="mono" style="margin-top:8px">复杂量子网络是建模传输现象的强大工具，特别是在生物系统中，能够研究多体量子系统中的涌现现象。高连通性和长程相互作用会对系统动力学施加强烈的约束。本文研究了由典型XXZ哈密顿量描述的具有非平凡图连通性和拓扑结构以及长程相互作用的量子网络的传输特性。我们展示了长程相互作用如何引发记忆保持效应，并通过合作屏蔽显著影响激发的传播。我们描述了在全连通正则网络中，距离无关耦合下的记忆保持效应。事实上，初始注入激发的数量记忆可以在长时间内被保持，体现在动力学中出现的频率数量上。有趣的是，我们发现记忆保持效应也出现在结构不那么规则的图中，例如具有幂律节点连通性或复杂、小世界型架构的量子网络。我们讨论了这些特性在生物学相关问题中的应用，如在神经科学中对韦伯定律的应用，以及通过仿生学在特定量子技术中的实现。此外，我们还展示了长程相互作用在小世界网络和幂律全连通耦合网络中对激发动力学的显著影响。事实上，由于合作屏蔽效应，随着连通性或相互作用范围的增加，初始激发在网络中的传播速度会减慢，并且其传播过程变得高度依赖于初始条件。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Complex quantum networks are powerful tools in the modeling of transport phenomena, particularly for biological systems, and enable the study of emergent phenomena in many-body quantum systems.</div>
</details>
</div>
<div class="card">
<div class="title">Eliciting Behaviors in Multi-Turn Conversations</div>
<div class="meta-line">Authors: Jing Huang, Shujian Zhang, Lun Wang, Andrew Hard, Rajiv Mathews, John Lambert</div>
<div class="meta-line">First: 2025-12-29T18:57:10+00:00 · Latest: 2025-12-29T18:57:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23701v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23701v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Identifying specific and often complex behaviors from large language models (LLMs) in conversational settings is crucial for their evaluation. Recent work proposes novel techniques to find natural language prompts that induce specific behaviors from a target model, yet they are mainly studied in single-turn settings. In this work, we study behavior elicitation in the context of multi-turn conversations. We first offer an analytical framework that categorizes existing methods into three families based on their interactions with the target model: those that use only prior knowledge, those that use offline interactions, and those that learn from online interactions. We then introduce a generalized multi-turn formulation of the online method, unifying single-turn and multi-turn elicitation. We evaluate all three families of methods on automatically generating multi-turn test cases. We investigate the efficiency of these approaches by analyzing the trade-off between the query budget, i.e., the number of interactions with the target model, and the success rate, i.e., the discovery rate of behavior-eliciting inputs. We find that online methods can achieve an average success rate of 45/19/77% with just a few thousand queries over three tasks where static methods from existing multi-turn conversation benchmarks find few or even no failure cases. Our work highlights a novel application of behavior elicitation methods in multi-turn conversation evaluation and the need for the community to move towards dynamic benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在多轮对话中引出行为</div>
<div class="mono" style="margin-top:8px">从大型语言模型（LLMs）在对话环境中的行为中识别出具体且往往复杂的行为对于其评估至关重要。近期的研究提出了新颖的技术，以找到能够诱导目标模型产生特定行为的自然语言提示，但这些技术主要是在单轮对话场景中进行研究。在本工作中，我们研究了多轮对话中的行为引出问题。我们首先提供了一个分析框架，根据其与目标模型的交互方式，将现有方法分为三类：仅使用先验知识的方法、使用离线交互的方法以及从在线交互中学习的方法。随后，我们引入了一种在线方法的通用多轮形式化表述，统一了单轮和多轮行为引出。我们在自动构建多轮测试用例上评估了这三类方法。我们通过分析查询预算（即与目标模型的交互次数）和成功率（即行为引出输入的发现率）之间的权衡，研究了这些方法的效率。我们发现，在三个任务中，仅使用几千次查询，基于在线的方法就能实现平均成功率为45/19/77%，而现有多轮对话基准中的静态方法却只能找到很少甚至没有失败案例。我们的工作突显了行为引出方法在多轮对话评估中的新应用，并强调了社区需要转向动态基准的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Reasoning for Diffusion Language Models via Group Diffusion Policy Optimization</div>
<div class="meta-line">Authors: Kevin Rojas, Jiahe Lin, Kashif Rasul, Anderson Schneider, Yuriy Nevmyvaka, Molei Tao, Wei Deng</div>
<div class="meta-line">First: 2025-10-09T17:58:07+00:00 · Latest: 2025-12-29T18:55:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.08554v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.08554v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion language models (DLMs) enable parallel, order-agnostic generation with iterative refinement, offering a flexible alternative to autoregressive large language models (LLMs). However, adapting reinforcement learning (RL) fine-tuning to DLMs remains an open challenge because of the intractable likelihood. Pioneering work such as diffu-GRPO estimated token-level likelihoods via one-step unmasking. While computationally efficient, this approach is severely biased. A more principled foundation lies in sequence-level likelihoods, where the evidence lower bound (ELBO) serves as a surrogate. Yet, despite this clean mathematical connection, ELBO-based methods have seen limited adoption due to the prohibitive cost of likelihood evaluation. In this work, we revisit ELBO estimation and disentangle its sources of variance. This decomposition motivates reducing variance through fast, deterministic integral approximations along a few pivotal dimensions. Building on this insight, we introduce Group Diffusion Policy Optimization (GDPO), a new RL algorithm tailored for DLMs. GDPO leverages simple yet effective Semi-deterministic Monte Carlo schemes to mitigate the variance explosion of ELBO estimators under vanilla double Monte Carlo sampling, yielding a provably lower-variance estimator under tight evaluation budgets. Empirically, GDPO achieves consistent gains over pretrained checkpoints and outperforms diffu-GRPO, one of the state-of-the-art baselines, on the majority of math, reasoning, and coding benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过组扩散策略优化提升扩散语言模型的推理能力</div>
<div class="mono" style="margin-top:8px">扩散语言模型（DLMs）能够通过迭代优化实现并行、顺序无关的生成，为自回归大语言模型（LLMs）提供了一种灵活的替代方案。然而，由于难以计算的似然值，将强化学习（RL）微调适配到DLMs仍是一个开放性挑战。早期工作如diffu-GRPO通过一步解掩码估计了token级别的似然值。虽然计算效率高，但这种方法存在严重的偏差。更合理的理论基础在于序列级别的似然值，其中证据下界（ELBO）作为替代指标。尽管有清晰的数学联系，但基于ELBO的方法由于似然评估成本过高而应用有限。在本工作中，我们重新审视ELBO估计，并分离其方差来源。这种分解促使我们通过在几个关键维度上使用快速、确定性的积分近似方法来降低方差。基于这一见解，我们提出了组扩散策略优化（GDPO），这是一种专为DLMs设计的新RL算法。GDPO利用简单而有效的半确定性蒙特卡洛方案，在常规双蒙特卡洛采样下缓解ELBO估计器的方差爆炸问题，从而在严格的评估预算下获得可证明的更低方差估计器。实验结果表明，GDPO在预训练检查点上实现了持续的性能提升，并在大多数数学、推理和编程基准测试中优于diffu-GRPO这一最先进的基线方法。</div>
</details>
</div>
<div class="card">
<div class="title">Bellman Calibration for V-Learning in Offline Reinforcement Learning</div>
<div class="meta-line">Authors: Lars van der Laan, Nathan Kallus</div>
<div class="meta-line">First: 2025-12-29T18:52:18+00:00 · Latest: 2025-12-29T18:52:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23694v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23694v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Iterated Bellman Calibration, a simple, model-agnostic, post-hoc procedure for calibrating off-policy value predictions in infinite-horizon Markov decision processes. Bellman calibration requires that states with similar predicted long-term returns exhibit one-step returns consistent with the Bellman equation under the target policy. We adapt classical histogram and isotonic calibration to the dynamic, counterfactual setting by repeatedly regressing fitted Bellman targets onto a model&#x27;s predictions, using a doubly robust pseudo-outcome to handle off-policy data. This yields a one-dimensional fitted value iteration scheme that can be applied to any value estimator. Our analysis provides finite-sample guarantees for both calibration and prediction under weak assumptions, and critically, without requiring Bellman completeness or realizability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>贝尔曼校准在离线强化学习中的V学习应用</div>
<div class="mono" style="margin-top:8px">我们引入了迭代贝尔曼校准，这是一种简单、模型无关的后处理方法，用于校准无限时间跨度马尔可夫决策过程中的离策略价值预测。贝尔曼校准要求具有相似长期预测回报的状态，其一步回报应与目标策略下的贝尔曼方程一致。我们通过反复将拟合的贝尔曼目标回归到模型的预测值，将经典直方图和等tonic校准方法适应到动态的反事实设置中，使用双重稳健伪结果来处理离策略数据。这产生了一种一维的拟合值迭代方案，可以应用于任何价值估计器。我们的分析在弱假设下提供了校准和预测的有限样本保证，且关键在于不需要贝尔曼完备性或可实现性。</div>
</details>
</div>
<div class="card">
<div class="title">PROFASR-BENCH: A Benchmark for Context-Conditioned ASR in High-Stakes Professional Speech</div>
<div class="meta-line">Authors: Deepak Babu Piskala</div>
<div class="meta-line">First: 2025-12-29T18:43:23+00:00 · Latest: 2025-12-29T18:43:23+00:00</div>
<div class="meta-line">Comments: Benchmark dataset and evaluation suite. Data and code available at: https://huggingface.co/datasets/prdeepakbabu/ProfASR-Bench https://github.com/prdeepakbabu/ProfASR-Bench</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23686v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23686v1">PDF</a> · <a href="https://huggingface.co/datasets/prdeepakbabu/ProfASR-Bench">Code1</a> · <a href="https://github.com/prdeepakbabu/ProfASR-Bench">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automatic Speech Recognition (ASR) in professional settings faces challenges that existing benchmarks underplay: dense domain terminology, formal register variation, and near-zero tolerance for critical entity errors. We present ProfASR-Bench, a professional-talk evaluation suite for high-stakes applications across finance, medicine, legal, and technology. Each example pairs a natural-language prompt (domain cue and/or speaker profile) with an entity-rich target utterance, enabling controlled measurement of context-conditioned recognition. The corpus supports conventional ASR metrics alongside entity-aware scores and slice-wise reporting by accent and gender. Using representative families Whisper (encoder-decoder ASR) and Qwen-Omni (audio language models) under matched no-context, profile, domain+profile, oracle, and adversarial conditions, we find a consistent pattern: lightweight textual context produces little to no change in average word error rate (WER), even with oracle prompts, and adversarial prompts do not reliably degrade performance. We term this the context-utilization gap (CUG): current systems are nominally promptable yet underuse readily available side information. ProfASR-Bench provides a standardized context ladder, entity- and slice-aware reporting with confidence intervals, and a reproducible testbed for comparing fusion strategies across model families.
  Dataset: https://huggingface.co/datasets/prdeepakbabu/ProfASR-Bench
  Code: https://github.com/prdeepakbabu/ProfASR-Bench</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PROFASR-BENCH：高风险专业语音中上下文条件语音识别的基准</div>
<div class="mono" style="margin-top:8px">自动语音识别（ASR）在专业环境中面临现有基准未充分强调的挑战：密集的领域术语、正式语体的变化以及对关键实体错误近乎零容忍。我们提出了ProfASR-Bench，这是一个用于金融、医疗、法律和技术等高风险应用的专业对话评估套件。每个示例都包含一个自然语言提示（领域提示和/或说话人简介）与一个实体丰富的目标语句，从而实现对上下文条件识别的可控测量。该语料库支持传统的ASR指标，同时提供基于实体和切片的报告，并包含置信区间。通过在无上下文、简介、领域+简介、Oracle和对抗条件下使用代表性的Whisper（编码器-解码器ASR）和Qwen-Omni（音频语言模型）家族，我们发现一个一致的模式：轻量级文本上下文对平均词错误率（WER）几乎没有影响，即使使用Oracle提示，对抗性提示也不可靠地降低性能。我们将这种现象称为上下文利用差距（CUG）：当前系统名义上可提示，但未能充分利用可轻易获取的辅助信息。ProfASR-Bench提供标准化的上下文梯度、基于实体和切片的报告以及置信区间，并为不同模型家族之间的融合策略比较提供可复现的测试平台。</div>
</details>
</div>
<div class="card">
<div class="title">Multilingual Hidden Prompt Injection Attacks on LLM-Based Academic Reviewing</div>
<div class="meta-line">Authors: Panagiotis Theocharopoulos, Ajinkya Kulkarni, Mathew Magimai. -Doss</div>
<div class="meta-line">First: 2025-12-29T18:43:05+00:00 · Latest: 2025-12-29T18:43:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23684v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23684v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly considered for use in high-impact workflows, including academic peer review. However, LLMs are vulnerable to document-level hidden prompt injection attacks. In this work, we construct a dataset of approximately 500 real academic papers accepted to ICML and evaluate the effect of embedding hidden adversarial prompts within these documents. Each paper is injected with semantically equivalent instructions in four different languages and reviewed using an LLM. We find that prompt injection induces substantial changes in review scores and accept/reject decisions for English, Japanese, and Chinese injections, while Arabic injections produce little to no effect. These results highlight the susceptibility of LLM-based reviewing systems to document-level prompt injection and reveal notable differences in vulnerability across languages.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于大语言模型的多语言隐式提示注入攻击在学术评审中的应用</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）正越来越多地被用于高影响的工作流程，包括学术同行评审。然而，LLMs容易受到文档级隐式提示注入攻击。在本研究中，我们构建了一个包含约500篇被ICML接受的学术论文的数据集，并评估在这些文档中嵌入隐式对抗性提示的影响。每篇论文都用四种不同的语言注入语义等价的指令，并由LLM进行评审。我们发现，提示注入对英语、日语和中文的评审得分和接受/拒绝决策产生了显著影响，而阿拉伯语的注入则几乎没有影响。这些结果突显了基于LLM的评审系统对文档级提示注入的易受攻击性，并揭示了不同语言在易受攻击性上的显著差异。</div>
</details>
</div>
<div class="card">
<div class="title">Edge of Stochastic Stability: Revisiting the Edge of Stability for SGD</div>
<div class="meta-line">Authors: Arseniy Andreyev, Pierfrancesco Beneventano</div>
<div class="meta-line">First: 2024-12-29T18:59:01+00:00 · Latest: 2025-12-29T18:39:34+00:00</div>
<div class="meta-line">Comments: 83 pages, 36 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2412.20553v5">Abs</a> · <a href="https://arxiv.org/pdf/2412.20553v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent findings by Cohen et al., 2021, demonstrate that when training neural networks using full-batch gradient descent with a step size of $η$, the largest eigenvalue $λ_{\max}$ of the full-batch Hessian consistently stabilizes around $2/η$. These results have significant implications for convergence and generalization. This, however, is not the case for mini-batch optimization algorithms, limiting the broader applicabilityof the consequences of these findings. We show mini-batch Stochastic Gradient Descent (SGD) trains in a different regime we term Edge of Stochastic Stability (EoSS). In this regime, what stabilizes at $2/η$ is Batch Sharpness: the expected directional curvature of mini-batch Hessians along their corresponding stochastic gradients. As a consequence $λ_{\max}$ -- which is generally smaller than Batch Sharpness -- is suppressed, aligning with the long-standing empirical observation that smaller batches and larger step sizes favor flatter minima. We further discuss implications for mathematical modeling of SGD trajectories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随机稳定性边缘：重新审视SGD的稳定性边缘</div>
<div class="mono" style="margin-top:8px">Cohen等人于2021年的最新研究发现，当使用步长为$η$的全批量梯度下降训练神经网络时，全批量Hessian矩阵的最大特征值$λ_{\max}$会稳定在$2/η$附近。这些结果对收敛性和泛化能力有重要影响。然而，这一现象并不适用于小批量优化算法，限制了这些发现的广泛适用性。我们展示了小批量随机梯度下降（SGD）在我们称之为随机稳定性边缘（EoSS）的不同运行机制下进行训练。在该机制中，稳定在$2/η$的是批量锐度：小批量Hessian矩阵沿其对应随机梯度方向的期望曲率。因此，$λ_{\max}$通常小于批量锐度，从而被抑制，这与长期以来的经验观察一致，即较小的批量和较大的步长更有利于找到平坦的极小值点。我们进一步讨论了这对SGD轨迹数学建模的影响。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent findings by Cohen et al., 2021, demonstrate that when training neural networks using full-batch gradient descent with a step size of $η$, the largest eigenvalue $λ_{\max}$ of the full-batch Hessian consistently stabilizes around $2/η$.</div>
</details>
</div>
<div class="card">
<div class="title">Web World Models</div>
<div class="meta-line">Authors: Jichen Feng, Yifan Zhang, Chenggong Zhang, Yifu Lu, Shilong Liu, Mengdi Wang</div>
<div class="meta-line">First: 2025-12-29T18:31:45+00:00 · Latest: 2025-12-29T18:31:45+00:00</div>
<div class="meta-line">Comments: Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23676v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23676v1">PDF</a> · <a href="https://github.com/Princeton-AI2-Lab/Web-World-Models">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language agents increasingly require persistent worlds in which they can act, remember, and learn. Existing approaches sit at two extremes: conventional web frameworks provide reliable but fixed contexts backed by databases, while fully generative world models aim for unlimited environments at the expense of controllability and practical engineering. In this work, we introduce the Web World Model (WWM), a middle ground where world state and ``physics&#x27;&#x27; are implemented in ordinary web code to ensure logical consistency, while large language models generate context, narratives, and high-level decisions on top of this structured latent state. We build a suite of WWMs on a realistic web stack, including an infinite travel atlas grounded in real geography, fictional galaxy explorers, web-scale encyclopedic and narrative worlds, and simulation- and game-like environments. Across these systems, we identify practical design principles for WWMs: separating code-defined rules from model-driven imagination, representing latent state as typed web interfaces, and utilizing deterministic generation to achieve unlimited but structured exploration. Our results suggest that web stacks themselves can serve as a scalable substrate for world models, enabling controllable yet open-ended environments. Project Page: https://github.com/Princeton-AI2-Lab/Web-World-Models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>网络世界模型</div>
<div class="mono" style="margin-top:8px">语言代理越来越多地需要持久的世界环境，以便在其中行动、记忆和学习。现有方法处于两个极端：传统网络框架提供可靠但固定的数据库支持的上下文，而完全生成式的世界模型则追求无限环境，但牺牲了可控性和实际工程可行性。在本工作中，我们引入了网络世界模型（Web World Model, WWM），它处于中间地带，通过普通网络代码实现世界状态和『物理』规则，以确保逻辑一致性，同时利用大语言模型在结构化的潜在状态之上生成上下文、叙事和高层决策。我们基于一个现实的网络堆栈构建了一系列WWM，包括基于真实地理的无限旅行地图、虚构的银河探险者、网络规模的百科全书式和叙事式世界，以及模拟和游戏类环境。在这些系统中，我们识别出网络世界模型的实用设计原则：将代码定义的规则与模型驱动的想象分离，将潜在状态表示为类型化的网络接口，并利用确定性生成实现无限但结构化的探索。我们的结果表明，网络堆栈本身可以作为世界模型的可扩展基础，从而实现可控但开放的环境。项目页面：https://github.com/Princeton-AI2-Lab/Web-World-Models。</div>
</details>
</div>
<div class="card">
<div class="title">Investigation of the Impact of Synthetic Training Data in the Industrial Application of Terminal Strip Object Detection</div>
<div class="meta-line">Authors: Nico Baumgart, Markus Lange-Hegermann, Mike Mücke</div>
<div class="meta-line">First: 2024-03-06T18:33:27+00:00 · Latest: 2025-12-29T18:31:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2403.04809v2">Abs</a> · <a href="https://arxiv.org/pdf/2403.04809v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In industrial manufacturing, deploying deep learning models for visual inspection is mostly hindered by the high and often intractable cost of collecting and annotating large-scale training datasets. While image synthesis from 3D CAD models is a common solution, the individual techniques of domain and rendering randomization to create rich synthetic training datasets have been well studied mainly in simple domains. Hence, their effectiveness on complex industrial tasks with densely arranged and similar objects remains unclear. In this paper, we investigate the sim-to-real generalization performance of standard object detectors on the complex industrial application of terminal strip object detection, carefully combining randomization and domain knowledge. We describe step-by-step the creation of our image synthesis pipeline that achieves high realism with minimal implementation effort and explain how this approach could be transferred to other industrial settings. Moreover, we created a dataset comprising 30.000 synthetic images and 300 manually annotated real images of terminal strips, which is publicly available for reference and future research. To provide a baseline as a lower bound of the expectable performance in these challenging industrial parts detection tasks, we show the sim-to-real generalization performance of standard object detectors on our dataset based on a fully synthetic training. While all considered models behave similarly, the transformer-based DINO model achieves the best score with 98.40 % mean average precision on the real test set, demonstrating that our pipeline enables high quality detections in complex industrial environments from existing CAD data and with a manageable image synthesis effort.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>合成训练数据在端子排目标检测工业应用中的影响研究</div>
<div class="mono" style="margin-top:8px">在工业制造中，部署深度学习模型进行视觉检测主要受到大规模训练数据集收集和标注的高成本和难以处理的限制。虽然从3D CAD模型生成图像合成是一种常见解决方案，但针对创建丰富合成训练数据集的领域和渲染随机化技术，其在简单领域中已被广泛研究。然而，这些技术在密集排列且相似目标的复杂工业任务中的有效性仍不清楚。本文研究了标准目标检测器在端子排目标检测复杂工业应用中的模拟到现实（sim-to-real）泛化性能，仔细结合了随机化与领域知识。我们逐步描述了所创建的图像合成流水线，该流水线在最小化实现工作量的前提下实现了高真实感，并解释了这种方法如何应用于其他工业场景。此外，我们创建了一个包含30,000张合成图像和300张人工标注真实图像的端子排数据集，该数据集可供参考和未来研究使用。为了提供一个性能下限的基准，我们展示了基于完全合成训练的模型在该数据集上的模拟到现实泛化性能。尽管所有考虑的模型表现相似，但基于Transformer的DINO模型在真实测试集上取得了最佳成绩，平均精度达到98.40%，这表明我们的流水线能够利用现有的CAD数据，在复杂工业环境中实现高质量的目标检测，并且图像合成工作量可控。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In industrial manufacturing, deploying deep learning models for visual inspection is mostly hindered by the high and often intractable cost of collecting and annotating large-scale training datasets.</div>
</details>
</div>
<div class="card">
<div class="title">End-to-End Test-Time Training for Long Context</div>
<div class="meta-line">Authors: Arnuv Tandon, Karan Dalal, Xinhao Li, Daniel Koceja, Marcel Rød, Sam Buchanan, Xiaolong Wang, Jure Leskovec, Sanmi Koyejo, Tatsunori Hashimoto, Carlos Guestrin, Jed McCaleb, Yejin Choi, Yu Sun</div>
<div class="meta-line">First: 2025-12-29T18:30:14+00:00 · Latest: 2025-12-29T18:30:14+00:00</div>
<div class="meta-line">Comments: Code: https://github.com/test-time-training/e2e</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23675v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23675v1">PDF</a> · <a href="https://github.com/test-time-training/e2e">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We formulate long-context language modeling as a problem in continual learning rather than architecture design. Under this formulation, we only use a standard architecture -- a Transformer with sliding-window attention. However, our model continues learning at test time via next-token prediction on the given context, compressing the context it reads into its weights. In addition, we improve the model&#x27;s initialization for learning at test time via meta-learning at training time. Overall, our method, a form of Test-Time Training (TTT), is End-to-End (E2E) both at test time (via next-token prediction) and training time (via meta-learning), in contrast to previous forms. We conduct extensive experiments with a focus on scaling properties. In particular, for 3B models trained with 164B tokens, our method (TTT-E2E) scales with context length in the same way as Transformer with full attention, while others, such as Mamba 2 and Gated DeltaNet, do not. However, similar to RNNs, TTT-E2E has constant inference latency regardless of context length, making it 2.7 times faster than full attention for 128K context. Our code is publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向长上下文的端到端测试时训练</div>
<div class="mono" style="margin-top:8px">我们将长上下文语言建模问题视为持续学习问题，而非架构设计问题。在此框架下，我们仅使用标准架构——带滑动窗口注意力的Transformer。然而，我们的模型通过在给定上下文中进行下一个token预测，在测试时持续学习，将读取的上下文压缩到模型权重中。此外，我们通过训练时的元学习改进模型的初始化，以支持测试时的学习。总体而言，我们的方法是一种测试时训练（TTT）形式，既在测试时（通过下一个token预测）又在训练时（通过元学习）实现端到端（E2E），与以往形式不同。我们进行了大量实验，重点关注扩展性。特别是，对于使用1640亿token训练的30亿参数模型，我们的方法（TTT-E2E）在上下文长度上的扩展性与全注意力Transformer相同，而其他方法如Mamba 2和Gated DeltaNet则不具备此特性。然而，与RNN类似，TTT-E2E的推理延迟与上下文长度无关，使其在128K上下文长度下比全注意力模型快2.7倍。我们的代码已公开。</div>
</details>
</div>
<div class="card">
<div class="title">Calibrated Multi-Level Quantile Forecasting</div>
<div class="meta-line">Authors: Tiffany Ding, Isaac Gibbs, Ryan J. Tibshirani</div>
<div class="meta-line">First: 2025-12-29T18:25:36+00:00 · Latest: 2025-12-29T18:25:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23671v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23671v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present an online method for guaranteeing calibration of quantile forecasts at multiple quantile levels simultaneously. A sequence of $α$-level quantile forecasts is calibrated if the forecasts are larger than the target value at an $α$-fraction of time steps. We introduce a lightweight method called Multi-Level Quantile Tracker (MultiQT) that wraps around any existing point or quantile forecaster to produce corrected forecasts guaranteed to achieve calibration, even against adversarial distribution shifts, while ensuring that the forecasts are ordered -- e.g., the 0.5-level quantile forecast is never larger than the 0.6-level forecast. Furthermore, the method comes with a no-regret guarantee that implies it will not worsen the performance of an existing forecaster, asymptotically, with respect to the quantile loss. In experiments, we find that MultiQT significantly improves the calibration of real forecasters in epidemic and energy forecasting problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>校准的多层级分位数预测</div>
<div class="mono" style="margin-top:8px">我们提出了一种在线方法，用于同时保证多个分位数层级的预测校准。一个α层级的分位数预测序列被校准，当且仅当预测值在α比例的时间步中大于目标值。我们引入了一种轻量级方法，称为多层级分位数追踪器（MultiQT），它可以包装任何现有的点预测或分位数预测模型，生成经过校正的预测结果，确保即使在对抗性分布变化的情况下也能实现校准，同时保持预测的有序性——例如，0.5层级的分位数预测值永远不会大于0.6层级的预测值。此外，该方法还提供了无遗憾保证，这意味着它在渐近意义上不会使现有预测模型的分位数损失性能变差。在实验中，我们发现MultiQT在流行病和能源预测问题中显著提升了实际预测模型的校准效果。</div>
</details>
</div>
<div class="card">
<div class="title">Random Controlled Differential Equations</div>
<div class="meta-line">Authors: Francesco Piatti, Thomas Cass, William F. Turner</div>
<div class="meta-line">First: 2025-12-29T18:25:10+00:00 · Latest: 2025-12-29T18:25:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23670v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23670v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce a training-efficient framework for time-series learning that combines random features with controlled differential equations (CDEs). In this approach, large randomly parameterized CDEs act as continuous-time reservoirs, mapping input paths to rich representations. Only a linear readout layer is trained, resulting in fast, scalable models with strong inductive bias. Building on this foundation, we propose two variants: (i) Random Fourier CDEs (RF-CDEs): these lift the input signal using random Fourier features prior to the dynamics, providing a kernel-free approximation of RBF-enhanced sequence models; (ii) Random Rough DEs (R-RDEs): these operate directly on rough-path inputs via a log-ODE discretization, using log-signatures to capture higher-order temporal interactions while remaining stable and efficient. We prove that in the infinite-width limit, these model induces the RBF-lifted signature kernel and the rough signature kernel, respectively, offering a unified perspective on random-feature reservoirs, continuous-time deep architectures, and path-signature theory.
  We evaluate both models across a range of time-series benchmarks, demonstrating competitive or state-of-the-art performance. These methods provide a practical alternative to explicit signature computations, retaining their inductive bias while benefiting from the efficiency of random features.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随机控制微分方程</div>
<div class="mono" style="margin-top:8px">我们提出了一种训练高效的框架，用于时间序列学习，该框架结合了随机特征与控制微分方程（CDEs）。在此方法中，大规模随机参数化的CDEs作为连续时间的储层，将输入路径映射为丰富的表示。仅需训练一个线性读出层，从而得到快速、可扩展的模型，并具有强大的归纳偏倚。在此基础上，我们提出了两种变体：(i) 随机傅里叶CDEs（RF-CDEs）：这些方法在动态之前使用随机傅里叶特征提升输入信号，提供了一种无需核函数的RBF增强序列模型的近似；(ii) 随机粗糙微分方程（R-RDEs）：这些方法通过log-ODE离散化直接在粗糙路径输入上操作，利用log签名捕捉高阶时间交互，同时保持稳定和高效。我们证明，在无限宽度极限下，这些模型分别诱导出RBF提升的签名核和粗糙签名核，从而为随机特征储层、连续时间深度架构和路径签名理论提供了一个统一的视角。我们评估了这两种模型在多个时间序列基准数据集上的表现，展示了其具有竞争力或最先进的性能。这些方法为显式签名计算提供了一个实用的替代方案，在保留其归纳偏倚的同时，受益于随机特征的高效性。</div>
</details>
</div>
<div class="card">
<div class="title">IDT: A Physically Grounded Transformer for Feed-Forward Multi-View Intrinsic Decomposition</div>
<div class="meta-line">Authors: Kang Du, Yirui Guan, Zeyu Wang</div>
<div class="meta-line">First: 2025-12-29T18:24:46+00:00 · Latest: 2025-12-29T18:24:46+00:00</div>
<div class="meta-line">Comments: 10 pages 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23667v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23667v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Intrinsic image decomposition is fundamental for visual understanding, as RGB images entangle material properties, illumination, and view-dependent effects. Recent diffusion-based methods have achieved strong results for single-view intrinsic decomposition; however, extending these approaches to multi-view settings remains challenging, often leading to severe view inconsistency. We propose \textbf{Intrinsic Decomposition Transformer (IDT)}, a feed-forward framework for multi-view intrinsic image decomposition. By leveraging transformer-based attention to jointly reason over multiple input images, IDT produces view-consistent intrinsic factors in a single forward pass, without iterative generative sampling. IDT adopts a physically grounded image formation model that explicitly decomposes images into diffuse reflectance, diffuse shading, and specular shading. This structured factorization separates Lambertian and non-Lambertian light transport, enabling interpretable and controllable decomposition of material and illumination effects across views. Experiments on both synthetic and real-world datasets demonstrate that IDT achieves cleaner diffuse reflectance, more coherent diffuse shading, and better-isolated specular components, while substantially improving multi-view consistency compared to prior intrinsic decomposition methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>IDT：一种用于前馈多视角内在图像分解的物理基础Transformer</div>
<div class="mono" style="margin-top:8px">内在图像分解是视觉理解的基础，因为RGB图像将材质属性、光照和视角相关效应纠缠在一起。最近基于扩散的方法在单视角内在图像分解中取得了显著成果；然而，将这些方法扩展到多视角场景仍具有挑战性，常常导致严重的视角不一致。我们提出\textbf{内在图像分解Transformer（IDT）}，这是一种用于多视角内在图像分解的前馈框架。通过利用基于Transformer的注意力机制，对多个输入图像进行联合推理，IDT可以在单次前向传播中生成视角一致的内在因子，而无需迭代生成采样。IDT采用了一种物理基础的图像形成模型，明确地将图像分解为漫反射反射率、漫反射光照和镜面光照。这种结构化的分解能够分离朗伯反射和非朗伯反射的光照传输，从而实现跨视角的可解释且可控的材质和光照效应分解。在合成和真实世界数据集上的实验表明，与之前的方法相比，IDT在多视角一致性方面有显著提升，同时实现了更干净的漫反射反射率、更连贯的漫反射光照以及更清晰的镜面成分。</div>
</details>
</div>
<div class="card">
<div class="title">Preconditioning for Accelerated Gradient Descent Optimization and Regularization</div>
<div class="meta-line">Authors: Qiang Ye</div>
<div class="meta-line">First: 2024-09-30T20:58:39+00:00 · Latest: 2025-12-29T18:23:17+00:00</div>
<div class="meta-line">Comments: 21 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.00232v2">Abs</a> · <a href="https://arxiv.org/pdf/2410.00232v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accelerated training algorithms, such as adaptive learning rates (or preconditioning) and various normalization methods, are widely used but not fully understood. When regularization is introduced, standard optimizers like adaptive learning rates may not perform effectively. This raises the need for alternative regularization approaches such as AdamW and the question of how to properly combine regularization with preconditioning. In this paper, we address these challenges using the theory of preconditioning as follows: (1) We explain how AdaGrad, RMSProp, and Adam accelerates training through improving Hessian conditioning; (2) We explore the interaction between $L_2$-regularization and preconditioning, demonstrating that AdamW amounts to selecting the underlying intrinsic parameters for regularization, and we derive a generalization for the $L_1$-regularization; and (3) We demonstrate how various normalization methods such as input data normalization, batch normalization, and layer normalization accelerate training by improving Hessian conditioning. Our analysis offers a unified mathematical framework for understanding various acceleration techniques or deriving appropriate regularization schemes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于加速梯度下降优化和正则化的预处理方法</div>
<div class="mono" style="margin-top:8px">加速训练算法，如自适应学习率（或预处理）和各种归一化方法，被广泛使用但尚未被完全理解。当引入正则化时，标准优化器如自适应学习率可能无法有效工作。这引发了对替代正则化方法如AdamW的需求，以及如何正确地将正则化与预处理结合的问题。本文通过预处理理论解决这些挑战：(1) 我们解释了AdaGrad、RMSProp和Adam如何通过改善Hessian条件来加速训练；(2) 我们探讨了$L_2$-正则化与预处理之间的相互作用，证明AdamW相当于为正则化选择底层内在参数，并推导出$L_1$-正则化的一般形式；(3) 我们展示了诸如输入数据归一化、批量归一化和层归一化等不同归一化方法如何通过改善Hessian条件来加速训练。我们的分析提供了一个统一的数学框架，用于理解各种加速技术或推导适当的正则化方案。</div>
</details>
</div>
<div class="card">
<div class="title">Dynamical incompatibilities in paced finger tapping experiments</div>
<div class="meta-line">Authors: Ariel D. Silva, Claudia R. González, Rodrigo Laje</div>
<div class="meta-line">First: 2025-12-29T18:14:04+00:00 · Latest: 2025-12-29T18:14:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23661v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23661v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The behavioral description of the sensorimotor synchronization phenomenon in humans is exhaustive, mostly by using variations of the traditional paced finger-tapping task. This task helps unveil the inner workings of the error-correction mechanism responsible for the resynchronization after a perturbation to the period of the stimuli sequence. Yet, fundamental contradictions still exist among different works in the literature. One of such contradictions only emerges after comparing the two most-common period perturbation types: step changes and phase shifts. The stimulus sequence is exactly the same in both perturbation types up to and including the (unexpected) perturbed stimulus. Why then would the timing of the next response be different between perturbation types, as observed? The explanation lies in the buildup of different temporal contexts during the experiments that recalibrate the error-correction mechanism. Here we show, both experimentally and theoretically, that responses to different perturbation types are dynamically incompatible when they occur in separate experiments. That is, they can&#x27;t be represented by the same underlying dynamical system, thus explaining many contradictory results and the difficulty in reproducing both types of perturbations with a single mathematical model. On the other hand, if both perturbation types are presented at random during the same experiment then the responses are compatible with each other and can be construed as produced by a unique underlying mechanism. We conclude that a single underlying dynamical system can represent the response to all perturbation types, signs, and sizes, which is nevertheless recalibrated by temporal context. Our results offer a ground for performing better comparisons in paced finger tapping and extend the usable range of data beyond the perturbed stimulus and into the information-rich resynchronization phase.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>节拍手指敲击实验中的动力学不兼容性</div>
<div class="mono" style="margin-top:8px">人类的传感器运动同步现象的行为描述主要通过传统节拍手指敲击任务的变体来完成。该任务有助于揭示在刺激序列周期受到干扰后重新同步的误差校正机制的内部运作。然而，文献中不同研究之间仍存在根本性的矛盾。其中一种矛盾仅在比较两种最常见的周期干扰类型（阶跃变化和相位偏移）时才会显现。在这两种干扰类型中，刺激序列在包括（意外的）干扰刺激之前是完全相同的。那么为何观察到的下一次反应时间在两种干扰类型之间存在差异？其解释在于实验过程中构建的不同时间背景会重新校准误差校正机制。我们通过实验和理论分析表明，当两种干扰类型在不同的实验中出现时，其反应在动力学上是不兼容的。也就是说，它们不能由同一个底层动力学系统来表示，从而解释了许多矛盾的结果以及用单一数学模型难以同时再现两种干扰类型的困难。另一方面，如果在同一次实验中随机呈现这两种干扰类型，其反应则彼此兼容，可以视为由一个统一的底层机制产生。我们得出结论：一个统一的动力学系统可以表示所有干扰类型、方向和大小的反应，但该系统会受到时间背景的重新校准。我们的研究结果为节拍手指敲击实验中的更精确比较提供了基础，并将数据的可用范围扩展到干扰刺激之后的信息丰富的重新同步阶段。</div>
</details>
</div>
<div class="card">
<div class="title">Less is more: Probabilistic reduction is best explained by small-scale predictability measures</div>
<div class="meta-line">Authors: Cassandra L. Jacobs, Andrés Buxó-Lugo, Anna K. Taylor, Marie Leopold-Hooke</div>
<div class="meta-line">First: 2025-12-29T18:12:37+00:00 · Latest: 2025-12-29T18:12:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23659v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23659v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The primary research questions of this paper center on defining the amount of context that is necessary and/or appropriate when investigating the relationship between language model probabilities and cognitive phenomena. We investigate whether whole utterances are necessary to observe probabilistic reduction and demonstrate that n-gram representations suffice as cognitive units of planning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>少即是多：概率缩减最好通过小规模可预测性度量来解释</div>
<div class="mono" style="margin-top:8px">本文的主要研究问题集中在定义在研究语言模型概率与认知现象之间关系时所需和/或适当的情境量。我们探讨是否需要整个句子来观察概率缩减现象，并证明n-gram表示足以作为规划的认知单位。</div>
</details>
</div>
<div class="card">
<div class="title">Application-Driven Innovation in Machine Learning</div>
<div class="meta-line">Authors: David Rolnick, Alan Aspuru-Guzik, Sara Beery, Bistra Dilkina, Priya L. Donti, Marzyeh Ghassemi, Hannah Kerner, Claire Monteleoni, Esther Rolf, Milind Tambe, Adam White</div>
<div class="meta-line">Venue: ICML 2024</div>
<div class="meta-line">First: 2024-03-26T04:59:27+00:00 · Latest: 2025-12-29T18:09:01+00:00</div>
<div class="meta-line">Comments: 12 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2403.17381v2">Abs</a> · <a href="https://arxiv.org/pdf/2403.17381v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this position paper, we argue that application-driven research has been systemically under-valued in the machine learning community. As applications of machine learning proliferate, innovative algorithms inspired by specific real-world challenges have become increasingly important. Such work offers the potential for significant impact not merely in domains of application but also in machine learning itself. In this paper, we describe the paradigm of application-driven research in machine learning, contrasting it with the more standard paradigm of methods-driven research. We illustrate the benefits of application-driven machine learning and how this approach can productively synergize with methods-driven work. Despite these benefits, we find that reviewing, hiring, and teaching practices in machine learning often hold back application-driven innovation. We outline how these processes may be improved.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>以应用为导向的机器学习创新</div>
<div class="mono" style="margin-top:8px">本文认为，应用驱动的研究在机器学习领域长期被系统性低估。随着机器学习应用的不断扩展，由具体现实挑战启发的创新算法变得越来越重要。此类研究不仅在应用领域可能产生重大影响，也对机器学习本身具有重要意义。本文描述了机器学习中应用驱动研究的范式，并将其与更传统的以方法为导向的研究范式进行对比。我们展示了应用驱动机器学习的优势，以及这种方法如何能与方法驱动研究产生建设性的协同作用。尽管有这些优势，我们发现机器学习领域的评审、招聘和教学实践往往阻碍了应用驱动的创新。我们提出了如何改进这些流程的建议。</div>
</details>
</div>
<div class="card">
<div class="title">Victor Calibration (VC): Multi-Pass Confidence Calibration and CP4.3 Governance Stress Test under Round-Table Orchestration</div>
<div class="meta-line">Authors: Victor Stasiuc</div>
<div class="meta-line">First: 2025-12-18T04:09:22+00:00 · Latest: 2025-12-29T18:07:41+00:00</div>
<div class="meta-line">Comments: 7 pages, 1 figure, 4 tables. Exploratory case study</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.17956v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.17956v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Safety alignment can make frontier LMs overly conservative, degrading collaboration via hedging or false refusals. We present a lightweight toolkit with three parts: (1) Victor Calibration (VC), a multi-pass protocol that elicits a scalar confidence proxy T (T0&lt;T1&lt;T2) through iterative evidence re-evaluation; (2) FD-Lite, a behavior-only phenomenology audit with a fixed anchor phrase and a meta-prefix trap to avoid anthropomorphic claims; and (3) CP4.3, a governance stress test for rank invariance and allocation monotonicity (M6). Across Claude 4.5 models (Haiku, Sonnet no-thinking, Sonnet thinking) and Opus, we observe monotonic VC trajectories without violating safety invariants, and stable CP4.3 behavior. (&quot;Opus&quot; here refers to a single Claude Opus 4.1 session accessed via a standard UI account, as reported in Table 1.) This work was conducted by a single operator (n=1) and is intended as hypothesis-generating; we explicitly invite replication, critique, and extension by the research community. We include prompt templates and an artifact plan to facilitate independent verification.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Victor 校准（VC）：多轮次置信度校准与 CP4.3 治理压力测试在圆桌协调下的实现</div>
<div class="mono" style="margin-top:8px">安全对齐可能导致前沿语言模型过于保守，从而通过模糊或虚假拒绝影响协作。我们提出了一种轻量级工具包，包含三个部分：(1) Victor 校准（VC），一种多轮次协议，通过迭代证据重新评估来提取标量置信度代理 T（T0 &lt; T1 &lt; T2）；(2) FD-Lite，一种仅基于行为的现象学审计，使用固定锚点短语和元前缀陷阱以避免人类特征的声称；(3) CP4.3，一种用于等级不变性和分配单调性（M6）的治理压力测试。在 Claude 4.5 模型（Haiku、Sonnet 无思考、Sonnet 思考）和 Opus 上，我们观察到在不违反安全不变量的情况下，VC 轨迹具有单调性，并且 CP4.3 行为稳定。（此处的 &quot;Opus&quot; 指的是通过标准 UI 账户访问的单个 Claude Opus 4.1 会话，如表 1 所述。）本研究由单个操作员（n=1）完成，旨在生成假设；我们明确邀请研究界进行复制、批评和扩展。我们提供了提示模板和一个成果计划，以促进独立验证。</div>
</details>
</div>
<div class="card">
<div class="title">RoboMirror: Understand Before You Imitate for Video to Humanoid Locomotion</div>
<div class="meta-line">Authors: Zhe Li, Cheng Chi, Yangyang Wei, Boan Zhu, Tao Huang, Zhenguo Sun, Yibo Peng, Pengwei Wang, Zhongyuan Wang, Fangzhou Liu, Chang Xu, Shanghang Zhang</div>
<div class="meta-line">First: 2025-12-29T17:59:19+00:00 · Latest: 2025-12-29T17:59:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23649v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23649v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans learn locomotion through visual observation, interpreting visual content first before imitating actions. However, state-of-the-art humanoid locomotion systems rely on either curated motion capture trajectories or sparse text commands, leaving a critical gap between visual understanding and control. Text-to-motion methods suffer from semantic sparsity and staged pipeline errors, while video-based approaches only perform mechanical pose mimicry without genuine visual understanding. We propose RoboMirror, the first retargeting-free video-to-locomotion framework embodying &quot;understand before you imitate&quot;. Leveraging VLMs, it distills raw egocentric/third-person videos into visual motion intents, which directly condition a diffusion-based policy to generate physically plausible, semantically aligned locomotion without explicit pose reconstruction or retargeting. Extensive experiments validate the effectiveness of RoboMirror, it enables telepresence via egocentric videos, drastically reduces third-person control latency by 80%, and achieves a 3.7% higher task success rate than baselines. By reframing humanoid control around video understanding, we bridge the visual understanding and action gap.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RoboMirror：先理解再模仿的视频到人形运动框架</div>
<div class="mono" style="margin-top:8px">人类通过视觉观察学习运动，先理解视觉内容再模仿动作。然而，最先进的类人运动系统依赖于人工整理的运动捕捉轨迹或稀疏的文本指令，导致视觉理解与控制之间存在关键差距。文本到运动的方法存在语义稀疏性和分阶段流程错误，而基于视频的方法仅能进行机械姿态模仿，缺乏真正的视觉理解。我们提出RoboMirror，这是首个无需重定向的视频到运动框架，体现了“先理解再模仿”的理念。通过利用视觉语言模型（VLMs），它将原始的第一视角或第三人称视频提炼为视觉运动意图，这些意图直接用于基于扩散模型的策略，生成物理上合理且语义一致的运动，无需显式的姿态重建或重定向。大量实验验证了RoboMirror的有效性，它能够通过第一视角视频实现远程临场感，将第三人称控制延迟降低了80%，并在任务成功率上比基线方法高出3.7%。通过将类人控制重新定义为基于视频理解，我们弥合了视觉理解和动作之间的鸿沟。</div>
</details>
</div>
<div class="card">
<div class="title">Nested Browser-Use Learning for Agentic Information Seeking</div>
<div class="meta-line">Authors: Baixuan Li, Jialong Wu, Wenbiao Yin, Kuan Li, Zhongwang Zhang, Huifeng Yin, Zhengwei Tao, Liwen Zhang, Pengjun Xie, Jingren Zhou, Yong Jiang</div>
<div class="meta-line">First: 2025-12-29T17:59:14+00:00 · Latest: 2025-12-29T17:59:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23647v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23647v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Information-seeking (IS) agents have achieved strong performance across a range of wide and deep search tasks, yet their tool use remains largely restricted to API-level snippet retrieval and URL-based page fetching, limiting access to the richer information available through real browsing. While full browser interaction could unlock deeper capabilities, its fine-grained control and verbose page content returns introduce substantial complexity for ReAct-style function-calling agents. To bridge this gap, we propose Nested Browser-Use Learning (NestBrowse), which introduces a minimal and complete browser-action framework that decouples interaction control from page exploration through a nested structure. This design simplifies agentic reasoning while enabling effective deep-web information acquisition. Empirical results on challenging deep IS benchmarks demonstrate that NestBrowse offers clear benefits in practice. Further in-depth analyses underscore its efficiency and flexibility.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于代理信息搜索的嵌套浏览器使用学习</div>
<div class="mono" style="margin-top:8px">信息搜索（IS）代理在各种广泛和深入的搜索任务中表现出色，但其工具使用仍主要局限于API级别的片段检索和基于URL的页面获取，这限制了其通过真实浏览获取更丰富信息的能力。虽然完整的浏览器交互可以解锁更深层次的能力，但其精细的控制和冗长的页面内容返回给ReAct风格的函数调用代理带来了显著的复杂性。为弥合这一差距，我们提出了嵌套浏览器使用学习（NestBrowse），该方法引入了一个最小且完整的浏览器操作框架，通过嵌套结构将交互控制与页面探索解耦。这种设计简化了代理推理，同时实现了有效的深度网络信息获取。在具有挑战性的深度信息搜索基准测试中的实证结果表明，NestBrowse在实践中具有明显优势。进一步的深入分析突显了其效率和灵活性。</div>
</details>
</div>
<div class="card">
<div class="title">OmniAgent: Audio-Guided Active Perception Agent for Omnimodal Audio-Video Understanding</div>
<div class="meta-line">Authors: Keda Tao, Wenjie Du, Bohan Yu, Weiqiang Wang, Jian Liu, Huan Wang</div>
<div class="meta-line">First: 2025-12-29T17:59:05+00:00 · Latest: 2025-12-29T17:59:05+00:00</div>
<div class="meta-line">Comments: Website:https://kd-tao.github.io/OmniAgent/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23646v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23646v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://kd-tao.github.io/OmniAgent/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Omnimodal large language models have made significant strides in unifying audio and visual modalities; however, they often lack the fine-grained cross-modal understanding and have difficulty with multimodal alignment. To address these limitations, we introduce OmniAgent, a fully audio-guided active perception agent that dynamically orchestrates specialized tools to achieve more fine-grained audio-visual reasoning. Unlike previous works that rely on rigid, static workflows and dense frame-captioning, this paper demonstrates a paradigm shift from passive response generation to active multimodal inquiry. OmniAgent employs dynamic planning to autonomously orchestrate tool invocation on demand, strategically concentrating perceptual attention on task-relevant cues. Central to our approach is a novel coarse-to-fine audio-guided perception paradigm, which leverages audio cues to localize temporal events and guide subsequent reasoning. Extensive empirical evaluations on three audio-video understanding benchmarks demonstrate that OmniAgent achieves state-of-the-art performance, surpassing leading open-source and proprietary models by substantial margins of 10% - 20% accuracy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OmniAgent：面向多模态音频-视频理解的音频引导主动感知代理</div>
<div class="mono" style="margin-top:8px">多模态大语言模型在统一音频和视觉模态方面取得了显著进展；然而，它们通常缺乏细粒度的跨模态理解，并且在多模态对齐方面存在困难。为了解决这些局限性，我们引入了OmniAgent，这是一个完全由音频引导的主动感知代理，能够动态协调专用工具以实现更细粒度的音频-视觉推理。与以往依赖于刚性、静态工作流和密集帧字幕的方法不同，本文展示了一种从被动响应生成向主动多模态查询的范式转变。OmniAgent采用动态规划，按需自主调用工具，战略性地将感知注意力集中在任务相关的线索上。我们的方法核心是一种新颖的由粗到细的音频引导感知范式，该范式利用音频线索定位时间事件，并引导后续推理。</div>
</details>
</div>
<div class="card">
<div class="title">Simultaneous Approximation of the Score Function and Its Derivatives by Deep Neural Networks</div>
<div class="meta-line">Authors: Konstantin Yakovlev, Nikita Puchkin</div>
<div class="meta-line">First: 2025-12-29T17:54:45+00:00 · Latest: 2025-12-29T17:54:45+00:00</div>
<div class="meta-line">Comments: 38 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23643v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23643v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a theory for simultaneous approximation of the score function and its derivatives, enabling the handling of data distributions with low-dimensional structure and unbounded support. Our approximation error bounds match those in the literature while relying on assumptions that relax the usual bounded support requirement. Crucially, our bounds are free from the curse of dimensionality. Moreover, we establish approximation guarantees for derivatives of any prescribed order, extending beyond the commonly considered first-order setting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度神经网络对得分函数及其导数的同时逼近</div>
<div class="mono" style="margin-top:8px">我们提出了一种关于得分函数及其导数的同时逼近理论，使得能够处理具有低维结构且支撑集无界的概率分布。我们的逼近误差界与文献中的结果一致，但依赖于比传统有界支撑集假设更宽松的条件。关键的是，我们的误差界不受维度灾难的影响。此外，我们还建立了对任意指定阶数导数的逼近保证，超越了通常考虑的一阶情形。</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking the Spatio-Temporal Alignment of End-to-End 3D Perception</div>
<div class="meta-line">Authors: Xiaoyu Li, Peidong Li, Xian Wu, Long Shi, Dedong Liu, Yitao Wu, Jiajia Fu, Dixiao Cui, Lijun Zhao, Lining Sun</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-12-29T17:48:56+00:00 · Latest: 2025-12-29T17:48:56+00:00</div>
<div class="meta-line">Comments: Accepted to AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23635v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23635v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information. Existing methods typically rely on the attention mechanism to align objects across frames, simplifying the motion model with a unified explicit physical model (constant velocity, etc.). These approaches prefer semantic features for implicit alignment, challenging the importance of explicit motion modeling in the traditional perception paradigm. However, variations in motion states and object features across categories and frames render this alignment suboptimal. To address this, we propose HAT, a spatio-temporal alignment module that allows each object to adaptively decode the optimal alignment proposal from multiple hypotheses without direct supervision. Specifically, HAT first utilizes multiple explicit motion models to generate spatial anchors and motion-aware feature proposals for historical instances. It then performs multi-hypothesis decoding by incorporating semantic and motion cues embedded in cached object queries, ultimately providing the optimal alignment proposal for the target frame. On nuScenes, HAT consistently improves 3D temporal detectors and trackers across diverse baselines. It achieves state-of-the-art tracking results with 46.0% AMOTA on the test set when paired with the DETR3D detector. In an object-centric E2E AD method, HAT enhances perception accuracy (+1.3% mAP, +3.1% AMOTA) and reduces the collision rate by 32%. When semantics are corrupted (nuScenes-C), the enhancement of motion modeling by HAT enables more robust perception and planning in the E2E AD.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考端到端三维感知的时空对齐</div>
<div class="mono" style="margin-top:8px">时空对齐对于自动驾驶（AD）中端到端（E2E）感知的时间建模至关重要，能够提供有价值的结构和纹理先验信息。现有方法通常依赖注意力机制在帧间对齐物体，并通过统一的显式物理模型（如恒定速度等）简化运动模型。这些方法倾向于使用语义特征进行隐式对齐，挑战了传统感知范式中显式运动建模的重要性。然而，不同类别和帧之间运动状态和物体特征的变化使得这种对齐效果不理想。为了解决这一问题，我们提出HAT，一个时空对齐模块，允许每个物体在无直接监督的情况下，自适应地从多个假设中解码最优的对齐方案。具体而言，HAT首先利用多个显式运动模型生成历史实例的空间锚点和运动感知特征提案。然后，通过结合缓存物体查询中嵌入的语义和运动线索，执行多假设解码，最终为目标帧提供最优的对齐提案。在nuScenes数据集上，HAT在多种基线模型中持续提升三维时间检测器和跟踪器的性能。当与DETR3D检测器结合时，在测试集上实现了46.0%的AMOTA，达到最先进的跟踪结果。在以物体为中心的端到端AD方法中，HAT提升了感知精度（+1.3% mAP，+3.1% AMOTA），并减少了碰撞率32%。当语义信息被破坏（nuScenes-C）时，HAT对运动建模的增强使得端到端AD中的感知和规划更加鲁棒。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Spatio-temporal alignment is crucial for temporal modeling of end-to-end (E2E) perception in autonomous driving (AD), providing valuable structural and textural prior information.</div>
</details>
</div>
<div class="card">
<div class="title">AI tutoring can safely and effectively support students: An exploratory RCT in UK classrooms</div>
<div class="meta-line">Authors: LearnLM Team, Eedi, :, Albert Wang, Aliya Rysbek, Andrea Huber, Anjali Nambiar, Anna Kenolty, Ben Caulfield, Beth Lilley-Draper, Bibi Groot, Brian Veprek, Chelsea Burdett, Claire Willis, Craig Barton, Digory Smith, George Mu, Harriet Walters, Irina Jurenka, Iris Hulls, James Stalley-Moores, Jonathan Caton, Julia Wilkowski, Kaiz Alarakyia, Kevin R. McKee, Liam McCafferty, Lucy Dalton, Markus Kunesch, Pauline Malubay, Rachel Kidson, Rich Wells, Sam Wheeler, Sara Wiltberger, Shakir Mohamed, Simon Woodhead, Vasco Brazão</div>
<div class="meta-line">First: 2025-12-29T17:44:03+00:00 · Latest: 2025-12-29T17:44:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23633v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23633v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">One-to-one tutoring is widely considered the gold standard for personalized education, yet it remains prohibitively expensive to scale. To evaluate whether generative AI might help expand access to this resource, we conducted an exploratory randomized controlled trial (RCT) with $N = 165$ students across five UK secondary schools. We integrated LearnLM -- a generative AI model fine-tuned for pedagogy -- into chat-based tutoring sessions on the Eedi mathematics platform. In the RCT, expert tutors directly supervised LearnLM, with the remit to revise each message it drafted until they would be satisfied sending it themselves. LearnLM proved to be a reliable source of pedagogical instruction, with supervising tutors approving 76.4% of its drafted messages making zero or minimal edits (i.e., changing only one or two characters). This translated into effective tutoring support: students guided by LearnLM performed at least as well as students chatting with human tutors on each learning outcome we measured. In fact, students who received support from LearnLM were 5.5 percentage points more likely to solve novel problems on subsequent topics (with a success rate of 66.2%) than those who received tutoring from human tutors alone (rate of 60.7%). In interviews, tutors highlighted LearnLM&#x27;s strength at drafting Socratic questions that encouraged deeper reflection from students, with multiple tutors even reporting that they learned new pedagogical practices from the model. Overall, our results suggest that pedagogically fine-tuned AI tutoring systems may play a promising role in delivering effective, individualized learning support at scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI辅导可以安全有效地支持学生：英国课堂中的探索性随机对照试验</div>
<div class="mono" style="margin-top:8px">一对一辅导被广泛认为是个性化教育的黄金标准，但要将其扩展到更大规模仍成本高昂。为评估生成式AI是否有助于扩大这一资源的可及性，我们在英国五所中学的165名学生中开展了一项探索性随机对照试验（RCT）。我们将LearnLM——一个针对教学进行微调的生成式AI模型——整合到Eedi数学平台的基于聊天的辅导会话中。在RCT中，专家辅导教师直接监督LearnLM，负责修改其生成的每条信息，直至他们满意并决定将其发送给学生。LearnLM被证明是可靠的教育指导来源，监督教师批准其生成信息中76.4%的内容无需或仅需极少修改（即仅更改一两个字符）。这转化为有效的辅导支持：在我们测量的每项学习成果上，由LearnLM引导的学生表现至少与与人类辅导教师聊天的学生相当。事实上，接受LearnLM支持的学生在后续主题中解决新问题的可能性比仅接受人类辅导的学生高出5.5个百分点（成功率66.2% vs 60.7%）。在访谈中，辅导教师强调LearnLM在起草苏格拉底式问题方面的优势，这些问题能鼓励学生进行更深层次的反思，多位教师甚至表示他们从模型中学到了新的教学方法。总体而言，我们的研究结果表明，经过教学微调的AI辅导系统可能在大规模提供有效个性化学习支持方面发挥重要作用。</div>
</details>
</div>
<div class="card">
<div class="title">BOAD: Discovering Hierarchical Software Engineering Agents via Bandit Optimization</div>
<div class="meta-line">Authors: Iris Xu, Guangtao Zeng, Zexue He, Charles Jin, Aldo Pareja, Dan Gutfreund, Chuang Gan, Zhang-Wei Hong</div>
<div class="meta-line">First: 2025-12-29T17:41:11+00:00 · Latest: 2025-12-29T17:41:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23631v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23631v1">PDF</a> · <a href="https://github.com/iamxjy/BOAD-SWE-Agent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) have shown strong reasoning and coding capabilities, yet they struggle to generalize to real-world software engineering (SWE) problems that are long-horizon and out of distribution. Existing systems often rely on a single agent to handle the entire workflow-interpreting issues, navigating large codebases, and implementing fixes-within one reasoning chain. Such monolithic designs force the model to retain irrelevant context, leading to spurious correlations and poor generalization. Motivated by how human engineers decompose complex problems, we propose structuring SWE agents as orchestrators coordinating specialized sub-agents for sub-tasks such as localization, editing, and validation. The challenge lies in discovering effective hierarchies automatically: as the number of sub-agents grows, the search space becomes combinatorial, and it is difficult to attribute credit to individual sub-agents within a team. We address these challenges by formulating hierarchy discovery as a multi-armed bandit (MAB) problem, where each arm represents a candidate sub-agent and the reward measures its helpfulness when collaborating with others. This framework, termed Bandit Optimization for Agent Design (BOAD), enables efficient exploration of sub-agent designs under limited evaluation budgets. On SWE-bench-Verified, BOAD outperforms single-agent and manually designed multi-agent systems. On SWE-bench-Live, featuring more recent and out-of-distribution issues, our 36B system ranks second on the leaderboard at the time of evaluation, surpassing larger models such as GPT-4 and Claude. These results demonstrate that automatically discovered hierarchical multi-agent systems significantly improve generalization on challenging long-horizon SWE tasks. Code is available at https://github.com/iamxjy/BOAD-SWE-Agent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BOAD: 通过多臂老虎机优化发现分层软件工程代理</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）展现了强大的推理和编码能力，但在长周期且分布外的现实软件工程（SWE）问题上泛化能力较差。现有系统通常依赖单一代理处理整个工作流程，包括问题解读、导航大型代码库和实施修复，这限制在单一推理链中。这种单体设计迫使模型保留不相关上下文，导致虚假相关性并降低泛化能力。受人类工程师分解复杂问题方式的启发，我们提出将SWE代理结构化为协调专用子代理的编排者，以处理定位、编辑和验证等子任务。挑战在于自动发现有效的层次结构：随着子代理数量的增加，搜索空间变得组合爆炸，且难以在团队中为个体子代理分配信用。我们通过将层次发现建模为多臂老虎机（MAB）问题来解决这些挑战，其中每个臂代表一个候选子代理，奖励衡量其与其他代理协作时的有用性。该框架称为代理设计的多臂老虎机优化（BOAD），能够在有限的评估预算下高效探索子代理设计。在SWE-bench-Verified上，BOAD优于单代理和手动设计的多代理系统。在包含更多近期和分布外问题的SWE-bench-Live上，我们的36B系统在评估时排名第二，超越了如GPT-4和Claude等更大的模型。这些结果表明，自动发现的分层多代理系统在具有挑战性的长周期SWE任务上显著提升了泛化能力。代码可在https://github.com/iamxjy/BOAD-SWE-Agent获取。</div>
</details>
</div>
<div class="card">
<div class="title">Memorization in 3D Shape Generation: An Empirical Study</div>
<div class="meta-line">Authors: Shu Pu, Boya Zeng, Kaichen Zhou, Mengyu Wang, Zhuang Liu</div>
<div class="meta-line">First: 2025-12-29T17:39:21+00:00 · Latest: 2025-12-29T17:39:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.23628v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.23628v1">PDF</a> · <a href="https://github.com/zlab-princeton/3d_mem">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generative models are increasingly used in 3D vision to synthesize novel shapes, yet it remains unclear whether their generation relies on memorizing training shapes. Understanding their memorization could help prevent training data leakage and improve the diversity of generated results. In this paper, we design an evaluation framework to quantify memorization in 3D generative models and study the influence of different data and modeling designs on memorization. We first apply our framework to quantify memorization in existing methods. Next, through controlled experiments with a latent vector-set (Vecset) diffusion model, we find that, on the data side, memorization depends on data modality, and increases with data diversity and finer-grained conditioning; on the modeling side, it peaks at a moderate guidance scale and can be mitigated by longer Vecsets and simple rotation augmentation. Together, our framework and analysis provide an empirical understanding of memorization in 3D generative models and suggest simple yet effective strategies to reduce it without degrading generation quality. Our code is available at https://github.com/zlab-princeton/3d_mem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>三维形状生成中的记忆现象：一项实证研究</div>
<div class="mono" style="margin-top:8px">生成模型在三维视觉中被越来越多地用于合成新颖形状，但其生成是否依赖于记忆训练数据仍不清楚。理解其记忆机制有助于防止训练数据泄露并提高生成结果的多样性。在本文中，我们设计了一个评估框架，用于量化三维生成模型中的记忆现象，并研究不同数据和建模设计对记忆的影响。我们首先将该框架应用于现有方法的记忆量化。随后，通过使用潜在向量集（Vecset）扩散模型的受控实验，我们发现，在数据方面，记忆依赖于数据模态，并随数据多样性及更细粒度的条件增强而增加；在建模方面，记忆在中等引导尺度下达到峰值，可以通过更长的Vecset和简单的旋转增强来缓解。综上，我们的框架和分析提供了对三维生成模型中记忆现象的实证理解，并提出了简单而有效的策略以减少记忆现象，而不会降低生成质量。我们的代码可在 https://github.com/zlab-princeton/3d_mem 获取。</div>
</details>
</div>
<div class="card">
<div class="title">Bayesian Image Mediation Analysis</div>
<div class="meta-line">Authors: Yuliang Xu, Timothy D Johnson, Mary Heitzeg, Jian Kang</div>
<div class="meta-line">First: 2023-10-25T01:35:29+00:00 · Latest: 2025-12-29T17:32:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2310.16284v2">Abs</a> · <a href="https://arxiv.org/pdf/2310.16284v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mediation analysis aims to separate the indirect effect through mediators from the direct effect of the exposure on the outcome. It is challenging to perform mediation analysis with neuroimaging data which involves high dimensionality, complex spatial correlations, sparse activation patterns and relatively low signal-to-noise ratio. To address these issues, we develop a new spatially varying coefficient structural equation model for Bayesian Image Mediation Analysis (BIMA). We define spatially varying mediation effects within the potential outcomes framework, employing a soft-thresholded Gaussian process prior for functional parameters. We establish posterior consistency for spatially varying mediation effects along with selection consistency on important regions that contribute to the mediation effects. We develop an efficient posterior computation algorithm scalable to analysis of large-scale imaging data. Through extensive simulations, we show that BIMA can improve the estimation accuracy and computational efficiency for high-dimensional mediation analysis over existing methods. We apply BIMA to analyze behavioral and fMRI data in the Adolescent Brain Cognitive Development (ABCD) study with a focus on inferring the mediation effects of the parental education level on the children&#x27;s general cognitive ability that are mediated through the working memory brain activity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>贝叶斯图像中介分析</div>
<div class="mono" style="margin-top:8px">中介分析旨在将暴露对结果的直接效应与通过中介变量的间接效应区分开来。然而，使用神经影像数据进行中介分析具有挑战性，因为这类数据通常具有高维性、复杂的空间相关性、稀疏的激活模式以及相对较低的信噪比。为了解决这些问题，我们开发了一种新的空间变化系数结构方程模型，用于贝叶斯图像中介分析（BIMA）。我们在潜在结果框架中定义了空间变化的中介效应，并采用软阈值高斯过程先验对功能参数进行建模。我们建立了空间变化中介效应的后验一致性以及对重要区域的选取一致性。此外，我们开发了一种高效的后验计算算法，可扩展用于大规模影像数据的分析。通过广泛的模拟实验，我们展示了BIMA在高维中介分析中相较于现有方法能够提高估计精度和计算效率。我们将BIMA应用于青少年大脑认知发展（ABCD）研究中的行为和fMRI数据，重点在于推断父母教育水平对儿童一般认知能力的中介效应，该效应通过工作记忆脑活动实现。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
