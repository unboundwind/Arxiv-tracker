<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-05 04:17</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260205_0417</div>
    <div class="row"><div class="card">
<div class="title">EventNeuS: 3D Mesh Reconstruction from a Single Event Camera</div>
<div class="meta-line">Authors: Shreyas Sachan, Viktor Rudnev, Mohamed Elgharib, Christian Theobalt, Vladislav Golyanik</div>
<div class="meta-line">Venue: International Conference on 3D Vision (3DV) 2026</div>
<div class="meta-line">First: 2026-02-03T18:59:57+00:00 · Latest: 2026-02-03T18:59:57+00:00</div>
<div class="meta-line">Comments: 13 pages, 10 figures, 3 tables; project page: https://4dqv.mpi-inf.mpg.de/EventNeuS/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03847v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03847v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Event cameras offer a considerable alternative to RGB cameras in many scenarios. While there are recent works on event-based novel-view synthesis, dense 3D mesh reconstruction remains scarcely explored and existing event-based techniques are severely limited in their 3D reconstruction accuracy. To address this limitation, we present EventNeuS, a self-supervised neural model for learning 3D representations from monocular colour event streams. Our approach, for the first time, combines 3D signed distance function and density field learning with event-based supervision. Furthermore, we introduce spherical harmonics encodings into our model for enhanced handling of view-dependent effects. EventNeuS outperforms existing approaches by a significant margin, achieving 34% lower Chamfer distance and 31% lower mean absolute error on average compared to the best previous method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EventNeuS：从单目事件相机进行3D网格重建</div>
<div class="mono" style="margin-top:8px">事件相机在许多场景中为RGB相机提供了有力的替代方案。尽管已有针对基于事件的新视角合成的近期研究，但密集的3D网格重建仍鲜有探索，且现有的基于事件的技术在3D重建精度方面存在严重限制。为了解决这一问题，我们提出了EventNeuS，这是一种自监督的神经模型，用于从单目彩色事件流中学习3D表示。我们的方法首次将3D有符号距离函数和密度场学习与基于事件的监督相结合。此外，我们还在模型中引入了球面调和编码，以增强对视角依赖效应的处理能力。EventNeuS在性能上显著优于现有方法，在最佳先前方法的基础上，平均实现了34%的Chamfer距离降低和31%的平均绝对误差降低。</div>
</details>
</div>
<div class="card">
<div class="title">PLATE: Plasticity-Tunable Efficient Adapters for Geometry-Aware Continual Learning</div>
<div class="meta-line">Authors: Romain Cosentino</div>
<div class="meta-line">First: 2026-02-03T18:59:42+00:00 · Latest: 2026-02-03T18:59:42+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03846v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03846v1">PDF</a> · <a href="https://github.com/SalesforceAIResearch/PLATE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We develop a continual learning method for pretrained models that \emph{requires no access to old-task data}, addressing a practical barrier in foundation model adaptation where pretraining distributions are often unavailable. Our key observation is that pretrained networks exhibit substantial \emph{geometric redundancy}, and that this redundancy can be exploited in two complementary ways. First, redundant neurons provide a proxy for dominant pretraining-era feature directions, enabling the construction of approximately protected update subspaces directly from pretrained weights. Second, redundancy offers a natural bias for \emph{where} to place plasticity: by restricting updates to a subset of redundant neurons and constraining the remaining degrees of freedom, we obtain update families with reduced functional drift on the old-data distribution and improved worst-case retention guarantees. These insights lead to \textsc{PLATE} (\textbf{Pla}sticity-\textbf{T}unable \textbf{E}fficient Adapters), a continual learning method requiring no past-task data that provides explicit control over the plasticity-retention trade-off. PLATE parameterizes each layer with a structured low-rank update $ΔW = B A Q^\top$, where $B$ and $Q$ are computed once from pretrained weights and kept frozen, and only $A$ is trained on the new task. The code is available at https://github.com/SalesforceAIResearch/PLATE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PLATE: 塑性可调的高效适配器用于几何感知的持续学习</div>
<div class="mono" style="margin-top:8px">我们开发了一种持续学习方法，用于预训练模型，该方法\emph{无需访问旧任务数据}，解决了基础模型适应中的一个实际障碍，即预训练分布通常不可用。我们的主要观察是，预训练网络表现出显著的\emph{几何冗余}，并且这种冗余可以通过两种互补的方式加以利用。首先，冗余神经元为预训练时期的主导特征方向提供了代理，从而能够直接从预训练权重中构建近似保护的更新子空间。其次，冗余为\emph{塑性放置位置}提供了自然的偏差：通过限制更新仅针对冗余神经元的子集，并约束剩余的自由度，我们获得了在旧数据分布上功能漂移减少且最坏情况保留保证增强的更新族。这些见解促成了\textsc{PLATE}（\textbf{Pla}sticity-\textbf{T}unable \textbf{E}fficient Adapters），一种无需过去任务数据的持续学习方法，能够显式地控制塑性与保留之间的权衡。PLATE 通过结构化的低秩更新 $ΔW = B A Q^\top$ 参数化每一层，其中 $B$ 和 $Q$ 从预训练权重中一次性计算并保持冻结，而仅 $A$ 在新任务上进行训练。代码可在 https://github.com/SalesforceAIResearch/PLATE 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We develop a continual learning method for pretrained models that \emph{requires no access to old-task data}, addressing a practical barrier in foundation model adaptation where pretraining distributions are often unavailable.</div>
</details>
</div>
<div class="card">
<div class="title">Reuse your FLOPs: Scaling RL on Hard Problems by Conditioning on Very Off-Policy Prefixes</div>
<div class="meta-line">Authors: Amrith Setlur, Zijian Wang, Andrew Cohen, Paria Rashidinejad, Sang Michael Xie</div>
<div class="meta-line">First: 2026-01-26T18:57:00+00:00 · Latest: 2026-02-03T18:58:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18795v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18795v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls. To bootstrap more efficient RL, we consider reusing old sampling FLOPs (from prior inference or RL training) in the form of off-policy traces. Standard off-policy methods supervise against off-policy data, causing instabilities during RL optimization. We introduce PrefixRL, where we condition on the prefix of successful off-policy traces and run on-policy RL to complete them, side-stepping off-policy instabilities. PrefixRL boosts the learning signal on hard problems by modulating the difficulty of the problem through the off-policy prefix length. We prove that the PrefixRL objective is not only consistent with the standard RL objective but also more sample efficient. Empirically, we discover back-generalization: training only on prefixed problems generalizes to out-of-distribution unprefixed performance, with learned strategies often differing from those in the prefix. In our experiments, we source the off-policy traces by rejection sampling with the base model, creating a self-improvement loop. On hard reasoning problems, PrefixRL reaches the same training reward 2x faster than the strongest baseline (SFT on off-policy data then RL), even after accounting for the compute spent on the initial rejection sampling, and increases the final reward by 3x. The gains transfer to held-out benchmarks, and PrefixRL is still effective when off-policy traces are derived from a different model family, validating its flexibility in practical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重用你的FLOPs：通过基于非常离策略前缀的条件来扩展RL在困难问题上的应用</div>
<div class="mono" style="margin-top:8px">典型的用于LLM推理的强化学习（RL）方法在处理困难问题时会浪费计算资源，这些问题中正确的策略轨迹稀少，策略梯度消失，学习停滞。为了启动更高效的RL，我们考虑以离策略轨迹的形式重用旧的采样FLOPs（来自先前推理或RL训练）。标准的离策略方法通过离策略数据进行监督，导致RL优化过程中出现不稳定性。我们引入了PrefixRL，通过条件在成功的离策略轨迹的前缀上，并运行策略轨迹来完成这些轨迹，从而绕过离策略的不稳定性。PrefixRL通过调节离策略前缀长度来改变问题的难度，从而增强在困难问题上的学习信号。我们证明了PrefixRL的目标不仅与标准RL目标一致，而且更加样本高效。实证上，我们发现了反向泛化现象：仅在前缀问题上训练的模型可以泛化到分布外的非前缀性能，且学到的策略通常与前缀中的策略不同。在我们的实验中，我们通过基础模型的拒绝采样来获取离策略轨迹，从而创建了一个自我提升的循环。在困难推理问题上，PrefixRL在考虑初始拒绝采样所消耗的计算资源后，比最强基线（先在离策略数据上进行SFT，再进行RL）快两倍达到相同的训练奖励，并将最终奖励提高了三倍。这些收益可以迁移到保留的基准测试中，即使离策略轨迹来源于不同的模型家族，PrefixRL仍然有效，验证了其在实际应用中的灵活性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Typical reinforcement learning (RL) methods for LLM reasoning waste compute on hard problems, where correct on-policy traces are rare, policy gradients vanish, and learning stalls.</div>
</details>
</div>
<div class="card">
<div class="title">Investigating Quantum Circuit Designs Using Neuro-Evolution</div>
<div class="meta-line">Authors: Devroop Kar, Daniel Krutz, Travis Desell</div>
<div class="meta-line">First: 2026-02-03T18:57:39+00:00 · Latest: 2026-02-03T18:57:39+00:00</div>
<div class="meta-line">Comments: Submitted to The Genetic and Evolutionary Computation Conference (GECCO) 2026. Under Review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03840v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03840v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Designing effective quantum circuits remains a central challenge in quantum computing, as circuit structure strongly influences expressivity, trainability, and hardware feasibility. Current approaches, whether using manually designed circuit templates, fixed heuristics, or automated rules, face limitations in scalability, flexibility, and adaptability, often producing circuits that are poorly matched to the specific problem or quantum hardware. In this work, we propose the Evolutionary eXploration of Augmenting Quantum Circuits (EXAQC), an evolutionary approach to the automated design and training of parameterized quantum circuits (PQCs) which leverages and extends on strategies from neuroevolution and genetic programming. The proposed method jointly searches over gate types, qubit connectivity, parameterization, and circuit depth while respecting hardware and noise constraints. The method supports both Qiskit and Pennylane libraries, allowing the user to configure every aspect. This work highlights evolutionary search as a critical tool for advancing quantum machine learning and variational quantum algorithms, providing a principled pathway toward scalable, problem-aware, and hardware-efficient quantum circuit design. Preliminary results demonstrate that circuits evolved on classification tasks are able to achieve over 90% accuracy on most of the benchmark datasets with a limited computational budget, and are able to emulate target circuit quantum states with high fidelity scores.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用神经进化探究量子电路设计</div>
<div class="mono" style="margin-top:8px">设计有效的量子电路仍然是量子计算中的核心挑战，因为电路结构强烈影响表达能力、可训练性和硬件可行性。当前的方法，无论是使用手动设计的电路模板、固定启发式规则还是自动化规则，都存在可扩展性、灵活性和适应性方面的局限，常常产生与特定问题或量子硬件不匹配的电路。在本工作中，我们提出了增强量子电路的进化探索（EXAQC），这是一种用于参数化量子电路（PQCs）自动化设计和训练的进化方法，借鉴并扩展了神经进化和遗传编程的策略。该方法在门类型、量子比特连接性、参数化方式和电路深度上进行联合搜索，同时尊重硬件和噪声约束。该方法支持Qiskit和Pennylane库，允许用户配置所有方面。本工作强调进化搜索作为推进量子机器学习和变分量子算法的关键工具，提供了一条原理性的路径，以实现可扩展、问题感知和硬件高效的量子电路设计。初步结果表明，在分类任务上进化得到的电路能够在有限的计算预算下在大多数基准数据集上达到超过90%的准确率，并且能够以高保真度模拟目标电路的量子态。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Designing effective quantum circuits remains a central challenge in quantum computing, as circuit structure strongly influences expressivity, trainability, and hardware feasibility.</div>
</details>
</div>
<div class="card">
<div class="title">Polynomial Neural Sheaf Diffusion: A Spectral Filtering Approach on Cellular Sheaves</div>
<div class="meta-line">Authors: Alessio Borgi, Fabrizio Silvestri, Pietro Liò</div>
<div class="meta-line">Venue: ICML 2026</div>
<div class="meta-line">First: 2025-11-28T23:10:54+00:00 · Latest: 2026-02-03T18:57:37+00:00</div>
<div class="meta-line">Comments: Under Review at ICML 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.00242v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.00242v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sheaf Neural Networks equip graph structures with a cellular sheaf: a geometric structure which assigns local vector spaces (stalks) and a linear learnable restriction/transport maps to nodes and edges, yielding an edge-aware inductive bias that handles heterophily and limits oversmoothing. However, common Neural Sheaf Diffusion implementations rely on SVD-based sheaf normalization and dense per-edge restriction maps, which scale with stalk dimension, require frequent Laplacian rebuilds, and yield brittle gradients. To address these limitations, we introduce Polynomial Neural Sheaf Diffusion (PolyNSD), a new sheaf diffusion approach whose propagation operator is a degree-K polynomial in a normalised sheaf Laplacian, evaluated via a stable three-term recurrence on a spectrally rescaled operator. This provides an explicit K-hop receptive field in a single layer (independently of the stalk dimension), with a trainable spectral response obtained as a convex mixture of K+1 orthogonal polynomial basis responses. PolyNSD enforces stability via convex mixtures, spectral rescaling, and residual/gated paths, reaching new state-of-the-art results on both homophilic and heterophilic benchmarks, inverting the Neural Sheaf Diffusion trend by obtaining these results with just diagonal restriction maps, decoupling performance from large stalk dimension, while reducing runtime and memory requirements.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多项式神经束扩散：基于细胞束的谱滤波方法</div>
<div class="mono" style="margin-top:8px">Sheaf Neural Networks 通过为图结构赋予细胞束：一种几何结构，它为节点和边分配局部向量空间（茎）和可学习的线性限制/传输映射，从而产生一种边感知的归纳偏差，处理异质性并限制过度平滑。然而，常见的神经束扩散实现依赖于基于奇异值分解（SVD）的束归一化和密集的每边限制映射，这些方法随着茎维度的增加而扩展，需要频繁重建拉普拉斯矩阵，并导致梯度脆弱。为了解决这些限制，我们引入了多项式神经束扩散（PolyNSD），这是一种新的束扩散方法，其传播算子是归一化束拉普拉斯矩阵的K次多项式，通过在谱重标定算子上的稳定三阶递推进行计算。这在单层中提供了显式的K跳感受野（与茎维度无关），其可训练的谱响应是K+1个正交多项式基响应的凸混合。PolyNSD 通过凸混合、谱重标定和残差/门控路径来确保稳定性，在同质性和异质性基准测试中均达到了新的最先进结果，逆转了神经束扩散的趋势，仅使用对角限制映射即可获得这些结果，将性能与大茎维度解耦，同时减少了运行时间和内存需求。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Sheaf Neural Networks equip graph structures with a cellular sheaf: a geometric structure which assigns local vector spaces (stalks) and a linear learnable restriction/transport maps to nodes and edges, yielding an edge-aware inductive bias that handles heterophily and limits oversmoothing.</div>
</details>
</div>
<div class="card">
<div class="title">Understanding and Exploiting Weight Update Sparsity for Communication-Efficient Distributed RL</div>
<div class="meta-line">Authors: Erfan Miahi, Eugene Belilovsky</div>
<div class="meta-line">First: 2026-02-03T18:56:48+00:00 · Latest: 2026-02-03T18:56:48+00:00</div>
<div class="meta-line">Comments: 32 pages, 14 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03839v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03839v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) is a critical component for post-training large language models (LLMs). However, in bandwidth-constrained distributed RL, scalability is often bottlenecked by the synchronization of policy weights from trainers to inference workers, particularly over commodity networks or in decentralized settings. While recent studies suggest that RL updates modify only a small fraction of model parameters, these observations are typically based on coarse checkpoint differences. We present a systematic empirical study of weight-update sparsity at both step-level and multi-step granularities, examining its evolution across training dynamics, off-policy delay, and model scale. We find that update sparsity is consistently high, frequently exceeding 99% across practically relevant settings. Leveraging this structure, we propose PULSE (Patch Updates via Lossless Sparse Encoding), a simple yet highly efficient lossless weight synchronization method that transmits only the indices and values of modified parameters. PULSE is robust to transmission errors and avoids floating-point drift inherent in additive delta schemes. In bandwidth-constrained decentralized environments, our approach achieves over 100x (14 GB to ~108 MB) communication reduction while maintaining bit-identical training dynamics and performance compared to full weight synchronization. By exploiting this structure, PULSE enables decentralized RL training to approach centralized throughput, reducing the bandwidth required for weight synchronization from 20 Gbit/s to 0.2 Gbit/s to maintain high GPU utilization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>理解与利用权重更新稀疏性以实现通信高效的分布式强化学习</div>
<div class="mono" style="margin-top:8px">强化学习（RL）是后训练大语言模型（LLMs）的关键组成部分。然而，在带宽受限的分布式RL中，可扩展性通常受到从训练器到推理工作者同步策略权重的限制，尤其是在商品网络或去中心化环境中。尽管近期研究表明RL更新通常只修改模型参数的一小部分，但这些观察通常基于粗略的检查点差异。我们系统地研究了在步骤级和多步骤粒度下的权重更新稀疏性，并分析其在训练动态、离策略延迟和模型规模中的演变。我们发现，更新稀疏性在实际相关场景中通常很高，经常超过99%。利用这一结构，我们提出了PULSE（通过无损稀疏编码进行补丁更新），这是一种简单但高度高效的无损权重同步方法，仅传输修改参数的索引和值。PULSE对传输错误具有鲁棒性，并避免了加法差分方案中固有的浮点漂移问题。在带宽受限的去中心化环境中，我们的方法在保持与完整权重同步相同的训练动态和性能的同时，实现了超过100倍（14GB到约108MB）的通信减少。通过利用这一结构，PULSE使去中心化RL训练能够接近中心化吞吐量，将权重同步所需的带宽从20 Gbit/s降低至0.2 Gbit/s，以维持高GPU利用率。</div>
</details>
</div>
<div class="card">
<div class="title">PrevizWhiz: Combining Rough 3D Scenes and 2D Video to Guide Generative Video Previsualization</div>
<div class="meta-line">Authors: Erzhen Hu, Frederik Brudy, David Ledo, George Fitzmaurice, Fraser Anderson</div>
<div class="meta-line">First: 2026-02-03T18:56:40+00:00 · Latest: 2026-02-03T18:56:40+00:00</div>
<div class="meta-line">Comments: 21 pages, 13 figures; accepted and to appear at CHI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03838v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03838v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In pre-production, filmmakers and 3D animation experts must rapidly prototype ideas to explore a film&#x27;s possibilities before fullscale production, yet conventional approaches involve trade-offs in efficiency and expressiveness. Hand-drawn storyboards often lack spatial precision needed for complex cinematography, while 3D previsualization demands expertise and high-quality rigged assets. To address this gap, we present PrevizWhiz, a system that leverages rough 3D scenes in combination with generative image and video models to create stylized video previews. The workflow integrates frame-level image restyling with adjustable resemblance, time-based editing through motion paths or external video inputs, and refinement into high-fidelity video clips. A study with filmmakers demonstrates that our system lowers technical barriers for film-makers, accelerates creative iteration, and effectively bridges the communication gap, while also surfacing challenges of continuity, authorship, and ethical consideration in AI-assisted filmmaking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PrevizWhiz：结合粗糙3D场景和2D视频以指导生成式视频预可视化</div>
<div class="mono" style="margin-top:8px">在前期制作阶段，电影制作人和3D动画专家需要快速原型化创意，以在大规模制作前探索电影的潜力。然而，传统方法在效率和表达性之间存在权衡。手绘故事板往往缺乏复杂 cinematography 所需的空间精度，而3D预可视化则需要专业知识和高质量的绑定资产。为了解决这一问题，我们提出了PrevizWhiz系统，该系统利用粗糙的3D场景结合生成图像和视频模型，创建风格化的视频预览。该工作流程集成了基于帧的图像重风格化（可调节相似度）、基于时间的编辑（通过运动路径或外部视频输入），以及精炼为高保真视频片段。一项针对电影制作人的研究显示，我们的系统降低了技术门槛，加快了创意迭代，并有效弥合了沟通差距，同时也揭示了AI辅助电影制作中连续性、作者权和伦理考量等方面的挑战。</div>
</details>
</div>
<div class="card">
<div class="title">MixGRPO: Unlocking Flow-based GRPO Efficiency with Mixed ODE-SDE</div>
<div class="meta-line">Authors: Junzhe Li, Yutao Cui, Tao Huang, Yinping Ma, Chun Fan, Miles Yang, Zhao Zhong</div>
<div class="meta-line">First: 2025-07-29T13:40:09+00:00 · Latest: 2026-02-03T18:56:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.21802v4">Abs</a> · <a href="https://arxiv.org/pdf/2507.21802v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Although GRPO substantially enhances flow matching models in human preference alignment of image generation, methods such as FlowGRPO and DanceGRPO still exhibit inefficiency due to the necessity of sampling and optimizing over all denoising steps specified by the Markov Decision Process (MDP). In this paper, we propose $\textbf{MixGRPO}$, a novel framework that leverages the flexibility of mixed sampling strategies through the integration of stochastic differential equations (SDE) and ordinary differential equations (ODE). This streamlines the optimization process within the MDP to improve efficiency and boost performance. Specifically, MixGRPO introduces a sliding window mechanism, using SDE sampling and GRPO-guided optimization only within the window, while applying ODE sampling outside. This design confines sampling randomness to the time-steps within the window, thereby reducing the optimization overhead, and allowing for more focused gradient updates to accelerate convergence. Additionally, as time-steps beyond the sliding window are not involved in optimization, higher-order solvers are supported for faster sampling. So we present a faster variant, termed $\textbf{MixGRPO-Flash}$, which further improves training efficiency while achieving comparable performance. MixGRPO exhibits substantial gains across multiple dimensions of human preference alignment, outperforming DanceGRPO in both effectiveness and efficiency, with nearly 50% lower training time. Notably, MixGRPO-Flash further reduces training time by 71%.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MixGRPO: 通过混合ODE-SDE提升基于流的GRPO效率</div>
<div class="mono" style="margin-top:8px">尽管GRPO在图像生成中的人类偏好对齐方面显著提升了流匹配模型的性能，但诸如FlowGRPO和DanceGRPO等方法由于需要在马尔可夫决策过程（MDP）指定的所有去噪步骤上进行采样和优化，仍然存在效率低下问题。本文提出了一种新颖的框架MixGRPO，通过整合随机微分方程（SDE）和常微分方程（ODE），利用混合采样策略的灵活性，从而优化MDP中的优化过程，提高效率和性能。具体而言，MixGRPO引入了一个滑动窗口机制，在窗口内使用SDE采样和GRPO引导的优化，而在窗口外使用ODE采样。这种设计将采样随机性限制在窗口内的时间步，从而减少优化开销，并允许更集中的梯度更新以加速收敛。此外，由于滑动窗口外的时间步不参与优化，支持更高阶的求解器以实现更快的采样。因此，我们提出了一个更快的变体MixGRPO-Flash，在提升训练效率的同时保持了相当的性能。MixGRPO在人类偏好对齐的多个维度上表现出显著的提升，在有效性和效率方面均优于DanceGRPO，训练时间几乎降低了50%。值得注意的是，MixGRPO-Flash进一步将训练时间减少了71%。</div>
</details>
</div>
<div class="card">
<div class="title">Accelerating Scientific Research with Gemini: Case Studies and Common Techniques</div>
<div class="meta-line">Authors: David P. Woodruff, Vincent Cohen-Addad, Lalit Jain, Jieming Mao, Song Zuo, MohammadHossein Bateni, Simina Branzei, Michael P. Brenner, Lin Chen, Ying Feng, Lance Fortnow, Gang Fu, Ziyi Guan, Zahra Hadizadeh, Mohammad T. Hajiaghayi, Mahdi JafariRaviz, Adel Javanmard, Karthik C. S., Ken-ichi Kawarabayashi, Ravi Kumar, Silvio Lattanzi, Euiwoong Lee, Yi Li, Ioannis Panageas, Dimitris Paparas, Benjamin Przybocki, Bernardo Subercaseaux, Ola Svensson, Shayan Taherijam, Xuan Wu, Eylon Yogev, Morteza Zadimoghaddam, Samson Zhou, Vahab Mirrokni</div>
<div class="meta-line">First: 2026-02-03T18:56:17+00:00 · Latest: 2026-02-03T18:56:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03837v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03837v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large language models (LLMs) have opened new avenues for accelerating scientific research. While models are increasingly capable of assisting with routine tasks, their ability to contribute to novel, expert-level mathematical discovery is less understood. We present a collection of case studies demonstrating how researchers have successfully collaborated with advanced AI models, specifically Google&#x27;s Gemini-based models (in particular Gemini Deep Think and its advanced variants), to solve open problems, refute conjectures, and generate new proofs across diverse areas in theoretical computer science, as well as other areas such as economics, optimization, and physics. Based on these experiences, we extract common techniques for effective human-AI collaboration in theoretical research, such as iterative refinement, problem decomposition, and cross-disciplinary knowledge transfer. While the majority of our results stem from this interactive, conversational methodology, we also highlight specific instances that push beyond standard chat interfaces. These include deploying the model as a rigorous adversarial reviewer to detect subtle flaws in existing proofs, and embedding it within a &quot;neuro-symbolic&quot; loop that autonomously writes and executes code to verify complex derivations. Together, these examples highlight the potential of AI not just as a tool for automation, but as a versatile, genuine partner in the creative process of scientific discovery.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用Gemini加速科学研究：案例分析与常用技术</div>
<div class="mono" style="margin-top:8px">近年来，大语言模型（LLMs）的进展为加速科学研究开辟了新途径。尽管这些模型在协助处理常规任务方面越来越强大，但它们在促进新颖、专家级数学发现方面的能力仍不为人所知。我们展示了一系列案例研究，说明研究人员如何成功与先进的AI模型合作，特别是基于Gemini的Google模型（尤其是Gemini Deep Think及其高级变体），以解决理论计算机科学及其他领域如经济学、优化和物理学中的开放性问题、反驳猜想并生成新证明。基于这些经验，我们提炼出在理论研究中有效的人机协作常用技术，例如迭代优化、问题分解和跨学科知识迁移。虽然我们大多数成果源于这种交互式、对话式的方法，但我们还强调了一些超越标准聊天界面的具体实例。这些实例包括将模型部署为严格的对抗性审稿人，以检测现有证明中的细微缺陷，以及将其嵌入到一个“神经符号”循环中，以自主编写和执行代码验证复杂推导。这些例子共同展示了AI不仅作为自动化工具的潜力，更作为科学发现创造性过程中的灵活且真实的合作伙伴。</div>
</details>
</div>
<div class="card">
<div class="title">Observers, $α$-parameters, and the Hartle-Hawking state</div>
<div class="meta-line">Authors: Daniel Harlow</div>
<div class="meta-line">First: 2026-02-03T18:54:28+00:00 · Latest: 2026-02-03T18:54:28+00:00</div>
<div class="meta-line">Comments: 47 pages, 22 figures. Many kinds of closed universe state</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03835v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03835v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper we extend recent ideas about observers and closed universes to theories where observers can be fluctuated into existence in the Hartle-Hawking state. This introduces a phenomenon that was not considered in these earlier discussions: the dominant transition from one cosmological state to another can go through a fluctuation that annihilates the universe and creates a new one. We nonetheless argue that the observer decoherence rule allows for the third-quantized description of such a theory to emerge from a factorizing holographic theory with a one-dimensional Hilbert space, without any need for $α$-parameters. We also point out a close analogy between the observer rule in this context and the coarse-graining of the spectral form factor at late times for AdS black holes. Along the way we clarify several aspects of the relationship between holography, the gravitational path integral, and $α$-parameters. We also explain why string theory scattering amplitudes do not lead to a one-dimensional Hilbert space on the worldsheet, despite being computed by a gravitational path integral with a sum over topology. Finally we point out that using the path integral to compute integrated local operators conditioned on an observer in the context of a theory with a landscape can lead to rather surprising conclusions. For example we argue that in a landscape with one AdS minimum and one dS minimum, both of which can support observers, an observer almost surely finds themself in dS and not AdS even if the boundary conditions are dual to a state with an observer in AdS.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>观察者、$α$-参数与哈特尔-霍金态</div>
<div class="mono" style="margin-top:8px">本文扩展了近期关于观察者和封闭宇宙的想法，将其应用于观察者可以在哈特尔-霍金态中被波动生成的理论。这引入了一个在早期讨论中未被考虑的现象：从一个宇宙态到另一个宇宙态的主导转变可能通过一个湮灭宇宙并创造新宇宙的波动过程实现。我们仍然认为，观察者退相干规则允许这种理论以第三量子化的方式从一个具有单维希尔伯特空间的因子化全息理论中出现，而无需任何$α$-参数。我们还指出，在这种背景下观察者规则与AdS黑洞在晚期时间的谱形式因子粗粒化之间存在密切类比。在过程中，我们澄清了全息原理、引力路径积分与$α$-参数之间关系的几个方面。我们还解释了为什么弦论散射振幅不会在世界面上导致单维希尔伯特空间，尽管它们是通过具有拓扑求和的引力路径积分计算得到的。最后，我们指出，在具有景观的理论中，使用路径积分计算受观察者条件限制的局部算符可能会得出相当令人惊讶的结论。例如，我们论证在一个具有一个AdS极小值和一个dS极小值的景观中，尽管边界条件与AdS中存在观察者的态对偶，观察者几乎确定会发现自己处于dS态而非AdS态。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this paper we extend recent ideas about observers and closed universes to theories where observers can be fluctuated into existence in the Hartle-Hawking state.</div>
</details>
</div>
<div class="card">
<div class="title">AutoFigure: Generating and Refining Publication-Ready Scientific Illustrations</div>
<div class="meta-line">Authors: Minjun Zhu, Zhen Lin, Yixuan Weng, Panzhong Lu, Qiujie Xie, Yifan Wei, Sifan Liu, Qiyao Sun, Yue Zhang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-03T18:41:43+00:00 · Latest: 2026-02-03T18:41:43+00:00</div>
<div class="meta-line">Comments: Accepted at the ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03828v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03828v1">PDF</a> · <a href="https://github.com/ResearAI/AutoFigure">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-quality scientific illustrations are crucial for effectively communicating complex scientific and technical concepts, yet their manual creation remains a well-recognized bottleneck in both academia and industry. We present FigureBench, the first large-scale benchmark for generating scientific illustrations from long-form scientific texts. It contains 3,300 high-quality scientific text-figure pairs, covering diverse text-to-illustration tasks from scientific papers, surveys, blogs, and textbooks. Moreover, we propose AutoFigure, the first agentic framework that automatically generates high-quality scientific illustrations based on long-form scientific text. Specifically, before rendering the final result, AutoFigure engages in extensive thinking, recombination, and validation to produce a layout that is both structurally sound and aesthetically refined, outputting a scientific illustration that achieves both structural completeness and aesthetic appeal. Leveraging the high-quality data from FigureBench, we conduct extensive experiments to test the performance of AutoFigure against various baseline methods. The results demonstrate that AutoFigure consistently surpasses all baseline methods, producing publication-ready scientific illustrations. The code, dataset and huggingface space are released in https://github.com/ResearAI/AutoFigure.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AutoFigure：生成和优化可用于出版的科学插图</div>
<div class="mono" style="margin-top:8px">高质量的科学插图对于有效传达复杂的科学和技术概念至关重要，但其手动创建仍然是学术界和工业界公认的瓶颈。我们提出了FigureBench，这是首个用于从长篇科学文本生成科学插图的大规模基准数据集。它包含3,300对高质量的科学文本与插图，涵盖了科学论文、综述、博客和教科书中的多样化文本到插图任务。此外，我们提出了AutoFigure，这是首个基于长篇科学文本自动生成高质量科学插图的代理框架。具体而言，在渲染最终结果之前，AutoFigure会进行广泛的思考、重组和验证，以生成结构合理且审美优化的布局，输出兼具结构完整性和审美吸引力的科学插图。借助FigureBench的高质量数据，我们进行了大量实验以测试AutoFigure相对于各种基线方法的性能。实验结果表明，AutoFigure在所有基线方法上表现一致优异，能够生成可用于出版的科学插图。代码、数据集和HuggingFace空间已发布在https://github.com/ResearAI/AutoFigure。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Agent Pathfinding Under Team-Connected Communication Constraint via Adaptive Path Expansion and Dynamic Leading</div>
<div class="meta-line">Authors: Hoang-Dung Bui, Erion Plaku, Gregoy J. Stein</div>
<div class="meta-line">First: 2025-01-06T05:21:18+00:00 · Latest: 2026-02-03T18:36:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.02770v5">Abs</a> · <a href="https://arxiv.org/pdf/2501.02770v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes a novel planning framework to handle a multi-agent pathfinding problem under team-connected communication constraint, where all agents must have a connected communication channel to the rest of the team during their entire movements. Standard multi-agent path finding approaches (e.g., priority-based search) have potential in this domain but fail when neighboring configurations at start and goal differ. Their single-expansion approach -- computing each agent&#x27;s path from the start to the goal in just a single expansion -- cannot reliably handle planning under communication constraints for agents as their neighbors change during navigating. Similarly, leader-follower approaches (e.g., platooning) are effective at maintaining team communication, but fixing the leader at the outset of planning can cause planning to become stuck in dense-clutter environments, limiting their practical utility. To overcome this limitation, we propose a novel two-level multi-agent pathfinding framework that integrates two techniques: adaptive path expansion to expand agent paths to their goals in multiple stages; and dynamic leading technique that enables the reselection of the leading agent during each agent path expansion whenever progress cannot be made. Simulation experiments show the efficiency of our planners, which can handle up to 25 agents across five environment types under a limited communication range constraint and up to 11-12 agents on three environment types under line-of-sight communication constraint, exceeding 90% success-rate where baselines routinely fail.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过自适应路径扩展和动态引导处理团队连接通信约束下的多智能体路径规划</div>
<div class="mono" style="margin-top:8px">本文提出了一种新颖的规划框架，用于处理团队连接通信约束下的多智能体路径规划问题，其中所有智能体在其整个移动过程中都必须与团队其余部分保持连接通信。标准的多智能体路径规划方法（如基于优先级的搜索）在此领域具有潜力，但在起始和目标状态的相邻配置不同时会失效。它们的单次扩展方法——即在单次扩展中为每个智能体计算从起点到目标的路径——无法可靠地处理通信约束下的规划，因为智能体在导航过程中邻居会不断变化。同样，领航-跟随方法（如车队行驶）在保持团队通信方面有效，但若在规划开始时固定领航智能体，可能会在密集障碍环境中陷入困境，从而限制其实用性。为克服这一限制，我们提出了一种新颖的两级多智能体路径规划框架，集成了两种技术：自适应路径扩展，用于分阶段扩展智能体路径至目标；以及动态引导技术，允许在每次智能体路径扩展过程中，当无法取得进展时重新选择引导智能体。仿真实验表明，我们的规划器在有限通信范围约束下，可在五种环境类型中处理多达25个智能体，在视线通信约束下可在三种环境类型中处理多达11-12个智能体，成功率超过90%，而基准方法通常会失败。</div>
</details>
</div>
<div class="card">
<div class="title">ME-IGM: Individual-Global-Max in Maximum Entropy Multi-Agent Reinforcement Learning</div>
<div class="meta-line">Authors: Wen-Tse Chen, Yuxuan Li, Shiyu Huang, Jiayu Chen, Jeff Schneider</div>
<div class="meta-line">Venue: Proc. of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026), Paphos, Cyprus, May 25 - 29, 2026, IFAAMAS, 19 pages</div>
<div class="meta-line">First: 2024-06-20T01:55:08+00:00 · Latest: 2026-02-03T18:35:29+00:00</div>
<div class="meta-line">Comments: Published in the Proceedings of the 25th International Conference on Autonomous Agents and Multiagent Systems (AAMAS 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2406.13930v4">Abs</a> · <a href="https://arxiv.org/pdf/2406.13930v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent credit assignment is a fundamental challenge for cooperative multi-agent reinforcement learning (MARL), where a team of agents learn from shared reward signals. The Individual-Global-Max (IGM) condition is a widely used principle for multi-agent credit assignment, requiring that the joint action determined by individual Q-functions maximizes the global Q-value. Meanwhile, the principle of maximum entropy has been leveraged to enhance exploration in MARL. However, we identify a critical limitation in existing maximum entropy MARL methods: a misalignment arises between local policies and the joint policy that maximizes the global Q-value, leading to violations of the IGM condition. To address this misalignment, we propose an order-preserving transformation. Building on it, we introduce ME-IGM, a novel maximum entropy MARL algorithm compatible with any credit assignment mechanism that satisfies the IGM condition while enjoying the benefits of maximum entropy exploration. We empirically evaluate two variants of ME-IGM: ME-QMIX and ME-QPLEX, in non-monotonic matrix games, and demonstrate their state-of-the-art performance across 17 scenarios in SMAC-v2 and Overcooked.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ME-IGM：最大熵多智能体强化学习中的个体-全局最大化</div>
<div class="mono" style="margin-top:8px">多智能体信用分配是合作多智能体强化学习（MARL）中的一个基本挑战，其中一组智能体从共享的奖励信号中学习。个体-全局最大化（IGM）条件是多智能体信用分配中广泛使用的原则，要求由个体Q函数决定的联合动作最大化全局Q值。同时，最大熵原则已被用于增强MARL中的探索能力。然而，我们发现现有最大熵MARL方法存在一个关键限制：局部策略与最大化全局Q值的联合策略之间出现不匹配，导致IGM条件被违反。为了解决这种不匹配，我们提出了一种保持顺序的变换方法。在此基础上，我们引入了ME-IGM，一种新型的最大熵MARL算法，它兼容任何满足IGM条件的信用分配机制，同时享有最大熵探索的优势。我们在非单调矩阵游戏中对ME-IGM的两个变体ME-QMIX和ME-QPLEX进行了实证评估，并在SMAC-v2和Overcooked的17个场景中展示了其最先进的性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-agent credit assignment is a fundamental challenge for cooperative multi-agent reinforcement learning (MARL), where a team of agents learn from shared reward signals.</div>
</details>
</div>
<div class="card">
<div class="title">Perfect Network Resilience in Polynomial Time</div>
<div class="meta-line">Authors: Matthias Bentert, Stefan Schmid</div>
<div class="meta-line">First: 2026-02-03T18:34:22+00:00 · Latest: 2026-02-03T18:34:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03827v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03827v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern communication networks support local fast rerouting mechanisms to quickly react to link failures: nodes store a set of conditional rerouting rules which define how to forward an incoming packet in case of incident link failures. The rerouting decisions at any node $v$ must rely solely on local information available at $v$: the link from which a packet arrived at $v$, the target of the packet, and the incident link failures at $v$. Ideally, such rerouting mechanisms provide perfect resilience: any packet is routed from its source to its target as long as the two are connected in the underlying graph after the link failures. Already in their seminal paper at ACM PODC &#x27;12, Feigenbaum, Godfrey, Panda, Schapira, Shenker, and Singla showed that perfect resilience cannot always be achieved. While the design of local rerouting algorithms has received much attention since then, we still lack a detailed understanding of when perfect resilience is achievable.
  This paper closes this gap and presents a complete characterization of when perfect resilience can be achieved. This characterization also allows us to design an $O(n)$-time algorithm to decide whether a given instance is perfectly resilient and an $O(nm)$-time algorithm to compute perfectly resilient rerouting rules whenever it is. Our algorithm is also attractive for the simple structure of the rerouting rules it uses, known as skipping in the literature: alternative links are chosen according to an ordered priority list (per in-port), where failed links are simply skipped. Intriguingly, our result also implies that in the context of perfect resilience, skipping rerouting rules are as powerful as more general rerouting rules. This partially answers a long-standing open question by Chiesa, Nikolaevskiy, Mitrovic, Gurtov, Madry, Schapira, and Shenker [IEEE/ACM Transactions on Networking, 2017] in the affirmative.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多项式时间内实现完美的网络弹性</div>
<div class="mono" style="margin-top:8px">现代通信网络支持本地快速重路由机制以快速应对链路故障：节点存储一组条件重路由规则，定义在发生链路故障时如何转发到达的分组。任何节点 $v$ 的重路由决策必须仅依赖于 $v$ 处的本地信息：分组到达 $v$ 的链路、分组的目标以及 $v$ 处的故障链路。理想情况下，这样的重路由机制可以提供完美的弹性：只要源和目标在链路故障后仍连接于底层图中，任何分组都能被正确路由。早在2012年ACM PODC会议上，Feigenbaum、Godfrey、Panda、Schapira、Shenker和Singla在他们的开创性论文中就表明，完美的弹性并非总能实现。尽管自那以后，本地重路由算法的设计受到了广泛关注，但我们仍然缺乏对何时可以实现完美弹性的详细理解。
  本文填补了这一空白，提出了一个关于何时可以实现完美弹性的完整特征描述。该特征描述还使我们能够设计一个 $O(n)$ 时间的算法来判断给定实例是否具有完美的弹性，并在具有完美弹性的条件下设计一个 $O(nm)$ 时间的算法来计算重路由规则。我们的算法因其使用的重路由规则（在文献中称为跳过规则）的简单结构而具有吸引力：替代链路根据每个入端口的优先级列表进行选择，其中故障链路仅被跳过。有趣的是，我们的结果还表明，在完美弹性的背景下，跳过重路由规则的威力与更一般的重路由规则相当。这在一定程度上肯定了Chiesa、Nikolaevskiy、Mitrovic、Gurtov、Madry、Schapira和Shenker [IEEE/ACM 通信汇刊，2017] 长期存在的一个开放问题。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modern communication networks support local fast rerouting mechanisms to quickly react to link failures: nodes store a set of conditional rerouting rules which define how to forward an incoming packet in case of incident link failures.</div>
</details>
</div>
<div class="card">
<div class="title">Continuous Control of Editing Models via Adaptive-Origin Guidance</div>
<div class="meta-line">Authors: Alon Wolf, Chen Katzir, Kfir Aberman, Or Patashnik</div>
<div class="meta-line">First: 2026-02-03T18:33:39+00:00 · Latest: 2026-02-03T18:33:39+00:00</div>
<div class="meta-line">Comments: Project page at https://adaor-paper.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03826v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03826v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://adaor-paper.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion-based editing models have emerged as a powerful tool for semantic image and video manipulation. However, existing models lack a mechanism for smoothly controlling the intensity of text-guided edits. In standard text-conditioned generation, Classifier-Free Guidance (CFG) impacts prompt adherence, suggesting it as a potential control for edit intensity in editing models. However, we show that scaling CFG in these models does not produce a smooth transition between the input and the edited result. We attribute this behavior to the unconditional prediction, which serves as the guidance origin and dominates the generation at low guidance scales, while representing an arbitrary manipulation of the input content. To enable continuous control, we introduce Adaptive-Origin Guidance (AdaOr), a method that adjusts this standard guidance origin with an identity-conditioned adaptive origin, using an identity instruction corresponding to the identity manipulation. By interpolating this identity prediction with the standard unconditional prediction according to the edit strength, we ensure a continuous transition from the input to the edited result. We evaluate our method on image and video editing tasks, demonstrating that it provides smoother and more consistent control compared to current slider-based editing approaches. Our method incorporates an identity instruction into the standard training framework, enabling fine-grained control at inference time without per-edit procedure or reliance on specialized datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过自适应源引导实现编辑模型的连续控制</div>
<div class="mono" style="margin-top:8px">基于扩散的编辑模型已成为语义图像和视频操作的强大工具。然而，现有模型缺乏对文本引导编辑强度进行平滑控制的机制。在标准的文本条件生成中，无条件预测作为引导源，主导生成过程，但在低引导尺度下无法实现输入与编辑结果之间的平滑过渡。我们引入自适应源引导（AdaOr）方法，通过身份指令对应的自适应源调整标准引导源，根据编辑强度对身份预测与标准无条件预测进行插值，从而确保输入与编辑结果之间的连续过渡。我们在图像和视频编辑任务中评估了该方法，证明其相比当前基于滑块的编辑方法提供了更平滑、更一致的控制。我们的方法将身份指令纳入标准训练框架，使得推理时能够实现细粒度控制，而无需针对每次编辑进行特殊处理或依赖专门的数据集。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Diffusion-based editing models have emerged as a powerful tool for semantic image and video manipulation.</div>
</details>
</div>
<div class="card">
<div class="title">Robust Intervention Learning from Emergency Stop Interventions</div>
<div class="meta-line">Authors: Ethan Pronovost, Khimya Khetarpal, Siddhartha Srinivasa</div>
<div class="meta-line">First: 2026-02-03T18:33:21+00:00 · Latest: 2026-02-03T18:33:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03825v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03825v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human interventions are a common source of data in autonomous systems during testing. These interventions provide an important signal about where the current policy needs improvement, but are often noisy and incomplete. We define Robust Intervention Learning (RIL) as the problem of learning from intervention data while remaining robust to the quality and informativeness of the intervention signal. In the best case, interventions are precise and avoiding them is sufficient to solve the task, but in many realistic settings avoiding interventions is necessary but not sufficient for achieving good performance. We study robust intervention learning in the context of emergency stop interventions and propose Residual Intervention Fine-Tuning (RIFT), a residual fine-tuning algorithm that treats intervention feedback as an incomplete learning signal and explicitly combines it with a prior policy. By framing intervention learning as a fine-tuning problem, our approach leverages structure encoded in the prior policy to resolve ambiguity when intervention signals under-specify the task. We provide theoretical analysis characterizing conditions under which this formulation yields principled policy improvement, and identify regimes where intervention learning is expected to fail. Our experiments reveal that residual fine-tuning enables robust and consistent policy improvement across a range of intervention strategies and prior policy qualities, and highlight robust intervention learning as a promising direction for future work.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从紧急停止干预中学习鲁棒干预</div>
<div class="mono" style="margin-top:8px">人类干预是自主系统测试中常见的数据来源。这些干预提供了当前策略需要改进的重要信号，但往往存在噪声和不完整性。我们将鲁棒干预学习（RIL）定义为从干预数据中学习的问题，同时保持对干预信号质量和信息量的鲁棒性。在理想情况下，干预是精确的，避免干预就足以解决问题，但在许多现实场景中，避免干预是必要的，但并不充分。我们在紧急停止干预的背景下研究鲁棒干预学习，并提出了残差干预微调（RIFT）算法，该算法将干预反馈视为不完整的学习信号，并显式地将其与先验策略结合。通过将干预学习框架为微调问题，我们的方法利用先验策略中编码的结构来解决干预信号不足以明确任务时的歧义。我们提供了理论分析，刻画了该公式在何种条件下能够实现有原则的策略改进，并识别了干预学习可能失败的场景。实验表明，残差微调能够在多种干预策略和先验策略质量下实现稳健且一致的策略改进，并突出了鲁棒干预学习作为未来研究的有前景方向。</div>
</details>
</div>
<div class="card">
<div class="title">Deep-learning-based pan-phenomic data reveals the explosive evolution of avian visual disparity</div>
<div class="meta-line">Authors: Jiao Sun</div>
<div class="meta-line">First: 2026-02-03T18:32:15+00:00 · Latest: 2026-02-03T18:32:15+00:00</div>
<div class="meta-line">Comments: Readers from the field of computer science may be interested in section 2.1, 2.2, 3.1, 4.1, 4.2. These sections discussed the interpretability and representation learning, especially the texture vs shape problem, highlighting our model&#x27;s ability of overcoming the texture biases and capturing overall shape features. (Although they&#x27;re put here to prove the biological validity of the model.)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03824v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03824v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The evolution of biological morphology is critical for understanding the diversity of the natural world, yet traditional analyses often involve subjective biases in the selection and coding of morphological traits. This study employs deep learning techniques, utilising a ResNet34 model capable of recognising over 10,000 bird species, to explore avian morphological evolution. We extract weights from the model&#x27;s final fully connected (fc) layer and investigate the semantic alignment between the high-dimensional embedding space learned by the model and biological phenotypes. The results demonstrate that the high-dimensional embedding space encodes phenotypic convergence. Subsequently, we assess the morphological disparity among various taxa and evaluate the association between morphological disparity and species richness, demonstrating that species richness is the primary driver of morphospace expansion. Moreover, the disparity-through-time analysis reveals a visual &quot;early burst&quot; after the K-Pg extinction.
  While mainly aimed at evolutionary analysis, this study also provides insights into the interpretability of Deep Neural Networks. We demonstrate that hierarchical semantic structures (biological taxonomy) emerged in the high-dimensional embedding space despite being trained on flat labels. Furthermore, through adversarial examples, we provide evidence that our model in this task can overcome texture bias and learn holistic shape representations (body plans), challenging the prevailing view that CNNs rely primarily on local textures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于深度学习的全表型数据揭示了鸟类视觉差异的爆发式演化</div>
<div class="mono" style="margin-top:8px">生物形态的演化对于理解自然界的多样性至关重要，但传统分析方法在形态特征的选择和编码过程中常带有主观偏见。本研究采用深度学习技术，利用能够识别超过10,000种鸟类的ResNet34模型，探讨鸟类形态的演化。我们提取模型最后一层全连接（fc）层的权重，并研究模型所学的高维嵌入空间与生物表型之间的语义对齐。结果表明，高维嵌入空间编码了表型趋同。随后，我们评估了不同类群的形态差异，并分析了形态差异与物种丰富度之间的关联，证明物种丰富度是形态空间扩展的主要驱动因素。此外，通过差异随时间变化的分析，我们发现白垩纪-古近纪（K-Pg）灭绝之后出现了一种视觉上的&quot;早期爆发&quot;。尽管本研究主要针对演化分析，但也为深度神经网络的可解释性提供了见解。我们展示了尽管模型是基于扁平标签训练的，但其高维嵌入空间中仍出现了层次化的语义结构（生物分类学）。此外，通过对抗样本，我们提供了证据表明该模型在本任务中能够克服纹理偏见，学习整体形状表示（身体计划），挑战了CNN主要依赖局部纹理的主流观点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The evolution of biological morphology is critical for understanding the diversity of the natural world, yet traditional analyses often involve subjective biases in the selection and coding of morphological traits.</div>
</details>
</div>
<div class="card">
<div class="title">Preference-based Conditional Treatment Effects and Policy Learning</div>
<div class="meta-line">Authors: Dovid Parnas, Mathieu Even, Julie Josse, Uri Shalit</div>
<div class="meta-line">First: 2026-02-03T18:31:26+00:00 · Latest: 2026-02-03T18:31:26+00:00</div>
<div class="meta-line">Comments: Accepted to AISTATS 2026; 10 pages + appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03823v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03823v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce a new preference-based framework for conditional treatment effect estimation and policy learning, built on the Conditional Preference-based Treatment Effect (CPTE). CPTE requires only that outcomes be ranked under a preference rule, unlocking flexible modeling of heterogeneous effects with multivariate, ordinal, or preference-driven outcomes. This unifies applications such as conditional probability of necessity and sufficiency, conditional Win Ratio, and Generalized Pairwise Comparisons. Despite the intrinsic non-identifiability of comparison-based estimands, CPTE provides interpretable targets and delivers new identifiability conditions for previous unidentifiable estimands. We present estimation strategies via matching, quantile, and distributional regression, and further design efficient influence-function estimators to correct plug-in bias and maximize policy value. Synthetic and semi-synthetic experiments demonstrate clear performance gains and practical impact.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于偏好的条件处理效应与政策学习</div>
<div class="mono" style="margin-top:8px">我们引入了一种基于偏好的新框架，用于条件处理效应估计和政策学习，该框架建立在条件偏好处理效应（CPTE）之上。CPTE仅要求在偏好规则下对结果进行排序，从而解锁了对异质处理效应的灵活建模，适用于多变量、序数或由偏好驱动的结果。该框架统一了诸如条件必要性与充分性概率、条件Win Ratio和广义成对比较等应用。尽管基于比较的估计量本质上是非可识别的，CPTE提供了可解释的目标，并为之前不可识别的估计量提供了新的可识别条件。我们通过匹配、分位数和分布回归提出了估计策略，并进一步设计了高效的影响力函数估计器以校正插件偏差并最大化政策价值。合成和半合成实验展示了显著的性能提升和实际影响。</div>
</details>
</div>
<div class="card">
<div class="title">They Said Memes Were Harmless-We Found the Ones That Hurt: Decoding Jokes, Symbols, and Cultural References</div>
<div class="meta-line">Authors: Sahil Tripathi, Gautam Siddharth Kashyap, Mehwish Nasim, Jian Yang, Jiechao Gao, Usman Naseem</div>
<div class="meta-line">Venue: The Web Conference 2026</div>
<div class="meta-line">First: 2026-02-03T18:29:46+00:00 · Latest: 2026-02-03T18:29:46+00:00</div>
<div class="meta-line">Comments: Accepted at the The Web Conference 2026 (Research Track)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03822v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03822v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Meme-based social abuse detection is challenging because harmful intent often relies on implicit cultural symbolism and subtle cross-modal incongruence. Prior approaches, from fusion-based methods to in-context learning with Large Vision-Language Models (LVLMs), have made progress but remain limited by three factors: i) cultural blindness (missing symbolic context), ii) boundary ambiguity (satire vs. abuse confusion), and iii) lack of interpretability (opaque model reasoning). We introduce CROSS-ALIGN+, a three-stage framework that systematically addresses these limitations: (1) Stage I mitigates cultural blindness by enriching multimodal representations with structured knowledge from ConceptNet, Wikidata, and Hatebase; (2) Stage II reduces boundary ambiguity through parameter-efficient LoRA adapters that sharpen decision boundaries; and (3) Stage III enhances interpretability by generating cascaded explanations. Extensive experiments on five benchmarks and eight LVLMs demonstrate that CROSS-ALIGN+ consistently outperforms state-of-the-art methods, achieving up to 17% relative F1 improvement while providing interpretable justifications for each decision.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>他们说表情包无害-我们发现了那些有害的：解码笑话、符号和文化引用</div>
<div class="mono" style="margin-top:8px">基于表情包的社会滥用检测具有挑战性，因为有害意图常常依赖于隐含的文化象征和微妙的跨模态不一致。以往的方法，从融合方法到使用大型视觉-语言模型（LVLMs）的上下文学习，虽有进展，但仍受三个因素限制：i）文化盲视（缺乏象征语境），ii）边界模糊（讽刺与滥用的混淆），iii）缺乏可解释性（模型推理过程不透明）。我们引入了CROSS-ALIGN+，一个三阶段框架，系统性地解决了这些限制：（1）第一阶段通过从ConceptNet、Wikidata和Hatebase中获取结构化知识来丰富多模态表示，从而缓解文化盲视；（2）第二阶段通过参数高效的LoRA适配器减少边界模糊，从而增强决策边界；（3）第三阶段通过生成级联解释来提升可解释性。在五个基准数据集和八个LVLMs上的大量实验表明，CROSS-ALIGN+在性能上持续优于最先进的方法，实现了最高达17%的相对F1提升，同时为每个决策提供可解释的依据。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Evidence Weighting for Audio-Spatiotemporal Fusion</div>
<div class="meta-line">Authors: Oscar Ovanger, Levi Harris, Timothy H. Keitt</div>
<div class="meta-line">First: 2026-02-03T18:21:13+00:00 · Latest: 2026-02-03T18:21:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03817v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03817v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs. In bioacoustic classification, species identity may be inferred both from the acoustic signal and from spatiotemporal context such as location and season; while Bayesian inference motivates multiplicative evidence combination, in practice we typically only have access to discriminative predictors rather than calibrated generative models. We introduce \textbf{F}usion under \textbf{IN}dependent \textbf{C}onditional \textbf{H}ypotheses (\textbf{FINCH}), an adaptive log-linear evidence fusion framework that integrates a pre-trained audio classifier with a structured spatiotemporal predictor. FINCH learns a per-sample gating function that estimates the reliability of contextual information from uncertainty and informativeness statistics. The resulting fusion family \emph{contains} the audio-only classifier as a special case and explicitly bounds the influence of contextual evidence, yielding a risk-contained hypothesis class with an interpretable audio-only fallback. Across benchmarks, FINCH consistently outperforms fixed-weight fusion and audio-only baselines, improving robustness and error trade-offs even when contextual information is weak in isolation. We achieve state-of-the-art performance on CBI and competitive or improved performance on several subsets of BirdSet using a lightweight, interpretable, evidence-based approach. Code is available: \texttt{\href{https://anonymous.4open.science/r/birdnoise-85CD/README.md}{anonymous-repository}}</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于音频-时空融合的自适应证据加权</div>
<div class="mono" style="margin-top:8px">许多机器学习系统可以获取同一预测目标的多个证据来源，但这些来源在不同输入上的可靠性和信息量往往不同。在生物声学分类中，物种身份可以从音频信号以及诸如位置和季节等时空上下文中推断出来；虽然贝叶斯推断鼓励乘法证据组合，但在实践中我们通常只能访问判别性预测器，而无法获得校准的生成模型。我们引入了\textbf{F}usion under \textbf{IN}dependent \textbf{C}onditional \textbf{H}ypotheses（\textbf{FINCH}），这是一种自适应对数线性证据融合框架，将预训练的音频分类器与结构化的时空预测器相结合。FINCH 学习一个样本级的门控函数，通过不确定性与信息量统计量来估计上下文信息的可靠性。最终的融合模型\emph{包含}仅音频分类器作为特例，并显式地限制了上下文证据的影响，从而产生一个具有可解释性的仅音频回退机制的风险可控假设类。在多个基准测试中，FINCH 一致优于固定权重融合和仅音频基线模型，即使在孤立上下文信息较弱的情况下，也能提升鲁棒性和误差权衡。我们采用一种轻量、可解释、基于证据的方法，在CBI数据集上实现了最先进的性能，并在BirdSet的多个子集上取得了具有竞争力或改进的性能。代码已发布：\texttt{\href{https://anonymous.4open.science/r/birdnoise-85CD/README.md}{anonymous-repository}}</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Many machine learning systems have access to multiple sources of evidence for the same prediction target, yet these sources often differ in reliability and informativeness across inputs.</div>
</details>
</div>
<div class="card">
<div class="title">SymPlex: A Structure-Aware Transformer for Symbolic PDE Solving</div>
<div class="meta-line">Authors: Yesom Park, Annie C. Lu, Shao-Ching Huang, Qiyang Hu, Y. Sungtaek Ju, Stanley Osher</div>
<div class="meta-line">First: 2026-02-03T18:18:30+00:00 · Latest: 2026-02-03T18:18:30+00:00</div>
<div class="meta-line">Comments: 27 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03816v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03816v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose SymPlex, a reinforcement learning framework for discovering analytical symbolic solutions to partial differential equations (PDEs) without access to ground-truth expressions. SymPlex formulates symbolic PDE solving as tree-structured decision-making and optimizes candidate solutions using only the PDE and its boundary conditions. At its core is SymFormer, a structure-aware Transformer that models hierarchical symbolic dependencies via tree-relative self-attention and enforces syntactic validity through grammar-constrained autoregressive decoding, overcoming the limited expressivity of sequence-based generators. Unlike numerical and neural approaches that approximate solutions in discretized or implicit function spaces, SymPlex operates directly in symbolic expression space, enabling interpretable and human-readable solutions that naturally represent non-smooth behavior and explicit parametric dependence. Empirical results demonstrate exact recovery of non-smooth and parametric PDE solutions using deep learning-based symbolic methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SymPlex：一种面向结构的Transformer用于符号偏微分方程求解</div>
<div class="mono" style="margin-top:8px">我们提出SymPlex，这是一种强化学习框架，用于在不访问真实表达式的情况下发现偏微分方程（PDE）的解析符号解。SymPlex将符号PDE求解建模为树结构的决策过程，并仅利用PDE及其边界条件优化候选解。其核心是SymFormer，一种结构感知的Transformer，通过树相对自注意力建模层次化的符号依赖关系，并通过语法约束的自回归解码确保语法有效性，克服了基于序列的生成器表达能力有限的问题。与在离散化或隐式函数空间中近似求解的数值和神经方法不同，SymPlex直接在符号表达空间中操作，从而实现可解释且易于人类阅读的解，自然地表示非光滑行为和显式的参数依赖关系。实证结果表明，使用基于深度学习的符号方法可以精确恢复非光滑和参数化的PDE解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We propose SymPlex, a reinforcement learning framework for discovering analytical symbolic solutions to partial differential equations (PDEs) without access to ground-truth expressions.</div>
</details>
</div>
<div class="card">
<div class="title">Fast-Slow Efficient Training for Multimodal Large Language Models via Visual Token Pruning</div>
<div class="meta-line">Authors: Dingkun Zhang, Shuhan Qi, Yulin Wu, Xinyu Xiao, Xuan Wang, Long Chen</div>
<div class="meta-line">First: 2026-02-03T18:18:11+00:00 · Latest: 2026-02-03T18:18:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03815v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03815v1">PDF</a> · <a href="https://github.com/dingkun-zhang/DualSpeed">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) suffer from severe training inefficiency issue, which is associated with their massive model sizes and visual token numbers. Existing efforts in efficient training focus on reducing model sizes or trainable parameters. Inspired by the success of Visual Token Pruning (VTP) in improving inference efficiency, we are exploring another substantial research direction for efficient training by reducing visual tokens. However, applying VTP at the training stage results in a training-inference mismatch: pruning-trained models perform poorly when inferring on non-pruned full visual token sequences. To close this gap, we propose DualSpeed, a fast-slow framework for efficient training of MLLMs. The fast-mode is the primary mode, which incorporates existing VTP methods as plugins to reduce visual tokens, along with a mode isolator to isolate the model&#x27;s behaviors. The slow-mode is the auxiliary mode, where the model is trained on full visual sequences to retain training-inference consistency. To boost its training, it further leverages self-distillation to learn from the sufficiently trained fast-mode. Together, DualSpeed can achieve both training efficiency and non-degraded performance. Experiments show DualSpeed accelerates the training of LLaVA-1.5 by 2.1$\times$ and LLaVA-NeXT by 4.0$\times$, retaining over 99% performance. Code: https://github.com/dingkun-zhang/DualSpeed</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过视觉标记修剪实现多模态大语言模型的快速-慢速高效训练</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）面临严重的训练效率问题，这与其庞大的模型规模和视觉标记数量有关。现有的高效训练方法主要集中在减少模型规模或可训练参数上。受视觉标记修剪（VTP）在提升推理效率方面的成功启发，我们探索了通过减少视觉标记来提高训练效率的另一重要研究方向。然而，在训练阶段应用VTP会导致训练-推理不匹配：修剪训练的模型在推理非修剪的完整视觉标记序列时表现不佳。为弥合这一差距，我们提出了DualSpeed，一种用于MLLMs高效训练的快速-慢速框架。快速模式是主要模式，它将现有的VTP方法作为插件以减少视觉标记，并通过模式隔离器隔离模型行为。慢速模式是辅助模式，其中模型在完整视觉序列上进行训练以保持训练-推理的一致性。为了提升其训练效果，它进一步利用自蒸馏技术从已充分训练的快速模式中学习。结合两者，DualSpeed能够在保持性能的同时实现高效的训练。实验表明，DualSpeed将LLaVA-1.5的训练速度提升了2.1倍，将LLaVA-NeXT的训练速度提升了4.0倍，同时保持超过99%的性能。代码：https://github.com/dingkun-zhang/DualSpeed</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) suffer from severe training inefficiency issue, which is associated with their massive model sizes and visual token numbers.</div>
</details>
</div>
<div class="card">
<div class="title">Conformal Thinking: Risk Control for Reasoning on a Compute Budget</div>
<div class="meta-line">Authors: Xi Wang, Anushri Suresh, Alvin Zhang, Rishi More, William Jurayj, Benjamin Van Durme, Mehrdad Farajtabar, Daniel Khashabi, Eric Nalisnick</div>
<div class="meta-line">First: 2026-02-03T18:17:22+00:00 · Latest: 2026-02-03T18:17:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03814v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03814v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning Large Language Models (LLMs) enable test-time scaling, with dataset-level accuracy improving as the token budget increases, motivating adaptive reasoning -- spending tokens when they improve reliability and stopping early when additional computation is unlikely to help. However, setting the token budget, as well as the threshold for adaptive reasoning, is a practical challenge that entails a fundamental risk-accuracy trade-off. We re-frame the budget setting problem as risk control, limiting the error rate while minimizing compute. Our framework introduces an upper threshold that stops reasoning when the model is confident (risking incorrect output) and a novel parametric lower threshold that preemptively stops unsolvable instances (risking premature stoppage). Given a target risk and a validation set, we use distribution-free risk control to optimally specify these stopping mechanisms. For scenarios with multiple budget controlling criteria, we incorporate an efficiency loss to select the most computationally efficient exiting mechanism. Empirical results across diverse reasoning tasks and models demonstrate the effectiveness of our risk control approach, demonstrating computational efficiency gains from the lower threshold and ensemble stopping mechanisms while adhering to the user-specified risk target.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>共形思维：基于计算预算的推理风险控制</div>
<div class="mono" style="margin-top:8px">推理大型语言模型（LLMs）能够实现测试时的扩展，随着token预算的增加，数据集级别的准确性也随之提升，这促使了自适应推理的发展——在token能提升可靠性时使用，在额外计算可能无益时提前停止。然而，设置token预算以及自适应推理的阈值是一个实际挑战，涉及根本的风险-准确性权衡。我们将预算设置问题重新框架为风险控制问题，在限制错误率的同时最小化计算资源。我们的框架引入了一个上限阈值，当模型足够自信时停止推理（可能产生错误输出），以及一个新颖的参数化下限阈值，提前停止无法解决的实例（可能造成过早终止）。在给定目标风险和验证集的情况下，我们使用无分布依赖的风险控制方法来最优地指定这些停止机制。对于具有多个预算控制标准的场景，我们引入效率损失以选择最计算高效的退出机制。在多种推理任务和模型上的实证结果展示了我们风险控制方法的有效性，同时在保持用户指定的风险目标的前提下，实现了计算效率的提升。</div>
</details>
</div>
<div class="card">
<div class="title">Antidistillation Fingerprinting</div>
<div class="meta-line">Authors: Yixuan Even Xu, John Kirchenbauer, Yash Savani, Asher Trockman, Alexander Robey, Tom Goldstein, Fei Fang, J. Zico Kolter</div>
<div class="meta-line">First: 2026-02-03T18:15:50+00:00 · Latest: 2026-02-03T18:15:50+00:00</div>
<div class="meta-line">Comments: 26 pages, 11 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03812v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03812v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Model distillation enables efficient emulation of frontier large language models (LLMs), creating a need for robust mechanisms to detect when a third-party student model has trained on a teacher model&#x27;s outputs. However, existing fingerprinting techniques that could be used to detect such distillation rely on heuristic perturbations that impose a steep trade-off between generation quality and fingerprinting strength, often requiring significant degradation of utility to ensure the fingerprint is effectively internalized by the student. We introduce antidistillation fingerprinting (ADFP), a principled approach that aligns the fingerprinting objective with the student&#x27;s learning dynamics. Building upon the gradient-based framework of antidistillation sampling, ADFP utilizes a proxy model to identify and sample tokens that directly maximize the expected detectability of the fingerprint in the student after fine-tuning, rather than relying on the incidental absorption of the un-targeted biases of a more naive watermark. Experiments on GSM8K and OASST1 benchmarks demonstrate that ADFP achieves a significant Pareto improvement over state-of-the-art baselines, yielding stronger detection confidence with minimal impact on utility, even when the student model&#x27;s architecture is unknown.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>反蒸馏指纹识别</div>
<div class="mono" style="margin-top:8px">模型蒸馏能够高效地模拟前沿的大语言模型（LLMs），因此需要稳健的机制来检测第三方学生模型是否基于教师模型的输出进行训练。然而，现有的指纹识别技术在检测此类蒸馏时依赖启发式扰动，这在生成质量与指纹识别强度之间造成了显著的权衡，通常需要显著降低实用性以确保指纹被学生模型有效内化。我们引入了反蒸馏指纹识别（ADFP），这是一种将指纹识别目标与学生学习动态对齐的原则性方法。基于反蒸馏采样的梯度框架，ADFP利用代理模型来识别并采样那些在微调后能直接最大化指纹在学生模型中可检测性的标记，而不是依赖于更简单水印模型中非目标偏差的偶然吸收。在GSM8K和OASST1基准上的实验表明，ADFP在实用性影响最小的情况下，显著优于现有最先进的基线方法，实现了更强的检测置信度，即使学生模型的架构未知。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Model distillation enables efficient emulation of frontier large language models (LLMs), creating a need for robust mechanisms to detect when a third-party student model has trained on a teacher model&#x27;s outputs.</div>
</details>
</div>
<div class="card">
<div class="title">Progressive Checkerboards for Autoregressive Multiscale Image Generation</div>
<div class="meta-line">Authors: David Eigen</div>
<div class="meta-line">First: 2026-02-03T18:15:27+00:00 · Latest: 2026-02-03T18:15:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03811v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03811v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A key challenge in autoregressive image generation is to efficiently sample independent locations in parallel, while still modeling mutual dependencies with serial conditioning. Some recent works have addressed this by conditioning between scales in a multiscale pyramid. Others have looked at parallelizing samples in a single image using regular partitions or randomized orders. In this work we examine a flexible, fixed ordering based on progressive checkerboards for multiscale autoregressive image generation. Our ordering draws samples in parallel from evenly spaced regions at each scale, maintaining full balance in all levels of a quadtree subdivision at each step. This enables effective conditioning both between and within scales. Intriguingly, we find evidence that in our balanced setting, a wide range of scale-up factors lead to similar results, so long as the total number of serial steps is constant. On class-conditional ImageNet, our method achieves competitive performance compared to recent state-of-the-art autoregressive systems with like model capacity, using fewer sampling steps.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>渐进式棋盘格用于自回归多尺度图像生成</div>
<div class="mono" style="margin-top:8px">自回归图像生成中的一个关键挑战是在并行采样独立位置的同时，仍能通过序列条件建模相互依赖关系。一些近期的工作通过多尺度金字塔中的尺度间条件来解决这一问题。其他工作则尝试在单张图像中使用规则分区或随机顺序来并行化采样。在本工作中，我们研究了一种基于渐进式棋盘格的灵活且固定顺序方法，用于多尺度自回归图像生成。我们的顺序在每个尺度上从均匀分布的区域并行采样，每一步都保持四叉树细分的所有层级的完全平衡。这使得在尺度之间和尺度内部都能有效建模条件关系。有趣的是，我们发现，在我们的平衡设置下，只要总序列步骤数保持不变，广泛的尺度扩展因子都能导致相似的结果。在类别条件下的ImageNet数据集上，我们的方法在使用更少采样步骤的情况下，与具有类似模型容量的近期最先进的自回归系统相比表现出竞争力。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Imbalanced Node Classification via Curriculum-Guided Feature Learning and Three-Stage Attention Network</div>
<div class="meta-line">Authors: Abdul Joseph Fofanah, Lian Wen, David Chen, Shaoyang Zhang</div>
<div class="meta-line">First: 2026-02-03T18:10:40+00:00 · Latest: 2026-02-03T18:10:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03808v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03808v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Imbalanced node classification in graph neural networks (GNNs) happens when some labels are much more common than others, which causes the model to learn unfairly and perform badly on the less common classes. To solve this problem, we propose a Curriculum-Guided Feature Learning and Three-Stage Attention Network (CL3AN-GNN), a learning network that uses a three-step attention system (Engage, Enact, Embed) similar to how humans learn. The model begins by engaging with structurally simpler features, defined as (1) local neighbourhood patterns (1-hop), (2) low-degree node attributes, and (3) class-separable node pairs identified via initial graph convolutional networks and graph attention networks (GCN and GAT) embeddings. This foundation enables stable early learning despite label skew. The Enact stage then addresses complicated aspects: (1) connections that require multiple steps, (2) edges that connect different types of nodes, and (3) nodes at the edges of minority classes by using adjustable attention weights. Finally, Embed consolidates these features via iterative message passing and curriculum-aligned loss weighting. We evaluate CL3AN-GNN on eight Open Graph Benchmark datasets spanning social, biological, and citation networks. Experiments show consistent improvements across all datasets in accuracy, F1-score, and AUC over recent state-of-the-art methods. The model&#x27;s step-by-step method works well with different types of graph datasets, showing quicker results than training everything at once, better performance on new, imbalanced graphs, and clear explanations of each step using gradient stability and attention correlation learning curves. This work provides both a theoretically grounded framework for curriculum learning in GNNs and practical evidence of its effectiveness against imbalances, validated through metrics, convergence speeds, and generalisation tests.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过课程引导特征学习和三阶段注意力网络增强图神经网络中的不平衡节点分类</div>
<div class="mono" style="margin-top:8px">在图神经网络（GNNs）中，不平衡节点分类是指某些标签远比其他标签更常见，这会导致模型学习过程不公平，并在较少见的类别上表现不佳。为了解决这一问题，我们提出了一种课程引导特征学习和三阶段注意力网络（CL3AN-GNN），该网络采用类似人类学习的三步注意力系统（Engage, Enact, Embed）。模型首先通过结构更简单的特征进行学习，这些特征包括（1）局部邻域模式（1跳），（2）低度节点属性，以及（3）通过初始图卷积网络和图注意力网络（GCN 和 GAT）嵌入识别出的类别可区分节点对。这为在标签偏倚下实现稳定的早期学习奠定了基础。Enact 阶段则处理更复杂的方面：（1）需要多步的连接，（2）连接不同类型节点的边，以及（3）少数类别边缘节点，通过使用可调整的注意力权重。最后，Embed 阶段通过迭代的消息传递和课程对齐的损失加权来整合这些特征。我们在八个Open Graph Benchmark数据集上评估了CL3AN-GNN，这些数据集涵盖社交、生物和引用网络。实验结果表明，在所有数据集上，CL3AN-GNN在准确率、F1分数和AUC指标上均优于最近的最先进方法。该模型的逐步方法在不同类型的图数据集上表现良好，相较于一次性训练，能够更快地得到结果，在新的不平衡图上表现更好，并且通过梯度稳定性和注意力相关性学习曲线清晰地解释了每一步。本工作为图神经网络中的课程学习提供了一个理论基础坚实的框架，并通过指标、收敛速度和泛化测试提供了其实效性的实证支持。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Imbalanced node classification in graph neural networks (GNNs) happens when some labels are much more common than others, which causes the model to learn unfairly and perform badly on the less common classes.</div>
</details>
</div>
<div class="card">
<div class="title">Bridging Online and Offline RL: Contextual Bandit Learning for Multi-Turn Code Generation</div>
<div class="meta-line">Authors: Ziru Chen, Dongdong Chen, Ruinan Jin, Yingbin Liang, Yujia Xie, Huan Sun</div>
<div class="meta-line">First: 2026-02-03T18:08:41+00:00 · Latest: 2026-02-03T18:08:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03806v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03806v1">PDF</a> · <a href="https://github.com/OSU-NLP-Group/cobalt">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation. While online RL tends to perform better than offline RL, its higher training cost and instability hinders wide adoption. In this paper, we build on the observation that multi-turn code generation can be formulated as a one-step recoverable Markov decision process and propose contextual bandit learning with offline trajectories (Cobalt), a new method that combines the benefits of online and offline RL. Cobalt first collects code generation trajectories using a reference LLM and divides them into partial trajectories as contextual prompts. Then, during online bandit learning, the LLM is trained to complete each partial trajectory prompt through single-step code generation. Cobalt outperforms two multi-turn online RL baselines based on GRPO and VeRPO, and substantially improves R1-Distill 8B and Qwen3 8B by up to 9.0 and 6.2 absolute Pass@1 scores on LiveCodeBench. Also, we analyze LLMs&#x27; in-context reward hacking behaviors and augment Cobalt training with perturbed trajectories to mitigate this issue. Overall, our results demonstrate Cobalt as a promising solution for iterative decision-making tasks like multi-turn code generation. Our code and data are available at https://github.com/OSU-NLP-Group/cobalt.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>连接在线与离线强化学习：用于多轮代码生成的上下文老虎机学习</div>
<div class="mono" style="margin-top:8px">最近，训练大型语言模型（LLMs）在现实任务（如多轮代码生成）上使用强化学习（RL）引起了广泛关注。虽然在线RL通常优于离线RL，但其较高的训练成本和不稳定性阻碍了其广泛应用。在本文中，我们基于多轮代码生成可以被建模为一步可恢复的马尔可夫决策过程的观察，提出了一种结合在线和离线RL优势的新方法——基于离线轨迹的上下文老虎机学习（Cobalt）。Cobalt首先使用参考LLM收集代码生成轨迹，并将这些轨迹分割为部分轨迹作为上下文提示。然后，在在线老虎机学习过程中，LLM被训练以通过单步代码生成完成每个部分轨迹提示。在LiveCodeBench上，Cobalt在两个多轮在线RL基线（基于GRPO和VeRPO）的基础上表现更优，并在R1-Distill 8B和Qwen3 8B上分别提升了高达9.0和6.2的绝对Pass@1得分。此外，我们分析了LLMs在上下文中的奖励欺骗行为，并通过扰动轨迹增强Cobalt的训练以缓解这一问题。总体而言，我们的结果表明Cobalt是多轮代码生成等迭代决策任务的有前景解决方案。我们的代码和数据可在https://github.com/OSU-NLP-Group/cobalt获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recently, there have been significant research interests in training large language models (LLMs) with reinforcement learning (RL) on real-world tasks, such as multi-turn code generation.</div>
</details>
</div>
<div class="card">
<div class="title">Measuring Agents in Production</div>
<div class="meta-line">Authors: Melissa Z. Pan, Negar Arabzadeh, Riccardo Cogo, Yuxuan Zhu, Alexander Xiong, Lakshya A Agrawal, Huanzhi Mao, Emma Shen, Sid Pallerla, Liana Patel, Shu Liu, Tianneng Shi, Xiaoyuan Liu, Jared Quincy Davis, Emmanuele Lacavalla, Alessandro Basile, Shuyi Yang, Paul Castro, Daniel Kang, Joseph E. Gonzalez, Koushik Sen, Dawn Song, Ion Stoica, Matei Zaharia, Marquita Ellis</div>
<div class="meta-line">First: 2025-12-02T16:45:10+00:00 · Latest: 2026-02-03T18:06:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.04123v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.04123v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based agents already operate in production across many industries, yet we lack an understanding of what technical methods make deployments successful. We present the first systematic study of Measuring Agents in Production, MAP, using first-hand data from agent developers. We conducted 20 case studies via in-depth interviews and surveyed 306 practitioners across 26 domains. We investigate why organizations build agents, how they build them, how they evaluate them, and their top development challenges. Our study finds that production agents are built using simple, controllable approaches: 68% execute at most 10 steps before human intervention, 70% rely on prompting off-the-shelf models instead of weight tuning, and 74% depend primarily on human evaluation. Reliability (consistent correct behavior over time) remains the top development challenge, which practitioners currently address through systems-level design. MAP documents the current state of production agents, providing the research community with visibility into deployment realities and under-explored research avenues.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生产中的智能体度量</div>
<div class="mono" style="margin-top:8px">基于大语言模型的智能体已经在许多行业中投入生产使用，但我们缺乏对哪些技术方法使部署成功的基本理解。我们通过第一手数据，提出了首个关于生产中智能体度量（MAP）的系统性研究。我们通过深入访谈进行了20个案例研究，并对26个领域中的306名从业者进行了调查。我们探讨了组织为何构建智能体、如何构建、如何评估以及他们面临的主要开发挑战。研究发现，生产智能体主要采用简单且可控的方法：68%的智能体在人类干预前最多执行10个步骤，70%依赖提示词调用现成模型而非权重调优，74%主要依赖人工评估。可靠性（随时间保持一致正确行为）仍然是主要的开发挑战，从业者目前主要通过系统级设计来应对。MAP记录了当前生产智能体的状态，为研究社区提供了对部署现实和尚未充分探索的研究方向的可见性。</div>
</details>
</div>
<div class="card">
<div class="title">Prediction of Critical Heat Flux in Rod Bundles Using Tube-Based Hybrid Machine Learning Models in CTF</div>
<div class="meta-line">Authors: Aidan Furlong, Robert Salko, Xingang Zhao, Xu Wu</div>
<div class="meta-line">First: 2026-02-03T18:05:16+00:00 · Latest: 2026-02-03T18:05:16+00:00</div>
<div class="meta-line">Comments: Submitted to the 2026 American Nuclear Society Annual Meeting</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03805v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03805v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The prediction of critical heat flux (CHF) using machine learning (ML) approaches has become a highly active research activity in recent years, the goal of which is to build models more accurate than current conventional approaches such as empirical correlations or lookup tables (LUTs). Previous work developed and deployed tube-based pure and hybrid ML models in the CTF subchannel code, however, full-scale reactor core simulations require the use of rod bundle geometries. Unlike isolated subchannels, rod bundles experience complex thermal hydraulic phenomena such as channel crossflow, spacer grid losses, and effects from unheated conductors. This study investigates the generalization of ML-based CHF prediction models in rod bundles after being trained on tube-based CHF data. A purely data-driven DNN and two hybrid bias-correction models were implemented in the CTF subchannel code and used to predict CHF location and magnitude in the Combustion Engineering 5-by-5 bundle CHF test series. The W-3 correlation, Bowring correlation, and Groeneveld LUT were used as baseline comparators. On average, all three ML-based approaches produced magnitude and location predictions more accurate than the baseline models, with the hybrid LUT model exhibiting the most favorable performance metrics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用基于管的混合机器学习模型在CTF中预测棒束临界热流</div>
<div class="mono" style="margin-top:8px">近年来，利用机器学习（ML）方法预测临界热流（CHF）已成为高度活跃的研究领域，其目标是构建比当前传统方法（如经验关联式或查找表（LUTs））更精确的模型。之前的工作已在CTF子通道代码中开发并部署了基于管的纯ML和混合ML模型，然而，全规模反应堆核心模拟需要使用棒束几何结构。与孤立子通道不同，棒束会经历复杂的热流现象，如通道交叉流、栅格损失以及未加热导体的影响。本研究探讨了在基于管的CHF数据训练后，ML基CHF预测模型在棒束中的泛化能力。纯数据驱动的深度神经网络（DNN）和两个混合偏差校正模型被应用于CTF子通道代码，用于预测Combustion Engineering 5×5棒束CHF测试系列中的CHF位置和数值。W-3关联式、Bowring关联式和Groeneveld查找表被用作基准比较器。平均而言，所有三种基于ML的方法在数值和位置预测上都比基准模型更准确，其中混合查找表模型表现出最佳的性能指标。</div>
</details>
</div>
<div class="card">
<div class="title">Do We Need Asynchronous SGD? On the Near-Optimality of Synchronous Methods</div>
<div class="meta-line">Authors: Grigory Begunov, Alexander Tyurin</div>
<div class="meta-line">First: 2026-02-03T18:02:14+00:00 · Latest: 2026-02-03T18:02:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03802v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03802v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern distributed optimization methods mostly rely on traditional synchronous approaches, despite substantial recent progress in asynchronous optimization. We revisit Synchronous SGD and its robust variant, called $m$-Synchronous SGD, and theoretically show that they are nearly optimal in many heterogeneous computation scenarios, which is somewhat unexpected. We analyze the synchronous methods under random computation times and adversarial partial participation of workers, and prove that their time complexities are optimal in many practical regimes, up to logarithmic factors. While synchronous methods are not universal solutions and there exist tasks where asynchronous methods may be necessary, we show that they are sufficient for many modern heterogeneous computation scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>我们需要异步SGD吗？关于同步方法的近最优性</div>
<div class="mono" style="margin-top:8px">尽管在异步优化方面取得了显著进展，现代分布式优化方法大多仍依赖传统的同步方法。我们重新审视同步SGD及其稳健变体m-同步SGD，并在理论上证明它们在许多异构计算场景中几乎是最佳的，这在某种程度上是令人意外的。我们在随机计算时间和对抗性部分参与的工况下分析同步方法，并证明其时间复杂度在许多实际范围内是最佳的，仅差对数因子。虽然同步方法并非万能解决方案，且存在某些任务需要异步方法，但我们证明它们足以应对许多现代异构计算场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modern distributed optimization methods mostly rely on traditional synchronous approaches, despite substantial recent progress in asynchronous optimization.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260204_0421.html">20260204_0421</a>
<a href="archive/20260202_0402.html">20260202_0402</a>
<a href="archive/20260201_0354.html">20260201_0354</a>
<a href="archive/20260131_0408.html">20260131_0408</a>
<a href="archive/20260130_0406.html">20260130_0406</a>
<a href="archive/20260129_0407.html">20260129_0407</a>
<a href="archive/20260128_0407.html">20260128_0407</a>
<a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
