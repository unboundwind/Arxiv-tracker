<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-19 04:19</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260219_0419</div>
    <div class="row"><div class="card">
<div class="title">Ensemble-size-dependence of deep-learning post-processing methods that minimize an (un)fair score: motivating examples and a proof-of-concept solution</div>
<div class="meta-line">Authors: Christopher David Roberts</div>
<div class="meta-line">First: 2026-02-17T18:59:55+00:00 · Latest: 2026-02-17T18:59:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15830v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15830v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fair scores reward ensemble forecast members that behave like samples from the same distribution as the verifying observations. They are therefore an attractive choice as loss functions to train data-driven ensemble forecasts or post-processing methods when large training ensembles are either unavailable or computationally prohibitive. The adjusted continuous ranked probability score (aCRPS) is fair and unbiased with respect to ensemble size, provided forecast members are exchangeable and interpretable as conditionally independent draws from an underlying predictive distribution. However, distribution-aware post-processing methods that introduce structural dependency between members can violate this assumption, rendering aCRPS unfair. We demonstrate this effect using two approaches designed to minimize the expected aCRPS of a finite ensemble: (1) a linear member-by-member calibration, which couples members through a common dependency on the sample ensemble mean, and (2) a deep-learning method, which couples members via transformer self-attention across the ensemble dimension. In both cases, the results are sensitive to ensemble size and apparent gains in aCRPS can correspond to systematic unreliability characterized by over-dispersion. We introduce trajectory transformers as a proof-of-concept that ensemble-size independence can be achieved. This approach is an adaptation of the Post-processing Ensembles with Transformers (PoET) framework and applies self-attention over lead time while preserving the conditional independence required by aCRPS. When applied to weekly mean $T_{2m}$ forecasts from the ECMWF subseasonal forecasting system, this approach successfully reduces systematic model biases whilst also improving or maintaining forecast reliability regardless of the ensemble size used in training (3 vs 9 members) or real-time forecasts (9 vs 100 members).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>最小化（不）公平评分的深度学习后处理方法的集合规模依赖性：动机性示例与概念验证方案</div>
<div class="mono" style="margin-top:8px">公平评分奖励那些行为类似于验证观测样本来自同一分布的集合预报成员。因此，它们是训练数据驱动集合预报或后处理方法时具有吸引力的损失函数，尤其是在大型训练集合不可用或计算成本过高时。只要预报成员是可交换的，并可解释为来自潜在预测分布的条件独立抽样，调整后的连续等级概率评分（aCRPS）在集合规模上是公平且无偏的。然而，引入成员间结构依赖性的分布感知后处理方法可能会违反这一假设，从而使aCRPS变得不公平。我们通过两种旨在最小化有限集合预期aCRPS的方法来展示这一效应：(1) 一种线性逐成员校准方法，该方法通过共同依赖于样本集合均值来耦合成员；(2) 一种深度学习方法，该方法通过在集合维度上使用Transformer自注意力机制来耦合成员。在两种情况下，结果都对集合规模敏感，aCRPS的显着提升可能对应于系统性不可靠性，表现为过度离散。我们引入轨迹Transformer作为概念验证方案，以实现集合规模的独立性。这种方法是对Transformer后处理集合（PoET）框架的适应，它在预报时效上应用自注意力机制，同时保留aCRPS所需的条件独立性。当应用于ECMWF次季节预报系统中的周平均$T_{2m}$预报时，该方法成功地减少了系统性模型偏差，同时在训练集合规模（3 vs 9成员）或实时预报集合规模（9 vs 100成员）下也提高了或保持了预报可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">Operationalising the Superficial Alignment Hypothesis via Task Complexity</div>
<div class="meta-line">Authors: Tomás Vergara-Browne, Darshan Patil, Ivan Titov, Siva Reddy, Tiago Pimentel, Marius Mosbach</div>
<div class="meta-line">First: 2026-02-17T18:59:39+00:00 · Latest: 2026-02-17T18:59:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15829v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15829v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The superficial alignment hypothesis (SAH) posits that large language models learn most of their knowledge during pre-training, and that post-training merely surfaces this knowledge. The SAH, however, lacks a precise definition, which has led to (i) different and seemingly orthogonal arguments supporting it, and (ii) important critiques to it. We propose a new metric called task complexity: the length of the shortest program that achieves a target performance on a task. In this framework, the SAH simply claims that pre-trained models drastically reduce the complexity of achieving high performance on many tasks. Our definition unifies prior arguments supporting the SAH, interpreting them as different strategies to find such short programs. Experimentally, we estimate the task complexity of mathematical reasoning, machine translation, and instruction following; we then show that these complexities can be remarkably low when conditioned on a pre-trained model. Further, we find that pre-training enables access to strong performances on our tasks, but it can require programs of gigabytes of length to access them. Post-training, on the other hand, collapses the complexity of reaching this same performance by several orders of magnitude. Overall, our results highlight that task adaptation often requires surprisingly little information -- often just a few kilobytes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过任务复杂度实现表层对齐假说</div>
<div class="mono" style="margin-top:8px">表层对齐假说（SAH）认为，大语言模型在预训练阶段学习了大部分知识，而微调只是将这些知识展现出来。然而，SAH缺乏精确的定义，导致了（i）支持它的不同且看似正交的论点，以及（ii）重要的批评。我们提出了一种新的度量标准，称为任务复杂度：实现某一任务目标性能的最短程序的长度。在此框架下，SAH仅声称预训练模型大幅降低了在许多任务上实现高性能的复杂度。我们的定义统一了之前支持SAH的各种论点，将其解释为寻找此类简短程序的不同策略。实验上，我们估计了数学推理、机器翻译和指令遵循任务的复杂度，然后表明在预训练模型的条件下，这些复杂度可以显著降低。此外，我们发现预训练使模型能够访问到强大的性能，但要实现这些性能可能需要数GB长度的程序。相比之下，微调后，达到相同性能的复杂度降低了几个数量级。总体而言，我们的结果表明，任务适应通常只需要非常少的信息——通常只有几KB。</div>
</details>
</div>
<div class="card">
<div class="title">Dex4D: Task-Agnostic Point Track Policy for Sim-to-Real Dexterous Manipulation</div>
<div class="meta-line">Authors: Yuxuan Kuang, Sungjae Park, Katerina Fragkiadaki, Shubham Tulsiani</div>
<div class="meta-line">First: 2026-02-17T18:59:31+00:00 · Latest: 2026-02-17T18:59:31+00:00</div>
<div class="meta-line">Comments: Project page: https://dex4d.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15828v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15828v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://dex4d.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning generalist policies capable of accomplishing a plethora of everyday tasks remains an open challenge in dexterous manipulation. In particular, collecting large-scale manipulation data via real-world teleoperation is expensive and difficult to scale. While learning in simulation provides a feasible alternative, designing multiple task-specific environments and rewards for training is similarly challenging. We propose Dex4D, a framework that instead leverages simulation for learning task-agnostic dexterous skills that can be flexibly recomposed to perform diverse real-world manipulation tasks. Specifically, Dex4D learns a domain-agnostic 3D point track conditioned policy capable of manipulating any object to any desired pose. We train this &#x27;Anypose-to-Anypose&#x27; policy in simulation across thousands of objects with diverse pose configurations, covering a broad space of robot-object interactions that can be composed at test time. At deployment, this policy can be zero-shot transferred to real-world tasks without finetuning, simply by prompting it with desired object-centric point tracks extracted from generated videos. During execution, Dex4D uses online point tracking for closed-loop perception and control. Extensive experiments in simulation and on real robots show that our method enables zero-shot deployment for diverse dexterous manipulation tasks and yields consistent improvements over prior baselines. Furthermore, we demonstrate strong generalization to novel objects, scene layouts, backgrounds, and trajectories, highlighting the robustness and scalability of the proposed framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Dex4D：一种与任务无关的点轨迹策略，用于模拟到现实的灵巧操作</div>
<div class="mono" style="margin-top:8px">学习能够完成各种日常任务的通用策略仍然是灵巧操作领域的一个开放性挑战。特别是，通过现实世界远程操作收集大规模操作数据既昂贵又难以扩展。虽然在模拟环境中学习提供了一个可行的替代方案，但设计多个任务特定的环境和奖励进行训练同样具有挑战性。我们提出 Dex4D，一个框架，通过模拟环境学习与任务无关的灵巧技能，并能够灵活组合以执行各种现实世界操作任务。具体而言，Dex4D 学习一种与领域无关的 3D 点轨迹条件策略，能够将任何物体移动到任意期望的姿态。我们在模拟环境中，针对数千种具有不同姿态配置的物体进行训练，覆盖了广泛的机器人-物体交互空间，这些交互可以在测试时组合使用。在部署时，该策略可以通过从生成视频中提取的期望物体中心点轨迹进行零样本迁移，无需微调。在执行过程中，Dex4D 使用在线点轨迹追踪实现闭环感知与控制。在模拟和真实机器人上的大量实验表明，我们的方法能够实现多种灵巧操作任务的零样本部署，并在多个先前基线方法上取得一致的性能提升。此外，我们展示了其对新物体、场景布局、背景和轨迹的强泛化能力，突显了所提框架的鲁棒性和可扩展性。</div>
</details>
</div>
<div class="card">
<div class="title">Perceptive Humanoid Parkour: Chaining Dynamic Human Skills via Motion Matching</div>
<div class="meta-line">Authors: Zhen Wu, Xiaoyu Huang, Lujie Yang, Yuanhang Zhang, Koushil Sreenath, Xi Chen, Pieter Abbeel, Rocky Duan, Angjoo Kanazawa, Carmelo Sferrazza, Guanya Shi, C. Karen Liu</div>
<div class="meta-line">First: 2026-02-17T18:59:11+00:00 · Latest: 2026-02-17T18:59:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15827v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15827v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While recent advances in humanoid locomotion have achieved stable walking on varied terrains, capturing the agility and adaptivity of highly dynamic human motions remains an open challenge. In particular, agile parkour in complex environments demands not only low-level robustness, but also human-like motion expressiveness, long-horizon skill composition, and perception-driven decision-making. In this paper, we present Perceptive Humanoid Parkour (PHP), a modular framework that enables humanoid robots to autonomously perform long-horizon, vision-based parkour across challenging obstacle courses. Our approach first leverages motion matching, formulated as nearest-neighbor search in a feature space, to compose retargeted atomic human skills into long-horizon kinematic trajectories. This framework enables the flexible composition and smooth transition of complex skill chains while preserving the elegance and fluidity of dynamic human motions. Next, we train motion-tracking reinforcement learning (RL) expert policies for these composed motions, and distill them into a single depth-based, multi-skill student policy, using a combination of DAgger and RL. Crucially, the combination of perception and skill composition enables autonomous, context-aware decision-making: using only onboard depth sensing and a discrete 2D velocity command, the robot selects and executes whether to step over, climb onto, vault or roll off obstacles of varying geometries and heights. We validate our framework with extensive real-world experiments on a Unitree G1 humanoid robot, demonstrating highly dynamic parkour skills such as climbing tall obstacles up to 1.25m (96% robot height), as well as long-horizon multi-obstacle traversal with closed-loop adaptation to real-time obstacle perturbations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>感知类人机器人跑酷：通过运动匹配串联动态人类技能</div>
<div class="mono" style="margin-top:8px">尽管近期在类人机器人运动方面取得了稳定在复杂地形上行走的进展，但捕捉高度动态人类动作的敏捷性和适应性仍然是一个开放性挑战。特别是，在复杂环境中进行敏捷跑酷不仅需要底层的鲁棒性，还需要类人动作的表达性、长时序技能组合以及感知驱动的决策能力。本文提出感知类人跑酷（Perceptive Humanoid Parkour, PHP），这是一个模块化框架，使类人机器人能够自主地在具有挑战性的障碍赛道上执行基于视觉的长时序跑酷动作。我们的方法首先利用运动匹配，将其建模为特征空间中的最近邻搜索，将重定向的原子人类技能组合成长时序运动轨迹。该框架支持复杂技能链的灵活组合和流畅过渡，同时保留动态人类动作的优雅性和流畅性。随后，我们使用DAgger和强化学习（RL）的结合方法，训练运动跟踪的专家策略，并将其提炼为一个基于深度的、多技能的学生策略。关键的是，感知与技能组合的结合使得机器人能够实现自主、上下文感知的决策：仅依靠机载深度感知和离散的2D速度指令，机器人即可选择并执行跨越、攀爬、跳越或滚过不同几何形状和高度的障碍物。我们在Unitree G1类人机器人上进行了大量真实世界实验，验证了该框架，展示了诸如攀爬高达1.25米（占机器人高度的96%）的障碍物等高度动态的跑酷技能，以及在多障碍物环境中进行长时序穿越并实时适应障碍物扰动的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Hunt Globally: Wide Search AI Agents for Drug Asset Scouting in Investing, Business Development, and Competitive Intelligence</div>
<div class="meta-line">Authors: Alisa Vinogradova, Vlad Vinogradov, Luba Greenwood, Ilya Yasny, Dmitry Kobyzev, Shoman Kasbekar, Kong Nguyen, Dmitrii Radkevich, Roman Doronin, Andrey Doronichev</div>
<div class="meta-line">First: 2026-02-16T18:57:49+00:00 · Latest: 2026-02-17T18:58:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15019v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.15019v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels. Recent data suggests that over 85% of patent filings originate outside the U.S., with China accounting for nearly half of the global total. A growing share of scholarly output is also non-U.S. Industry estimates put China at 30% of global drug development, spanning 1,200+ novel candidates. In this high-stakes environment, failing to surface &quot;under-the-radar&quot; assets creates multi-billion-dollar risk for investors and business development teams, making asset scouting a coverage-critical competition where speed and completeness drive value. Yet today&#x27;s Deep Research AI agents still lag human experts in achieving high recall discovery across heterogeneous, multilingual sources without hallucination. We propose a benchmarking methodology for drug asset scouting and a tuned, tree-based self-learning Bioptic Agent aimed at complete, non-hallucinated scouting. We construct a challenging completeness benchmark using a multilingual multi-agent pipeline: complex user queries paired with ground-truth assets that are largely outside U.S.-centric radar. To reflect real-deal complexity, we collected screening queries from expert investors, BD, and VC professionals and used them as priors to conditionally generate benchmark queries. For grading, we use LLM-as-judge evaluation calibrated to expert opinions. On this benchmark, our Bioptic Agent achieves 79.7% F1 score, outperforming Claude Opus 4.6 (56.2%), Gemini 3 Pro + Deep Research (50.6%), OpenAI GPT-5.2 Pro (46.6%), Perplexity Deep Research (44.2%), and Exa Websets (26.9%). Performance improves steeply with additional compute, supporting the view that more compute yields better results.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>全球搜寻：用于投资、业务拓展和竞争情报的广泛搜索药物资产AI代理</div>
<div class="mono" style="margin-top:8px">生物制药创新已发生变化：许多新药资产现在源自美国以外地区，并主要通过区域性的非英语渠道披露。最新数据显示，超过85%的专利申请来自美国以外，其中中国占全球总量的近一半。学术产出中也有越来越多的非美国内容。行业估计显示，中国占全球药物开发的30%，涵盖1,200多个新药候选项目。在这一高风险环境中，未能发现&quot;低调&quot;的资产会给投资者和业务拓展团队带来数十亿美元的风险，使资产搜寻成为一项关键的覆盖竞争，其中速度和完整性决定价值。然而，当前的深度研究AI代理在跨异构、多语言来源中实现高召回率发现时，仍落后于人类专家，且容易产生幻觉。我们提出了一种药物资产搜寻的基准方法，并开发了一种经过调优的基于树的自学习Bioptic代理，旨在实现完整且无幻觉的资产搜寻。我们使用多语言多代理流程构建了一个具有挑战性的完整性基准：复杂的用户查询与大量位于美国中心视野之外的真实资产配对。为了反映真实复杂性，我们从专家投资者、业务拓展和风投专业人士中收集了筛选查询，并将其作为先验条件来生成基准查询。在评估中，我们采用LLM作为裁判的评估方法，并根据专家意见进行校准。在该基准上，我们的Bioptic代理实现了79.7%的F1分数，优于Claude Opus 4.6（56.2%）、Gemini 3 Pro + Deep Research（50.6%）、OpenAI GPT-5.2 Pro（46.6%）、Perplexity Deep Research（44.2%）和Exa Websets（26.9%）。随着计算资源的增加，性能显著提升，支持了&quot;更多计算资源带来更好结果&quot;的观点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Bio-pharmaceutical innovation has shifted: many new drug assets now originate outside the United States and are disclosed primarily via regional, non-English channels.</div>
</details>
</div>
<div class="card">
<div class="title">stable-worldmodel-v1: Reproducible World Modeling Research and Evaluation</div>
<div class="meta-line">Authors: Lucas Maes, Quentin Le Lidec, Dan Haramati, Nassim Massaudi, Damien Scieur, Yann LeCun, Randall Balestriero</div>
<div class="meta-line">First: 2026-02-09T18:04:22+00:00 · Latest: 2026-02-17T18:58:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08968v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08968v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">World Models have emerged as a powerful paradigm for learning compact, predictive representations of environment dynamics, enabling agents to reason, plan, and generalize beyond direct experience. Despite recent interest in World Models, most available implementations remain publication-specific, severely limiting their reusability, increasing the risk of bugs, and reducing evaluation standardization. To mitigate these issues, we introduce stable-worldmodel (SWM), a modular, tested, and documented world-model research ecosystem that provides efficient data-collection tools, standardized environments, planning algorithms, and baseline implementations. In addition, each environment in SWM enables controllable factors of variation, including visual and physical properties, to support robustness and continual learning research. Finally, we demonstrate the utility of SWM by using it to study zero-shot robustness in DINO-WM.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>stable-worldmodel-v1: 可复现的世界建模研究与评估</div>
<div class="mono" style="margin-top:8px">世界模型已成为学习环境动态的紧凑、预测性表示的强大范式，使智能体能够推理、规划并在直接经验之外进行泛化。尽管近期对世界模型的兴趣增加，但大多数现有实现仍局限于特定论文，严重限制了其可重用性，增加了出错的风险，并降低了评估的标准化程度。为了解决这些问题，我们引入了 stable-worldmodel（SWM），这是一个模块化、经过测试并有文档支持的世界模型研究生态系统，提供了高效的数据收集工具、标准化环境、规划算法和基线实现。此外，SWM 中的每个环境都支持可控的变化因素，包括视觉和物理属性，以支持鲁棒性和持续学习的研究。最后，我们通过使用 SWM 来研究 DINO-WM 中的零样本鲁棒性，展示了其实用性。</div>
</details>
</div>
<div class="card">
<div class="title">CrispEdit: Low-Curvature Projections for Scalable Non-Destructive LLM Editing</div>
<div class="meta-line">Authors: Zarif Ikram, Arad Firouzkouhi, Stephen Tu, Mahdi Soltanolkotabi, Paria Rashidinejad</div>
<div class="meta-line">First: 2026-02-17T18:58:04+00:00 · Latest: 2026-02-17T18:58:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15823v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15823v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking. We present CrispEdit, a scalable and principled second-order editing algorithm that treats capability preservation as an explicit constraint, unifying and generalizing several existing editing approaches. CrispEdit formulates editing as constrained optimization and enforces the constraint by projecting edit updates onto the low-curvature subspace of the capability-loss landscape. At the crux of CrispEdit is expressing capability constraint via Bregman divergence, whose quadratic form yields the Gauss-Newton Hessian exactly and even when the base model is not trained to convergence. We make this second-order procedure efficient at the LLM scale using Kronecker-factored approximate curvature (K-FAC) and a novel matrix-free projector that exploits Kronecker structure to avoid constructing massive projection matrices. Across standard model-editing benchmarks, CrispEdit achieves high edit success while keeping capability degradation below 1% on average across datasets, significantly improving over prior editors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CrispEdit：用于可扩展非破坏性LLM编辑的低曲率投影</div>
<div class="mono" style="margin-top:8px">在大型语言模型（LLM）编辑中，一个核心挑战是能力保持：成功改变目标行为的方法可能会悄悄地利用编辑代理进行游戏，从而破坏一般能力，产生类似代理/奖励黑客行为的退化行为。我们提出了CrispEdit，这是一种可扩展且基于原理的二阶编辑算法，将能力保持视为显式约束，统一并推广了多种现有的编辑方法。CrispEdit将编辑建模为带约束的优化问题，并通过将编辑更新投影到能力损失景观的低曲率子空间来强制执行该约束。CrispEdit的核心在于通过Bregman散度表达能力约束，其二次形式能够精确地得到Gauss-Newton Hessian，即使基础模型未训练至收敛。我们利用Kronecker因子近似曲率（K-FAC）和一种新颖的矩阵无关投影器，使这一二阶过程在LLM规模上高效运行，该投影器利用Kronecker结构避免构建大规模的投影矩阵。在标准的模型编辑基准测试中，CrispEdit实现了高编辑成功率，同时在各数据集上平均保持能力退化低于1%，显著优于之前的编辑器。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">A central challenge in large language model (LLM) editing is capability preservation: methods that successfully change targeted behavior can quietly game the editing proxy and corrupt general capabilities, producing degenerate behaviors reminiscent of proxy/reward hacking.</div>
</details>
</div>
<div class="card">
<div class="title">Stabilizing Test-Time Adaptation of High-Dimensional Simulation Surrogates via D-Optimal Statistics</div>
<div class="meta-line">Authors: Anna Zimmel, Paul Setinek, Gianluca Galletti, Johannes Brandstetter, Werner Zellinger</div>
<div class="meta-line">First: 2026-02-17T18:55:18+00:00 · Latest: 2026-02-17T18:55:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15820v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15820v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine learning surrogates are increasingly used in engineering to accelerate costly simulations, yet distribution shifts between training and deployment often cause severe performance degradation (e.g., unseen geometries or configurations). Test-Time Adaptation (TTA) can mitigate such shifts, but existing methods are largely developed for lower-dimensional classification with structured outputs and visually aligned input-output relationships, making them unstable for the high-dimensional, unstructured and regression problems common in simulation. We address this challenge by proposing a TTA framework based on storing maximally informative (D-optimal) statistics, which jointly enables stable adaptation and principled parameter selection at test time. When applied to pretrained simulation surrogates, our method yields up to 7% out-of-distribution improvements at negligible computational cost. To the best of our knowledge, this is the first systematic demonstration of effective TTA for high-dimensional simulation regression and generative design optimization, validated on the SIMSHIFT and EngiBench benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过D-最优统计稳定高维模拟替代模型的测试时适应</div>
<div class="mono" style="margin-top:8px">机器学习替代模型在工程中被越来越多地用于加速昂贵的模拟，但训练与部署之间的分布偏移常常导致严重性能下降（例如未见过的几何形状或配置）。测试时适应（TTA）可以缓解这种偏移，但现有方法主要针对低维分类任务，具有结构化输出和视觉对齐的输入-输出关系，因此在高维、非结构化和回归问题上不够稳定。我们通过提出一种基于存储最大信息量（D-最优）统计的TTA框架来解决这一挑战，该框架能够实现测试时的稳定适应和参数选择。当应用于预训练的模拟替代模型时，我们的方法在计算成本可忽略的情况下，实现了高达7%的分布外性能提升。据我们所知，这是首次系统性地展示高维模拟回归和生成设计优化中有效TTA的应用，已在SIMSHIFT和EngiBench基准上得到验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Machine learning surrogates are increasingly used in engineering to accelerate costly simulations, yet distribution shifts between training and deployment often cause severe performance degradation (e.g., unseen geometries or configurations).</div>
</details>
</div>
<div class="card">
<div class="title">VideoSketcher: Video Models Prior Enable Versatile Sequential Sketch Generation</div>
<div class="meta-line">Authors: Hui Ren, Yuval Alaluf, Omer Bar Tal, Alexander Schwing, Antonio Torralba, Yael Vinker</div>
<div class="meta-line">First: 2026-02-17T18:55:03+00:00 · Latest: 2026-02-17T18:55:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15819v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15819v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sketching is inherently a sequential process, in which strokes are drawn in a meaningful order to explore and refine ideas. However, most generative models treat sketches as static images, overlooking the temporal structure that underlies creative drawing. We present a data-efficient approach for sequential sketch generation that adapts pretrained text-to-video diffusion models to generate sketching processes. Our key insight is that large language models and video diffusion models offer complementary strengths for this task: LLMs provide semantic planning and stroke ordering, while video diffusion models serve as strong renderers that produce high-quality, temporally coherent visuals. We leverage this by representing sketches as short videos in which strokes are progressively drawn on a blank canvas, guided by text-specified ordering instructions. We introduce a two-stage fine-tuning strategy that decouples the learning of stroke ordering from the learning of sketch appearance. Stroke ordering is learned using synthetic shape compositions with controlled temporal structure, while visual appearance is distilled from as few as seven manually authored sketching processes that capture both global drawing order and the continuous formation of individual strokes. Despite the extremely limited amount of human-drawn sketch data, our method generates high-quality sequential sketches that closely follow text-specified orderings while exhibiting rich visual detail. We further demonstrate the flexibility of our approach through extensions such as brush style conditioning and autoregressive sketch generation, enabling additional controllability and interactive, collaborative drawing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoSketcher：视频模型先验实现多功能的序列草图生成</div>
<div class="mono" style="margin-top:8px">素描本质上是一个序列过程，其中笔画按照有意义的顺序绘制以探索和精炼想法。然而，大多数生成模型将素描视为静态图像，忽略了创造性绘画背后的时序结构。我们提出了一种数据高效的序列素描生成方法，通过适配预训练的文本到视频扩散模型来生成素描过程。我们的关键洞察是，大语言模型和视频扩散模型在该任务中具有互补的优势：LLMs 提供语义规划和笔画顺序，而视频扩散模型则作为强大的渲染器，生成高质量且时间连贯的视觉效果。我们通过将素描表示为短视频来利用这一优势，其中笔画在空白画布上逐步绘制，由文本指定的顺序指令引导。我们引入了一种两阶段微调策略，将笔画顺序的学习与素描外观的学习分离。笔画顺序通过具有可控时序结构的合成形状组合进行学习，而视觉外观则通过仅需七个手动编写的素描过程进行蒸馏，这些过程捕捉了全局绘制顺序和单个笔画的连续形成。尽管人类绘制的素描数据极其有限，我们的方法仍能生成高质量的序列素描，这些素描紧密遵循文本指定的顺序，同时展现出丰富的视觉细节。我们进一步通过诸如画笔风格条件控制和自回归素描生成等扩展，展示了该方法的灵活性，从而实现额外的可控性和交互式、协作式绘画。</div>
</details>
</div>
<div class="card">
<div class="title">Solving Parameter-Robust Avoid Problems with Unknown Feasibility using Reinforcement Learning</div>
<div class="meta-line">Authors: Oswin So, Eric Yang Yu, Songyuan Zhang, Matthew Cleaveland, Mitchell Black, Chuchu Fan</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-17T18:53:31+00:00 · Latest: 2026-02-17T18:53:31+00:00</div>
<div class="meta-line">Comments: ICLR 2026. The project page can be found at https://oswinso.xyz/fge</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15817v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15817v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in deep reinforcement learning (RL) have achieved strong results on high-dimensional control tasks, but applying RL to reachability problems raises a fundamental mismatch: reachability seeks to maximize the set of states from which a system remains safe indefinitely, while RL optimizes expected returns over a user-specified distribution. This mismatch can result in policies that perform poorly on low-probability states that are still within the safe set. A natural alternative is to frame the problem as a robust optimization over a set of initial conditions that specify the initial state, dynamics and safe set, but whether this problem has a solution depends on the feasibility of the specified set, which is unknown a priori. We propose Feasibility-Guided Exploration (FGE), a method that simultaneously identifies a subset of feasible initial conditions under which a safe policy exists, and learns a policy to solve the reachability problem over this set of initial conditions. Empirical results demonstrate that FGE learns policies with over 50% more coverage than the best existing method for challenging initial conditions across tasks in the MuJoCo simulator and the Kinetix simulator with pixel observations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用强化学习解决具有未知可行性参数鲁棒避障问题</div>
<div class="mono" style="margin-top:8px">深度强化学习（RL）的最新进展在高维控制任务中取得了显著成果，但将RL应用于可达性问题时存在根本性的不匹配：可达性问题旨在最大化系统在安全集合内无限期保持安全的状态集合，而RL则是在用户指定的分布上优化期望回报。这种不匹配可能导致在低概率但仍在安全集合内的状态上表现不佳的策略。一种自然的替代方法是将问题框架为在一组初始条件上的鲁棒优化，这些初始条件指定了初始状态、动态和安全集合。然而，该问题是否有解取决于所指定集合的可行性，而这在事先是未知的。我们提出了一种名为可行性引导探索（Feasibility-Guided Exploration, FGE）的方法，该方法同时识别出存在安全策略的初始条件子集，并在这些初始条件上学习解决可达性问题的策略。实验证明，在MuJoCo模拟器和具有像素观测的Kinetix模拟器中，FGE在具有挑战性的初始条件下，其策略的覆盖范围比现有最佳方法高出超过50%。</div>
</details>
</div>
<div class="card">
<div class="title">Developing AI Agents with Simulated Data: Why, what, and how?</div>
<div class="meta-line">Authors: Xiaoran Liu, Istvan David</div>
<div class="meta-line">First: 2026-02-17T18:53:27+00:00 · Latest: 2026-02-17T18:53:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15816v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15816v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As insufficient data volume and quality remain the key impediments to the adoption of modern subsymbolic AI, techniques of synthetic data generation are in high demand. Simulation offers an apt, systematic approach to generating diverse synthetic data. This chapter introduces the reader to the key concepts, benefits, and challenges of simulation-based synthetic data generation for AI training purposes, and to a reference framework to describe, design, and analyze digital twin-based AI simulation solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用模拟数据开发AI代理：为何、何为以及如何？</div>
<div class="mono" style="margin-top:8px">由于数据量和质量不足仍然是现代子符号AI采用的关键障碍，合成数据生成技术需求迫切。模拟提供了一种恰当且系统的方法来生成多样化的合成数据。本章旨在向读者介绍基于模拟的合成数据生成在AI训练中的关键概念、优势与挑战，并提供一个参考框架，用于描述、设计和分析基于数字孪生的AI模拟解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Token-Based Audio Inpainting via Discrete Diffusion</div>
<div class="meta-line">Authors: Tali Dror, Iftach Shoham, Moshe Buchris, Oren Gal, Haim Permuter, Gilad Katz, Eliya Nachmani</div>
<div class="meta-line">First: 2025-07-11T06:25:49+00:00 · Latest: 2026-02-17T18:53:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.08333v4">Abs</a> · <a href="https://arxiv.org/pdf/2507.08333v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Audio inpainting seeks to restore missing segments in degraded recordings. Previous diffusion-based methods exhibit impaired performance when the missing region is large. We introduce the first approach that applies discrete diffusion over tokenized music representations from a pre-trained audio tokenizer, enabling stable and semantically coherent restoration of long gaps. Our method further incorporates two training approaches: a derivative-based regularization loss that enforces smooth temporal dynamics, and a span-based absorbing transition that provides structured corruption during diffusion. Experiments on the MusicNet and MAESTRO datasets with gaps up to 750 ms show that our approach consistently outperforms strong baselines across range of gap lengths, for gaps of 150 ms and above. This work advances musical audio restoration and introduces new directions for discrete diffusion model training. Visit our project page for examples and code.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于离散扩散的基于令牌的音频补全</div>
<div class="mono" style="margin-top:8px">音频补全旨在修复退化录音中的缺失片段。先前基于扩散的方法在缺失区域较大时表现出性能下降。我们引入了首个方法，通过预训练音频分词器对分词后的音乐表示应用离散扩散，从而实现长间隙的稳定且语义连贯的修复。我们的方法进一步结合了两种训练策略：基于导数的正则化损失，用于强制执行平滑的时间动态；以及基于跨度的吸收转移，用于在扩散过程中提供结构化的破坏。在MusicNet和MAESTRO数据集上的实验显示，我们的方法在各种间隙长度下，对于150毫秒及以上的间隙，均优于强大的基线方法。本工作推动了音乐音频修复的发展，并为离散扩散模型的训练引入了新方向。访问我们的项目页面以获取示例和代码。</div>
</details>
</div>
<div class="card">
<div class="title">Avey-B</div>
<div class="meta-line">Authors: Devang Acharya, Mohammad Hammoud</div>
<div class="meta-line">First: 2026-02-17T18:50:40+00:00 · Latest: 2026-02-17T18:50:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15814v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15814v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Compact pretrained bidirectional encoders remain the backbone of industrial NLP under tight compute and memory budgets. Their effectiveness stems from self-attention&#x27;s ability to deliver high-quality bidirectional contextualization with sequence-level parallelism, as popularized by BERT-style architectures. Recently, Avey was introduced as an autoregressive, attention-free alternative that naturally admits an encoder-only adaptation. In this paper, we reformulate Avey for the encoder-only paradigm and propose several innovations to its architecture, including decoupled static and dynamic parameterizations, stability-oriented normalization, and neural compression. Results show that this reformulated architecture compares favorably to four widely used Transformer-based encoders, consistently outperforming them on standard token-classification and information-retrieval benchmarks while scaling more efficiently to long contexts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Avey-B</div>
<div class="mono" style="margin-top:8px">在计算和内存资源受限的情况下，紧凑型预训练双向编码器仍然是工业自然语言处理的核心。其有效性源于自注意力机制能够在序列级并行处理中提供高质量的双向上下文表示，这一特性在BERT等架构中得到了广泛推广。最近，Avey被引入作为一种自回归且无注意力机制的替代方案，其天然适合编码器-only的变体。本文将Avey重新构建为编码器-only范式，并提出对其架构的多项创新，包括解耦的静态与动态参数化、以稳定性为导向的归一化方法以及神经压缩技术。实验结果表明，这种重构后的架构在多个标准的词分类和信息检索基准测试中均优于四种常用的基于Transformer的编码器，同时在处理长上下文时具有更高的效率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Compact pretrained bidirectional encoders remain the backbone of industrial NLP under tight compute and memory budgets.</div>
</details>
</div>
<div class="card">
<div class="title">Horizon Imagination: Efficient On-Policy Rollout in Diffusion World Models</div>
<div class="meta-line">Authors: Lior Cohen, Ofir Nabati, Kaixin Wang, Navdeep Kumar, Shie Mannor</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-08T16:07:04+00:00 · Latest: 2026-02-17T18:50:34+00:00</div>
<div class="meta-line">Comments: This paper will be published in the ICLR 2026 proceedings</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08032v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08032v2">PDF</a> · <a href="https://github.com/leor-c/horizon-imagination">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study diffusion-based world models for reinforcement learning, which offer high generative fidelity but face critical efficiency challenges in control. Current methods either require heavyweight models at inference or rely on highly sequential imagination, both of which impose prohibitive computational costs. We propose Horizon Imagination (HI), an on-policy imagination process for discrete stochastic policies that denoises multiple future observations in parallel. HI incorporates a stabilization mechanism and a novel sampling schedule that decouples the denoising budget from the effective horizon over which denoising is applied while also supporting sub-frame budgets. Experiments on Atari 100K and Craftium show that our approach maintains control performance with a sub-frame budget of half the denoising steps and achieves superior generation quality under varied schedules. Code is available at https://github.com/leor-c/horizon-imagination.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>地平线想象：扩散世界模型中的高效在线策略展开</div>
<div class="mono" style="margin-top:8px">我们研究基于扩散的世界模型在强化学习中的应用，这些模型具有高生成保真度，但在控制方面面临关键的效率挑战。当前方法要么需要在推理时使用重型模型，要么依赖高度序列化的想象过程，这两种方法都会带来计算成本过高的问题。我们提出了一种名为 Horizon Imagination（HI）的在线想象过程，适用于离散随机策略，能够并行去噪多个未来观测。HI 引入了稳定机制和一种新颖的采样时间表，将去噪预算与实际应用的去噪时间范围解耦，同时支持子帧预算。在 Atari 100K 和 Craftium 上的实验表明，我们的方法在仅使用一半去噪步骤的子帧预算下仍能保持控制性能，并在不同时间表下实现更优的生成质量。代码可在 https://github.com/leor-c/horizon-imagination 获取。</div>
</details>
</div>
<div class="card">
<div class="title">FAST-EQA: Efficient Embodied Question Answering with Global and Local Region Relevancy</div>
<div class="meta-line">Authors: Haochen Zhang, Nirav Savaliya, Faizan Siddiqui, Enna Sachdeva</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2026-02-17T18:49:43+00:00 · Latest: 2026-02-17T18:49:43+00:00</div>
<div class="meta-line">Comments: WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15813v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15813v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Embodied Question Answering (EQA) combines visual scene understanding, goal-directed exploration, spatial and temporal reasoning under partial observability. A central challenge is to confine physical search to question-relevant subspaces while maintaining a compact, actionable memory of observations. Furthermore, for real-world deployment, fast inference time during exploration is crucial. We introduce FAST-EQA, a question-conditioned framework that (i) identifies likely visual targets, (ii) scores global regions of interest to guide navigation, and (iii) employs Chain-of-Thought (CoT) reasoning over visual memory to answer confidently. FAST-EQA maintains a bounded scene memory that stores a fixed-capacity set of region-target hypotheses and updates them online, enabling robust handling of both single and multi-target questions without unbounded growth. To expand coverage efficiently, a global exploration policy treats narrow openings and doors as high-value frontiers, complementing local target seeking with minimal computation. Together, these components focus the agent&#x27;s attention, improve scene coverage, and improve answer reliability while running substantially faster than prior approaches. On HMEQA and EXPRESS-Bench, FAST-EQA achieves state-of-the-art performance, while performing competitively on OpenEQA and MT-HM3D.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FAST-EQA：基于全局和局部区域相关性的高效具身问答</div>
<div class="mono" style="margin-top:8px">具身问答（EQA）结合了视觉场景理解、目标导向探索、在部分可观测条件下的空间和时间推理。一个核心挑战是在保持紧凑且可操作的观察记忆的同时，将物理搜索限制在与问题相关的子空间中。此外，为了实际部署，探索过程中的快速推理时间至关重要。我们引入了FAST-EQA，这是一个基于问题的框架，其特点包括：(i) 识别可能的视觉目标；(ii) 评估全局感兴趣区域以指导导航；(iii) 在视觉记忆上使用思维链（Chain-of-Thought, CoT）推理来自信作答。FAST-EQA维护一个有界的场景记忆，存储固定容量的区域-目标假设集，并在线更新这些假设，从而能够稳健地处理单目标和多目标问题，而不会出现无限制增长。为了高效扩展覆盖范围，全局探索策略将狭窄开口和门视为高价值前沿，以最小的计算量补充局部目标搜索。这些组件共同作用，使智能体的注意力更加集中，提高场景覆盖范围和答案可靠性，同时显著优于以往方法。在HMEQA和EXPRESS-Bench数据集上，FAST-EQA实现了最先进的性能，同时在OpenEQA和MT-HM3D上也表现出竞争力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Embodied Question Answering (EQA) combines visual scene understanding, goal-directed exploration, spatial and temporal reasoning under partial observability.</div>
</details>
</div>
<div class="card">
<div class="title">Task-Agnostic Continual Learning for Chest Radiograph Classification</div>
<div class="meta-line">Authors: Muthu Subash Kavitha, Anas Zafar, Amgad Muneer, Jia Wu</div>
<div class="meta-line">First: 2026-02-17T18:47:30+00:00 · Latest: 2026-02-17T18:47:30+00:00</div>
<div class="meta-line">Comments: 12 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15811v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15811v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Clinical deployment of chest radiograph classifiers requires models that can be updated as new datasets become available without retraining on previously ob- served data or degrading validated performance. We study, for the first time, a task-incremental continual learning setting for chest radiograph classification, in which heterogeneous chest X-ray datasets arrive sequentially and task identifiers are unavailable at inference. We propose a continual adapter-based routing learning strategy for Chest X-rays (CARL-XRay) that maintains a fixed high-capacity backbone and incrementally allocates lightweight task-specific adapters and classifier heads. A latent task selector operates on task-adapted features and leverages both current and historical context preserved through compact prototypes and feature-level experience replay. This design supports stable task identification and adaptation across sequential updates while avoiding raw-image storage. Experiments on large-scale public chest radiograph datasets demonstrate robust performance retention and reliable task-aware inference under continual dataset ingestion. CARL-XRay outperforms joint training under task-unknown deployment, achieving higher routing accuracy (75.0\% vs.\ 62.5\%), while maintaining competitive diagnostic performance with AUROC of 0.74 in the oracle setting with ground-truth task identity and 0.75 under task-unknown inference, using significantly fewer trainable parameters. Finally, the proposed framework provides a practical alternative to joint training and repeated full retraining in continual clinical deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于胸部X光分类的无任务意识持续学习</div>
<div class="mono" style="margin-top:8px">在临床部署胸部X光分类器时，需要模型能够在新数据集出现时进行更新，而无需在之前观察到的数据上重新训练或降低已验证的性能。我们首次研究了胸部X光分类的基于任务增量的持续学习场景，其中异构胸部X光数据集按顺序到达，且推理时无任务标识符。我们提出了一种基于持续适配器的路由学习策略（CARL-XRay），该策略保持固定高容量主干网络，并逐步分配轻量级任务特定适配器和分类头。一个潜在的任务选择器基于任务适应的特征运行，并利用通过紧凑原型和特征级经验回放保存的当前和历史上下文。这种设计支持在连续更新中稳定地进行任务识别和适应，同时避免存储原始图像。在大规模公开胸部X光数据集上的实验表明，在持续数据集摄入下，该方法能够保持稳健的性能并实现可靠的基于任务的推理。在任务未知部署下，CARL-XRay的路由准确率（75.0% vs. 62.5%）优于联合训练，同时在拥有真实任务标识符的oracle设置下保持与之相当的诊断性能（AUROC为0.74），在任务未知推理下达到0.75，且使用显著更少的可训练参数。最后，所提出的框架为持续临床部署提供了一种实用的替代方案，避免了联合训练和重复完整再训练。</div>
</details>
</div>
<div class="card">
<div class="title">Decision Quality Evaluation Framework at Pinterest</div>
<div class="meta-line">Authors: Yuqi Tian, Robert Paine, Attila Dobi, Kevin O&#x27;Sullivan, Aravindh Manickavasagam, Faisal Farooq</div>
<div class="meta-line">First: 2026-02-17T18:45:55+00:00 · Latest: 2026-02-17T18:45:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15809v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15809v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Online platforms require robust systems to enforce content safety policies at scale. A critical component of these systems is the ability to evaluate the quality of moderation decisions made by both human agents and Large Language Models (LLMs). However, this evaluation is challenging due to the inherent trade-offs between cost, scale, and trustworthiness, along with the complexity of evolving policies. To address this, we present a comprehensive Decision Quality Evaluation Framework developed and deployed at Pinterest. The framework is centered on a high-trust Golden Set (GDS) curated by subject matter experts (SMEs), which serves as a ground truth benchmark. We introduce an automated intelligent sampling pipeline that uses propensity scores to efficiently expand dataset coverage. We demonstrate the framework&#x27;s practical application in several key areas: benchmarking the cost-performance trade-offs of various LLM agents, establishing a rigorous methodology for data-driven prompt optimization, managing complex policy evolution, and ensuring the integrity of policy content prevalence metrics via continuous validation. The framework enables a shift from subjective assessments to a data-driven and quantitative practice for managing content safety systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Pinterest的决策质量评估框架</div>
<div class="mono" style="margin-top:8px">在线平台需要强大的系统来大规模执行内容安全政策。这些系统的一个关键组成部分是评估人类代理和大型语言模型（LLMs）所做内容审核决策的质量。然而，由于成本、规模和可信度之间的固有权衡，以及政策不断演变的复杂性，这种评估具有挑战性。为了解决这一问题，我们提出了一个全面的决策质量评估框架，该框架已在Pinterest中开发和部署。该框架以高可信度的黄金集（GDS）为核心，该黄金集由主题专家（SMEs）精心策划，作为基准真值。我们引入了一个自动化的智能抽样流程，利用倾向得分来高效扩展数据集的覆盖范围。我们展示了该框架在多个关键领域的实际应用：评估不同LLM代理的成本-性能权衡，建立数据驱动的提示优化方法，管理复杂的政策演变，以及通过持续验证确保政策内容普及度指标的完整性。该框架使内容安全系统的管理从主观评估转向数据驱动和定量实践。</div>
</details>
</div>
<div class="card">
<div class="title">Should You Use Your Large Language Model to Explore or Exploit?</div>
<div class="meta-line">Authors: Keegan Harris, Aleksandrs Slivkins</div>
<div class="meta-line">First: 2025-01-31T23:42:53+00:00 · Latest: 2026-02-17T18:41:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.00225v3">Abs</a> · <a href="https://arxiv.org/pdf/2502.00225v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We evaluate the ability of the current generation of large language models (LLMs) to help a decision-making agent facing an exploration-exploitation tradeoff. While previous work has largely study the ability of LLMs to solve combined exploration-exploitation tasks, we take a more systematic approach and use LLMs to explore and exploit in silos in various (contextual) bandit tasks. We find that reasoning models show the most promise for solving exploitation tasks, although they are still too expensive or too slow to be used in many practical settings. Motivated by this, we study tool use and in-context summarization using non-reasoning models. We find that these mitigations may be used to substantially improve performance on medium-difficulty tasks, however even then, all LLMs we study perform worse than a simple linear regression, even in non-linear settings. On the other hand, we find that LLMs do help at exploring large action spaces with inherent semantics, by suggesting suitable candidates to explore.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>你应该使用大型语言模型进行探索还是利用？</div>
<div class="mono" style="margin-top:8px">我们评估当前一代大型语言模型（LLMs）在帮助决策代理处理探索与利用权衡时的能力。虽然以往的研究主要探讨了LLMs解决结合探索与利用任务的能力，但我们采取了更系统的方法，在各种（上下文）多臂老虎机任务中分别利用LLMs进行探索和利用。我们发现，推理模型在解决利用任务方面最有前景，尽管它们在许多实际场景中仍然过于昂贵或过于缓慢。受此启发，我们研究了非推理模型的工具使用和上下文内摘要方法。我们发现，这些缓解措施可能显著提升中等难度任务的性能，但即使如此，我们研究的所有LLMs在非线性设置中仍表现不如简单的线性回归。另一方面，我们发现LLMs在探索具有内在语义的大动作空间时确实有所帮助，通过建议合适的探索候选方案。</div>
</details>
</div>
<div class="card">
<div class="title">The Geometry of Alignment Collapse: When Fine-Tuning Breaks Safety</div>
<div class="meta-line">Authors: Max Springer, Chung Peng Lee, Blossom Metevier, Jane Castleman, Bohdan Turbal, Hayoung Jung, Zeyu Shen, Aleksandra Korolova</div>
<div class="meta-line">First: 2026-02-17T18:39:15+00:00 · Latest: 2026-02-17T18:39:15+00:00</div>
<div class="meta-line">Comments: 27 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15799v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15799v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning aligned language models on benign tasks unpredictably degrades safety guardrails, even when training data contains no harmful content and developers have no adversarial intent. We show that the prevailing explanation, that fine-tuning updates should be orthogonal to safety-critical directions in high-dimensional parameter space, offers false reassurance: we show this orthogonality is structurally unstable and collapses under the dynamics of gradient descent. We then resolve this through a novel geometric analysis, proving that alignment concentrates in low-dimensional subspaces with sharp curvature, creating a brittle structure that first-order methods cannot detect or defend. While initial fine-tuning updates may indeed avoid these subspaces, the curvature of the fine-tuning loss generates second-order acceleration that systematically steers trajectories into alignment-sensitive regions. We formalize this mechanism through the Alignment Instability Condition, three geometric properties that, when jointly satisfied, lead to safety degradation. Our main result establishes a quartic scaling law: alignment loss grows with the fourth power of training time, governed by the sharpness of alignment geometry and the strength of curvature coupling between the fine-tuning task and safety-critical parameters. These results expose a structural blind spot in the current safety paradigm. The dominant approaches to safe fine-tuning address only the initial snapshot of a fundamentally dynamic problem. Alignment fragility is not a bug to be patched; it is an intrinsic geometric property of gradient descent on curved manifolds. Our results motivate the development of curvature-aware methods, and we hope will further enable a shift in alignment safety analysis from reactive red-teaming to predictive diagnostics for open-weight model deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对齐坍缩的几何学：当微调破坏安全性时</div>
<div class="mono" style="margin-top:8px">在无害任务上微调对齐的语言模型会不可预测地降低安全性保障，即使训练数据不含有害内容且开发者没有敌意。我们表明，主流解释——即微调更新应与高维参数空间中的安全性关键方向正交——提供了错误的安心：我们证明这种正交性在结构上是不稳定的，并在梯度下降的动力学下发生坍缩。我们通过一种新颖的几何分析解决了这一问题，证明了对齐集中在具有尖锐曲率的低维子空间中，形成一种脆弱的结构，一阶方法无法检测或防御。虽然初始的微调更新可能确实避免这些子空间，但微调损失的曲率会产生二阶加速，系统性地引导轨迹进入对齐敏感区域。我们通过“对齐不稳定性条件”正式化了这一机制，该条件包含三个几何特性，当它们共同满足时会导致安全性下降。我们的主要结果确立了一个四次方缩放定律：对齐损失随着训练时间的四次方增长，由对齐几何的尖锐度和微调任务与安全性关键参数之间的曲率耦合强度所决定。这些结果揭示了当前安全性范式中的结构性盲点。主流的微调安全性方法仅关注该问题的初始快照，而该问题本质上是动态的。对齐脆弱性不是需要修补的错误，而是梯度下降在曲面流形上的固有几何属性。我们的结果推动了对曲率感知方法的开发，并希望进一步促进对齐安全性分析从被动的红队测试转向主动的预测性诊断，以支持开放权重模型的部署。</div>
</details>
</div>
<div class="card">
<div class="title">Learning depth-3 circuits via quantum agnostic boosting</div>
<div class="meta-line">Authors: Srinivasan Arunachalam, Arkopal Dutt, Alexandru Gheorghiu, Michael de Oliveira</div>
<div class="meta-line">First: 2025-09-17T22:28:29+00:00 · Latest: 2026-02-17T18:38:29+00:00</div>
<div class="meta-line">Comments: 53 pages; Typos fixed for depth-3 circuits result</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.14461v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.14461v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We initiate the study of quantum agnostic learning of phase states with respect to a function class $\mathsf{C}\subseteq \{c:\{0,1\}^n\rightarrow \{0,1\}\}$: given copies of an unknown $n$-qubit state $|ψ\rangle$ which has fidelity $\textsf{opt}$ with a phase state $|φ_c\rangle=\frac{1}{\sqrt{2^n}}\sum_{x\in \{0,1\}^n}(-1)^{c(x)}|x\rangle$ for some $c\in \mathsf{C}$, output $|φ\rangle$ which has fidelity $|\langle φ| ψ\rangle|^2 \geq \textsf{opt}-\varepsilon$. To this end, we give agnostic learning protocols for the following classes: (i) Size-$t$ decision trees which runs in time $\textsf{poly}(n,t,1/\varepsilon)$. This also implies $k$-juntas can be agnostically learned in time $\textsf{poly}(n,2^k,1/\varepsilon)$. (ii) $s$-term DNF formulas in time $\textsf{poly}(n,(s/\varepsilon)^{\log \log (s/\varepsilon) \cdot \log(1/\varepsilon)})$.
  Our main technical contribution is a quantum agnostic boosting protocol which converts a weak agnostic learner, which outputs a parity state $|φ\rangle$ such that $|\langle φ|ψ\rangle|^2\geq \textsf{opt}/\textsf{poly}(n)$, into a strong learner which outputs a superposition of parity states $|φ&#x27;\rangle$ such that $|\langle φ&#x27;|ψ\rangle|^2\geq \textsf{opt} - \varepsilon$.
  Using quantum agnostic boosting, we obtain a $n^{O(\log(n/\varepsilon) \cdot \log \log n)}$-time algorithm for $\varepsilon$-learning $\textsf{poly}(n)$-sized depth-$3$ circuits (consisting of $\textsf{AND}$, $\textsf{OR}$, $\textsf{NOT}$ gates) in the uniform $\textsf{PAC}$ model given quantum examples. Classically, obtaining an algorithm with a similar complexity has been an open question in the $\textsf{PAC}$ model and our work answers this given quantum examples.</div></details>
</div>
<div class="card">
<div class="title">GenDA: Generative Data Assimilation on Complex Urban Areas via Classifier-Free Diffusion Guidance</div>
<div class="meta-line">Authors: Francisco Giral, Álvaro Manzano, Ignacio Gómez, Ricardo Vinuesa, Soledad Le Clainche</div>
<div class="meta-line">First: 2026-01-16T17:02:00+00:00 · Latest: 2026-02-17T18:27:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11440v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.11440v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Urban wind flow reconstruction is essential for assessing air quality, heat dispersion, and pedestrian comfort, yet remains challenging when only sparse sensor data are available. We propose GenDA, a generative data assimilation framework that reconstructs high-resolution wind fields on unstructured meshes from limited observations. The model employs a multiscale graph-based diffusion architecture trained on computational fluid dynamics (CFD) simulations and interprets classifier-free guidance as a learned posterior reconstruction mechanism: the unconditional branch learns a geometry-aware flow prior, while the sensor-conditioned branch injects observational constraints during sampling. This formulation enables obstacle-aware reconstruction and generalization across unseen geometries, wind directions, and mesh resolutions without retraining. We consider both sparse fixed sensors and trajectory-based observations using the same reconstruction procedure. When evaluated against supervised graph neural network (GNN) baselines and classical reduced-order data assimilation methods, GenDA reduces the relative root-mean-square error (RRMSE) by 25-57% and increases the structural similarity index (SSIM) by 23-33% across the tested meshes. Experiments are conducted on Reynolds-averaged Navier-Stokes (RANS) simulations of a real urban neighbourhood in Bristol, United Kingdom, at a characteristic Reynolds number of $\mathrm{Re}\approx2\times10^{7}$, featuring complex building geometry and irregular terrain. The proposed framework provides a scalable path toward generative, geometry-aware data assimilation for environmental monitoring in complex domains.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenDA：通过无分类器扩散引导在复杂城市区域进行生成式数据同化</div>
<div class="mono" style="margin-top:8px">城市风流重建对于评估空气质量、热量扩散和行人舒适度至关重要，但在仅有稀疏传感器数据的情况下仍具挑战性。我们提出GenDA，一种生成式数据同化框架，能够从有限的观测数据中重建非结构化网格上的高分辨率风场。该模型采用多尺度图扩散架构，基于计算流体力学（CFD）模拟进行训练，并将无分类器引导解释为一种学习的后验重建机制：无条件分支学习几何感知的风场先验，而传感器条件分支在采样过程中注入观测约束。这种形式使得模型能够感知障碍物并推广到未见过的几何结构、风向和网格分辨率，而无需重新训练。我们使用相同的重建过程处理稀疏固定传感器和基于轨迹的观测。在与监督图神经网络（GNN）基线和经典降阶数据同化方法的对比评估中，GenDA在测试网格上将相对均方根误差（RRMSE）降低了25-57%，同时将结构相似性指数（SSIM）提高了23-33%。实验基于英国布里斯托尔一个真实城市社区的雷诺平均纳维-斯托克斯（RANS）模拟，其特征雷诺数为$\mathrm{Re}\approx2\times10^{7}$，包含复杂的建筑几何和不规则地形。所提出的框架为复杂领域中生成式、几何感知的数据同化提供了可扩展的路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Urban wind flow reconstruction is essential for assessing air quality, heat dispersion, and pedestrian comfort, yet remains challenging when only sparse sensor data are available.</div>
</details>
</div>
<div class="card">
<div class="title">Large Language Models and Impossible Language Acquisition: &quot;False Promise&quot; or an Overturn of our Current Perspective towards AI</div>
<div class="meta-line">Authors: Ziyan Wang, Longlong Ma</div>
<div class="meta-line">First: 2026-02-09T09:50:12+00:00 · Latest: 2026-02-17T18:26:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08437v4">Abs</a> · <a href="https://arxiv.org/pdf/2602.08437v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In Chomsky&#x27;s provocative critique &quot;The False Promise of CHATGPT,&quot; Large Language Models (LLMs) are characterized as mere pattern predictors that do not acquire languages via intrinsic causal and self-correction structures like humans, therefore are not able to distinguish impossible languages. It stands as a representative in a fundamental challenge to the intellectual foundations of AI, for it integrally synthesizes major issues in methodologies within LLMs and possesses an iconic a priori rationalist perspective. We examine this famous critique from both the perspective in pre-existing literature of linguistics and psychology as well as a research based on an experiment inquiring into the capacity of learning both possible and impossible languages among LLMs. We constructed a set of syntactically impossible languages by applying certain transformations to English. These include reversing whole sentences, and adding negation based on word-count parity. Two rounds of controlled experiments were each conducted on GPT-2 small models and long short-term memory (LSTM) models. Statistical analysis (Welch&#x27;s t-test) shows GPT2 small models underperform in learning all of the impossible languages compared to their performance on the possible language (p&lt;.001). On the other hand, LSTM models&#x27; performance tallies with Chomsky&#x27;s argument, suggesting the irreplaceable role of the evolution of transformer architecture. Based on theoretical analysis and empirical findings, we propose a new vision within Chomsky&#x27;s theory towards LLMs, and a shift of theoretical paradigm outside Chomsky, from his &quot;rationalist-romantics&quot; paradigm to functionalism and empiricism in LLMs research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大型语言模型与不可能的语言习得：&quot;虚假承诺&quot;还是对当前人工智能认知基础的颠覆</div>
<div class="mono" style="margin-top:8px">在乔姆斯基的挑衅性批评《CHATGPT的虚假承诺》中，大型语言模型（LLMs）被描述为仅仅是模式预测器，它们不像人类那样通过内在因果和自我修正的结构来习得语言，因此无法区分不可能的语言。这代表了对人工智能知识基础的根本性挑战，因为它综合了LLMs方法论中的主要问题，并具有标志性的先验理性主义视角。我们从语言学和心理学既有文献的角度，以及基于实验探究LLMs学习可能和不可能语言能力的研究角度，审视这一著名批评。我们通过在英语上应用某些转换构建了一组句法上不可能的语言，包括反转整个句子和基于单词数量奇偶性添加否定。两轮受控实验分别在GPT-2小型模型和长短期记忆（LSTM）模型上进行。统计分析（Welch&#x27;s t-test）显示，GPT2小型模型在学习所有不可能语言方面的表现均不如其在可能语言上的表现（p&lt;.001）。另一方面，LSTM模型的表现与乔姆斯基的论点一致，表明转换器架构的演进在LLMs研究中不可替代的作用。基于理论分析和实证发现，我们提出了乔姆斯基理论框架下对LLMs的新愿景，并提出了从乔姆斯基的&quot;理性主义-浪漫主义&quot;范式向LLMs研究中的功能主义和经验主义范式转变的理论范式转换。</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Building Semantics Preservation in AI Model Training with Large Language Model Encodings</div>
<div class="meta-line">Authors: Suhyung Jang, Ghang Lee, Jaekun Lee, Hyunjun Lee</div>
<div class="meta-line">First: 2026-02-17T18:26:36+00:00 · Latest: 2026-02-17T18:26:36+00:00</div>
<div class="meta-line">Comments: 42nd International Symposium on Automation and Robotics in Construction (ISARC 2025)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15791v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15791v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate representation of building semantics, encompassing both generic object types and specific subtypes, is essential for effective AI model training in the architecture, engineering, construction, and operation (AECO) industry. Conventional encoding methods (e.g., one-hot) often fail to convey the nuanced relationships among closely related subtypes, limiting AI&#x27;s semantic comprehension. To address this limitation, this study proposes a novel training approach that employs large language model (LLM) embeddings (e.g., OpenAI GPT and Meta LLaMA) as encodings to preserve finer distinctions in building semantics. We evaluated the proposed method by training GraphSAGE models to classify 42 building object subtypes across five high-rise residential building information models (BIMs). Various embedding dimensions were tested, including original high-dimensional LLM embeddings (1,536, 3,072, or 4,096) and 1,024-dimensional compacted embeddings generated via the Matryoshka representation model. Experimental results demonstrated that LLM encodings outperformed the conventional one-hot baseline, with the llama-3 (compacted) embedding achieving a weighted average F1-score of 0.8766, compared to 0.8475 for one-hot encoding. The results underscore the promise of leveraging LLM-based encodings to enhance AI&#x27;s ability to interpret complex, domain-specific building semantics. As the capabilities of LLMs and dimensionality reduction techniques continue to evolve, this approach holds considerable potential for broad application in semantic elaboration tasks throughout the AECO industry.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用大语言模型编码增强建筑语义在AI模型训练中的保留</div>
<div class="mono" style="margin-top:8px">在建筑、工程、施工和运营（AECO）行业中，对建筑语义的准确表示，包括通用对象类型和具体子类型，对于有效的AI模型训练至关重要。传统的编码方法（如one-hot）往往无法传达紧密相关子类型之间的细微关系，限制了AI对语义的理解。为了解决这一局限性，本研究提出了一种新颖的训练方法，利用大语言模型（LLM）嵌入（如OpenAI GPT和Meta LLaMA）作为编码，以保留更细致的建筑语义区分。我们通过训练GraphSAGE模型对五个高层住宅建筑信息模型（BIMs）中的42种建筑对象子类型进行分类，评估了所提出的方法。测试了多种嵌入维度，包括原始高维LLM嵌入（1,536、3,072或4,096维）以及通过Matryoshka表示模型生成的1,024维压缩嵌入。实验结果表明，LLM编码优于传统的one-hot基线，其中llama-3（压缩）嵌入的加权平均F1分数达到0.8766，而one-hot编码仅为0.8475。结果突显了利用基于LLM的编码来增强AI对复杂、领域特定建筑语义理解的潜力。随着LLM能力和降维技术的不断发展，这种方法在AECO行业的语义细化任务中具有广泛的应用前景。</div>
</details>
</div>
<div class="card">
<div class="title">Timelike bounce hypersurfaces in charged null dust collapse</div>
<div class="meta-line">Authors: David Bick</div>
<div class="meta-line">First: 2026-02-17T18:19:25+00:00 · Latest: 2026-02-17T18:19:25+00:00</div>
<div class="meta-line">Comments: 44 pages, 13 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15786v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15786v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We establish results on the dynamics of interacting charged null fluids in general relativity, specifically in the context of the bouncing continuation proposed in [Ori91]. In this model - the setting for a number of prominent case studies on black hole formation - charged massless particles may instantaneously change direction (bounce) after losing all their 4-momentum due to electrostatic repulsion. We initiate the study of timelike bounce hypersurfaces in spherical symmetry: scenarios in which an incoming beam of charged null dust changes direction along a timelike surface $\mathcal{B}$, which is the (free) boundary of an interacting 2-dust region. We identify a novel decoupling of the equations of motion in this region. First, it is shown that every timelike curve segment $γ$ in the spherically symmetric quotient of Minkowski or Reissner-Nordström spacetimes arises as the bounce hypersurface $\mathcal{B}$ of a charged null dust beam incident from past null infinity $\mathcal{I}^-$. We construct a spacetime $(\mathcal{M},g_{μν})$ describing the full trajectory of the beam, which includes gluing to Reissner-Nordström and Vaidya regions. Across $\mathcal{B}$ the metric has regularity $g_{μν}\in C^{2,1}$ and satisfies Einstein&#x27;s equation classically, while $C^\infty$ gluing may be achieved across all other interfaces. We also obtain examples of timelike bounce hypersurfaces terminating in a null point. Since these constructions are teleological, we secondly consider a given charged incoming beam from past null infinity. We formulate and solve a free boundary problem which represents the formation of a timelike bounce hypersurface. The result is conditional, applying only in the exterior region of a Reissner-Nordström spacetime, and subject to a technical regularity condition.</div></details>
</div>
<div class="card">
<div class="title">Thermal Field Theory in the Presence of a Background Magnetic Field and its Application to QCD</div>
<div class="meta-line">Authors: Munshi G. Mustafa, Aritra Bandyopadhyay, Chowdhury Aminul Islam</div>
<div class="meta-line">First: 2025-02-27T19:00:08+00:00 · Latest: 2026-02-17T18:19:05+00:00</div>
<div class="meta-line">Comments: Invited review in Progress in Particle and Nuclear Physics, modified and refined to match the journal version</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.00075v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.00075v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This review has explored the fundamental principles of thermal field theory in the context of a background magnetic field, highlighting its theoretical framework and some of its applications to the thermo-magnetic QCD plasma generated in heavy-ion collisions. Our discussion has been limited to equilibrium systems for clarity and conciseness. We analysed bulk thermodynamic characteristics, including the phase diagram as well as real-time observables, shedding light on the behaviour and dynamics of the thermo-magnetic QCD medium relevant to heavy-ion physics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在背景磁场存在下的热场理论及其在QCD中的应用</div>
<div class="mono" style="margin-top:8px">本综述探讨了在背景磁场环境下热场理论的基本原理，突出了其理论框架以及在重离子碰撞中产生的热磁QCD等离子体的一些应用。为清晰和简洁起见，我们的讨论限于平衡系统。我们分析了体热力学特性，包括相图以及实时可观测量，揭示了与重离子物理相关的热磁QCD介质的行为和动力学。</div>
</details>
</div>
<div class="card">
<div class="title">This human study did not involve human subjects: Validating LLM simulations as behavioral evidence</div>
<div class="meta-line">Authors: Jessica Hullman, David Broska, Huaman Sun, Aaron Shaw</div>
<div class="meta-line">First: 2026-02-17T18:18:38+00:00 · Latest: 2026-02-17T18:18:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15785v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15785v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A growing literature uses large language models (LLMs) as synthetic participants to generate cost-effective and nearly instantaneous responses in social science experiments. However, there is limited guidance on when such simulations support valid inference about human behavior. We contrast two strategies for obtaining valid estimates of causal effects and clarify the assumptions under which each is suitable for exploratory versus confirmatory research. Heuristic approaches seek to establish that simulated and observed human behavior are interchangeable through prompt engineering, model fine-tuning, and other repair strategies designed to reduce LLM-induced inaccuracies. While useful for many exploratory tasks, heuristic approaches lack the formal statistical guarantees typically required for confirmatory research. In contrast, statistical calibration combines auxiliary human data with statistical adjustments to account for discrepancies between observed and simulated responses. Under explicit assumptions, statistical calibration preserves validity and provides more precise estimates of causal effects at lower cost than experiments that rely solely on human participants. Yet the potential of both approaches depends on how well LLMs approximate the relevant populations. We consider what opportunities are overlooked when researchers focus myopically on substituting LLMs for human participants in a study.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>本项人类研究未涉及人类受试者：验证大型语言模型模拟作为行为证据的有效性</div>
<div class="mono" style="margin-top:8px">越来越多的文献使用大型语言模型（LLMs）作为合成参与者，以在社会科学实验中生成成本效益高且几乎即时的响应。然而，关于何时此类模拟能够有效推断人类行为的指导仍有限。我们对比了两种获取因果效应有效估计的策略，并阐明了每种策略适用于探索性研究与验证性研究的假设条件。启发式方法通过提示工程、模型微调等修复策略，试图建立模拟行为与观察到的人类行为之间的可替换性。尽管启发式方法对许多探索性任务有用，但它们缺乏验证性研究通常所需的正式统计保证。相比之下，统计校准结合辅助人类数据与统计调整，以弥补观察响应与模拟响应之间的差异。在明确的假设下，统计校准能够保持有效性，并以低于仅依赖人类受试者的实验成本，提供更精确的因果效应估计。然而，这两种方法的潜力都取决于LLMs在多大程度上能够近似相关人群。我们探讨了当研究者过于狭隘地关注用LLMs替代人类受试者时，可能忽略哪些研究机会。</div>
</details>
</div>
<div class="card">
<div class="title">Context-aware Skin Cancer Epithelial Cell Classification with Scalable Graph Transformers</div>
<div class="meta-line">Authors: Lucas Sancéré, Noémie Moreau, Katarzyna Bozek</div>
<div class="meta-line">First: 2026-02-17T18:17:52+00:00 · Latest: 2026-02-17T18:17:52+00:00</div>
<div class="meta-line">Comments: 17 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15783v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15783v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Whole-slide images (WSIs) from cancer patients contain rich information that can be used for medical diagnosis or to follow treatment progress. To automate their analysis, numerous deep learning methods based on convolutional neural networks and Vision Transformers have been developed and have achieved strong performance in segmentation and classification tasks. However, due to the large size and complex cellular organization of WSIs, these models rely on patch-based representations, losing vital tissue-level context. We propose using scalable Graph Transformers on a full-WSI cell graph for classification. We evaluate this methodology on a challenging task: the classification of healthy versus tumor epithelial cells in cutaneous squamous cell carcinoma (cSCC), where both cell types exhibit very similar morphologies and are therefore difficult to differentiate for image-based approaches. We first compared image-based and graph-based methods on a single WSI. Graph Transformer models SGFormer and DIFFormer achieved balanced accuracies of $85.2 \pm 1.5$ ($\pm$ standard error) and $85.1 \pm 2.5$ in 3-fold cross-validation, respectively, whereas the best image-based method reached $81.2 \pm 3.0$. By evaluating several node feature configurations, we found that the most informative representation combined morphological and texture features as well as the cell classes of non-epithelial cells, highlighting the importance of the surrounding cellular context. We then extended our work to train on several WSIs from several patients. To address the computational constraints of image-based models, we extracted four $2560 \times 2560$ pixel patches from each image and converted them into graphs. In this setting, DIFFormer achieved a balanced accuracy of $83.6 \pm 1.9$ (3-fold cross-validation), while the state-of-the-art image-based model CellViT256 reached $78.1 \pm 0.5$.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于可扩展图变换器的上下文感知皮肤癌上皮细胞分类</div>
<div class="mono" style="margin-top:8px">来自癌症患者的全切片图像（WSIs）包含丰富的信息，可用于医学诊断或跟踪治疗进展。为了自动化分析这些图像，已经开发了多种基于卷积神经网络和视觉变换器的深度学习方法，并在分割和分类任务中取得了优异的性能。然而，由于WSIs的尺寸较大且细胞组织结构复杂，这些模型依赖于基于补丁的表示方法，从而丢失了关键的组织级上下文信息。我们提出使用可扩展的图变换器对完整的WSI细胞图进行分类。我们在一项具有挑战性的任务上评估了该方法：区分表皮细胞在皮肤鳞状细胞癌（cSCC）中的健康与肿瘤类型，其中两种细胞类型在形态上非常相似，因此对于基于图像的方法来说难以区分。我们首先在单个WSI上比较了基于图像和基于图的方法。在3折交叉验证中，图变换器模型SGFormer和DIFFormer分别达到了85.2±1.5和85.1±2.5的平衡准确率，而最佳的基于图像的方法仅达到81.2±3.0。通过评估几种节点特征配置，我们发现最具有信息量的表示结合了形态学和纹理特征，以及非上皮细胞的类别，突显了周围细胞上下文的重要性。随后，我们将工作扩展到使用多个患者提供的多个WSI进行训练。为了解决基于图像模型的计算限制，我们从每张图像中提取了四个2560×2560像素的补丁，并将其转换为图。在此设置下，DIFFormer达到了83.6±1.9的平衡准确率（3折交叉验证），而最先进的基于图像模型CellViT256仅达到78.1±0.5。</div>
</details>
</div>
<div class="card">
<div class="title">Meteorological data and Sky Images meets Neural Models for Photovoltaic Power Forecasting</div>
<div class="meta-line">Authors: Ines Montoya-Espinagosa, Antonio Agudo</div>
<div class="meta-line">First: 2026-02-17T18:14:15+00:00 · Latest: 2026-02-17T18:14:15+00:00</div>
<div class="meta-line">Comments: CAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15782v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15782v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Due to the rise in the use of renewable energies as an alternative to traditional ones, and especially solar energy, there is increasing interest in studying how to address photovoltaic forecasting in the face of the challenge of variability in photovoltaic energy production, using different methodologies. This work develops a hybrid approach for short and long-term forecasting based on two studies with the same purpose. A multimodal approach that combines images of the sky and photovoltaic energy history with meteorological data is proposed. The main goal is to improve the accuracy of ramp event prediction, increase the robustness of forecasts in cloudy conditions, and extend capabilities beyond nowcasting, to support more efficient operation of the power grid and better management of solar variability. Deep neural models are used for both nowcasting and forecasting solutions, incorporating individual and multiple meteorological variables, as well as an analytical solar position. The results demonstrate that the inclusion of meteorological data, particularly the surface long-wave, radiation downwards, and the combination of wind and solar position, significantly improves current predictions in both nowcasting and forecasting tasks, especially on cloudy days. This study highlights the importance of integrating diverse data sources to improve the reliability and interpretability of solar energy prediction models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>气象数据与天空图像结合神经模型用于光伏功率预测</div>
<div class="mono" style="margin-top:8px">随着可再生能源，尤其是太阳能的使用增加，人们越来越关注如何应对光伏能源生产波动性挑战，采用不同方法进行光伏预测。本研究开发了一种混合方法，用于短期和长期预测，基于两个具有相同目标的研究。提出了一种多模态方法，结合天空图像、光伏能源历史数据和气象数据。主要目标是提高坡度事件预测的准确性，增强阴天条件下的预测鲁棒性，并拓展能力超越现在预测，以支持电力系统的更高效运行和更好地管理太阳能波动性。现在预测和预测解决方案均采用深度神经模型，结合单个和多个气象变量以及分析性的太阳位置。结果表明，在现在预测和预测任务中，包含气象数据，特别是地表长波辐射和风速与太阳位置的结合，显著提高了当前预测的准确性，尤其是在阴天情况下。本研究强调了整合多样化数据源对于提高太阳能预测模型可靠性和可解释性的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Neural Scaling Laws for Boosted Jet Tagging</div>
<div class="meta-line">Authors: Matthias Vigl, Nicole Hartman, Michael Kagan, Lukas Heinrich</div>
<div class="meta-line">First: 2026-02-17T18:13:01+00:00 · Latest: 2026-02-17T18:13:01+00:00</div>
<div class="meta-line">Comments: 9 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15781v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15781v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The success of Large Language Models (LLMs) has established that scaling compute, through joint increases in model capacity and dataset size, is the primary driver of performance in modern machine learning. While machine learning has long been an integral component of High Energy Physics (HEP) data analysis workflows, the compute used to train state-of-the-art HEP models remains orders of magnitude below that of industry foundation models. With scaling laws only beginning to be studied in the field, we investigate neural scaling laws for boosted jet classification using the public JetClass dataset. We derive compute optimal scaling laws and identify an effective performance limit that can be consistently approached through increased compute. We study how data repetition, common in HEP where simulation is expensive, modifies the scaling yielding a quantifiable effective dataset size gain. We then study how the scaling coefficients and asymptotic performance limits vary with the choice of input features and particle multiplicity, demonstrating that increased compute reliably drives performance toward an asymptotic limit, and that more expressive, lower-level features can raise the performance limit and improve results at fixed dataset size.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于增强喷注分类的神经扩展定律</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的成功表明，通过同时增加模型容量和数据集规模来扩展计算资源是现代机器学习性能提升的主要驱动力。尽管机器学习长期以来一直是高能物理（HEP）数据分析流程的重要组成部分，但用于训练最先进的HEP模型的计算资源仍远低于工业基础模型。随着扩展定律在该领域的研究刚刚起步，我们利用公开的JetClass数据集，研究用于增强喷注分类的神经扩展定律。我们推导出计算最优的扩展定律，并确定一个可以通过增加计算资源一致接近的有效性能上限。我们研究了在高能物理中常见的数据重复现象如何改变扩展行为，从而带来可量化的有效数据集规模增益。随后，我们研究了扩展系数和渐近性能上限如何随输入特征和粒子多重性选择而变化，证明了增加计算资源能够可靠地将性能推向渐近极限，并且更具表现力的低级特征可以提高性能上限并在固定数据集规模下改善结果。</div>
</details>
</div>
<div class="card">
<div class="title">GlobeDiff: State Diffusion Process for Partial Observability in Multi-Agent Systems</div>
<div class="meta-line">Authors: Yiqin Yang, Xu Yang, Yuhua Jiang, Ni Mu, Hao Hu, Runpeng Xie, Ziyou Zhang, Siyuan Li, Yuan-Hua Ni, Qianchuan Zhao, Bo Xu</div>
<div class="meta-line">Venue: ICLR</div>
<div class="meta-line">First: 2026-02-17T18:05:48+00:00 · Latest: 2026-02-17T18:05:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.15776v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.15776v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the realm of multi-agent systems, the challenge of \emph{partial observability} is a critical barrier to effective coordination and decision-making. Existing approaches, such as belief state estimation and inter-agent communication, often fall short. Belief-based methods are limited by their focus on past experiences without fully leveraging global information, while communication methods often lack a robust model to effectively utilize the auxiliary information they provide. To solve this issue, we propose Global State Diffusion Algorithm~(GlobeDiff) to infer the global state based on the local observations. By formulating the state inference process as a multi-modal diffusion process, GlobeDiff overcomes ambiguities in state estimation while simultaneously inferring the global state with high fidelity. We prove that the estimation error of GlobeDiff under both unimodal and multi-modal distributions can be bounded. Extensive experimental results demonstrate that GlobeDiff achieves superior performance and is capable of accurately inferring the global state.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GlobeDiff：多智能体系统中部分可观测性的状态扩散过程</div>
<div class="mono" style="margin-top:8px">在多智能体系统领域，部分可观测性是有效协调和决策的关键障碍。现有方法，如信念状态估计和智能体间通信，往往存在局限。基于信念的方法受限于仅关注过去经验而未能充分利用全局信息，而通信方法通常缺乏一个稳健的模型来有效利用所提供的辅助信息。为了解决这一问题，我们提出全局状态扩散算法（GlobeDiff），通过局部观测来推断全局状态。通过将状态推断过程建模为多模态扩散过程，GlobeDiff克服了状态估计中的歧义，同时以高保真度推断全局状态。我们证明了在单模态和多模态分布下，GlobeDiff的估计误差是可以被限制的。大量实验结果表明，GlobeDiff表现出优越的性能，并能够准确推断全局状态。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260218_0416.html">20260218_0416</a>
<a href="archive/20260217_0410.html">20260217_0410</a>
<a href="archive/20260216_0403.html">20260216_0403</a>
<a href="archive/20260215_0401.html">20260215_0401</a>
<a href="archive/20260213_0421.html">20260213_0421</a>
<a href="archive/20260212_0425.html">20260212_0425</a>
<a href="archive/20260210_0435.html">20260210_0435</a>
<a href="archive/20260209_0404.html">20260209_0404</a>
<a href="archive/20260208_0353.html">20260208_0353</a>
<a href="archive/20260207_0410.html">20260207_0410</a>
<a href="archive/20260206_0412.html">20260206_0412</a>
<a href="archive/20260205_0417.html">20260205_0417</a>
<a href="archive/20260204_0421.html">20260204_0421</a>
<a href="archive/20260202_0402.html">20260202_0402</a>
<a href="archive/20260201_0354.html">20260201_0354</a>
<a href="archive/20260131_0408.html">20260131_0408</a>
<a href="archive/20260130_0406.html">20260130_0406</a>
<a href="archive/20260129_0407.html">20260129_0407</a>
<a href="archive/20260128_0407.html">20260128_0407</a>
<a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
