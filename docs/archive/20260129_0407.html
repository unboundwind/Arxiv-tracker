<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-29 04:07</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260129_0407</div>
    <div class="row"><div class="card">
<div class="title">DuwatBench: Bridging Language and Visual Heritage through an Arabic Calligraphy Benchmark for Multimodal Understanding</div>
<div class="meta-line">Authors: Shubham Patle, Sara Ghaboura, Hania Tariq, Mohammad Usman Khan, Omkar Thawakar, Rao Muhammad Anwer, Salman Khan</div>
<div class="meta-line">First: 2026-01-27T18:59:19+00:00 · Latest: 2026-01-27T18:59:19+00:00</div>
<div class="meta-line">Comments: Accepted to EACL-2026 (Main Track)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19898v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19898v1">PDF</a> · <a href="https://huggingface.co/datasets/MBZUAI/DuwatBench">Code1</a> · <a href="https://github.com/mbzuai-oryx/DuwatBench">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Arabic calligraphy represents one of the richest visual traditions of the Arabic language, blending linguistic meaning with artistic form. Although multimodal models have advanced across languages, their ability to process Arabic script, especially in artistic and stylized calligraphic forms, remains largely unexplored. To address this gap, we present DuwatBench, a benchmark of 1,272 curated samples containing about 1,475 unique words across six classical and modern calligraphic styles, each paired with sentence-level detection annotations. The dataset reflects real-world challenges in Arabic writing, such as complex stroke patterns, dense ligatures, and stylistic variations that often challenge standard text recognition systems. Using DuwatBench, we evaluated 13 leading Arabic and multilingual multimodal models and showed that while they perform well on clean text, they struggle with calligraphic variation, artistic distortions, and precise visual-text alignment. By publicly releasing DuwatBench and its annotations, we aim to advance culturally grounded multimodal research, foster fair inclusion of the Arabic language and visual heritage in AI systems, and support continued progress in this area. Our dataset (https://huggingface.co/datasets/MBZUAI/DuwatBench) and evaluation suit (https://github.com/mbzuai-oryx/DuwatBench) are publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DuwatBench：通过阿拉伯书法基准促进语言与视觉遗产的多模态理解</div>
<div class="mono" style="margin-top:8px">阿拉伯书法代表了阿拉伯语言最丰富的视觉传统之一，将语言意义与艺术形式相结合。尽管多模态模型在多种语言上取得了进展，但它们处理阿拉伯文字，尤其是艺术化和风格化的书法形式的能力仍 largely unexplored。为了解决这一问题，我们提出了 DuwatBench，这是一个包含 1,272 个精选样本的基准数据集，涵盖六种经典和现代书法风格，共约 1,475 个独特词汇，每个样本都配有句子级别的检测标注。该数据集反映了阿拉伯书写中的现实挑战，如复杂的笔画模式、密集的连字以及风格变化，这些常常对标准文本识别系统构成困难。通过使用 DuwatBench，我们评估了 13 个领先的阿拉伯语和多语言多模态模型，并表明尽管它们在干净文本上表现良好，但在书法变体、艺术扭曲和精确的视觉-文本对齐方面存在困难。通过公开发布 DuwatBench 及其标注，我们旨在推动以文化为基础的多模态研究，促进阿拉伯语言和视觉遗产在人工智能系统中的公平纳入，并支持该领域持续的发展。我们的数据集（https://huggingface.co/datasets/MBZUAI/DuwatBench）和评估工具（https://github.com/mbzuai-oryx/DuwatBench）已公开。</div>
</details>
</div>
<div class="card">
<div class="title">Self-Distillation Enables Continual Learning</div>
<div class="meta-line">Authors: Idan Shenfeld, Mehul Damani, Jonas Hübotter, Pulkit Agrawal</div>
<div class="meta-line">First: 2026-01-27T18:59:08+00:00 · Latest: 2026-01-27T18:59:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19897v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19897v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual learning, enabling models to acquire new skills and knowledge without degrading existing capabilities, remains a fundamental challenge for foundation models. While on-policy reinforcement learning can reduce forgetting, it requires explicit reward functions that are often unavailable. Learning from expert demonstrations, the primary alternative, is dominated by supervised fine-tuning (SFT), which is inherently off-policy. We introduce Self-Distillation Fine-Tuning (SDFT), a simple method that enables on-policy learning directly from demonstrations. SDFT leverages in-context learning by using a demonstration-conditioned model as its own teacher, generating on-policy training signals that preserve prior capabilities while acquiring new skills. Across skill learning and knowledge acquisition tasks, SDFT consistently outperforms SFT, achieving higher new-task accuracy while substantially reducing catastrophic forgetting. In sequential learning experiments, SDFT enables a single model to accumulate multiple skills over time without performance regression, establishing on-policy distillation as a practical path to continual learning from demonstrations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自蒸馏实现持续学习</div>
<div class="mono" style="margin-top:8px">持续学习，使模型能够在不损害已有能力的前提下获取新技能和知识，仍然是基础模型的核心挑战。虽然基于策略的强化学习可以减少遗忘，但它需要显式的奖励函数，而这些函数往往不可用。相比之下，从专家演示中学习是主要的替代方法，但其主要依赖于监督微调（SFT），而SFT本质上是离策略的。我们引入了自蒸馏微调（SDFT），这是一种简单的方法，使模型能够直接从演示中进行基于策略的学习。SDFT通过使用以演示为条件的模型作为自身的教师，利用上下文学习生成基于策略的训练信号，在保留已有能力的同时获取新技能。在技能学习和知识获取任务中，SDFT始终优于SFT，实现了更高的新任务准确率，同时显著减少了灾难性遗忘。在序列学习实验中，SDFT使单个模型能够随时间积累多项技能，而不会出现性能退化，确立了基于策略的蒸馏作为从演示中实现持续学习的实用途径。</div>
</details>
</div>
<div class="card">
<div class="title">Post-LayerNorm Is Back: Stable, ExpressivE, and Deep</div>
<div class="meta-line">Authors: Chen Chen, Lai Wei</div>
<div class="meta-line">First: 2026-01-27T18:58:46+00:00 · Latest: 2026-01-27T18:58:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19895v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19895v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM) scaling is hitting a wall. Widening models yields diminishing returns, and extending context length does not improve fundamental expressivity. In contrast, depth scaling offers theoretically superior expressivity, yet current Transformer architectures struggle to train reliably at extreme depths. We revisit the Post-LayerNorm (Post-LN) formulation, whose instability at scale caused its replacement by Pre-LN in modern LLMs. We show that the central failure mode of Post-LN arises from the ResNet-style residual pathway, which introduces gradient vanishing in deep networks. We present Keel, a Post-LN Transformer that replaces this residual path with a Highway-style connection. This modification preserves the gradient flow through the residual branch, preventing signal vanishing from the top layers to the bottom. Unlike prior methods, Keel enables stable training at extreme depths without requiring specialized initialization or complex optimization tricks. Keel trains robustly at depths exceeding 1000 layers and consistently improves perplexity and depth-scaling characteristics over Pre-LN. These findings indicate that Post-LN, when paired with a Highway-style connection, provides a simple and effective foundation for building deeply scalable LLMs, opening the possibility for future infinite-depth architectures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>后层归一化回归：稳定、表达力强且深层</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLM）的扩展遇到了瓶颈。扩大模型规模带来的收益逐渐减少，而增加上下文长度并不能提升基本的表达能力。相比之下，深度扩展在理论上提供了更优的表达能力，但当前的Transformer架构在极端深度下难以可靠地进行训练。我们重新审视了后层归一化（Post-LayerNorm，Post-LN）的结构，其在大规模应用中的不稳定性导致其被现代LLM中的前层归一化（Pre-LN）所取代。我们发现，Post-LN的主要失效模式源于ResNet风格的残差路径，这会导致深层网络中梯度消失。我们提出了Keel，一种采用后层归一化结构的Transformer，它用Highway风格的连接替换了传统的残差路径。这种修改保留了残差分支中的梯度流动，防止了从顶层到底层的信号消失。与之前的方法不同，Keel可以在极端深度下实现稳定训练，而无需特殊的初始化或复杂的优化技巧。Keel能够在超过1000层的深度下稳健训练，并且在Perplexity和深度扩展特性上持续优于Pre-LN。这些发现表明，当与Highway风格的连接结合时，Post-LN为构建深层可扩展的LLM提供了一个简单而有效的基础，为未来无限深度的架构打开了可能性。</div>
</details>
</div>
<div class="card">
<div class="title">&quot;Not in My Backyard&quot;: LLMs Uncover Online and Offline Social Biases Against Homelessnes</div>
<div class="meta-line">Authors: Jonathan A. Karr, Benjamin F. Herbst, Matthew L. Sisk, Xueyun Li, Ting Hua, Matthew Hauenstein, Georgina Curto, Nitesh V. Chawla</div>
<div class="meta-line">First: 2025-08-14T17:58:34+00:00 · Latest: 2026-01-27T18:56:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.13187v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.13187v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Homelessness is a persistent social challenge, impacting millions worldwide. Over 876,000 people experienced homelessness (PEH) in the U.S. in 2025. Social bias is a significant barrier to alleviation, shaping public perception and influencing policymaking. Given that online textual media and offline city council discourse reflect and influence part of public opinion, it provides valuable insights to identify and track social biases against PEH. We present a new, manually-annotated multi-domain dataset compiled from Reddit, X (formerly Twitter), news articles, and city council meeting minutes across ten U.S. cities. Our 16-category multi-label taxonomy creates a challenging long-tail classification problem: some categories appear in less than 1% of samples, while others exceed 70%. We find that small human-annotated datasets (1,702 samples) are insufficient for training effective classifiers, whether used to fine-tune encoder models or as few-shot examples for LLMs. To address this, we use GPT-4.1 to generate pseudo-labels on a larger unlabeled corpus. Training on this expanded dataset enables even small encoder models (ModernBERT, 150M parameters) to achieve 35.23 macro-F1, approaching GPT-4.1&#x27;s 41.57. This demonstrates that \textbf{data quantity matters more than model size}, enabling low-cost, privacy-preserving deployment without relying on commercial APIs. Our results reveal that negative bias against PEH is prevalent both offline and online (especially on Reddit), with &quot;not in my backyard&quot; narratives showing the highest engagement. These findings uncover a type of ostracism that directly impacts poverty-reduction policymaking and provide actionable insights for practitioners addressing homelessness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>&quot;Not in My Backyard&quot;: LLMs Uncover Online and Offline Social Biases Against Homelessness</div>
<div class="mono" style="margin-top:8px">无家可归是一个持续的社会挑战，影响着全球数百万人。2025年，美国有超过876,000人经历了无家可归（PEH）。社会偏见是缓解这一问题的重要障碍，影响公众认知并塑造政策制定。鉴于在线文本媒体和线下城市委员会讨论反映了并影响了部分公众意见，识别和追踪针对PEH的社会偏见具有重要价值。我们提出了一种新的、人工标注的多领域数据集，该数据集从Reddit、X（原Twitter）、新闻文章和十个美国城市的市政会议记录中收集而来。我们的16类多标签分类体系构成了一个具有挑战性的长尾分类问题：某些类别在样本中出现频率低于1%，而其他类别则超过70%。我们发现，小型人工标注数据集（1,702个样本）不足以训练有效的分类器，无论是用于微调编码器模型还是作为LLMs的少样本示例。为了解决这一问题，我们使用GPT-4.1在更大的未标注语料库上生成伪标签。在该扩展数据集上进行训练，即使使用小型编码器模型（ModernBERT，1.5亿参数）也能达到35.23的宏F1值，接近GPT-4.1的41.57。这表明\textbf{数据量比模型大小更重要}，使得低成本、隐私保护的部署成为可能，而无需依赖商业API。我们的研究结果揭示了针对PEH的负面偏见在离线和在线环境中都普遍存在（尤其是在Reddit上），其中&quot;Not in My Backyard&quot;（不在我后院）的叙事获得了最高参与度。这些发现揭示了一种直接对减贫政策制定产生影响的排斥现象，并为解决无家可归问题的实践者提供了可操作的见解。</div>
</details>
</div>
<div class="card">
<div class="title">M-SGWR: Multiscale Similarity and Geographically Weighted Regression</div>
<div class="meta-line">Authors: M. Naser Lessani, Zhenlong Li, Manzhu Yu, Helen Greatrex, Chan Shen</div>
<div class="meta-line">First: 2026-01-27T18:55:12+00:00 · Latest: 2026-01-27T18:55:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19888v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19888v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The first law of geography is a cornerstone of spatial analysis, emphasizing that nearby and related locations tend to be more similar, however, defining what constitutes &quot;near&quot; and &quot;related&quot; remains challenging, as different phenomena exhibit distinct spatial patterns. Traditional local regression models, such as Geographically Weighted Regression (GWR) and Multiscale GWR (MGWR), quantify spatial relationships solely through geographic proximity. In an era of globalization and digital connectivity, however, geographic proximity alone may be insufficient to capture how locations are interconnected. To address this limitation, we propose a new multiscale local regression framework, termed M-SGWR, which characterizes spatial interaction across two dimensions: geographic proximity and attribute (variable) similarity. For each predictor, geographic and attribute-based weight matrices are constructed separately and then combined using an optimized parameter, alpha, which governs their relative contribution to local model fitting. Analogous to variable-specific bandwidths in MGWR, the optimal alpha varies by predictor, allowing the model to flexibly account for geographic, mixed, or non-spatial (remote similarity) effects. Results from two simulation experiments and one empirical application demonstrate that M-SGWR consistently outperforms GWR, SGWR, and MGWR across all goodness-of-fit metrics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>M-SGWR：多尺度相似性与地理加权回归</div>
<div class="mono" style="margin-top:8px">地理第一定律是空间分析的基石，强调相近且相关的地点往往更为相似，然而，如何定义&quot;近&quot;和&quot;相关&quot;仍具挑战性，因为不同现象表现出不同的空间模式。传统的局部回归模型，如地理加权回归（GWR）和多尺度地理加权回归（MGWR），仅通过地理邻近性来量化空间关系。然而，在全球化和数字连接的时代，仅依靠地理邻近性可能不足以捕捉地点之间的互联性。为了解决这一局限性，我们提出了一种新的多尺度局部回归框架，称为M-SGWR，该框架通过地理邻近性和属性（变量）相似性两个维度来刻画空间交互。对于每个预测变量，分别构建地理权重矩阵和属性权重矩阵，然后通过一个优化参数alpha进行组合，该参数控制其在局部模型拟合中的相对贡献。类似于MGWR中的变量特异性带宽，最优的alpha值因预测变量而异，使模型能够灵活地考虑地理、混合或非空间（远程相似性）效应。两个模拟实验和一个实证应用的结果表明，M-SGWR在所有拟合优度指标上均优于GWR、SGWR和MGWR。</div>
</details>
</div>
<div class="card">
<div class="title">VGGT-SLAM 2.0: Real time Dense Feed-forward Scene Reconstruction</div>
<div class="meta-line">Authors: Dominic Maggio, Luca Carlone</div>
<div class="meta-line">First: 2026-01-27T18:54:29+00:00 · Latest: 2026-01-27T18:54:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19887v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19887v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present VGGT-SLAM 2.0, a real time RGB feed-forward SLAM system which substantially improves upon VGGT-SLAM for incrementally aligning submaps created from VGGT. Firstly, we remove high-dimensional 15-degree-of-freedom drift and planar degeneracy from VGGT-SLAM by creating a new factor graph design while still addressing the reconstruction ambiguity of VGGT given unknown camera intrinsics. Secondly, by studying the attention layers of VGGT, we show that one of the layers is well suited to assist in image retrieval verification for free without additional training, which enables both rejecting false positive matches and allows for completing more loop closures. Finally, we conduct a suite of experiments which includes showing VGGT-SLAM 2.0 can easily be adapted for open-set object detection and demonstrating real time performance while running online onboard a ground robot using a Jetson Thor. We also test in environments ranging from cluttered indoor apartments and office scenes to a 4,200 square foot barn, and we also demonstrate VGGT-SLAM 2.0 achieves the highest accuracy on the TUM dataset with about 23 percent less pose error than VGGT-SLAM. Code will be released upon publication.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VGGT-SLAM 2.0：实时密集前馈场景重建</div>
<div class="mono" style="margin-top:8px">我们提出了VGGT-SLAM 2.0，这是一个实时RGB前馈SLAM系统，显著改进了VGGT-SLAM，使其能够更有效地对由VGGT创建的子地图进行增量对齐。首先，我们通过设计新的因子图来消除VGGT-SLAM中的高维15自由度漂移和平面退化，同时仍然解决VGGT在未知相机内参情况下的重建歧义问题。其次，通过研究VGGT的注意力层，我们发现其中一层非常适合在无需额外训练的情况下协助图像检索验证，这使得系统能够拒绝误匹配并完成更多的闭环检测。最后，我们进行了一系列实验，包括展示VGGT-SLAM 2.0可以轻松适应开放集目标检测，并在搭载Jetson Thor的地面机器人上实现实时性能。我们还在从杂乱的室内公寓和办公室场景到4200平方英尺谷仓的不同环境中进行了测试，并展示了VGGT-SLAM 2.0在TUM数据集上实现了比VGGT-SLAM高约23%的位姿误差精度的最高准确性。代码将在发表后发布。</div>
</details>
</div>
<div class="card">
<div class="title">Demystifying the Roles of LLM Layers in Retrieval, Knowledge, and Reasoning</div>
<div class="meta-line">Authors: Xinyuan Song, Keyu Wang, PengXiang Li, Lu Yin, Shiwei Liu</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-10-02T14:57:13+00:00 · Latest: 2026-01-27T18:53:30+00:00</div>
<div class="meta-line">Comments: Accepted by ICASSP 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.02091v4">Abs</a> · <a href="https://arxiv.org/pdf/2510.02091v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent studies suggest that the deeper layers of Large Language Models (LLMs) contribute little to representation learning and can often be removed without significant performance loss. However, such claims are typically drawn from narrow evaluations and may overlook important aspects of model behavior. In this work, we present a systematic study of depth utilization across diverse dimensions, including evaluation protocols, task categories, and model architectures. Our analysis confirms that very deep layers are generally less effective than earlier ones, but their contributions vary substantially with the evaluation setting. Under likelihood-based metrics without generation, pruning most layers preserves performance, with only the initial few being critical. By contrast, generation-based evaluation uncovers indispensable roles for middle and deeper layers in enabling reasoning and maintaining long-range coherence. We further find that knowledge and retrieval are concentrated in shallow components, whereas reasoning accuracy relies heavily on deeper layers -- yet can be reshaped through distillation. These results highlight that depth usage in LLMs is highly heterogeneous and context-dependent, underscoring the need for task-, metric-, and model-aware perspectives in both interpreting and compressing large models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>解析大语言模型层数在检索、知识和推理中的作用</div>
<div class="mono" style="margin-top:8px">近期研究表明，大语言模型（LLMs）的深层结构对表征学习贡献有限，通常可以移除而不显著影响性能。然而，这些结论通常基于狭窄的评估方式，可能忽略了模型行为的重要方面。在本工作中，我们系统地研究了不同维度下的深度利用情况，包括评估协议、任务类型和模型架构。我们的分析确认，非常深层的结构通常不如早期层有效，但其贡献会随着评估设置而显著变化。在不涉及生成的基于似然的指标下，修剪大部分层可以保持性能，仅有最初的几层是关键。相比之下，基于生成的评估揭示了中间层和深层结构在推理能力和维持长程连贯性中的不可或缺作用。我们进一步发现，知识和检索主要集中在浅层组件中，而推理准确性则高度依赖深层结构，但可以通过蒸馏进行重塑。这些结果表明，大语言模型中的深度使用具有高度异质性和上下文依赖性，强调了在解释和压缩大型模型时需要任务、指标和模型感知的视角。</div>
</details>
</div>
<div class="card">
<div class="title">AI Cap-and-Trade: Efficiency Incentives for Accessibility and Sustainability</div>
<div class="meta-line">Authors: Marco Bornstein, Amrit Singh Bedi</div>
<div class="meta-line">First: 2026-01-27T18:53:21+00:00 · Latest: 2026-01-27T18:53:21+00:00</div>
<div class="meta-line">Comments: 22 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19886v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19886v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The race for artificial intelligence (AI) dominance often prioritizes scale over efficiency. Hyper-scaling is the common industry approach: larger models, more data, and as many computational resources as possible. Using more resources is a simpler path to improved AI performance. Thus, efficiency has been de-emphasized. Consequently, the need for costly computational resources has marginalized academics and smaller companies. Simultaneously, increased energy expenditure, due to growing AI use, has led to mounting environmental costs. In response to accessibility and sustainability concerns, we argue for research into, and implementation of, market-based methods that incentivize AI efficiency. We believe that incentivizing efficient operations and approaches will reduce emissions while opening new opportunities for academics and smaller companies. As a call to action, we propose a cap-and-trade system for AI. Our system provably reduces computations for AI deployment, thereby lowering emissions and monetizing efficiency to the benefit of of academics and smaller companies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AI 配额交易：提升可访问性与可持续性的效率激励</div>
<div class="mono" style="margin-top:8px">在争夺人工智能（AI）主导权的竞争中，往往优先考虑规模而非效率。行业普遍采用超大规模扩展的方法：更大的模型、更多的数据以及尽可能多的计算资源。使用更多资源是提升AI性能的简单途径。因此，效率被弱化。结果，对昂贵计算资源的需求使学术界和小型公司处于不利地位。同时，由于AI使用量的增加，能源消耗也随之上升，导致环境成本不断累积。为应对可访问性和可持续性问题，我们主张研究并实施基于市场的机制，以激励AI效率。我们认为，激励高效操作和方法将减少排放，同时为学术界和小型公司创造新的机会。作为一项行动呼吁，我们提出一种AI的配额交易系统。我们的系统可证明地减少AI部署的计算量，从而降低排放，并将效率货币化，使学术界和小型公司受益。</div>
</details>
</div>
<div class="card">
<div class="title">SONIC: Spectral Oriented Neural Invariant Convolutions</div>
<div class="meta-line">Authors: Gijs Joppe Moens, Regina Beets-Tan, Eduardo H. P. Pooch</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-27T18:51:11+00:00 · Latest: 2026-01-27T18:51:11+00:00</div>
<div class="meta-line">Comments: 10 pages, 4 figures. Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19884v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19884v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures. Vision Transformers (ViTs), in turn, provide global connectivity but lack spatial inductive bias, depend on explicit positional encodings, and remain tied to the initial patch size. Bridging these limitations requires a representation that is both structured and global. We introduce SONIC (Spectral Oriented Neural Invariant Convolutions), a continuous spectral parameterisation that models convolutional operators using a small set of shared, orientation-selective components. These components define smooth responses across the full frequency domain, yielding global receptive fields and filters that adapt naturally across resolutions. Across synthetic benchmarks, large-scale image classification, and 3D medical datasets, SONIC shows improved robustness to geometric transformations, noise, and resolution shifts, and matches or exceeds convolutional, attention-based, and prior spectral architectures with an order of magnitude fewer parameters. These results demonstrate that continuous, orientation-aware spectral parameterisations provide a principled and scalable alternative to conventional spatial and spectral operators.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SONIC: 基于频谱导向的神经不变卷积</div>
<div class="mono" style="margin-top:8px">卷积神经网络（CNNs）依赖固定大小的卷积核扫描局部区域，这限制了其在不采用非常深架构的情况下捕捉全局上下文或长距离依赖的能力。另一方面，视觉Transformer（ViTs）虽然提供了全局连接性，但缺乏空间归纳偏置，依赖显式的方位编码，并且仍然受限于初始的图像块大小。克服这些限制需要一种既结构化又具有全局性的表示方法。我们引入SONIC（频谱导向的神经不变卷积），这是一种连续的频谱参数化方法，通过一组共享的、具有方向选择性的组件来建模卷积操作。这些组件在完整的频域中定义平滑响应，从而产生全局感受野和适应不同分辨率的滤波器。在合成基准测试、大规模图像分类和3D医学数据集上，SONIC表现出对几何变换、噪声和分辨率变化更强的鲁棒性，并且在参数数量仅为传统方法十分之一的情况下，其性能与卷积、注意力机制和先前频谱架构相当或更优。这些结果表明，连续的、具有方向感知的频谱参数化方法为传统空间和频谱操作提供了一种原理性且可扩展的替代方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Convolutional Neural Networks (CNNs) rely on fixed-size kernels scanning local patches, which limits their ability to capture global context or long-range dependencies without very deep architectures.</div>
</details>
</div>
<div class="card">
<div class="title">Probing Spin Defects via Single Spin Relaxometry</div>
<div class="meta-line">Authors: Alex L. Melendez, Ruotian Gong, Guanghui He, Yan Wang, Yueh-Chun Wu, Thomas Poirier, Steven Randolph, Sujoy Ghosh, Liangbo Liang, Stephen Jesse, An-Ping Li, Joshua T. Damron, Benjamin J. Lawrie, James H. Edgar, Ivan V. Vlassiouk, Chong Zu, Huan Zhao</div>
<div class="meta-line">First: 2025-04-13T04:47:13+00:00 · Latest: 2026-01-27T18:40:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.09432v3">Abs</a> · <a href="https://arxiv.org/pdf/2504.09432v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spin defects in solids offer promising platforms for quantum sensing and memory due to their long coherence times and optical addressability. Here, we integrate a single nitrogen-vacancy (NV) center in diamond with scanning probe microscopy to discover, read out, and spatially map arbitrary spin-based quantum sensors at the nanoscale. Using the boron vacancy ($\mathrm{V}_\mathrm{B}^-$) center in hexagonal boron nitride$\unicode{x2013}$an emerging two-dimensional spin system$\unicode{x2013}$as a model, we detect its electron spin resonance indirectly via changes in the spin relaxation time ($T_1$) of a nearby NV center, eliminating the need for optical excitation or fluorescence detection of the $\mathrm{V}_\mathrm{B}^-$. Cross-relaxation between NV and $\mathrm{V}_\mathrm{B}^-$ ensembles significantly reduces NV $T_1$, enabling quantitative nanoscale mapping of defect densities beyond the optical diffraction limit and clear resolution of hyperfine splitting in isotopically enriched h$^{10}$B$^{15}$N. Our method demonstrates interactions between 3D and 2D spin sensors, establishing NV centers as versatile probes for characterizing otherwise inaccessible spin defects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过单个自旋弛豫测量探测自旋缺陷</div>
<div class="mono" style="margin-top:8px">由于其长相干时间和光学可寻址性，固体中的自旋缺陷为量子传感和存储提供了有前景的平台。在此，我们通过将单个氮空位（NV）中心与扫描探针显微镜集成，实现了在纳米尺度上发现、读取和空间映射任意基于自旋的量子传感器。以六方氮化硼中的硼空位（$\mathrm{V}_\mathrm{B}^-$）中心——一种新兴的二维自旋系统——作为模型，我们通过检测附近NV中心的自旋弛豫时间（$T_1$）的变化间接探测其电子自旋共振，从而无需对$\mathrm{V}_\mathrm{B}^-$进行光学激发或荧光检测。NV与$\mathrm{V}_\mathrm{B}^-$集合体之间的交叉弛豫显著降低了NV的$T_1$，使得我们能够超越光学衍射极限进行缺陷密度的定量纳米尺度映射，并清晰地分辨同位素富集的h$^{10}$B$^{15}$N中的超精细分裂。我们的方法展示了三维和二维自旋传感器之间的相互作用，确立了NV中心作为表征其他不可接触自旋缺陷的多功能探针。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Spin defects in solids offer promising platforms for quantum sensing and memory due to their long coherence times and optical addressability.</div>
</details>
</div>
<div class="card">
<div class="title">RHSIA: Real-time Hemodynamics Surrogation for Non-idealized Intracranial Aneurysms</div>
<div class="meta-line">Authors: Yiying Sheng, Wenhao Ding, Dylan Roi, Leonard Leong Litt Yeo, Hwa Liang Leo, Choon Hwai Yap</div>
<div class="meta-line">First: 2026-01-27T18:39:58+00:00 · Latest: 2026-01-27T18:39:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19876v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19876v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Extensive studies suggested that fluid mechanical markers of intracranial aneurysms (IAs) derived from Computational Fluid Dynamics (CFD) can indicate disease progression risks, but to date this has not been translated clinically. This is because CFD requires specialized expertise and is time-consuming and low throughput, making it difficult to support clinical trials. A deep learning model that maps IA morphology to biomechanical markers can address this, enabling physicians to obtain these markers in real time without performing CFD. Here, we show that a Graph Transformer model that incorporates temporal information, which is supervised by large CFD data, can accurately predict Wall Shear Stress (WSS) across the cardiac cycle from IA surface meshes. The model effectively captures the temporal variations of the WSS pattern, achieving a Structural Similarity Index (SSIM) of up to 0.981 and a maximum-based relative L2 error of 2.8%. Ablation studies and SOTA comparison confirmed its optimality. Further, as pulsatile CFD data is computationally expensive to generate and sample sizes are limited, we engaged a strategy of injecting a large amount of steady-state CFD data, which are extremely low-cost to generate, as augmentation. This approach enhances network performance substantially when pulsatile CFD data sample size is small. Our study provides a proof of concept that temporal sequences cardiovascular fluid mechanical parameters can be computed in real time using a deep learning model from the geometric mesh, and this is achievable even with small pulsatile CFD sample size. Our approach is likely applicable to other cardiovascular scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RHSIA：用于非理想化颅内动脉瘤的实时血流动力学替代方法</div>
<div class="mono" style="margin-top:8px">大量研究表明，基于计算流体力学（CFD）的颅内动脉瘤（IAs）流体机械标志物可以指示疾病进展风险，但至今尚未在临床中应用。这是因为CFD需要专业技能，且耗时且低吞吐量，难以支持临床试验。一种将动脉瘤形态映射到生物力学标志物的深度学习模型可以解决这一问题，使医生能够在不进行CFD的情况下实时获取这些标志物。在此，我们展示了一种结合时间信息的图变换模型，该模型由大量CFD数据监督训练，能够从动脉瘤表面网格中准确预测整个心动周期内的壁切应力（WSS）。该模型有效捕捉了WSS模式的时间变化，实现了高达0.981的结构相似性指数（SSIM）和最大相对L2误差为2.8%的预测精度。消融研究和与当前最优方法的比较验证了其优越性。此外，由于脉动CFD数据生成成本高且样本量有限，我们采用了一种策略，即注入大量低成本生成的稳态CFD数据作为数据增强。这种方法在脉动CFD数据样本量较小时显著提升了网络性能。我们的研究证明了，通过深度学习模型从几何网格中实时计算心血管流体机械参数的时间序列是可行的，即使在脉动CFD样本量较小的情况下也能实现。我们的方法可能适用于其他心血管场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Extensive studies suggested that fluid mechanical markers of intracranial aneurysms (IAs) derived from Computational Fluid Dynamics (CFD) can indicate disease progression risks, but to date this has not been translated clinically.</div>
</details>
</div>
<div class="card">
<div class="title">A simple algorithm for output range analysis for deep neural networks</div>
<div class="meta-line">Authors: Helder Rojas, Nilton Rojas, Espinoza J. B., Luis Huamanchumo</div>
<div class="meta-line">First: 2024-07-02T22:47:40+00:00 · Latest: 2026-01-27T18:39:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2407.02700v4">Abs</a> · <a href="https://arxiv.org/pdf/2407.02700v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a novel approach for the output range estimation problem in Deep Neural Networks (DNNs) by integrating a Simulated Annealing (SA) algorithm tailored to operate within constrained domains and ensure convergence towards global optima. The method effectively addresses the challenges posed by the lack of local geometric information and the high non-linearity inherent to DNNs, making it applicable to a wide variety of architectures, with a special focus on Residual Networks (ResNets) due to their practical importance. Unlike existing methods, our algorithm imposes minimal assumptions on the internal architecture of neural networks, thereby extending its usability to complex models. Theoretical analysis guarantees convergence, while extensive empirical evaluations-including optimization tests involving functions with multiple local minima-demonstrate the robustness of our algorithm in navigating non-convex response surfaces. The experimental results highlight the algorithm&#x27;s efficiency in accurately estimating DNN output ranges, even in scenarios characterized by high non-linearity and complex constraints. For reproducibility, Python codes and datasets used in the experiments are publicly available through our GitHub repository.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种用于深度神经网络输出范围分析的简单算法</div>
<div class="mono" style="margin-top:8px">本文提出了一种新颖的方法，通过结合一种针对受限域设计并能收敛至全局最优的模拟退火（SA）算法，解决深度神经网络（DNNs）中的输出范围估计问题。该方法有效应对了缺乏局部几何信息和DNNs固有的高非线性所带来的挑战，适用于各种网络架构，特别关注残差网络（ResNets），因其在实际中的重要性。与现有方法不同，我们的算法对神经网络的内部结构做出最少假设，从而扩展了其在复杂模型中的适用性。理论分析保证了算法的收敛性，而广泛的实证评估（包括涉及多个局部极小值的优化测试）展示了该算法在非凸响应曲面上的鲁棒性。实验结果突显了该算法在准确估计DNN输出范围方面的高效性，即使在高非线性和复杂约束的情境下也能保持良好性能。为确保可复现性，我们通过GitHub仓库公开了实验中使用的Python代码和数据集。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper presents a novel approach for the output range estimation problem in Deep Neural Networks (DNNs) by integrating a Simulated Annealing (SA) algorithm tailored to operate within constrained domains and ensure convergence towards global optima.</div>
</details>
</div>
<div class="card">
<div class="title">LOGICAL-COMMONSENSEQA: A Benchmark for Logical Commonsense Reasoning</div>
<div class="meta-line">Authors: Obed Junias, Maria Leonor Pacheco</div>
<div class="meta-line">First: 2026-01-23T07:07:19+00:00 · Latest: 2026-01-27T18:33:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.16504v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.16504v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Commonsense reasoning often involves evaluating multiple plausible interpretations rather than selecting a single atomic answer, yet most benchmarks rely on single-label evaluation, obscuring whether statements are jointly plausible, mutually exclusive, or jointly implausible. We introduce LOGICAL-COMMONSENSEQA, a benchmark that re-frames commonsense reasoning as logical composition over pairs of atomic statements using plausibility-level operators (AND, OR, NEITHER/NOR). Evaluating instruction-tuned, reasoning-specialized, and fine-tuned models under zero-shot, few-shot, and chain-of-thought prompting, we find that while models perform reasonably on conjunctive and moderately on disjunctive reasoning, performance degrades sharply on negation-based questions. LOGICAL-COMMONSENSEQA exposes fundamental reasoning limitations and provides a controlled framework for advancing compositional commonsense reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LOGICAL-COMMONSENSEQA：逻辑常识推理的基准测试</div>
<div class="mono" style="margin-top:8px">常识推理通常涉及评估多个合理的解释，而非选择单一的原子答案，但大多数基准测试依赖于单标签评估，掩盖了陈述是否共同合理、相互排斥或共同不合理。我们引入了LOGICAL-COMMONSENSEQA基准测试，将常识推理重新定义为基于原子陈述对的逻辑组合，使用合理性操作符（AND，OR，NEITHER/NOR）。在零样本、少样本和思维链提示下评估指令调优、推理专用和微调模型，我们发现尽管模型在合取推理上表现尚可，在析取推理上表现中等，但在基于否定的问题上表现急剧下降。LOGICAL-COMMONSENSEQA揭示了推理的基本局限性，并提供了一个受控框架以推动组合式常识推理的发展。</div>
</details>
</div>
<div class="card">
<div class="title">Parameter-Efficient MoE LoRA for Few-Shot Multi-Style Editing</div>
<div class="meta-line">Authors: Cong Cao, Yujie Xu, Xiaodong Xu</div>
<div class="meta-line">First: 2025-11-14T12:40:21+00:00 · Latest: 2026-01-27T18:27:31+00:00</div>
<div class="meta-line">Comments: Technical report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.11236v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.11236v3">PDF</a> · <a href="https://github.com/cao-cong/FSMSE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In recent years, image editing has garnered growing attention. However, general image editing models often fail to produce satisfactory results when confronted with new styles. The challenge lies in how to effectively fine-tune general image editing models to new styles using only a limited amount of paired data. To address this issue, this paper proposes a novel few-shot style editing framework. For this task, we construct a benchmark dataset that encompasses five distinct styles. Correspondingly, we propose a parameter-efficient multi-style Mixture-of-Experts Low-Rank Adaptation (MoE LoRA) with style-specific and style-shared routing mechanisms for jointly fine-tuning multiple styles. The style-specific routing ensures that different styles do not interfere with one another, while the style-shared routing adaptively allocates shared MoE LoRAs to learn common patterns. Our MoE LoRA can automatically determine the optimal ranks for each layer through a novel metric-guided approach that estimates the importance score of each single-rank component. Additionally, we explore the optimal location to insert LoRA within the Diffusion in Transformer (DiT) model and integrate adversarial learning and flow matching to guide the diffusion training process. Experimental results demonstrate that our proposed method outperforms existing state-of-the-art approaches with significantly fewer LoRA parameters. Our code and dataset are available at https://github.com/cao-cong/FSMSE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>参数高效的MoE LoRA少样本多风格编辑</div>
<div class="mono" style="margin-top:8px">近年来，图像编辑受到了越来越多的关注。然而，通用图像编辑模型在面对新风格时往往难以产生令人满意的结果。挑战在于如何仅使用有限的配对数据，有效地对通用图像编辑模型进行微调以适应新风格。为了解决这一问题，本文提出了一种新颖的少样本风格编辑框架。为此，我们构建了一个包含五种不同风格的基准数据集。相应地，我们提出了一种参数高效的多风格Mixture-of-Experts Low-Rank Adaptation（MoE LoRA），并设计了风格特定和风格共享的路由机制，以联合微调多种风格。风格特定的路由机制确保不同风格之间不会相互干扰，而风格共享的路由机制则能自适应地分配共享的MoE LoRA，以学习通用模式。我们的MoE LoRA通过一种新颖的度量引导方法，自动确定每一层的最优秩，该方法估计每个单秩组件的重要性得分。此外，我们探索了在Diffusion in Transformer（DiT）模型中插入LoRA的最佳位置，并结合对抗学习和流匹配来指导扩散训练过程。实验结果表明，我们的方法在使用显著更少的LoRA参数的情况下，优于现有的最先进方法。我们的代码和数据集可在https://github.com/cao-cong/FSMSE获取。</div>
</details>
</div>
<div class="card">
<div class="title">Bandits in Flux: Adversarial Constraints in Dynamic Environments</div>
<div class="meta-line">Authors: Tareq Si Salem</div>
<div class="meta-line">First: 2026-01-27T18:26:07+00:00 · Latest: 2026-01-27T18:26:07+00:00</div>
<div class="meta-line">Comments: Accepted to AISTATS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19867v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19867v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate the challenging problem of adversarial multi-armed bandits operating under time-varying constraints, a scenario motivated by numerous real-world applications. To address this complex setting, we propose a novel primal-dual algorithm that extends online mirror descent through the incorporation of suitable gradient estimators and effective constraint handling. We provide theoretical guarantees establishing sublinear dynamic regret and sublinear constraint violation for our proposed policy. Our algorithm achieves state-of-the-art performance in terms of both regret and constraint violation. Empirical evaluations demonstrate the superiority of our approach.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>变化的强盗：动态环境中的对抗性约束</div>
<div class="mono" style="margin-top:8px">我们研究了在时变约束下运行的对抗性多臂老虎机这一具有挑战性的问题，这一场景受到众多现实世界应用的启发。为了解决这一复杂环境，我们提出了一种新颖的对偶算法，通过引入合适的梯度估计器和有效的约束处理机制，扩展了在线镜像下降方法。我们提供了理论保证，证明了所提出的策略在动态遗憾和约束违反方面具有次线性性能。我们的算法在遗憾和约束违反方面均实现了最先进的性能。实证评估展示了我们方法的优越性。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Semantic Inference over the Air: An Efficient Task-Oriented Communication System</div>
<div class="meta-line">Authors: Chenyang Wang, Roger Olsson, Stefan Forsström, Qing He</div>
<div class="meta-line">First: 2025-08-18T09:18:07+00:00 · Latest: 2026-01-27T18:26:00+00:00</div>
<div class="meta-line">Comments: Accepted at WCNC 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.12748v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.12748v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Empowered by deep learning, semantic communication marks a paradigm shift from transmitting raw data to conveying task-relevant meaning, enabling more efficient and intelligent wireless systems. In this study, we explore a deep learning-based task-oriented communication framework that jointly considers classification performance, computational latency, and communication cost. We evaluate ResNets-based models on the CIFAR-10 and CIFAR-100 datasets to simulate real-world classification tasks in wireless environments. We partition the model at various points to simulate split inference across a wireless channel. By varying the split location and the size of the transmitted semantic feature vector, we systematically analyze the trade-offs between task accuracy and resource efficiency. Experimental results show that, with appropriate model partitioning and semantic feature compression, the system can retain over 85\% of baseline accuracy while significantly reducing both computational load and communication overhead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>空中深度语义推理：一种高效的面向任务的通信系统</div>
<div class="mono" style="margin-top:8px">借助深度学习，语义通信标志着从传输原始数据到传达任务相关语义的范式转变，使无线系统更加高效和智能化。在本研究中，我们探索了一种基于深度学习的面向任务的通信框架，同时考虑分类性能、计算延迟和通信成本。我们在CIFAR-10和CIFAR-100数据集上评估了基于ResNets的模型，以模拟无线环境中真实世界的分类任务。我们通过在不同位置分割模型来模拟无线信道上的分割推理。通过调整分割位置和传输的语义特征向量大小，我们系统地分析了任务准确率与资源效率之间的权衡。实验结果表明，通过适当的模型分割和语义特征压缩，系统可以在保持超过85\%的基线准确率的同时，显著降低计算负载和通信开销。</div>
</details>
</div>
<div class="card">
<div class="title">Calibration without Ground Truth</div>
<div class="meta-line">Authors: Yuqing Kong, Mingyu Song, Yizhou Wang, Yifan Wu</div>
<div class="meta-line">First: 2026-01-27T18:18:47+00:00 · Latest: 2026-01-27T18:18:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19862v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19862v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Villalobos et al. [2024] predict that publicly available human text will be exhausted within the next decade. Thus, improving models without access to ground-truth labels becomes increasingly important. We propose a label-free post-processing framework that improves a strong but miscalibrated model using a weaker yet better-calibrated reference. Our framework guarantees a strict performance improvement under any proper loss. Our approach is based on a characterization of when strict improvement is possible: when the strong and reference models are not mutually calibrated. We formalize this condition, connect it to arbitrage and no-trade results from economics, and develop an efficient Bregman projection algorithm that guarantees worst-case loss reduction without labels. Experiments on representative LLMs across varying scales demonstrate that our label-free method significantly reduces proper losses and calibration errors, achieving performance competitive with supervised baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无需真实标签的校准</div>
<div class="mono" style="margin-top:8px">Villalobos 等人 [2024] 预测，公开可用的人类文本将在未来十年内耗尽。因此，改进无法获取真实标签的模型变得越来越重要。我们提出了一种无标签后处理框架，利用一个较弱但校准更好的参考模型来提升一个强大但校准不准确的模型。我们的框架在任何合理的损失函数下都能保证性能的严格提升。我们的方法基于严格提升可能性的条件：当强大模型和参考模型并非相互校准时。我们形式化了这一条件，将其与经济学中的套利和不交易结果相联系，并开发了一种高效的 Bregman 投影算法，该算法在无标签的情况下保证最坏情况下的损失减少。在不同规模的代表性大语言模型上的实验表明，我们的无标签方法显著降低了合理损失和校准误差，其性能可与监督基线相媲美。</div>
</details>
</div>
<div class="card">
<div class="title">MIP against Agent: Malicious Image Patches Hijacking Multimodal OS Agents</div>
<div class="meta-line">Authors: Lukas Aichberger, Alasdair Paren, Guohao Li, Philip Torr, Yarin Gal, Adel Bibi</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-03-13T18:59:12+00:00 · Latest: 2026-01-27T18:10:17+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.10809v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.10809v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in operating system (OS) agents have enabled vision-language models (VLMs) to directly control a user&#x27;s computer. Unlike conventional VLMs that passively output text, OS agents autonomously perform computer-based tasks in response to a single user prompt. OS agents do so by capturing, parsing, and analysing screenshots and executing low-level actions via application programming interfaces (APIs), such as mouse clicks and keyboard inputs. This direct interaction with the OS significantly raises the stakes, as failures or manipulations can have immediate and tangible consequences. In this work, we uncover a novel attack vector against these OS agents: Malicious Image Patches (MIPs), adversarially perturbed screen regions that, when captured by an OS agent, induce it to perform harmful actions by exploiting specific APIs. For instance, a MIP can be embedded in a desktop wallpaper or shared on social media to cause an OS agent to exfiltrate sensitive user data. We show that MIPs generalise across user prompts and screen configurations, and that they can hijack multiple OS agents even during the execution of benign instructions. These findings expose critical security vulnerabilities in OS agents that have to be carefully addressed before their widespread deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MIP对抗代理：恶意图像补丁劫持多模态操作系统代理</div>
<div class="mono" style="margin-top:8px">近年来，操作系统（OS）代理的进步使得视觉-语言模型（VLMs）能够直接控制用户的计算机。与传统被动输出文本的VLMs不同，OS代理能够根据单个用户提示自主执行基于计算机的任务。OS代理通过捕获、解析和分析屏幕截图，并通过应用程序编程接口（APIs）执行低级操作，如鼠标点击和键盘输入。这种直接与操作系统交互的方式显著提高了风险，因为失败或被操控可能导致立竿见影且实际的后果。在本工作中，我们揭示了一种针对这些OS代理的新攻击向量：恶意图像补丁（MIPs），即经过对抗性扰动的屏幕区域，当被OS代理捕获时，会利用特定API诱导其执行有害操作。例如，MIP可以嵌入到桌面壁纸或社交媒体中，导致OS代理泄露敏感用户数据。我们展示了MIPs能够跨用户提示和屏幕配置泛化，并且即使在执行良性指令期间，也能劫持多个OS代理。这些发现暴露了OS代理中的关键安全漏洞，在其广泛部署之前必须谨慎处理。</div>
</details>
</div>
<div class="card">
<div class="title">MLVTG: Mamba-Based Feature Alignment and LLM-Driven Purification for Multi-Modal Video Temporal Grounding</div>
<div class="meta-line">Authors: Zhiyi Zhu, Xiaoyu Wu, Zihao Liu, Linlin Yang</div>
<div class="meta-line">First: 2025-06-10T07:20:12+00:00 · Latest: 2026-01-27T18:07:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.08512v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.08512v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video Temporal Grounding (VTG), which aims to localize video clips corresponding to natural language queries, is a fundamental yet challenging task in video understanding. Existing Transformer-based methods often suffer from redundant attention and suboptimal multi-modal alignment. To address these limitations, we propose MLVTG, a novel framework that integrates two key modules: MambaAligner and LLMRefiner. MambaAligner uses stacked Vision Mamba blocks as a backbone instead of Transformers to model temporal dependencies and extract robust video representations for multi-modal alignment. LLMRefiner leverages the specific frozen layer of a pre-trained Large Language Model (LLM) to implicitly transfer semantic priors, enhancing multi-modal alignment without fine-tuning. This dual alignment strategy, temporal modeling via structured state-space dynamics and semantic purification via textual priors, enables more precise localization. Extensive experiments on QVHighlights, Charades-STA, and TVSum demonstrate that MLVTG achieves state-of-the-art performance and significantly outperforms existing baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MLVTG：基于Mamba的多模态视频时间定位与大语言模型驱动的语义净化</div>
<div class="mono" style="margin-top:8px">视频时间定位（Video Temporal Grounding, VTG）旨在定位与自然语言查询对应的视频片段，是视频理解中的基础且具有挑战性的任务。现有的基于Transformer的方法通常存在冗余注意力和次优的多模态对齐问题。为了解决这些问题，我们提出了MLVTG，一个结合两个关键模块MambaAligner和LLMRefiner的新框架。MambaAligner采用堆叠的Vision Mamba块作为主干，替代Transformer来建模时间依赖性并提取鲁棒的视频表示以实现多模态对齐。LLMRefiner利用预训练大语言模型（LLM）的特定冻结层，隐式地迁移语义先验，从而在不进行微调的情况下增强多模态对齐。这种双阶段对齐策略，通过结构化的状态空间动态建模时间信息，并通过文本先验进行语义净化，实现了更精确的定位。在QVHighlights、Charades-STA和TVSum等数据集上的大量实验表明，MLVTG取得了最先进的性能，并显著优于现有基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Video Temporal Grounding (VTG), which aims to localize video clips corresponding to natural language queries, is a fundamental yet challenging task in video understanding.</div>
</details>
</div>
<div class="card">
<div class="title">Generative Latent Alignment for Interpretable Radar Based Occupancy Detection in Ambient Assisted Living</div>
<div class="meta-line">Authors: Huy Trinh</div>
<div class="meta-line">First: 2026-01-27T18:06:51+00:00 · Latest: 2026-01-27T18:06:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19853v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19853v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we study how to make mmWave radar presence detection more interpretable for Ambient Assisted Living (AAL) settings, where camera-based sensing raises privacy concerns. We propose a Generative Latent Alignment (GLA) framework that combines a lightweight convolutional variational autoencoder with a frozen CLIP text encoder to learn a low-dimensional latent representation of radar Range-Angle (RA) heatmaps. The latent space is softly aligned with two semantic anchors corresponding to &quot;empty room&quot; and &quot;person present&quot;, and Grad-CAM is applied in this aligned latent space to visualize which spatial regions support each presence decision. On our mmWave radar dataset, we qualitatively observe that the &quot;person present&quot; class produces compact Grad-CAM blobs that coincide with strong RA returns, whereas &quot;empty room&quot; samples yield diffuse or no evidence. We also conduct an ablation study using unrelated text prompts, which degrades both reconstruction and localization, suggesting that radar-specific anchors are important for meaningful explanations in this setting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于环境辅助生活中可解释的雷达基于占用检测的生成式潜在对齐</div>
<div class="mono" style="margin-top:8px">在本工作中，我们研究如何使毫米波雷达存在检测在环境辅助生活（AAL）场景中更具可解释性，其中基于摄像头的感知会引发隐私问题。我们提出了一种生成式潜在对齐（GLA）框架，结合轻量级卷积变分自编码器与冻结的CLIP文本编码器，以学习雷达距离-角度（RA）热图的低维潜在表示。该潜在空间通过与&quot;空房间&quot;和&quot;有人存在&quot;两个语义锚点进行软对齐，并在该对齐的潜在空间中应用Grad-CAM以可视化哪些空间区域支持每个存在决策。在我们的毫米波雷达数据集上，我们定性观察到&quot;有人存在&quot;类别会产生紧凑的Grad-CAM blobs，与强RA返回相吻合，而&quot;空房间&quot;样本则产生扩散或无证据的 blobs。我们还使用无关文本提示进行了消融研究，结果表明这会同时降低重建和定位性能，说明在该设置中雷达特定的锚点对于有意义的解释至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this work, we study how to make mmWave radar presence detection more interpretable for Ambient Assisted Living (AAL) settings, where camera-based sensing raises privacy concerns.</div>
</details>
</div>
<div class="card">
<div class="title">Will It Zero-Shot?: Predicting Zero-Shot Classification Performance For Arbitrary Queries</div>
<div class="meta-line">Authors: Kevin Robbins, Xiaotong Liu, Yu Wu, Le Sun, Grady McPeak, Abby Stylianou, Robert Pless</div>
<div class="meta-line">First: 2026-01-24T17:30:23+00:00 · Latest: 2026-01-27T18:04:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.17535v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.17535v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models like CLIP create aligned embedding spaces for text and images, making it possible for anyone to build a visual classifier by simply naming the classes they want to distinguish. However, a model that works well in one domain may fail in another, and non-expert users have no straightforward way to assess whether their chosen VLM will work on their problem. We build on prior work using text-only comparisons to evaluate how well a model works for a given natural language task, and explore approaches that also generate synthetic images relevant to that task to evaluate and refine the prediction of zero-shot accuracy. We show that generated imagery to the baseline text-only scores substantially improves the quality of these predictions. Additionally, it gives a user feedback on the kinds of images that were used to make the assessment. Experiments on standard CLIP benchmark datasets demonstrate that the image-based approach helps users predict, without any labeled examples, whether a VLM will be effective for their application.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>零样本分类性能预测：任意查询的零样本能力评估</div>
<div class="mono" style="margin-top:8px">像CLIP这样的视觉-语言模型创建了文本和图像的对齐嵌入空间，使得任何人都可以通过简单地命名他们想要区分的类别来构建视觉分类器。然而，一个在某一领域表现良好的模型可能在另一领域失效，非专家用户没有直接的方法来评估他们选择的VLM是否适用于他们的具体问题。我们基于先前仅使用文本比较的工作，探索如何评估模型在特定自然语言任务上的表现，并进一步生成与该任务相关的合成图像以评估和优化零样本准确率的预测。我们表明，基于生成图像的方法显著提升了这些预测的质量。此外，它还为用户提供用于评估的图像类型反馈。在标准CLIP基准数据集上的实验表明，基于图像的方法可以帮助用户在没有任何标注示例的情况下预测VLM是否适用于他们的应用。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Dynamic Representations via An Optimally-Weighted Maximum Mean Discrepancy Optimization Framework for Continual Learning</div>
<div class="meta-line">Authors: KaiHui Huang, RunQing Wu, JinHui Sheng, HanYi Zhang, Ling Ge, JinYu Guo, Fei Ye</div>
<div class="meta-line">First: 2025-01-21T13:33:45+00:00 · Latest: 2026-01-27T18:04:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.12121v5">Abs</a> · <a href="https://arxiv.org/pdf/2501.12121v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual learning has emerged as a pivotal area of research, primarily due to its advantageous characteristic that allows models to persistently acquire and retain information. However, catastrophic forgetting can severely impair model performance. In this study, we address network forgetting by introducing a novel framework termed Optimally-Weighted Maximum Mean Discrepancy (OWMMD), which imposes penalties on representation alterations via a Multi-Level Feature Matching Mechanism (MLFMM). Furthermore, we propose an Adaptive Regularization Optimization (ARO) strategy to refine the adaptive weight vectors, which autonomously assess the significance of each feature layer throughout the optimization process, The proposed ARO approach can relieve the over-regularization problem and promote the future task learning. We conduct a comprehensive series of experiments, benchmarking our proposed method against several established baselines. The empirical findings indicate that our approach achieves state-of-the-art performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过最优加权最大均值差异优化框架学习动态表示用于持续学习</div>
<div class="mono" style="margin-top:8px">持续学习已成为一个关键的研究领域，主要因其具有使模型持续获取和保留信息的优势特性。然而，灾难性遗忘会严重损害模型性能。在本研究中，我们通过引入一种新颖的框架——最优加权最大均值差异（OWMMD），来解决网络遗忘问题，该框架通过多级特征匹配机制（MLFMM）对表示变化施加惩罚。此外，我们提出了一种自适应正则化优化（ARO）策略，以优化自适应权重向量，该策略在优化过程中自主评估每个特征层的重要性。所提出的ARO方法可以缓解过度正则化问题，并促进未来任务的学习。我们进行了一系列全面的实验，将我们的方法与多个现有基线进行基准测试。实证结果表明，我们的方法达到了最先进的性能。</div>
</details>
</div>
<div class="card">
<div class="title">TableMaster: A Recipe to Advance Table Understanding with Language Models</div>
<div class="meta-line">Authors: Lang Cao, Hanbing Liu</div>
<div class="meta-line">First: 2025-01-31T18:31:31+00:00 · Latest: 2026-01-27T18:04:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.19378v4">Abs</a> · <a href="https://arxiv.org/pdf/2501.19378v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tables serve as a fundamental format for representing structured relational data. While current language models (LMs) excel at many text-based tasks, they still face challenges in table understanding due to the complex characteristics of tabular data, such as their structured nature. In this paper, we aim to enhance LMs for improved table understanding. We identify four key challenges: 1) difficulty in locating target data, 2) deficiency in table semantics, 3) numerical inaccuracies in textual reasoning, and 4) semantic inflexibility in symbolic reasoning. To address these issues, we propose TableMaster, a recipe and comprehensive framework that integrates multiple solutions to overcome these obstacles. TableMaster first extracts relevant table content and verbalizes it with enriched semantic context. Additionally, we introduce adaptive reasoning, a flexible approach that dynamically adjusts between textual and symbolic reasoning, tailoring the reasoning process to each query. Extensive analyses and experiments demonstrate our findings and the effectiveness of TableMaster. On the WikiTQ dataset, TableMaster achieves an accuracy of 78.13% using GPT-4o-mini, surpassing existing baselines. We hope this work will serve as a practical step toward more robust and reliable table understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TableMaster：一种提升语言模型表格理解能力的方案</div>
<div class="mono" style="margin-top:8px">表格是表示结构化关系数据的基本格式。尽管当前的语言模型（LMs）在许多基于文本的任务中表现出色，但它们在表格理解方面仍面临挑战，这主要归因于表格数据的复杂特性，如其结构化特征。本文旨在提升语言模型的表格理解能力。我们识别出四个关键挑战：1）难以定位目标数据，2）表格语义的不足，3）文本推理中的数值不准确，4）符号推理中的语义僵化。为了解决这些问题，我们提出了TableMaster，这是一种集成了多种解决方案的方案和综合框架，以克服这些障碍。TableMaster首先提取相关表格内容，并通过增强的语义上下文进行语言化表达。此外，我们引入了自适应推理，这是一种灵活的方法，能够在文本推理和符号推理之间动态调整，使推理过程适应每个查询。广泛的分析和实验验证了我们的发现以及TableMaster的有效性。在WikiTQ数据集上，TableMaster使用GPT-4o-mini实现了78.13%的准确率，超过了现有的基线方法。我们希望这项工作能成为迈向更强大、更可靠表格理解的实用一步。</div>
</details>
</div>
<div class="card">
<div class="title">Hyperdisorder in tumor growth</div>
<div class="meta-line">Authors: Arturo Tozzi</div>
<div class="meta-line">First: 2026-01-27T18:01:43+00:00 · Latest: 2026-01-27T18:01:43+00:00</div>
<div class="meta-line">Comments: 10 pages, 1 figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19852v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19852v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tumor growth is constrained by spatial, mechanical, and metabolic factors whose alignment progressively breaks down across cellular, mesoscopic, and tissue scales as tumors expand. We hypothesize that this misalignment drives tumors toward a distinct architectural regime, termed hyperdisorder. Hyperdisorder is not defined by increased heterogeneity alone, but by the coexistence of elevated disorder across scales and spatial nonstationarity within the same tumor. Unlike ordinary randomness, where independent fluctuations diminish under spatial averaging, disorder here persists, reorganizes, or even amplifies with increasing observation scale, preventing convergence toward a stable architectural description. Using hematoxylin and eosin stained whole-slide images of gastric cancer from The Cancer Genome Atlas, we quantify tumor architecture using tile-based metrics that capture complementary aspects of organization, including texture entropy, microstructural fragmentation, orientation isotropy, and multiscale entropy variation. These measures are combined into a standardized hyperdisorder index, enabling unsupervised comparison across spatial regions. We find that architectural disruption is unevenly distributed and partially decoupled across scales within individual slides, consistent with growth-driven multiscale incoherence rather than uniform stochastic variability. Testable consequences include anomalous scaling of heterogeneity with sampling size, failure of coarse graining to converge, and systematic differences between tumor cores and invasive fronts. In diagnostic and clinical contexts, this framework clarifies when measurements from limited tissue samples are representative of the whole tumor and when they are dominated by scale- and location-dependent effects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>肿瘤生长中的超无序</div>
<div class="mono" style="margin-top:8px">肿瘤生长受到空间、机械和代谢因素的限制，随着肿瘤的扩张，这些因素在细胞、介观和组织尺度上的协调逐渐瓦解。我们假设这种失调推动肿瘤进入一种独特的建筑模式，称为超无序。超无序不仅仅由异质性增加所定义，而是指在不同尺度上存在高无序性，并且在同个肿瘤中表现出空间非平稳性。与普通随机性不同，普通随机性中独立波动在空间平均下会减弱，而这里的无序性则会持续、重组甚至放大，随着观察尺度的增加，阻碍了向稳定建筑描述的收敛。我们利用癌症基因组图谱（The Cancer Genome Atlas）中胃癌的苏木精-伊红染色全切片图像，通过基于瓦片的度量方法量化肿瘤结构，这些方法捕捉了组织组织的互补性特征，包括纹理熵、微观结构碎片化、方向各向同性和多尺度熵变化。这些指标被整合为一个标准化的超无序指数，从而实现不同空间区域的无监督比较。我们发现，建筑破坏在不同切片中分布不均，并且在不同尺度上部分解耦，这与生长驱动的多尺度不协调性一致，而非均匀的随机变化。可验证的后果包括异质性与采样规模的异常标度关系、粗粒化方法无法收敛，以及肿瘤核心与侵袭前沿之间的系统性差异。在诊断和临床背景下，该框架明确了有限组织样本的测量何时能代表整个肿瘤，何时则主要受尺度和位置相关效应的影响。</div>
</details>
</div>
<div class="card">
<div class="title">The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs</div>
<div class="meta-line">Authors: Piotr Nawrot, Robert Li, Renjie Huang, Sebastian Ruder, Kelly Marchisio, Edoardo M. Ponti</div>
<div class="meta-line">First: 2025-04-24T17:39:25+00:00 · Latest: 2026-01-27T17:59:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.17768v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.17768v2">PDF</a> · <a href="https://github.com/PiotrNawrot/sparse-frontier">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its efficiency-accuracy trade-offs remain unclear due to the lack of comprehensive evaluation. We address this gap with the largest-scale empirical analysis to date of training-free sparse attention, evaluating six methods across multiple model families and sizes, sequences up to 128K tokens, and sparsity levels up to 0.95 (i.e., $1/20$ attention budget) on nine diverse tasks. We first organise the rapidly evolving landscape of sparse attention methods into a taxonomy along four design axes. Our analysis then yields actionable insights: 1) sparse attention is effective -- larger sparse models outperform smaller dense ones at equivalent cost, improving the Pareto frontier; 2) due to computational constraints, token-to-page importance estimation is unfeasible during prefilling, where the choice of an alternative solution (global-to-token or block-to-block) depends on the task, but is possible during decoding, enabling better generalisation and tolerance to higher sparsity; 3) longer sequences tolerate higher sparsity, suggesting that fixed-budget methods in production are suboptimal. Together, these findings provide practical guidance for deploying sparse attention and methodological recommendations for future evaluations. Our code is available at https://github.com/PiotrNawrot/sparse-frontier.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稀疏前沿：Transformer大语言模型中的稀疏注意力权衡</div>
<div class="mono" style="margin-top:8px">稀疏注意力为扩展Transformer大语言模型的长上下文能力提供了一种有前景的策略，但由于缺乏全面的评估，其效率与准确性的权衡仍不明确。我们通过迄今为止最大规模的无训练稀疏注意力实证分析来填补这一空白，评估了六种方法在多个模型家族和规模、长达128K个标记的序列以及最高0.95（即1/20的注意力预算）稀疏度水平下的表现。我们首先将稀疏注意力方法的快速演进格局按照四个设计轴组织成一个分类体系。我们的分析得出了一些可操作的见解：1）稀疏注意力是有效的——在相同成本下，更大的稀疏模型优于较小的密集模型，从而改善帕累托前沿；2）由于计算限制，在预填充阶段无法进行标记到页面的重要性估计，其中替代方案（全局到标记或块到块）的选择取决于任务，但在解码阶段是可行的，从而实现更好的泛化能力和对更高稀疏度的容忍；3）更长的序列可以容忍更高的稀疏度，这表明生产环境中固定预算的方法是次优的。这些发现共同为稀疏注意力的部署提供了实用指导，并为未来评估提供了方法论建议。我们的代码可在https://github.com/PiotrNawrot/sparse-frontier获取。</div>
</details>
</div>
<div class="card">
<div class="title">EgoHandICL: Egocentric 3D Hand Reconstruction with In-Context Learning</div>
<div class="meta-line">Authors: Binzhu Xie, Shi Qiu, Sicheng Zhang, Yinqiao Wang, Hao Xu, Muzammal Naseer, Chi-Wing Fu, Pheng-Ann Heng</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-01-27T17:58:12+00:00 · Latest: 2026-01-27T17:58:12+00:00</div>
<div class="meta-line">Comments: Accepted in ICLR 2026, Codebase: https://github.com/Nicous20/EgoHandICL</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19850v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19850v1">PDF</a> · <a href="https://github.com/Nicous20/EgoHandICL">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Robust 3D hand reconstruction in egocentric vision is challenging due to depth ambiguity, self-occlusion, and complex hand-object interactions. Prior methods mitigate these issues by scaling training data or adding auxiliary cues, but they often struggle in unseen contexts. We present EgoHandICL, the first in-context learning (ICL) framework for 3D hand reconstruction that improves semantic alignment, visual consistency, and robustness under challenging egocentric conditions. EgoHandICL introduces complementary exemplar retrieval guided by vision-language models (VLMs), an ICL-tailored tokenizer for multimodal context, and a masked autoencoder (MAE)-based architecture trained with hand-guided geometric and perceptual objectives. Experiments on ARCTIC and EgoExo4D show consistent gains over state-of-the-art methods. We also demonstrate real-world generalization and improve EgoVLM hand-object interaction reasoning by using reconstructed hands as visual prompts. Code and data: https://github.com/Nicous20/EgoHandICL</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EgoHandICL: 基于情境学习的自摄视角三维手部重建</div>
<div class="mono" style="margin-top:8px">由于深度模糊性、自遮挡以及复杂的手-物体交互，自摄视角下的鲁棒三维手部重建具有挑战性。先前方法通过扩展训练数据或添加辅助线索来缓解这些问题，但在未见过的情境中往往表现不佳。我们提出了EgoHandICL，这是首个针对三维手部重建的情境学习（ICL）框架，通过提升语义对齐、视觉一致性以及在困难自摄条件下的鲁棒性。EgoHandICL引入了由视觉-语言模型（VLMs）引导的互补示例检索，一个针对情境学习的多模态上下文专用分词器，以及基于掩码自编码器（MAE）的架构，该架构使用手引导的几何和感知目标进行训练。在ARCTIC和EgoExo4D数据集上的实验表明，该方法在现有最佳方法之上取得了持续的性能提升。我们还展示了其在现实世界中的泛化能力，并通过使用重建的手作为视觉提示，改进了EgoVLM的手-物体交互推理。代码和数据：https://github.com/Nicous20/EgoHandICL</div>
</details>
</div>
<div class="card">
<div class="title">HexFormer: Hyperbolic Vision Transformer with Exponential Map Aggregation</div>
<div class="meta-line">Authors: Haya Alyoussef, Ahmad Bdeir, Diego Coello de Portugal Mecke, Tom Hanika, Niels Landwehr, Lars Schmidt-Thieme</div>
<div class="meta-line">First: 2026-01-27T17:56:49+00:00 · Latest: 2026-01-27T17:56:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19849v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19849v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Data across modalities such as images, text, and graphs often contains hierarchical and relational structures, which are challenging to model within Euclidean geometry. Hyperbolic geometry provides a natural framework for representing such structures. Building on this property, this work introduces HexFormer, a hyperbolic vision transformer for image classification that incorporates exponential map aggregation within its attention mechanism. Two designs are explored: a hyperbolic ViT (HexFormer) and a hybrid variant (HexFormer-Hybrid) that combines a hyperbolic encoder with an Euclidean linear classification head. HexFormer incorporates a novel attention mechanism based on exponential map aggregation, which yields more accurate and stable aggregated representations than standard centroid based averaging, showing that simpler approaches retain competitive merit. Experiments across multiple datasets demonstrate consistent performance improvements over Euclidean baselines and prior hyperbolic ViTs, with the hybrid variant achieving the strongest overall results. Additionally, this study provides an analysis of gradient stability in hyperbolic transformers. The results reveal that hyperbolic models exhibit more stable gradients and reduced sensitivity to warmup strategies compared to Euclidean architectures, highlighting their robustness and efficiency in training. Overall, these findings indicate that hyperbolic geometry can enhance vision transformer architectures by improving gradient stability and accuracy. In addition, relatively simple mechanisms such as exponential map aggregation can provide strong practical benefits.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HexFormer: 基于指数映射聚合的超几何视觉变换器</div>
<div class="mono" style="margin-top:8px">跨模态的数据如图像、文本和图通常包含层次化和关系结构，这些在欧几里得几何中难以建模。超几何几何为表示此类结构提供了自然的框架。基于这一特性，本文提出了HexFormer，这是一种用于图像分类的超几何视觉变换器，在其注意力机制中引入了指数映射聚合。研究探讨了两种设计：一种是纯超几何的ViT（HexFormer），另一种是混合变体（HexFormer-Hybrid），它结合了超几何编码器和欧几里得线性分类头。HexFormer引入了一种基于指数映射聚合的新颖注意力机制，相较于标准的基于质心的平均方法，能够产生更准确且稳定的聚合表示，表明简单的方法仍具有竞争力。在多个数据集上的实验表明，HexFormer在性能上优于欧几里得基线和先前的超几何ViT。此外，本研究还分析了超几何变换器中的梯度稳定性。结果表明，与欧几里得架构相比，超几何模型表现出更稳定的梯度和对预热策略的更低敏感性，突显了其在训练中的鲁棒性和效率。总体而言，这些发现表明超几何几何可以通过提升梯度稳定性和准确性来增强视觉变换器架构。此外，像指数映射聚合这样相对简单的机制也能提供显著的实用优势。</div>
</details>
</div>
<div class="card">
<div class="title">Chaotic Hedging with Iterated Integrals and Neural Networks</div>
<div class="meta-line">Authors: Ariel Neufeld, Philipp Schmocker</div>
<div class="meta-line">First: 2022-09-21T07:57:07+00:00 · Latest: 2026-01-27T17:46:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2209.10166v5">Abs</a> · <a href="https://arxiv.org/pdf/2209.10166v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper, we derive an $L^p$-chaos expansion based on iterated Stratonovich integrals with respect to a given exponentially integrable continuous semimartingale. By omitting the orthogonality of the expansion, we show that every $p$-integrable functional, $p \in [1,\infty)$, can be approximated by a finite sum of iterated Stratonovich integrals. Using (possibly random) neural networks as integrands, we therefere obtain universal approximation results for $p$-integrable financial derivatives in the $L^p$-sense. Moreover, we can approximately solve the $L^p$-hedging problem (coinciding for $p = 2$ with the quadratic hedging problem), where the approximating hedging strategy can be computed in closed form within short runtime.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用迭代积分和神经网络的混沌对冲</div>
<div class="mono" style="margin-top:8px">本文基于给定的指数可积连续半鞅，推导出一个$L^p$-混沌展开。通过忽略展开的正交性，我们证明了每个$p$-可积泛函，$p \in [1,\infty)$，都可以被有限个迭代Stratonovich积分近似。利用（可能随机的）神经网络作为积分因子，我们从而在$L^p$意义下获得了对$p$-可积金融衍生品的通用逼近结果。此外，我们还可以近似求解$L^p$-对冲问题（当$p=2$时，与二次对冲问题一致），其中近似对冲策略可以在短时间内以闭式形式计算。</div>
</details>
</div>
<div class="card">
<div class="title">HARMONI: Multimodal Personalization of Multi-User Human-Robot Interactions with LLMs</div>
<div class="meta-line">Authors: Jeanne Malécot, Hamed Rahimi, Jeanne Cattoni, Marie Samson, Mouad Abrini, Mahdi Khoramshahi, Maribel Pino, Mohamed Chetouani</div>
<div class="meta-line">First: 2026-01-27T17:45:04+00:00 · Latest: 2026-01-27T17:45:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.19839v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.19839v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing human-robot interaction systems often lack mechanisms for sustained personalization and dynamic adaptation in multi-user environments, limiting their effectiveness in real-world deployments. We present HARMONI, a multimodal personalization framework that leverages large language models to enable socially assistive robots to manage long-term multi-user interactions. The framework integrates four key modules: (i) a perception module that identifies active speakers and extracts multimodal input; (ii) a world modeling module that maintains representations of the environment and short-term conversational context; (iii) a user modeling module that updates long-term speaker-specific profiles; and (iv) a generation module that produces contextually grounded and ethically informed responses. Through extensive evaluation and ablation studies on four datasets, as well as a real-world scenario-driven user-study in a nursing home environment, we demonstrate that HARMONI supports robust speaker identification, online memory updating, and ethically aligned personalization, outperforming baseline LLM-driven approaches in user modeling accuracy, personalization quality, and user satisfaction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>HARMONI：利用大语言模型实现多用户人机交互的多模态个性化</div>
<div class="mono" style="margin-top:8px">现有的人机交互系统在多用户环境中往往缺乏持续个性化和动态适应的机制，限制了其在现实部署中的有效性。我们提出了HARMONI，这是一个多模态个性化框架，利用大语言模型使社会辅助机器人能够管理长期的多用户交互。该框架集成了四个关键模块：(i) 一个感知模块，用于识别主动说话者并提取多模态输入；(ii) 一个世界建模模块，用于维护环境和短期对话上下文的表示；(iii) 一个用户建模模块，用于更新长期说话者特定的个人资料；以及 (iv) 一个生成模块，用于生成上下文相关且符合伦理的响应。通过在四个数据集上的广泛评估和消融研究，以及在养老院环境中的现实场景驱动用户研究，我们证明HARMONI支持稳健的说话者识别、在线记忆更新和符合伦理的个性化，其在用户建模准确性、个性化质量和用户满意度方面均优于基线的LLM驱动方法。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient and Transferable Agentic Knowledge Graph RAG via Reinforcement Learning</div>
<div class="meta-line">Authors: Jinyeop Song, Song Wang, Julian Shun, Yada Zhu</div>
<div class="meta-line">First: 2025-09-30T15:14:24+00:00 · Latest: 2026-01-27T17:44:43+00:00</div>
<div class="meta-line">Comments: Wrong numbers are reported for main results</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.26383v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.26383v4">PDF</a> · <a href="https://github.com/Jinyeop3110/KG-R1">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Knowledge-graph retrieval-augmented generation (KG-RAG) couples large language models (LLMs) with structured, verifiable knowledge graphs (KGs) to reduce hallucinations and expose reasoning traces. However, many KG-RAG systems compose multiple LLM modules (e.g planning, reasoning, and responding), inflating inference cost and binding behavior to a specific target KG. To address this, we introduce KG-R1, an agentic KG retrieval-augmented generation (KG-RAG) framework through reinforcement learning (RL). KG-R1 utilizes a single agent that interacts with KGs as its environment, learning to retrieve at each step and incorporating the retrieved information into its reasoning and generation. The process is optimized through end-to-end RL. In controlled experiments across Knowledge-Graph Question Answering (KGQA) benchmarks, our method demonstrates both efficiency and transferability: Using Qwen-2.5-3B, KG-R1 improves answer accuracy with fewer generation tokens than prior multi-module workflow methods that use larger foundation or fine-tuned models. Furthermore, KG-R1 enables plug and play: after training, it maintains strong accuracy on new KGs without modification. These properties make KG-R1 a promising KG-RAG framework for real-world deployment. Our code is publicly available at https://github.com/Jinyeop3110/KG-R1.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过强化学习实现高效且可迁移的代理知识图谱RAG</div>
<div class="mono" style="margin-top:8px">知识图谱检索增强生成（KG-RAG）将大型语言模型（LLMs）与结构化、可验证的知识图谱（KGs）结合，以减少幻觉并暴露推理轨迹。然而，许多KG-RAG系统由多个LLM模块（如规划、推理和响应）组成，导致推理成本增加且行为绑定到特定目标KG。为了解决这一问题，我们引入了KG-R1，这是一个通过强化学习（RL）实现的代理式KG-RAG框架。KG-R1使用一个单一代理与知识图谱交互作为其环境，在每一步学习检索信息并将其整合到推理和生成过程中。整个过程通过端到端强化学习进行优化。在知识图谱问答（KGQA）基准的受控实验中，我们的方法展示了高效性和可迁移性：使用Qwen-2.5-3B模型，KG-R1在生成令牌数量更少的情况下提高了回答准确性。此外，KG-R1支持即插即用：训练后，它可以在新知识图谱上保持高准确性而无需修改。这些特性使KG-R1成为现实部署中具有前景的KG-RAG框架。我们的代码可在https://github.com/Jinyeop3110/KG-R1公开获取。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260128_0407.html">20260128_0407</a>
<a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
