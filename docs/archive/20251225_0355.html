<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-25 03:55</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251225_0355</div>
    <div class="row"><div class="card">
<div class="title">SemanticGen: Video Generation in Semantic Space</div>
<div class="meta-line">Authors: Jianhong Bai, Xiaoshi Wu, Xintao Wang, Fu Xiao, Yuanxing Zhang, Qinghe Wang, Xiaoyu Shi, Menghan Xia, Zuozhu Liu, Haoji Hu, Pengfei Wan, Kun Gai</div>
<div class="meta-line">First: 2025-12-23T18:59:56+00:00 · Latest: 2025-12-23T18:59:56+00:00</div>
<div class="meta-line">Comments: Project page: https://jianhongbai.github.io/SemanticGen/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20619v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20619v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://jianhongbai.github.io/SemanticGen/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State-of-the-art video generative models typically learn the distribution of video latents in the VAE space and map them to pixels using a VAE decoder. While this approach can generate high-quality videos, it suffers from slow convergence and is computationally expensive when generating long videos. In this paper, we introduce SemanticGen, a novel solution to address these limitations by generating videos in the semantic space. Our main insight is that, due to the inherent redundancy in videos, the generation process should begin in a compact, high-level semantic space for global planning, followed by the addition of high-frequency details, rather than directly modeling a vast set of low-level video tokens using bi-directional attention. SemanticGen adopts a two-stage generation process. In the first stage, a diffusion model generates compact semantic video features, which define the global layout of the video. In the second stage, another diffusion model generates VAE latents conditioned on these semantic features to produce the final output. We observe that generation in the semantic space leads to faster convergence compared to the VAE latent space. Our method is also effective and computationally efficient when extended to long video generation. Extensive experiments demonstrate that SemanticGen produces high-quality videos and outperforms state-of-the-art approaches and strong baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SemanticGen：语义空间中的视频生成</div>
<div class="mono" style="margin-top:8px">当前最先进的视频生成模型通常在VAE空间中学习视频潜变量的分布，并通过VAE解码器将其映射到像素。虽然这种方法可以生成高质量的视频，但在生成长视频时收敛速度慢且计算成本高。本文中，我们提出了SemanticGen，一种新颖的解决方案，通过在语义空间中生成视频来解决这些限制。我们的主要见解是，由于视频本身存在固有的冗余性，生成过程应首先在紧凑的、高层次的语义空间中进行全局规划，然后逐步添加高频细节，而不是直接使用双向注意力建模大量低层次的视频标记。SemanticGen采用两阶段生成过程。第一阶段，一个扩散模型生成紧凑的语义视频特征，定义视频的全局布局。第二阶段，另一个扩散模型根据这些语义特征生成VAE潜变量以产生最终输出。我们观察到，在语义空间中进行生成比在VAE潜空间中更快收敛。当扩展到长视频生成时，我们的方法同样有效且计算高效。大量实验表明，SemanticGen能够生成高质量的视频，并优于最先进的方法和强基线。</div>
</details>
</div>
<div class="card">
<div class="title">LongVideoAgent: Multi-Agent Reasoning with Long Videos</div>
<div class="meta-line">Authors: Runtao Liu, Ziyi Liu, Jiaqi Tang, Yue Ma, Renjie Pi, Jipeng Zhang, Qifeng Chen</div>
<div class="meta-line">First: 2025-12-23T18:59:49+00:00 · Latest: 2025-12-23T18:59:49+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20618v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20618v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://longvideoagent.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in multimodal LLMs and systems that use tools for long-video QA point to the promise of reasoning over hour-long episodes. However, many methods still compress content into lossy summaries or rely on limited toolsets, weakening temporal grounding and missing fine-grained cues. We propose a multi-agent framework in which a master LLM coordinates a grounding agent to localize question-relevant segments and a vision agent to extract targeted textual observations. The master agent plans with a step limit, and is trained with reinforcement learning to encourage concise, correct, and efficient multi-agent cooperation. This design helps the master agent focus on relevant clips via grounding, complements subtitles with visual detail, and yields interpretable trajectories. On our proposed LongTVQA and LongTVQA+ which are episode-level datasets aggregated from TVQA/TVQA+, our multi-agent system significantly outperforms strong non-agent baselines. Experiments also show reinforcement learning further strengthens reasoning and planning for the trained agent. Code and data will be shared at https://longvideoagent.github.io/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LongVideoAgent：基于长视频的多智能体推理</div>
<div class="mono" style="margin-top:8px">近年来，多模态大语言模型和用于长视频问答的系统在长时序片段上的推理展现出巨大潜力。然而，许多方法仍压缩内容为有损摘要或依赖有限的工具集，削弱了时间定位并遗漏了细粒度信息。我们提出了一种多智能体框架，其中主LLM协调一个定位智能体以识别与问题相关的时间片段，并协调一个视觉智能体以提取目标文本观察。主智能体通过步数限制进行规划，并使用强化学习训练以鼓励简洁、准确且高效的多智能体协作。这种设计有助于主智能体通过定位聚焦于相关片段，用视觉细节补充字幕，并产生可解释的推理轨迹。在我们提出的基于TVQA/TVQA+构建的长时序数据集LongTVQA和LongTVQA+上，我们的多智能体系统显著优于强大的非智能体基线。实验还表明，强化学习进一步增强了训练智能体的推理和规划能力。代码和数据将在https://longvideoagent.github.io/上公开。</div>
</details>
</div>
<div class="card">
<div class="title">SpatialTree: How Spatial Abilities Branch Out in MLLMs</div>
<div class="meta-line">Authors: Yuxi Xiao, Longfei Li, Shen Yan, Xinhang Liu, Sida Peng, Yunchao Wei, Xiaowei Zhou, Bingyi Kang</div>
<div class="meta-line">First: 2025-12-23T18:59:46+00:00 · Latest: 2025-12-23T18:59:46+00:00</div>
<div class="meta-line">Comments: webpage: https://spatialtree.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20617v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20617v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://spatialtree.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cognitive science suggests that spatial ability develops progressively-from perception to reasoning and interaction. Yet in multimodal LLMs (MLLMs), this hierarchy remains poorly understood, as most studies focus on a narrow set of tasks. We introduce SpatialTree, a cognitive-science-inspired hierarchy that organizes spatial abilities into four levels: low-level perception (L1), mental mapping (L2), simulation (L3), and agentic competence (L4). Based on this taxonomy, we construct the first capability-centric hierarchical benchmark, thoroughly evaluating mainstream MLLMs across 27 sub-abilities. The evaluation results reveal a clear structure: L1 skills are largely orthogonal, whereas higher-level skills are strongly correlated, indicating increasing interdependency. Through targeted supervised fine-tuning, we uncover a surprising transfer dynamic-negative transfer within L1, but strong cross-level transfer from low- to high-level abilities with notable synergy. Finally, we explore how to improve the entire hierarchy. We find that naive RL that encourages extensive &quot;thinking&quot; is unreliable: it helps complex reasoning but hurts intuitive perception. We propose a simple auto-think strategy that suppresses unnecessary deliberation, enabling RL to consistently improve performance across all levels. By building SpatialTree, we provide a proof-of-concept framework for understanding and systematically scaling spatial abilities in MLLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpatialTree：多模态大语言模型中空间能力的层级发展</div>
<div class="mono" style="margin-top:8px">认知科学表明，空间能力是逐步发展的，从感知到推理再到交互。然而，在多模态大语言模型（MLLMs）中，这种层级结构仍不明确，因为大多数研究集中在有限的任务集上。我们提出了SpatialTree，一个受认知科学启发的层级框架，将空间能力划分为四个层级：低级感知（L1）、心理映射（L2）、模拟（L3）和代理能力（L4）。基于这一分类体系，我们构建了首个以能力为中心的层级基准测试，全面评估主流MLLMs在27个子能力上的表现。评估结果揭示了一个清晰的结构：L1技能大多相互独立，而高级技能则高度相关，表明其相互依赖性增强。通过有针对性的监督微调，我们发现了一个令人惊讶的迁移动态：L1内部存在负迁移，但低级到高级能力之间存在显著的跨层级迁移，并表现出明显的协同效应。最后，我们探讨了如何提升整个层级结构。我们发现，鼓励广泛“思考”的简单强化学习（RL）是不可靠的：它有助于复杂推理，却损害了直观感知。我们提出了一种简单的自动思考策略，抑制不必要的深思，使强化学习能够在所有层级上持续提升性能。通过构建SpatialTree，我们提供了一个理解并系统性扩展多模态大语言模型中空间能力的概念验证框架。</div>
</details>
</div>
<div class="card">
<div class="title">Active Intelligence in Video Avatars via Closed-loop World Modeling</div>
<div class="meta-line">Authors: Xuanhua He, Tianyu Yang, Ke Cao, Ruiqi Wu, Cheng Meng, Yong Zhang, Zhuoliang Kang, Xiaoming Wei, Qifeng Chen</div>
<div class="meta-line">First: 2025-12-23T18:59:16+00:00 · Latest: 2025-12-23T18:59:16+00:00</div>
<div class="meta-line">Comments: Project Page: https://xuanhuahe.github.io/ORCA/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20615v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20615v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://xuanhuahe.github.io/ORCA/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current video avatar generation methods excel at identity preservation and motion alignment but lack genuine agency, they cannot autonomously pursue long-term goals through adaptive environmental interaction. We address this by introducing L-IVA (Long-horizon Interactive Visual Avatar), a task and benchmark for evaluating goal-directed planning in stochastic generative environments, and ORCA (Online Reasoning and Cognitive Architecture), the first framework enabling active intelligence in video avatars. ORCA embodies Internal World Model (IWM) capabilities through two key innovations: (1) a closed-loop OTAR cycle (Observe-Think-Act-Reflect) that maintains robust state tracking under generative uncertainty by continuously verifying predicted outcomes against actual generations, and (2) a hierarchical dual-system architecture where System 2 performs strategic reasoning with state prediction while System 1 translates abstract plans into precise, model-specific action captions. By formulating avatar control as a POMDP and implementing continuous belief updating with outcome verification, ORCA enables autonomous multi-step task completion in open-domain scenarios. Extensive experiments demonstrate that ORCA significantly outperforms open-loop and non-reflective baselines in task success rate and behavioral coherence, validating our IWM-inspired design for advancing video avatar intelligence from passive animation to active, goal-oriented behavior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过闭环世界建模实现视频化身中的主动智能</div>
<div class="mono" style="margin-top:8px">当前的视频化身生成方法在身份保持和动作对齐方面表现优异，但缺乏真正的自主性，无法通过适应性环境交互自主追求长期目标。我们通过引入L-IVA（Long-horizon Interactive Visual Avatar），一个用于评估随机生成环境中目标导向规划的任务和基准，以及ORCA（Online Reasoning and Cognitive Architecture），首个实现视频化身中主动智能的框架，来解决这一问题。ORCA通过两个关键创新体现内部世界模型（IWM）的能力：(1) 一个闭环OTAR循环（观察-思考-行动-反思），通过持续将预测结果与实际生成进行对比，以维持在生成不确定性下的稳健状态跟踪；(2) 一个分层双系统架构，其中System 2进行基于状态预测的战略推理，而System 1将抽象计划转化为精确的、模型特定的动作描述。通过将化身控制建模为POMDP，并实现基于结果验证的连续信念更新，ORCA能够在开放领域场景中实现自主的多步任务完成。大量实验表明，ORCA在任务成功率和行为一致性方面显著优于开环和非反思性基线，验证了我们基于IWM的设计理念，推动视频化身智能从被动动画向主动、目标导向行为的演进。</div>
</details>
</div>
<div class="card">
<div class="title">FedPOD: the deployable units of training for federated learning</div>
<div class="meta-line">Authors: Daewoon Kim, Si Young Yie, Jae Sung Lee</div>
<div class="meta-line">Venue: MICCAI</div>
<div class="meta-line">First: 2025-12-23T18:57:53+00:00 · Latest: 2025-12-23T18:57:53+00:00</div>
<div class="meta-line">Comments: 12 pages, 12 figures, MICCAI</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20610v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20610v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper proposes FedPOD (Proportionally Orchestrated Derivative) for optimizing learning efficiency and communication cost in federated learning among multiple clients. Inspired by FedPIDAvg, we define a round-wise task for FedPOD to enhance training efficiency. FedPIDAvg achieved performance improvement by incorporating the training loss reduction for prediction entropy as weights using differential terms. Furthermore, by modeling data distribution with a Poisson distribution and using a PID controller, it reduced communication costs even in skewed data distribution. However, excluding participants classified as outliers based on the Poisson distribution can limit data utilization. Additionally, PID controller requires the same participants to be maintained throughout the federated learning process as it uses previous rounds&#x27; learning information in the current round. In our approach, FedPOD addresses these issues by including participants excluded as outliers, eliminating dependency on previous rounds&#x27; learning information, and applying a method for calculating validation loss at each round. In this challenge, FedPOD presents comparable performance to FedPIDAvg in metrics of Dice score, 0.78, 0.71 and 0.72 for WT, ET and TC in average, and projected convergence score, 0.74 in average. Furthermore, the concept of FedPOD draws inspiration from Kubernetes&#x27; smallest computing unit, POD, designed to be compatible with Kubernetes auto-scaling. Extending round-wise tasks of FedPOD to POD units allows flexible design by applying scale-out similar to Kubernetes&#x27; auto-scaling. This work demonstrated the potentials of FedPOD to enhance federated learning by improving efficiency, flexibility, and performance in metrics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FedPOD：联邦学习中训练的可部署单元</div>
<div class="mono" style="margin-top:8px">本文提出FedPOD（比例协调导数），用于优化多客户端联邦学习中的学习效率和通信成本。受FedPIDAvg启发，我们为FedPOD定义了按轮次的任务以提高训练效率。FedPIDAvg通过将预测熵的训练损失减少作为权重，利用微分项实现了性能提升。此外，通过使用泊松分布建模数据分布，并结合PID控制器，即使在数据分布倾斜的情况下，也降低了通信成本。然而，根据泊松分布排除被分类为异常值的参与者可能会限制数据的使用。此外，PID控制器需要在整个联邦学习过程中保持相同的参与者，因为它在当前轮次中使用了之前轮次的学习信息。在我们的方法中，FedPOD通过包含被排除为异常值的参与者、消除对之前轮次学习信息的依赖，并应用每轮计算验证损失的方法解决了这些问题。在本挑战中，FedPOD在Dice分数等指标上与FedPIDAvg表现相当，平均分别为WT、ET和TC的0.78、0.71和0.72，以及平均收敛分数0.74。此外，FedPOD的概念借鉴了Kubernetes中最小的计算单元POD，该单元设计为兼容Kubernetes的自动扩展。将FedPOD的按轮次任务扩展到POD单元，允许通过类似于Kubernetes自动扩展的扩展方式实现灵活设计。本工作展示了FedPOD在提升联邦学习效率、灵活性和性能方面的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper proposes FedPOD (Proportionally Orchestrated Derivative) for optimizing learning efficiency and communication cost in federated learning among multiple clients.</div>
</details>
</div>
<div class="card">
<div class="title">Saddle-to-Saddle Dynamics Explains A Simplicity Bias Across Neural Network Architectures</div>
<div class="meta-line">Authors: Yedi Zhang, Andrew Saxe, Peter E. Latham</div>
<div class="meta-line">First: 2025-12-23T18:55:30+00:00 · Latest: 2025-12-23T18:55:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20607v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20607v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural networks trained with gradient descent often learn solutions of increasing complexity over time, a phenomenon known as simplicity bias. Despite being widely observed across architectures, existing theoretical treatments lack a unifying framework. We present a theoretical framework that explains a simplicity bias arising from saddle-to-saddle learning dynamics for a general class of neural networks, incorporating fully-connected, convolutional, and attention-based architectures. Here, simple means expressible with few hidden units, i.e., hidden neurons, convolutional kernels, or attention heads. Specifically, we show that linear networks learn solutions of increasing rank, ReLU networks learn solutions with an increasing number of kinks, convolutional networks learn solutions with an increasing number of convolutional kernels, and self-attention models learn solutions with an increasing number of attention heads. By analyzing fixed points, invariant manifolds, and dynamics of gradient descent learning, we show that saddle-to-saddle dynamics operates by iteratively evolving near an invariant manifold, approaching a saddle, and switching to another invariant manifold. Our analysis also illuminates the effects of data distribution and weight initialization on the duration and number of plateaus in learning, dissociating previously confounding factors. Overall, our theory offers a framework for understanding when and why gradient descent progressively learns increasingly complex solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鞍点到鞍点动力学解释了神经网络架构中的简单性偏倚</div>
<div class="mono" style="margin-top:8px">使用梯度下降训练的神经网络通常会随时间学习越来越复杂的解，这种现象被称为简单性偏倚。尽管这一现象在各种架构中被广泛观察到，但现有的理论分析缺乏一个统一的框架。我们提出了一种理论框架，用于解释一类通用神经网络中由鞍点到鞍点学习动力学产生的简单性偏倚，包括全连接、卷积和基于注意力的架构。在这里，简单意味着可以用少量隐藏单元来表达，即隐藏神经元、卷积核或注意力头。具体而言，我们表明线性网络学习的解秩逐渐增加，ReLU网络学习的解的折角数量逐渐增加，卷积网络学习的解的卷积核数量逐渐增加，自注意力模型学习的解的注意力头数量逐渐增加。通过分析固定点、不变流形和梯度下降学习的动力学，我们展示了鞍点到鞍点动力学是如何通过在不变流形附近迭代演化、接近一个鞍点并切换到另一个不变流形来运作的。我们的分析还阐明了数据分布和权重初始化对学习过程中平台持续时间和数量的影响，区分了之前相互混淆的因素。总体而言，我们的理论提供了一个理解梯度下降为何和何时逐步学习越来越复杂解的框架。</div>
</details>
</div>
<div class="card">
<div class="title">Repurposing Video Diffusion Transformers for Robust Point Tracking</div>
<div class="meta-line">Authors: Soowon Son, Honggyu An, Chaehyun Kim, Hyunah Ko, Jisu Nam, Dahyun Chung, Siyoon Jin, Jung Yi, Jaewon Min, Junhwa Hur, Seungryong Kim</div>
<div class="meta-line">First: 2025-12-23T18:54:10+00:00 · Latest: 2025-12-23T18:54:10+00:00</div>
<div class="meta-line">Comments: Project Page: https://cvlab-kaist.github.io/DiTracker/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20606v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20606v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://cvlab-kaist.github.io/DiTracker/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Point tracking aims to localize corresponding points across video frames, serving as a fundamental task for 4D reconstruction, robotics, and video editing. Existing methods commonly rely on shallow convolutional backbones such as ResNet that process frames independently, lacking temporal coherence and producing unreliable matching costs under challenging conditions. Through systematic analysis, we find that video Diffusion Transformers (DiTs), pre-trained on large-scale real-world videos with spatio-temporal attention, inherently exhibit strong point tracking capability and robustly handle dynamic motions and frequent occlusions. We propose DiTracker, which adapts video DiTs through: (1) query-key attention matching, (2) lightweight LoRA tuning, and (3) cost fusion with a ResNet backbone. Despite training with 8 times smaller batch size, DiTracker achieves state-of-the-art performance on challenging ITTO benchmark and matches or outperforms state-of-the-art models on TAP-Vid benchmarks. Our work validates video DiT features as an effective and efficient foundation for point tracking.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将视频扩散变压器重新用于鲁棒点跟踪</div>
<div class="mono" style="margin-top:8px">点跟踪旨在对视频帧中的对应点进行定位，是4D重建、机器人和视频编辑等领域的基础任务。现有方法通常依赖于浅层卷积主干（如ResNet），独立处理每一帧，缺乏时间连贯性，并且在复杂条件下会产生不可靠的匹配代价。通过系统分析，我们发现经过时空注意力预训练的视频扩散变压器（DiTs）本身具有强大的点跟踪能力，并能稳健地处理动态运动和频繁遮挡。我们提出了DiTracker，通过以下三种方式对视频DiTs进行适配：(1) 查询-键注意力匹配，(2) 轻量级LoRA微调，(3) 与ResNet主干结合的成本融合。尽管训练时使用了8倍更小的批量大小，DiTracker在具有挑战性的ITTO基准上达到了最先进的性能，并在TAP-Vid基准上与最先进的模型表现相当或更优。我们的工作验证了视频DiT特征作为点跟踪有效且高效的基座。</div>
</details>
</div>
<div class="card">
<div class="title">Emergent temporal abstractions in autoregressive models enable hierarchical reinforcement learning</div>
<div class="meta-line">Authors: Seijin Kobayashi, Yanick Schimpf, Maximilian Schlegel, Angelika Steger, Maciej Wolczyk, Johannes von Oswald, Nino Scherre, Kaitlin Maile, Guillaume Lajoie, Blake A. Richards, Rif A. Saurous, James Manyika, Blaise Agüera y Arcas, Alexander Meulemans, João Sacramento</div>
<div class="meta-line">First: 2025-12-23T18:51:50+00:00 · Latest: 2025-12-23T18:51:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20605v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20605v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale autoregressive models pretrained on next-token prediction and finetuned with reinforcement learning (RL) have achieved unprecedented success on many problem domains. During RL, these models explore by generating new outputs, one token at a time. However, sampling actions token-by-token can result in highly inefficient learning, particularly when rewards are sparse. Here, we show that it is possible to overcome this problem by acting and exploring within the internal representations of an autoregressive model. Specifically, to discover temporally-abstract actions, we introduce a higher-order, non-causal sequence model whose outputs control the residual stream activations of a base autoregressive model. On grid world and MuJoCo-based tasks with hierarchical structure, we find that the higher-order model learns to compress long activation sequence chunks onto internal controllers. Critically, each controller executes a sequence of behaviorally meaningful actions that unfold over long timescales and are accompanied with a learned termination condition, such that composing multiple controllers over time leads to efficient exploration on novel tasks. We show that direct internal controller reinforcement, a process we term &quot;internal RL&quot;, enables learning from sparse rewards in cases where standard RL finetuning fails. Our results demonstrate the benefits of latent action generation and reinforcement in autoregressive models, suggesting internal RL as a promising avenue for realizing hierarchical RL within foundation models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自回归模型中的涌现时间抽象可实现分层强化学习</div>
<div class="mono" style="margin-top:8px">在多个问题领域中，基于下一个标记预测进行预训练并使用强化学习（RL）微调的大型自回归模型取得了前所未有的成功。在RL过程中，这些模型通过逐标记生成新输出来进行探索。然而，逐标记采样动作可能导致学习效率极低，尤其是在奖励稀疏的情况下。本文表明，可以通过在自回归模型的内部表示中进行行动和探索来克服这一问题。具体而言，为了发现时间抽象动作，我们引入了一个高阶、非因果序列模型，其输出控制基础自回归模型的残差流激活。在具有分层结构的网格世界和MuJoCo任务中，我们发现该高阶模型能够将长激活序列块压缩到内部控制器上。关键的是，每个控制器执行一系列具有行为意义的动作，这些动作跨越长时间尺度，并伴随着学习到的终止条件，使得在新任务中通过时间组合多个控制器可以实现高效的探索。我们展示了直接对内部控制器进行强化（我们称之为&quot;内部RL&quot;）能够在标准RL微调失败的情况下，从稀疏奖励中学习。我们的结果表明，在自回归模型中生成潜在动作并进行强化具有优势，这提示内部RL是实现基础模型中分层RL的有前景途径。</div>
</details>
</div>
<div class="card">
<div class="title">MoE-DiffuSeq: Enhancing Long-Document Diffusion Models with Sparse Attention and Mixture of Experts</div>
<div class="meta-line">Authors: Alexandros Christoforos, Chadbourne Davis</div>
<div class="meta-line">First: 2025-12-23T18:50:54+00:00 · Latest: 2025-12-23T18:50:54+00:00</div>
<div class="meta-line">Comments: Under submission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20604v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20604v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present MoE-DiffuSeq, a mixture of experts based framework for enhancing diffusion models in long document generation. Existing diffusion based text generation models, such as DiffuSeq, suffer from high computational cost and memory overhead when applied to extended sequences. To address these challenges, MoE-DiffuSeq integrates sparse attention with a mixture of experts architecture, enabling efficient and scalable long sequence modeling. Our approach introduces a customized sparse attention mechanism designed to reduce computational complexity while preserving text quality and coherence. In addition, we incorporate a soft absorbing state within the diffusion process to accelerate sequence reconstruction and improve generation precision. Extensive experiments demonstrate that MoE-DiffuSeq significantly improves training efficiency and sampling speed compared to existing diffusion models. These advantages are particularly effective for long document scenarios, including scientific article generation, code repository modeling, and long form dialogue generation. Benchmark results further show that MoE-DiffuSeq improves efficiency, speed, accuracy, and expressiveness, advancing the practical applicability of diffusion models for high quality long form text generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MoE-DiffuSeq: 通过稀疏注意力和专家混合增强长文档扩散模型</div>
<div class="mono" style="margin-top:8px">我们提出了MoE-DiffuSeq，这是一种基于专家混合的框架，用于增强长文档生成中的扩散模型。现有的基于扩散的文本生成模型，如DiffuSeq，在处理扩展序列时面临高计算成本和内存开销的问题。为了解决这些挑战，MoE-DiffuSeq结合了稀疏注意力机制与专家混合架构，实现了高效且可扩展的长序列建模。我们的方法引入了一种定制化的稀疏注意力机制，旨在降低计算复杂度同时保持文本质量和连贯性。此外，我们在扩散过程中引入了软吸收状态，以加速序列重建并提高生成精度。大量实验表明，与现有扩散模型相比，MoE-DiffuSeq显著提升了训练效率和采样速度。这些优势在长文档场景中尤为有效，包括科学论文生成、代码仓库建模和长文本对话生成。基准测试结果进一步表明，MoE-DiffuSeq在效率、速度、准确性和表达性方面均有提升，推动了扩散模型在高质量长文本生成中的实际应用。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforcement Learning From State and Temporal Differences</div>
<div class="meta-line">Authors: Lex Weaver, Jonathan Baxter</div>
<div class="meta-line">First: 2025-12-09T17:48:28+00:00 · Latest: 2025-12-23T18:50:53+00:00</div>
<div class="meta-line">Comments: Technical Report, Department of Computer Science, Australian National University, May 1999 New version uploaded 2025 after original source taken offline</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.08855v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.08855v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">TD($λ$) with function approximation has proved empirically successful for some complex reinforcement learning problems. For linear approximation, TD($λ$) has been shown to minimise the squared error between the approximate value of each state and the true value. However, as far as policy is concerned, it is error in the relative ordering of states that is critical, rather than error in the state values. We illustrate this point, both in simple two-state and three-state systems in which TD($λ$)--starting from an optimal policy--converges to a sub-optimal policy, and also in backgammon. We then present a modified form of TD($λ$), called STD($λ$), in which function approximators are trained with respect to relative state values on binary decision problems. A theoretical analysis, including a proof of monotonic policy improvement for STD($λ$) in the context of the two-state system, is presented, along with a comparison with Bertsekas&#x27; differential training method [1]. This is followed by successful demonstrations of STD($λ$) on the two-state system and a variation on the well known acrobot problem.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于状态和时间差分的强化学习</div>
<div class="mono" style="margin-top:8px">TD($λ$)结合函数逼近在一些复杂的强化学习问题中已被实证证明是成功的。对于线性逼近，TD($λ$)已被证明可以最小化每个状态的近似值与真实值之间的平方误差。然而，从策略的角度来看，关键的是状态相对顺序的误差，而不是状态值本身的误差。我们在简单的两状态和三状态系统中展示了这一点，其中TD($λ$)从最优策略出发却收敛到次优策略，也在backgammon问题中进行了说明。随后，我们提出了一种改进的TD($λ$)形式，称为STD($λ$)，它在二元决策问题中通过相对状态值来训练函数逼近器。我们提供了理论分析，包括在两状态系统中STD($λ$)单调策略改进的证明，并与Bertsekas的微分训练方法[1]进行了比较。最后，我们在两状态系统和一个著名的acrobot问题变种上成功展示了STD($λ$)的效果。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Informative Attention Weights for Person Re-Identification</div>
<div class="meta-line">Authors: Yancheng Wang, Nebojsa Jojic, Yingzhen Yang</div>
<div class="meta-line">First: 2025-05-13T21:01:53+00:00 · Latest: 2025-12-23T18:50:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.08961v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.08961v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Attention mechanisms have been widely used in deep learning, and recent efforts have been devoted to incorporating attention modules into deep neural networks (DNNs) for person Re-Identification (Re-ID) to enhance their discriminative feature learning capabilities. Existing attention modules, including self-attention and channel attention, learn attention weights that quantify the importance of feature tokens or feature channels. However, existing attention methods do not explicitly ensure that the attention weights are informative for predicting the identity of the person in the input image, and may consequently introduce noisy information from the input image. To address this issue, we propose a novel method termed Reduction of Information Bottleneck loss (RIB), motivated by the principle of the Information Bottleneck (IB). A novel distribution-free and efficient variational upper bound for the IB loss (IBB), which can be optimized by standard SGD, is derived and incorporated into the training loss of the RIB models. RIB is applied to DNNs with self-attention modules through a novel Differentiable Channel Selection Attention module, or DCS-Attention, that selects the most informative channels for computing attention weights, leading to competitive models termed RIB-DCS. RIB is also incorporated into DNNs with existing channel attention modules to promote the learning of informative channel attention weights, leading to models termed RIB-CA. Both RIB-DCS and RIB-CA are applied to fixed neural network backbones and learnable backbones with Differentiable Neural Architecture Search (DNAS). Extensive experiments on multiple person Re-ID benchmarks show that RIB significantly enhances the prediction accuracy of DNNs for person Re-ID, even for the occluded person Re-ID.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习具有信息量的注意力权重用于行人重识别</div>
<div class="mono" style="margin-top:8px">注意力机制在深度学习中被广泛应用，最近的研究致力于将注意力模块整合到深度神经网络（DNN）中以提升行人重识别（Re-ID）的判别特征学习能力。现有的注意力模块，包括自注意力和通道注意力，学习的注意力权重用于量化特征标记或特征通道的重要性。然而，现有注意力方法并未显式确保这些权重对预测输入图像中行人的身份具有信息量，可能导致引入输入图像中的噪声信息。为了解决这一问题，我们提出了一种新的方法，称为信息瓶颈损失的减少（RIB），其灵感来源于信息瓶颈（IB）原理。我们推导出了一种新颖的无分布假设且高效的变分上界（IBB），该上界可通过标准SGD进行优化，并被整合到RIB模型的训练损失中。通过一种新的可微通道选择注意力模块（DCS-Attention），RIB被应用于带有自注意力模块的DNN，从而选择最具信息量的通道来计算注意力权重，得到具有竞争力的模型RIB-DCS。RIB也被整合到带有现有通道注意力模块的DNN中，以促进具有信息量的通道注意力权重的学习，得到模型RIB-CA。RIB-DCS和RIB-CA均应用于固定神经网络主干和可学习主干，并通过可微神经网络架构搜索（DNAS）进行优化。在多个行人重识别基准数据集上的大量实验表明，RIB显著提升了DNN在行人重识别任务中的预测准确性，即使在遮挡情况下也是如此。</div>
</details>
</div>
<div class="card">
<div class="title">Cube Bench: A Benchmark for Spatial Visual Reasoning in MLLMs</div>
<div class="meta-line">Authors: Dhruv Anand, Ehsan Shareghi</div>
<div class="meta-line">First: 2025-12-23T18:43:05+00:00 · Latest: 2025-12-23T18:43:05+00:00</div>
<div class="meta-line">Comments: 27 pages, 5 figures, 9 tables. Cube available at https://github.com/dana-23/cube-bench</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20595v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20595v1">PDF</a> · <a href="https://github.com/dana-23/cube-bench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Cube Bench, a Rubik&#x27;s-cube benchmark for evaluating spatial and sequential reasoning in multimodal large language models (MLLMs). The benchmark decomposes performance into five skills: (i) reconstructing cube faces from images and text, (ii) choosing the optimal next move, (iii) predicting the outcome of a candidate move without applying it, (iv) executing multi-step plans while recovering from mistakes, and (v) detecting and revising one&#x27;s own errors. Using a shared set of scrambled cube states, identical prompts and parsers, and a single distance-to-solved metric, we compare recent MLLMs side by side as a function of scramble depth. Across seven MLLMs, accuracy drops sharply with depth; once a trajectory stalls or diverges, models rarely recover, and high face-reconstruction accuracy does not guarantee competent action selection or multi-step execution. A pronounced closed- vs open-source gap emerges: the strongest closed model leads on both single-step perception tasks and multi-step control tasks, while open-weight models cluster near chance on the hardest settings; yet even the best MLLM degrades at higher cube complexity. A simple self-correction via reflective thinking yields modest gains but can also introduce overthinking. Cube Bench offers a compact, reproducible probe of sequential spatial reasoning in MLLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Cube Bench：一种用于多模态大语言模型空间视觉推理的基准测试</div>
<div class="mono" style="margin-top:8px">我们引入了Cube Bench，这是一个用于评估多模态大语言模型（MLLMs）空间和顺序推理能力的魔方基准测试。该基准测试将性能分解为五个技能：(i) 从图像和文本中重建魔方面，(ii) 选择最优下一步操作，(iii) 预测候选操作的结果而不实际执行，(iv) 在执行多步计划时从错误中恢复，(v) 检测并修正自己的错误。我们使用共享的魔方打乱状态集、相同的提示和解析器以及单一的到解距离度量，将最近的MLLMs按打乱深度进行比较。在七个MLLMs中，准确率随着深度增加而急剧下降；一旦轨迹停滞或偏离，模型很少能恢复，而高魔方面重建准确率并不能保证有效的动作选择或多步执行。一个显著的闭源与开源模型之间的差距显现出来：最强的闭源模型在单步感知任务和多步控制任务中均表现优异，而开源权重模型在最难的设置中接近随机水平；然而，即使是最优的MLLM，在更高复杂度的魔方问题上也会出现性能下降。通过简单的自我修正和反思性思维，可以带来适度的提升，但也可能引入过度思考。Cube Bench为MLLMs中的顺序空间推理提供了一个紧凑且可复现的测试工具。</div>
</details>
</div>
<div class="card">
<div class="title">LightTact: A Visual-Tactile Fingertip Sensor for Deformation-Independent Contact Sensing</div>
<div class="meta-line">Authors: Changyi Lin, Boda Huo, Mingyang Yu, Emily Ruppel, Bingqing Chen, Jonathan Francis, Ding Zhao</div>
<div class="meta-line">First: 2025-12-23T18:38:25+00:00 · Latest: 2025-12-23T18:38:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20591v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20591v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contact often occurs without macroscopic surface deformation, such as during interaction with liquids, semi-liquids, or ultra-soft materials. Most existing tactile sensors rely on deformation to infer contact, making such light-contact interactions difficult to perceive robustly. To address this, we present LightTact, a visual-tactile fingertip sensor that makes contact directly visible via a deformation-independent, optics-based principle. LightTact uses an ambient-blocking optical configuration that suppresses both external light and internal illumination at non-contact regions, while transmitting only the diffuse light generated at true contacts. As a result, LightTact produces high-contrast raw images in which non-contact pixels remain near-black (mean gray value &lt; 3) and contact pixels preserve the natural appearance of the contacting surface. Built on this, LightTact achieves accurate pixel-level contact segmentation that is robust to material properties, contact force, surface appearance, and environmental lighting. We further integrate LightTact on a robotic arm and demonstrate manipulation behaviors driven by extremely light contact, including water spreading, facial-cream dipping, and thin-film interaction. Finally, we show that LightTact&#x27;s spatially aligned visual-tactile images can be directly interpreted by existing vision-language models, enabling resistor value reasoning for robotic sorting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LightTact：一种用于无形变接触感知的视觉-触觉指尖传感器</div>
<div class="mono" style="margin-top:8px">接触常常不伴随宏观表面形变，例如在与液体、半流体或超柔软材料交互时。大多数现有触觉传感器依赖形变来推断接触，这使得轻接触交互难以稳健地感知。为了解决这一问题，我们提出了LightTact，一种基于光学原理的视觉-触觉指尖传感器，能够通过与形变无关的方式直接可视化接触。LightTact采用环境光阻挡光学配置，在非接触区域抑制外部光和内部照明，仅传输真实接触点产生的漫反射光。因此，LightTact生成的原始图像具有高对比度，非接触像素保持接近黑色（平均灰度值 &lt; 3），接触像素保留接触表面的自然外观。在此基础上，LightTact实现了对接触的像素级准确分割，且对材料特性、接触力、表面外观和环境光照具有鲁棒性。我们进一步将LightTact集成到机械臂上，并展示了由极轻接触驱动的操作行为，包括水扩散、面霜蘸取和薄膜交互。最后，我们表明LightTact的空间对齐视觉-触觉图像可以直接被现有的视觉-语言模型解析，从而实现用于机器人分拣的电阻值推理。</div>
</details>
</div>
<div class="card">
<div class="title">Leveraging High-Fidelity Digital Models and Reinforcement Learning for Mission Engineering: A Case Study of Aerial Firefighting Under Perfect Information</div>
<div class="meta-line">Authors: İbrahim Oğuz Çetinkaya, Sajad Khodadadian, Taylan G. Topçu</div>
<div class="meta-line">First: 2025-12-23T18:36:07+00:00 · Latest: 2025-12-23T18:36:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20589v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20589v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As systems engineering (SE) objectives evolve from design and operation of monolithic systems to complex System of Systems (SoS), the discipline of Mission Engineering (ME) has emerged which is increasingly being accepted as a new line of thinking for the SE community. Moreover, mission environments are uncertain, dynamic, and mission outcomes are a direct function of how the mission assets will interact with this environment. This proves static architectures brittle and calls for analytically rigorous approaches for ME. To that end, this paper proposes an intelligent mission coordination methodology that integrates digital mission models with Reinforcement Learning (RL), that specifically addresses the need for adaptive task allocation and reconfiguration. More specifically, we are leveraging a Digital Engineering (DE) based infrastructure that is composed of a high-fidelity digital mission model and agent-based simulation; and then we formulate the mission tactics management problem as a Markov Decision Process (MDP), and employ an RL agent trained via Proximal Policy Optimization. By leveraging the simulation as a sandbox, we map the system states to actions, refining the policy based on realized mission outcomes. The utility of the RL-based intelligent mission coordinator is demonstrated through an aerial firefighting case study. Our findings indicate that the RL-based intelligent mission coordinator not only surpasses baseline performance but also significantly reduces the variability in mission performance. Thus, this study serves as a proof of concept demonstrating that DE-enabled mission simulations combined with advanced analytical tools offer a mission-agnostic framework for improving ME practice; which can be extended to more complicated fleet design and selection problems in the future from a mission-first perspective.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用高保真数字模型和强化学习进行任务工程：以完美信息条件下的空中灭火为案例研究</div>
<div class="mono" style="margin-top:8px">随着系统工程（SE）目标从单一系统的设计和操作演变为复杂系统群（SoS），任务工程（ME）作为一门新兴学科，正逐渐被SE社区接受为新的思维方式。此外，任务环境具有不确定性与动态性，任务结果直接取决于任务资产如何与该环境互动。这表明静态架构的脆弱性，并呼吁采用分析严谨的方法进行ME。为此，本文提出了一种智能任务协调方法，将数字任务模型与强化学习（RL）相结合，专门解决任务分配和重新配置的适应性需求。具体而言，我们利用基于数字工程（DE）的基础设施，该基础设施由高保真数字任务模型和基于代理的仿真组成；然后将任务战术管理问题建模为马尔可夫决策过程（MDP），并采用通过近端策略优化（PPO）训练的RL代理。通过将仿真作为沙盒，我们将系统状态映射到动作，并根据实际任务结果优化策略。本文通过空中灭火案例研究展示了基于RL的智能任务协调器的实用性。我们的研究结果表明，基于RL的智能任务协调器不仅超越了基准性能，还显著降低了任务性能的变异性。因此，本研究作为概念验证，证明了结合数字工程任务仿真和先进分析工具的方法，为改进任务工程实践提供了一个任务无关的框架；该框架可在未来从任务优先的角度扩展到更复杂的舰队设计和选择问题。</div>
</details>
</div>
<div class="card">
<div class="title">Automated stereotactic radiosurgery planning using a human-in-the-loop reasoning large language model agent</div>
<div class="meta-line">Authors: Humza Nusrat, Luke Francisco, Bing Luo, Hassan Bagher-Ebadian, Joshua Kim, Karen Chin-Snyder, Salim Siddiqui, Mira Shah, Eric Mellon, Mohammad Ghassemi, Anthony Doemer, Benjamin Movsas, Kundan Thind</div>
<div class="meta-line">First: 2025-12-23T18:32:17+00:00 · Latest: 2025-12-23T18:32:17+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20586v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20586v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Stereotactic radiosurgery (SRS) demands precise dose shaping around critical structures, yet black-box AI systems have limited clinical adoption due to opacity concerns. We tested whether chain-of-thought reasoning improves agentic planning in a retrospective cohort of 41 patients with brain metastases treated with 18 Gy single-fraction SRS. We developed SAGE (Secure Agent for Generative Dose Expertise), an LLM-based planning agent for automated SRS treatment planning. Two variants generated plans for each case: one using a non-reasoning model, one using a reasoning model. The reasoning variant showed comparable plan dosimetry relative to human planners on primary endpoints (PTV coverage, maximum dose, conformity index, gradient index; all p &gt; 0.21) while reducing cochlear dose below human baselines (p = 0.022). When prompted to improve conformity, the reasoning model demonstrated systematic planning behaviors including prospective constraint verification (457 instances) and trade-off deliberation (609 instances), while the standard model exhibited none of these deliberative processes (0 and 7 instances, respectively). Content analysis revealed that constraint verification and causal explanation concentrated in the reasoning agent. The optimization traces serve as auditable logs, offering a path toward transparent automated planning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用人类在环推理大语言模型代理的自动立体定向放射外科计划</div>
<div class="mono" style="margin-top:8px">立体定向放射外科（SRS）需要在关键结构周围精确地塑造剂量，但由于黑箱AI系统的不透明性，其在临床上的采用受到限制。我们在一项回顾性队列研究中测试了链式思维推理是否能提高代理计划的质量，该研究涉及41名接受18 Gy单次分割SRS治疗的脑转移患者。我们开发了SAGE（用于生成剂量专家的可信赖代理），这是一个基于大语言模型（LLM）的计划代理，用于自动化SRS治疗计划。两种变体分别为每个病例生成计划：一种使用非推理模型，另一种使用推理模型。推理变体在主要终点（靶区覆盖、最大剂量、符合指数、梯度指数；所有p &gt; 0.21）上表现出与人类规划者相当的计划剂量学性能，同时将耳蜗剂量降低至低于人类基准水平（p = 0.022）。当被提示改进符合性时，推理模型展示了系统性的规划行为，包括前瞻性约束验证（457次）和权衡讨论（609次），而标准模型则未表现出这些推理过程（分别为0次和7次）。内容分析显示，约束验证和因果解释主要集中在推理代理中。优化轨迹可作为可审计日志，为实现透明的自动化计划提供了一条路径。</div>
</details>
</div>
<div class="card">
<div class="title">Making Sense of Private Advertising: A Principled Approach to a Complex Ecosystem</div>
<div class="meta-line">Authors: Kyle Hogan, Alishah Chator, Gabriel Kaptchuk, Mayank Varia, Srinivas Devadas</div>
<div class="meta-line">First: 2025-12-23T18:28:24+00:00 · Latest: 2025-12-23T18:28:24+00:00</div>
<div class="meta-line">Comments: To appear in PETS 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20583v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20583v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we model the end-to-end pipeline of the advertising ecosystem, allowing us to identify two main issues with the current trajectory of private advertising proposals. First, prior work has largely considered ad targeting and engagement metrics individually rather than in composition. This has resulted in privacy notions that, while reasonable for each protocol in isolation, fail to compose to a natural notion of privacy for the ecosystem as a whole, permitting advertisers to extract new information about the audience of their advertisements. The second issue serves to explain the first: we prove that \textit{perfect} privacy is impossible for any, even minimally, useful advertising ecosystem, due to the advertisers&#x27; expectation of conducting market research on the results.
  Having demonstrated that leakage is inherent in advertising, we re-examine what privacy could realistically mean in advertising, building on the well-established notion of \textit{sensitive} data in a specific context. We identify that fundamentally new approaches are needed when designing privacy-preserving advertising subsystems in order to ensure that the privacy properties of the end-to-end advertising system are well aligned with people&#x27;s privacy desires.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>理解私有广告：对复杂生态系统的有原则方法</div>
<div class="mono" style="margin-top:8px">在这项工作中，我们建模了广告生态系统的端到端流程，从而能够识别当前私有广告提案发展路径中的两个主要问题。首先，以往的研究大多将广告定向和参与度指标视为独立问题，而非整体组合。这导致了隐私概念，虽然在每个协议独立运行时是合理的，但无法组合成整个生态系统自然的隐私观念，允许广告商提取关于其广告受众的新信息。第二个问题解释了第一个问题：我们证明了，对于任何哪怕是最基本有用的广告生态系统，\textit{完美}隐私都是不可能实现的，因为广告商期望对结果进行市场调研。
在证明了广告中信息泄露是不可避免的情况下，我们重新审视广告中隐私的现实含义，基于特定情境下\textit{敏感}数据的已有概念进行构建。我们发现，设计隐私保护的广告子系统时，需要从根本上采用新的方法，以确保端到端广告系统的隐私属性与人们的隐私期望良好对齐。</div>
</details>
</div>
<div class="card">
<div class="title">Relu and softplus neural nets as zero-sum turn-based games</div>
<div class="meta-line">Authors: Stephane Gaubert, Yiannis Vlassopoulos</div>
<div class="meta-line">First: 2025-12-23T18:27:41+00:00 · Latest: 2025-12-23T18:27:41+00:00</div>
<div class="meta-line">Comments: 24 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20582v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20582v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We show that the output of a ReLU neural network can be interpreted as the value of a zero-sum, turn-based, stopping game, which we call the ReLU net game. The game runs in the direction opposite to that of the network, and the input of the network serves as the terminal reward of the game. In fact, evaluating the network is the same as running the Shapley-Bellman backward recursion for the value of the game. Using the expression of the value of the game as an expected total payoff with respect to the path measure induced by the transition probabilities and a pair of optimal policies, we derive a discrete Feynman-Kac-type path-integral formula for the network output. This game-theoretic representation can be used to derive bounds on the output from bounds on the input, leveraging the monotonicity of Shapley operators, and to verify robustness properties using policies as certificates. Moreover, training the neural network becomes an inverse game problem: given pairs of terminal rewards and corresponding values, one seeks transition probabilities and rewards of a game that reproduces them. Finally, we show that a similar approach applies to neural networks with Softplus activation functions, where the ReLU net game is replaced by its entropic regularization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ReLU和Softplus神经网络作为零和回合制游戏</div>
<div class="mono" style="margin-top:8px">我们证明，ReLU神经网络的输出可以被解释为一个零和、回合制、停止游戏的值，我们称之为ReLU网络游戏。该游戏的运行方向与网络方向相反，网络的输入作为游戏的终端奖励。实际上，评估网络等同于对游戏值运行Shapley-Bellman逆递归。利用游戏值的表达式作为路径测度下相对于转移概率和一对最优策略的期望总收益，我们推导出一个离散的Feynman-Kac型路径积分公式用于网络输出。这种博弈论表示可以用于从输入的界限推导输出的界限，利用Shapley算子的单调性，并通过策略作为证书来验证鲁棒性属性。此外，神经网络的训练变成了一个逆博弈问题：给定终端奖励和对应值的配对，寻求一个能够重现这些值的游戏的转移概率和奖励。最后，我们展示了类似的方法也适用于具有Softplus激活函数的神经网络，其中ReLU网络游戏被其熵正则化版本所替代。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We show that the output of a ReLU neural network can be interpreted as the value of a zero-sum, turn-based, stopping game, which we call the ReLU net game.</div>
</details>
</div>
<div class="card">
<div class="title">Behavioral Machine Learning? Regularization and Forecast Bias</div>
<div class="meta-line">Authors: Murray Z. Frank, Jing Gao, Keer Yang</div>
<div class="meta-line">First: 2023-03-25T03:06:43+00:00 · Latest: 2025-12-23T18:23:14+00:00</div>
<div class="meta-line">Comments: stock analysts, machine learning, behavioral, overreaction</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2303.16158v4">Abs</a> · <a href="https://arxiv.org/pdf/2303.16158v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Standard forecast efficiency tests interpret violations as evidence of behavioral bias. We show theoretically and empirically that rational forecasters using optimal regularization systematically violate these tests. Machine learning forecasts show near zero bias at one year horizon, but strong overreaction at two years, consistent with predictions from a model of regularization and measurement noise. We provide three complementary tests: experimental variation in regularization parameters, cross-sectional heterogeneity in firm signal quality, and quasi-experimental evidence from ML adoption around 2013. Technically trained analysts shift sharply toward overreaction post-2013. Our findings suggest reported violations may reflect statistical sophistication rather than cognitive failure.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>行为机器学习？正则化与预测偏差</div>
<div class="mono" style="margin-top:8px">标准的预测效率测试将违规视为行为偏差的证据。我们通过理论和实证方法表明，使用最优正则化的理性预测者会系统性地违反这些测试。机器学习预测在一年时间范围内表现出接近零的偏差，但在两年时间范围内则出现显著的过度反应，这与正则化和测量噪声模型的预测一致。我们提供了三种互补的测试：正则化参数的实验性变化、公司信号质量的跨截面异质性，以及2013年左右机器学习采用的准实验证据。技术训练的分析师在2013年后明显转向过度反应。我们的研究结果表明，报告的违规现象可能反映了统计学上的熟练程度，而非认知失败。</div>
</details>
</div>
<div class="card">
<div class="title">Improving ML Training Data with Gold-Standard Quality Metrics</div>
<div class="meta-line">Authors: Leslie Barrett, Michael W. Sherman</div>
<div class="meta-line">Venue: KDD</div>
<div class="meta-line">First: 2025-12-23T18:21:24+00:00 · Latest: 2025-12-23T18:21:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20577v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20577v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hand-tagged training data is essential to many machine learning tasks. However, training data quality control has received little attention in the literature, despite data quality varying considerably with the tagging exercise. We propose methods to evaluate and enhance the quality of hand-tagged training data using statistical approaches to measure tagging consistency and agreement. We show that agreement metrics give more reliable results if recorded over multiple iterations of tagging, where declining variance in such recordings is an indicator of increasing data quality. We also show one way a tagging project can collect high-quality training data without requiring multiple tags for every work item, and that a tagger burn-in period may not be sufficient for minimizing tagger errors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过黄金标准质量指标改进机器学习训练数据</div>
<div class="mono" style="margin-top:8px">手工标注的训练数据对许多机器学习任务至关重要。然而，尽管数据质量在标注过程中存在显著差异，训练数据的质量控制在文献中却很少受到关注。我们提出使用统计方法来评估和提升手工标注训练数据的质量，通过测量标注的一致性和一致性来实现。我们表明，如果在多次标注迭代中记录一致性指标，这些指标将给出更可靠的结果，其中此类记录的方差下降是数据质量提高的指标。我们还展示了一种方法，使标注项目能够在不为每个工作项要求多个标注的情况下收集高质量的训练数据，并指出标注员的磨合期可能不足以最小化标注错误。</div>
</details>
</div>
<div class="card">
<div class="title">Performative Policy Gradient: Optimality in Performative Reinforcement Learning</div>
<div class="meta-line">Authors: Debabrota Basu, Udvas Das, Brahim Driss, Uddalak Mukherjee</div>
<div class="meta-line">First: 2025-12-23T18:20:06+00:00 · Latest: 2025-12-23T18:20:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20576v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20576v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-deployment machine learning algorithms often influence the environments they act in, and thus shift the underlying dynamics that the standard reinforcement learning (RL) methods ignore. While designing optimal algorithms in this performative setting has recently been studied in supervised learning, the RL counterpart remains under-explored. In this paper, we prove the performative counterparts of the performance difference lemma and the policy gradient theorem in RL, and further introduce the Performative Policy Gradient algorithm (PePG). PePG is the first policy gradient algorithm designed to account for performativity in RL. Under softmax parametrisation, and also with and without entropy regularisation, we prove that PePG converges to performatively optimal policies, i.e. policies that remain optimal under the distribution shifts induced by themselves. Thus, PePG significantly extends the prior works in Performative RL that achieves performative stability but not optimality. Furthermore, our empirical analysis on standard performative RL environments validate that PePG outperforms standard policy gradient algorithms and the existing performative RL algorithms aiming for stability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>表演性策略梯度：表演性强化学习中的最优性</div>
<div class="mono" style="margin-top:8px">部署后的机器学习算法常常会影响其运行的环境，从而改变标准强化学习（RL）方法所忽略的底层动态。尽管在监督学习中，针对这种表演性设置设计最优算法的研究最近有所进展，但强化学习的对应问题仍被较少探索。在本文中，我们证明了强化学习中表演性版本的性能差异引理和策略梯度定理，并进一步引入了表演性策略梯度算法（PePG）。PePG 是首个旨在考虑强化学习中表演性的策略梯度算法。在 softmax 参数化下，且在有无熵正则化的情况下，我们证明了 PePG 收敛于表演性最优策略，即在自身引发的分布变化下仍保持最优性的策略。因此，PePG 显著扩展了之前在表演性强化学习中实现表演性稳定但未达到最优性的研究成果。此外，我们在标准的表演性强化学习环境中进行的实证分析验证了 PePG 在性能上优于标准策略梯度算法以及现有的旨在实现稳定性的表演性强化学习算法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Post-deployment machine learning algorithms often influence the environments they act in, and thus shift the underlying dynamics that the standard reinforcement learning (RL) methods ignore.</div>
</details>
</div>
<div class="card">
<div class="title">Fail Fast, Win Big: Rethinking the Drafting Strategy in Speculative Decoding via Diffusion LLMs</div>
<div class="meta-line">Authors: Rui Pan, Zhuofu Chen, Ravi Netravali</div>
<div class="meta-line">First: 2025-12-23T18:16:58+00:00 · Latest: 2025-12-23T18:16:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20573v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20573v1">PDF</a> · <a href="https://github.com/ruipeterpan/failfast">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion Large Language Models (dLLMs) offer fast, parallel token generation, but their standalone use is plagued by an inherent efficiency-quality tradeoff. We show that, if carefully applied, the attributes of dLLMs can actually be a strength for drafters in speculative decoding with autoregressive (AR) verifiers. Our core insight is that dLLM&#x27;s speed from parallel decoding drastically lowers the risk of costly rejections, providing a practical mechanism to effectively realize the (elusive) lengthy drafts that lead to large speedups with speculative decoding. We present FailFast, a dLLM-based speculative decoding framework that realizes this approach by dynamically adapting its speculation length. It &quot;fails fast&quot; by spending minimal compute in hard-to-speculate regions to shrink speculation latency and &quot;wins big&quot; by aggressively extending draft lengths in easier regions to reduce verification latency (in many cases, speculating and accepting 70 tokens at a time!). Without any fine-tuning, FailFast delivers lossless acceleration of AR LLMs and achieves up to 4.9$\times$ speedup over vanilla decoding, 1.7$\times$ over the best naive dLLM drafter, and 1.4$\times$ over EAGLE-3 across diverse models and workloads. We open-source FailFast at https://github.com/ruipeterpan/failfast.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>快速失败，大幅收益：通过扩散大语言模型重新思考投机解码策略</div>
<div class="mono" style="margin-top:8px">扩散大语言模型（dLLMs）能够实现快速、并行的token生成，但其独立使用存在固有的效率与质量的权衡。我们表明，如果仔细应用，dLLMs的特性实际上可以成为在使用自回归（AR）验证器进行投机解码时的优势。我们的核心洞察是，dLLM的并行解码速度大幅降低了昂贵拒绝的风险，提供了一种实用机制，以实现（难以实现的）长篇草稿，从而在投机解码中获得显著的速度提升。我们提出了FailFast，这是一个基于dLLM的投机解码框架，通过动态调整其投机长度来实现这一方法。它在难以投机的区域消耗极少的计算资源，从而缩短投机延迟（即快速失败）；在容易投机的区域则积极延长草稿长度，以减少验证延迟（在许多情况下，每次可推测并接受70个token！）。无需任何微调，FailFast即可实现自回归LLM的无损加速，并在多种模型和任务中，相比普通解码实现最高达4.9倍的速度提升，相比最佳的朴素dLLM草稿生成器实现1.7倍的速度提升，相比EAGLE-3实现1.4倍的速度提升。我们已在https://github.com/ruipeterpan/failfast开源FailFast。</div>
</details>
</div>
<div class="card">
<div class="title">Distilling to Hybrid Attention Models via KL-Guided Layer Selection</div>
<div class="meta-line">Authors: Yanhong Li, Songlin Yang, Shawn Tan, Mayank Mishra, Rameswar Panda, Jiawei Zhou, Yoon Kim</div>
<div class="meta-line">First: 2025-12-23T18:12:22+00:00 · Latest: 2025-12-23T18:12:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20569v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20569v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Distilling pretrained softmax attention Transformers into more efficient hybrid architectures that interleave softmax and linear attention layers is a promising approach for improving the inference efficiency of LLMs without requiring expensive pretraining from scratch. A critical factor in the conversion process is layer selection, i.e., deciding on which layers to convert to linear attention variants. This paper describes a simple and efficient recipe for layer selection that uses layer importance scores derived from a small amount of training on generic text data. Once the layers have been selected we use a recent pipeline for the distillation process itself \citep[RADLADS;][]{goldstein2025radlads}, which consists of attention weight transfer, hidden state alignment, KL-based distribution matching, followed by a small amount of finetuning. We find that this approach is more effective than existing approaches for layer selection, including heuristics that uniformly interleave linear attentions based on a fixed ratio, as well as more involved approaches that rely on specialized diagnostic datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过KL引导的层选择将预训练softmax注意力Transformer提炼为混合注意力模型</div>
<div class="mono" style="margin-top:8px">将预训练的softmax注意力Transformer提炼为更高效的混合架构，该架构交替嵌入softmax和线性注意力层，是一种在不重新进行昂贵预训练的情况下提高大语言模型推理效率的有前景方法。转换过程中的一个关键因素是层选择，即决定哪些层转换为线性注意力变体。本文描述了一种简单且高效的层选择方法，该方法基于在通用文本数据上进行少量训练得到的层重要性分数。一旦层被选定，我们使用一种最近的提炼流程（RADLADS）来进行提炼过程，该流程包括注意力权重转移、隐藏状态对齐、基于KL的分布匹配，随后进行少量微调。我们发现，这种方法在层选择方面比现有方法更有效，包括基于固定比例均匀交替线性注意力的启发式方法，以及依赖专用诊断数据集的更复杂方法。</div>
</details>
</div>
<div class="card">
<div class="title">Similarity Field Theory: A Mathematical Framework for Intelligence</div>
<div class="meta-line">Authors: Kei-Sing Ng</div>
<div class="meta-line">First: 2025-09-21T22:34:00+00:00 · Latest: 2025-12-23T18:09:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.18218v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.18218v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We posit that persisting and transforming similarity relations form the structural basis of any comprehensible dynamic system. This paper introduces Similarity Field Theory, a mathematical framework that formalizes the principles governing similarity values among entities and their evolution. We define: (1) a similarity field $S: U \times U \to [0,1]$ over a universe of entities $U$, satisfying reflexivity $S(E,E)=1$ and treated as a directed relational field (asymmetry and non-transitivity are allowed); (2) the evolution of a system through a sequence $Z_p=(X_p,S^{(p)})$ indexed by $p=0,1,2,\ldots$; (3) concepts $K$ as entities that induce fibers $F_α(K)={E\in U \mid S(E,K)\ge α}$, i.e., superlevel sets of the unary map $S_K(E):=S(E,K)$; and (4) a generative operator $G$ that produces new entities. Within this framework, we formalize a generative definition of intelligence: an operator $G$ is intelligent with respect to a concept $K$ if, given a system containing entities belonging to the fiber of $K$, it generates new entities that also belong to that fiber. Similarity Field Theory thus offers a foundational language for characterizing, comparing, and constructing intelligent systems. At a high level, this framework reframes intelligence and interpretability as geometric problems on similarity fields--preserving and composing level-set fibers--rather than purely statistical ones. We prove two theorems: (i) asymmetry blocks mutual inclusion; and (ii) stability implies either an anchor coordinate or asymptotic confinement to the target level (up to arbitrarily small tolerance). Together, these results constrain similarity-field evolution and motivate an interpretive lens that can be applied to large language models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>相似性场理论：一种智能的数学框架</div>
<div class="mono" style="margin-top:8px">我们认为，持续且转换相似性关系构成了任何可理解动态系统的结构基础。本文引入相似性场理论，这是一种数学框架，用于形式化实体间相似性值的原理及其演化。我们定义：(1) 一个相似性场 $S: U \times U \to [0,1]$，定义在实体集合 $U$ 上，满足自反性 $S(E,E)=1$，并被视为一个有向关系场（允许非对称性和非传递性）；(2) 通过序列 $Z_p=(X_p,S^{(p)})$ 演化系统，其中 $p=0,1,2,\ldots$；(3) 概念 $K$ 作为能够诱导纤维 $F_α(K)={E\in U \mid S(E,K)\ge α}$ 的实体，即一元映射 $S_K(E):=S(E,K)$ 的上水平集；(4) 一个生成算子 $G$，用于生成新实体。在此框架下，我们形式化了智能的生成定义：如果一个系统包含属于概念 $K$ 纤维的实体，并能生成也属于该纤维的新实体，则该算子 $G$ 相对于概念 $K$ 是智能的。因此，相似性场理论为描述、比较和构建智能系统提供了一种基础语言。从高层次来看，该框架将智能和可解释性重新定义为相似性场上的几何问题——即保持和组合上水平集纤维——而不是纯粹的统计问题。我们证明了两个定理：(i) 非对称性会阻止相互包含；(ii) 稳定性意味着要么存在一个锚定坐标，要么实体趋于目标水平（允许任意小的误差）。这些结果共同约束了相似性场的演化，并激励了一种可应用于大型语言模型的解释视角。</div>
</details>
</div>
<div class="card">
<div class="title">LEAD: Minimizing Learner-Expert Asymmetry in End-to-End Driving</div>
<div class="meta-line">Authors: Long Nguyen, Micha Fauth, Bernhard Jaeger, Daniel Dauner, Maximilian Igl, Andreas Geiger, Kashyap Chitta</div>
<div class="meta-line">First: 2025-12-23T18:07:43+00:00 · Latest: 2025-12-23T18:07:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20563v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20563v1">PDF</a> · <a href="https://github.com/autonomousvision/lead">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Simulators can generate virtually unlimited driving data, yet imitation learning policies in simulation still struggle to achieve robust closed-loop performance. Motivated by this gap, we empirically study how misalignment between privileged expert demonstrations and sensor-based student observations can limit the effectiveness of imitation learning. More precisely, experts have significantly higher visibility (e.g., ignoring occlusions) and far lower uncertainty (e.g., knowing other vehicles&#x27; actions), making them difficult to imitate reliably. Furthermore, navigational intent (i.e., the route to follow) is under-specified in student models at test time via only a single target point. We demonstrate that these asymmetries can measurably limit driving performance in CARLA and offer practical interventions to address them. After careful modifications to narrow the gaps between expert and student, our TransFuser v6 (TFv6) student policy achieves a new state of the art on all major publicly available CARLA closed-loop benchmarks, reaching 95 DS on Bench2Drive and more than doubling prior performances on Longest6~v2 and Town13. Additionally, by integrating perception supervision from our dataset into a shared sim-to-real pipeline, we show consistent gains on the NAVSIM and Waymo Vision-Based End-to-End driving benchmarks. Our code, data, and models are publicly available at https://github.com/autonomousvision/lead.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LEAD: 减少学习者与专家之间的不对称性以实现端到端驾驶</div>
<div class="mono" style="margin-top:8px">模拟器可以生成几乎无限的驾驶数据，但模拟中的模仿学习策略仍然难以实现稳健的闭环性能。受这一差距的启发，我们实证研究了特权专家演示与基于传感器的学生观测之间的不匹配如何限制模仿学习的有效性。更具体地说，专家具有显著更高的可见性（例如忽略遮挡）和远更低的不确定性（例如知道其他车辆的行为），这使得他们难以被可靠地模仿。此外，学生模型在测试时仅通过一个目标点来指定导航意图（即应遵循的路线），这导致其定义不足。我们证明这些不对称性会可测量地限制驾驶性能，并提出了实际的干预措施来解决这些问题。在仔细修改以缩小专家与学生之间的差距后，我们的TransFuser v6（TFv6）学生策略在所有主要的公开CARLA闭环基准测试中均达到了新的最佳性能，其中在Bench2Drive上达到95 DS，在Longest6~v2和Town13上性能超过之前结果的两倍。此外，通过将我们数据集中的感知监督整合到共享的模拟到现实（sim-to-real）管道中，我们在NAVSIM和Waymo基于视觉的端到端驾驶基准测试中展示了持续的性能提升。我们的代码、数据和模型可在https://github.com/autonomousvision/lead上公开获取。</div>
</details>
</div>
<div class="card">
<div class="title">Shallow Neural Networks Learn Low-Degree Spherical Polynomials with Learnable Channel Attention</div>
<div class="meta-line">Authors: Yingzhen Yang</div>
<div class="meta-line">First: 2025-12-23T18:05:55+00:00 · Latest: 2025-12-23T18:05:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20562v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20562v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the problem of learning a low-degree spherical polynomial of degree $\ell_0 = Θ(1) \ge 1$ defined on the unit sphere in $\RR^d$ by training an over-parameterized two-layer neural network (NN) with channel attention in this paper. Our main result is the significantly improved sample complexity for learning such low-degree polynomials. We show that, for any regression risk $\eps \in (0,1)$, a carefully designed two-layer NN with channel attention and finite width of $m \ge Θ({n^4 \log (2n/δ)}/{d^{2\ell_0}})$ trained by the vanilla gradient descent (GD) requires the lowest sample complexity of $n \asymp Θ(d^{\ell_0}/\eps)$ with probability $1-δ$ for every $δ\in (0,1)$, in contrast with the representative sample complexity $Θ\pth{d^{\ell_0} \max\set{\eps^{-2},\log d}}$, where $n$ is the training daata size. Moreover, such sample complexity is not improvable since the trained network renders a sharp rate of the nonparametric regression risk of the order $Θ(d^{\ell_0}/{n})$ with probability at least $1-δ$. On the other hand, the minimax optimal rate for the regression risk with a kernel of rank $Θ(d^{\ell_0})$ is $Θ(d^{\ell_0}/{n})$, so that the rate of the nonparametric regression risk of the network trained by GD is minimax optimal. The training of the two-layer NN with channel attention consists of two stages. In Stage 1, a provable learnable channel selection algorithm identifies the ground-truth channel number $\ell_0$ from the initial $L \ge \ell_0$ channels in the first-layer activation, with high probability. This learnable selection is achieved by an efficient one-step GD update on both layers, enabling feature learning for low-degree polynomial targets. In Stage 2, the second layer is trained by standard GD using the activation function with the selected channels.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>浅层神经网络通过可学习通道注意力学习低次球面多项式</div>
<div class="mono" style="margin-top:8px">本文研究了通过训练一个过参数化的两层神经网络（NN）并结合通道注意力机制，在 $\RR^d$ 中的单位球上学习一个低次球面多项式的问题，其中多项式的次数 $\ell_0 = Θ(1) \ge 1$。我们的主要结果是显著改进了学习此类低次多项式的样本复杂度。我们证明，对于任意回归风险 $\eps \in (0,1)$，一个精心设计的两层NN，其第一层激活的初始通道数为 $L \ge \ell_0$，且宽度为 $m \ge Θ({n^4 \log (2n/δ)}/{d^{2\ell_0}})$，使用原始梯度下降（GD）训练后，其样本复杂度为 $n \asymp Θ(d^{\ell_0}/\eps)$，概率为 $1-δ$。这与代表性样本复杂度 $Θ\pth{d^{\ell_0} \max\set{\eps^{-2},\log d}}$ 相比有显著提升。此外，这种样本复杂度无法进一步改进，因为训练后的网络在概率至少为 $1-δ$ 的情况下，非参数回归风险的速率达到 $Θ(d^{\ell_0}/{n})$。另一方面，对于具有秩 $Θ(d^{\ell_0})$ 的核，回归风险的最小最大最优速率也是 $Θ(d^{\ell_0}/{n})$，因此通过GD训练的网络的非参数回归风险速率是最小最大最优的。两层NN的训练包含两个阶段。在第一阶段，一个可证明的可学习通道选择算法以高概率从第一层激活的初始 $L \ge \ell_0$ 个通道中识别出真实通道数 $\ell_0$。这种可学习选择通过在两层上进行高效的一步GD更新实现，从而实现对低次多项式目标的特征学习。在第二阶段，使用选定通道的激活函数，通过标准GD训练第二层。</div>
</details>
</div>
<div class="card">
<div class="title">FlashVLM: Text-Guided Visual Token Selection for Large Multimodal Models</div>
<div class="meta-line">Authors: Kaitong Cai, Jusheng Zhang, Jing Yang, Yijia Fan, Pengtao Xie, Jian Wang, Keze Wang</div>
<div class="meta-line">First: 2025-12-23T18:05:43+00:00 · Latest: 2025-12-23T18:05:43+00:00</div>
<div class="meta-line">Comments: Under submission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20561v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20561v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large vision-language models (VLMs) typically process hundreds or thousands of visual tokens per image or video frame, incurring quadratic attention cost and substantial redundancy. Existing token reduction methods often ignore the textual query or rely on deep attention maps, whose instability under aggressive pruning leads to degraded semantic alignment.
  We propose FlashVLM, a text guided visual token selection framework that dynamically adapts visual inputs to the query. Instead of relying on noisy attention weights, FlashVLM computes an explicit cross modal similarity between projected image tokens and normalized text embeddings in the language model space. This extrinsic relevance is fused with intrinsic visual saliency using log domain weighting and temperature controlled sharpening. In addition, a diversity preserving partition retains a minimal yet representative set of background tokens to maintain global context.
  Under identical token budgets and evaluation protocols, FlashVLM achieves beyond lossless compression, slightly surpassing the unpruned baseline while pruning up to 77.8 percent of visual tokens on LLaVA 1.5, and maintaining 92.8 percent accuracy even under 94.4 percent compression. Extensive experiments on 14 image and video benchmarks demonstrate that FlashVLM delivers state of the art efficiency performance trade offs while maintaining strong robustness and generalization across mainstream VLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlashVLM：用于大型多模态模型的文本引导视觉标记选择</div>
<div class="mono" style="margin-top:8px">大型视觉-语言模型（VLMs）通常对每张图像或视频帧处理数百或数千个视觉标记，导致二次注意力成本和大量冗余。现有的标记压缩方法往往忽略文本查询，或依赖深层注意力图，其在激进剪枝下的不稳定性会导致语义对齐下降。
我们提出FlashVLM，这是一个文本引导的视觉标记选择框架，能够动态地将视觉输入适应于查询。与依赖噪声注意力权重的方法不同，FlashVLM在语言模型空间中计算投影图像标记与归一化文本嵌入之间的显式跨模态相似性。该外在相关性通过日志域加权和温度控制锐化方法与内在视觉显著性进行融合。此外，一个多样性保留的分区机制保留了少量但具有代表性的背景标记，以维持全局上下文。
在相同的标记预算和评估协议下，FlashVLM实现了超越无损压缩的效果，在LLaVA 1.5上最多可剪枝77.8%的视觉标记，同时保持92.8%的准确率，即使在94.4%的压缩率下。在14个图像和视频基准上的大量实验表明，FlashVLM在保持主流VLMs强大鲁棒性和泛化能力的同时，提供了最先进的效率性能权衡。</div>
</details>
</div>
<div class="card">
<div class="title">Compute-in-Memory Implementation of State Space Models for Event Sequence Processing</div>
<div class="meta-line">Authors: Xiaoyu Zhang, Mingtao Hu, Sen Lu, Soohyeon Kim, Eric Yeu-Jer Lee, Yuyang Liu, Wei D. Lu</div>
<div class="meta-line">First: 2025-11-17T21:06:52+00:00 · Latest: 2025-12-23T18:00:52+00:00</div>
<div class="meta-line">Comments: Xiaoyu Zhang and Mingtao Hu contributed equally to this work</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.13912v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.13912v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">State space models (SSMs) have recently emerged as a powerful framework for long sequence processing, outperforming traditional methods on diverse benchmarks. Fundamentally, SSMs can generalize both recurrent and convolutional networks and have been shown to even capture key functions of biological systems. Here we report an approach to implement SSMs in energy-efficient compute-in-memory (CIM) hardware to achieve real-time, event-driven processing. Our work re-parameterizes the model to function with real-valued coefficients and shared decay constants, reducing the complexity of model mapping onto practical hardware systems. By leveraging device dynamics and diagonalized state transition parameters, the state evolution can be natively implemented in crossbar-based CIM systems combined with memristors exhibiting short-term memory effects. Through this algorithm and hardware co-design, we show the proposed system offers both high accuracy and high energy efficiency while supporting fully asynchronous processing for event-based vision and audio tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于事件序列处理的状态空间模型的存算一体实现</div>
<div class="mono" style="margin-top:8px">状态空间模型（SSMs）最近已成为处理长序列的强大框架，在各种基准测试中优于传统方法。本质上，SSMs可以同时泛化循环网络和卷积网络，并已被证明能够捕捉生物系统的关键功能。本文报告了一种在能效高的存算一体（CIM）硬件上实现SSMs的方法，以实现实时、事件驱动的处理。我们的工作重新参数化模型，使其能够使用实值系数和共享衰减常数运行，从而降低模型映射到实际硬件系统的复杂性。通过利用器件动态和对角化的状态转移参数，状态演化可以原生地在基于交叉阵列的CIM系统中实现，该系统结合了具有短期记忆效应的忆阻器。通过这种算法与硬件协同设计，我们展示了所提出的系统在支持基于事件的视觉和音频任务的全异步处理的同时，既具有高精度又具有高能效。</div>
</details>
</div>
<div class="card">
<div class="title">Learning to Reason in 4D: Dynamic Spatial Understanding for Vision Language Models</div>
<div class="meta-line">Authors: Shengchao Zhou, Yuxin Chen, Yuying Ge, Wei Huang, Jiehong Lin, Ying Shan, Xiaojuan Qi</div>
<div class="meta-line">First: 2025-12-23T17:56:36+00:00 · Latest: 2025-12-23T17:56:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20557v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20557v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLM) excel at general understanding yet remain weak at dynamic spatial reasoning (DSR), i.e., reasoning about the evolvement of object geometry and relationship in 3D space over time, largely due to the scarcity of scalable 4D-aware training resources. To bridge this gap across aspects of dataset, benchmark and model, we introduce DSR Suite. First, we propose an automated pipeline that generates multiple-choice question-answer pairs from in-the-wild videos for DSR. By leveraging modern vision foundation models, the pipeline extracts rich geometric and motion information, including camera poses, local point clouds, object masks, orientations, and 3D trajectories. These geometric cues enable the construction of DSR-Train for learning and further human-refined DSR-Bench for evaluation. Compared with previous works, our data emphasize (i) in-the-wild video sources, (ii) object- and scene-level 3D requirements, (iii) viewpoint transformations, (iv) multi-object interactions, and (v) fine-grained, procedural answers. Beyond data, we propose a lightweight Geometry Selection Module (GSM) to seamlessly integrate geometric priors into VLMs, which condenses question semantics and extracts question-relevant knowledge from pretrained 4D reconstruction priors into a compact set of geometry tokens. This targeted extraction avoids overwhelming the model with irrelevant knowledge. Experiments show that integrating DSR-Train and GSM into Qwen2.5-VL-7B significantly enhances its dynamic spatial reasoning capability, while maintaining accuracy on general video understanding benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在四维空间中学习推理：面向视觉语言模型的动态空间理解</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLM）在一般理解方面表现出色，但在动态空间推理（DSR）方面表现较弱，即对三维空间中物体几何形状和关系随时间演变的推理，这主要归因于可扩展的四维感知训练资源稀缺。为弥合数据集、基准测试和模型方面的差距，我们引入了DSR Suite。首先，我们提出了一种自动化流程，从野外视频中生成多选问答对用于DSR。通过利用现代视觉基础模型，该流程提取了丰富的几何和运动信息，包括相机姿态、局部点云、物体掩膜、方向和三维轨迹。这些几何提示使得构建用于学习的DSR-Train和用于评估的人工优化DSR-Bench成为可能。与以往的工作相比，我们的数据强调了以下几点：(i) 野外视频来源，(ii) 物体和场景级别的三维需求，(iii) 视点变换，(iv) 多物体交互，(v) 细粒度、程序化的答案。除了数据，我们还提出了一种轻量级的几何选择模块（GSM），用于无缝地将几何先验知识整合到VLM中，该模块将问题语义压缩，并从预训练的四维重建先验知识中提取与问题相关的信息，形成一组紧凑的几何标记。这种有针对性的提取避免了模型被无关知识淹没。实验表明，将DSR-Train和GSM整合到Qwen2.5-VL-7B中显著增强了其动态空间推理能力，同时在一般视频理解基准测试中保持了准确性。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Grained Text-Guided Image Fusion for Multi-Exposure and Multi-Focus Scenarios</div>
<div class="meta-line">Authors: Mingwei Tang, Jiahao Nie, Guang Yang, Ziqing Cui, Jie Li</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2025-12-23T17:55:35+00:00 · Latest: 2025-12-23T17:55:35+00:00</div>
<div class="meta-line">Comments: Accepted to WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.20556v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.20556v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Image fusion aims to synthesize a single high-quality image from a pair of inputs captured under challenging conditions, such as differing exposure levels or focal depths. A core challenge lies in effectively handling disparities in dynamic range and focus depth between the inputs. With the advent of vision-language models, recent methods incorporate textual descriptions as auxiliary guidance to enhance fusion quality. However, simply incorporating coarse-grained descriptions hampers the understanding of fine-grained details and poses challenges for precise cross-modal alignment. To address these limitations, we propose Multi-grained Text-guided Image Fusion (MTIF), a novel fusion paradigm with three key designs. First, it introduces multi-grained textual descriptions that separately capture fine details, structural cues, and semantic content, guiding image fusion through a hierarchical cross-modal modulation module. Second, it involves supervision signals at each granularity to facilitate alignment between visual and textual features and enhance the utility of auxiliary text. Third, it adopts a saliency-driven enrichment module to augment training data with dense semantic content, further strengthening the cross-modal modulation and alignment. Extensive experiments show that MTIF consistently outperforms previous methods on both multi-exposure and multi-focus image fusion tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多粒度文本引导的多曝光和多焦图像融合</div>
<div class="mono" style="margin-top:8px">图像融合旨在从在具有挑战性条件（如不同曝光水平或焦深）下捕获的两个输入图像中合成一张高质量图像。一个核心挑战在于有效处理输入图像之间的动态范围和焦深差异。随着视觉-语言模型的发展，近期方法引入文本描述作为辅助引导以提升融合质量。然而，仅使用粗粒度描述会阻碍对细粒度细节的理解，并对精确的跨模态对齐造成困难。为了解决这些局限性，我们提出了一种新的融合范式——多粒度文本引导图像融合（MTIF），其包含三个关键设计。首先，它引入了多粒度文本描述，分别捕捉细节、结构线索和语义内容，并通过分层跨模态调制模块引导图像融合。其次，它在每个粒度级别引入监督信号，以促进视觉与文本特征的对齐并增强辅助文本的实用性。第三，它采用基于显著性的丰富模块，通过密集语义内容增强训练数据，进一步加强跨模态调制与对齐。大量实验表明，MTIF在多曝光和多焦图像融合任务中均优于以往方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Image fusion aims to synthesize a single high-quality image from a pair of inputs captured under challenging conditions, such as differing exposure levels or focal depths.</div>
</details>
</div>
<div class="card">
<div class="title">Resolution scaling governs DINOv3 transfer performance in chest radiograph classification</div>
<div class="meta-line">Authors: Soroosh Tayebi Arasteh, Mina Shaigan, Christiane Kuhl, Jakob Nikolas Kather, Sven Nebelung, Daniel Truhn</div>
<div class="meta-line">First: 2025-10-08T16:25:04+00:00 · Latest: 2025-12-23T17:45:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.07191v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.07191v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Self-supervised learning (SSL) has advanced visual representation learning, but its value in chest radiography, a high-volume imaging modality with fine-grained findings, remains unclear. Meta&#x27;s DINOv3 extends earlier SSL models through Gram-anchored self-distillation. Whether these design choices improve transfer learning for chest radiography has not been systematically tested. We benchmarked DINOv3 against DINOv2 and ImageNet initialization across seven datasets (n&gt;814,000). Two representative backbones were evaluated: ViT-B/16 and ConvNeXt-B. Images were analyzed at 224x224, 512x512, and 1024x1024 pixels. We additionally assessed frozen features from a 7B model. The primary outcome was mean AUROC across labels. At 224x224, DINOv3 and DINOv2 achieved comparable performance on adult datasets. Increasing resolution to 512x512 yielded consistent improvements for DINOv3 over both DINOv2 and ImageNet. In contrast, results in pediatric cohort showed no differences across initializations. Across all settings, ConvNeXt-B outperformed ViT-B/16. Models using frozen DINOv3-7B features underperformed relative to fully finetuned 86-89M-parameter backbones, highlighting the importance of domain adaptation. Scaling to 1024x1024 did not further improve accuracy. Resolution-related gains were most evident for boundary-dependent and small focal abnormalities. In chest radiography, higher input resolution is critical for leveraging the benefits of modern self-supervised models. 512x512 pixels represent a practical upper limit where DINOv3-initialized ConvNeXt-B networks provide the strongest performance, while larger inputs offer minimal return on cost. Clinically, these findings support use of finetuned, mid-sized backbones at 512x512 for chest radiograph interpretation, with the greatest gains expected in detecting subtle or boundary-centered lesions relevant to emergency and critical care settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>分辨率缩放在胸部X光分类中的DINOv3迁移性能</div>
<div class="mono" style="margin-top:8px">自监督学习（SSL）推动了视觉表征学习的发展，但其在胸部X光检查中的价值仍不清楚，胸部X光是一种高体积、细粒度发现的影像模态。Meta的DINOv3通过Gram锚定的自蒸馏扩展了早期的SSL模型。这些设计选择是否能提升胸部X光的迁移学习效果尚未系统测试。我们在七个数据集（n&gt;814,000）上对DINOv3与DINOv2和ImageNet初始化进行了基准测试。评估了两个代表性主干网络：ViT-B/16和ConvNeXt-B。图像在224x224、512x512和1024x1024像素下进行分析。我们还评估了7B模型的冻结特征。主要结果是标签平均AUC。在224x224像素下，DINOv3和DINOv2在成人数据集上表现相当。将分辨率提升到512x512像素时，DINOv3在两个模型上均表现出一致的提升。相比之下，儿童队列的结果显示初始化之间没有差异。在所有设置中，ConvNeXt-B均优于ViT-B/16。使用冻结的DINOv3-7B特征的模型在与完全微调的86-89M参数主干网络相比时表现较差，突显了领域适应的重要性。将分辨率扩展到1024x1024像素并未进一步提高准确性。分辨率相关的增益在边界依赖和小焦点异常中最为明显。在胸部X光检查中，更高的输入分辨率对于利用现代自监督模型的优势至关重要。512x512像素代表了一个实用的上限，其中DINOv3初始化的ConvNeXt-B网络表现最强，而更大的输入则在成本上回报有限。在临床方面，这些发现支持在512x512像素下使用微调的中等规模主干网络进行胸部X光解读，预期在检测与急诊和重症监护环境相关的细微或边界中心病变时获得最大增益。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Self-supervised learning (SSL) has advanced visual representation learning, but its value in chest radiography, a high-volume imaging modality with fine-grained findings, remains unclear.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
