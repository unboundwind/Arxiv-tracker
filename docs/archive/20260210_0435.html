<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-10 04:35</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260210_0435</div>
    <div class="row"><div class="card">
<div class="title">MedMO: Grounding and Understanding Multimodal Large Language Model for Medical Images</div>
<div class="meta-line">Authors: Ankan Deria, Komal Kumar, Adinath Madhavrao Dukre, Eran Segal, Salman Khan, Imran Razzak</div>
<div class="meta-line">First: 2026-02-06T18:59:59+00:00 · Latest: 2026-02-06T18:59:59+00:00</div>
<div class="meta-line">Comments: 21 pages, 6 figures and 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06965v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06965v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://genmilab.github.io/MedMO-Page">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal large language models (MLLMs) have rapidly advanced, yet their adoption in medicine remains limited by gaps in domain coverage, modality alignment, and grounded reasoning. In this work, we introduce MedMO, a medical foundation model built upon a generalized MLLM architecture and trained exclusively on large-scale, domain-specific data. MedMO follows a multi-stage training recipe: (i) cross-modal pretraining to align heterogeneous visual encoders with a medical language backbone; (ii) instruction tuning on multi-task supervision that spans captioning, VQA, report generation, retrieval, and grounded disease localization with bounding boxes; and (iii) reinforcement learning with verifiable rewards that combine factuality checks with a box-level GIoU reward to strengthen spatial grounding and step-by-step reasoning in complex clinical scenarios. MedMO consistently outperforms strong open-source medical MLLMs across multiple modalities and tasks. On VQA benchmarks, MedMO achieves an average accuracy improvement of +13.7% over the baseline and performs within 1.9% of the SOTA Fleming-VL. For text-based QA, it attains +6.9% over the baseline and +14.5% over Fleming-VL. In medical report generation, MedMO delivers significant gains in both semantic and clinical accuracy. Moreover, it exhibits strong grounding capability, achieving an IoU improvement of +40.4 over the baseline and +37.0% over Fleming-VL, underscoring its robust spatial reasoning and localization performance. Evaluations across radiology, ophthalmology, and pathology-microscopy confirm MedMO&#x27;s broad cross-modality generalization. We release two versions of MedMO: 4B and 8B. Project is available at https://genmilab.github.io/MedMO-Page</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MedMO：面向医学图像的多模态大语言模型的定位与理解</div>
<div class="mono" style="margin-top:8px">尽管多模态大语言模型（MLLMs）迅速发展，但其在医学领域的应用仍受限于领域覆盖不足、模态对齐和基于上下文的推理能力的差距。本文提出MedMO，这是一个基于通用MLLM架构并仅在大规模、领域特定数据上训练的医学基础模型。MedMO采用多阶段训练方案：(i) 跨模态预训练，将异构视觉编码器与医学语言主干对齐；(ii) 在涵盖图像描述、视觉问答（VQA）、报告生成、检索和基于边界框的疾病定位等多任务监督下进行指令调优；(iii) 采用可验证奖励的强化学习，结合事实性检查和框级GIoU奖励，以增强复杂临床场景中的空间定位和逐步推理能力。MedMO在多个模态和任务上均优于现有的强大开源医学MLLMs。在VQA基准测试中，MedMO在基线基础上平均准确率提升了13.7%，且性能接近当前最优的Fleming-VL模型（仅差1.9%）。在基于文本的问答任务中，其准确率分别比基线提升6.9%和比Fleming-VL提升14.5%。在医学报告生成任务中，MedMO在语义和临床准确性方面均取得显著提升。此外，它展现出强大的定位能力，IoU指标分别比基线提升40.4%和比Fleming-VL提升37.0%，突显了其在空间推理和定位方面的优越性能。在放射学、眼科和病理学显微镜领域的评估进一步验证了MedMO在跨模态任务中的广泛泛化能力。我们发布了两个版本的MedMO：4B和8B。项目地址为：https://genmilab.github.io/MedMO-Page</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multimodal large language models (MLLMs) have rapidly advanced, yet their adoption in medicine remains limited by gaps in domain coverage, modality alignment, and grounded reasoning.</div>
</details>
</div>
<div class="card">
<div class="title">Learning a Generative Meta-Model of LLM Activations</div>
<div class="meta-line">Authors: Grace Luo, Jiahai Feng, Trevor Darrell, Alec Radford, Jacob Steinhardt</div>
<div class="meta-line">First: 2026-02-06T18:59:56+00:00 · Latest: 2026-02-06T18:59:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06964v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06964v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://generative-latent-prior.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing approaches for analyzing neural network activations, such as PCA and sparse autoencoders, rely on strong structural assumptions. Generative models offer an alternative: they can uncover structure without such assumptions and act as priors that improve intervention fidelity. We explore this direction by training diffusion models on one billion residual stream activations, creating &quot;meta-models&quot; that learn the distribution of a network&#x27;s internal states. We find that diffusion loss decreases smoothly with compute and reliably predicts downstream utility. In particular, applying the meta-model&#x27;s learned prior to steering interventions improves fluency, with larger gains as loss decreases. Moreover, the meta-model&#x27;s neurons increasingly isolate concepts into individual units, with sparse probing scores that scale as loss decreases. These results suggest generative meta-models offer a scalable path toward interpretability without restrictive structural assumptions. Project page: https://generative-latent-prior.github.io.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习LLM激活的生成元模型</div>
<div class="mono" style="margin-top:8px">现有的神经网络激活分析方法，如PCA和稀疏自编码器，依赖于强结构假设。生成模型提供了一种替代方法：它们可以在不依赖这些假设的情况下揭示结构，并作为先验知识来提高干预的保真度。我们通过在十亿个残差流激活上训练扩散模型，探索这一方向，创建了能够学习网络内部状态分布的&quot;元模型&quot;。我们发现扩散损失随着计算量的增加而平滑下降，并且能够可靠地预测下游性能。特别是，将元模型学习到的先验应用于干预引导，可以提高流畅性，损失越低，效果越显著。此外，元模型的神经元会越来越多地将概念隔离为独立单元，其稀疏探测得分随着损失的降低而提升。这些结果表明，生成元模型提供了一条无需严格结构假设即可实现可解释性的可扩展路径。项目页面：https://generative-latent-prior.github.io.</div>
</details>
</div>
<div class="card">
<div class="title">InftyThink+: Effective and Efficient Infinite-Horizon Reasoning via Reinforcement Learning</div>
<div class="meta-line">Authors: Yuchen Yan, Liang Jiang, Jin Jiang, Shuaicheng Li, Zujie Wen, Zhiqiang Zhang, Jun Zhou, Jian Shao, Yueting Zhuang, Yongliang Shen</div>
<div class="meta-line">First: 2026-02-06T18:59:27+00:00 · Latest: 2026-02-06T18:59:27+00:00</div>
<div class="meta-line">Comments: Project Page: https://zju-real.github.io/InftyThink-Plus Code: https://github.com/ZJU-REAL/InftyThink-Plus</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06960v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06960v1">PDF</a> · <a href="https://github.com/ZJU-REAL/InftyThink-Plus">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://zju-real.github.io/InftyThink-Plus">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects. Iterative reasoning mitigates these issues by periodically summarizing intermediate thoughts, yet existing methods rely on supervised learning or fixed heuristics and fail to optimize when to summarize, what to preserve, and how to resume reasoning. We propose InftyThink+, an end-to-end reinforcement learning framework that optimizes the entire iterative reasoning trajectory, building on model-controlled iteration boundaries and explicit summarization. InftyThink+ adopts a two-stage training scheme with supervised cold-start followed by trajectory-level reinforcement learning, enabling the model to learn strategic summarization and continuation decisions. Experiments on DeepSeek-R1-Distill-Qwen-1.5B show that InftyThink+ improves accuracy by 21% on AIME24 and outperforms conventional long chain-of-thought reinforcement learning by a clear margin, while also generalizing better to out-of-distribution benchmarks. Moreover, InftyThink+ significantly reduces inference latency and accelerates reinforcement learning training, demonstrating improved reasoning efficiency alongside stronger performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InftyThink+: 通过强化学习实现高效有效的无限时域推理</div>
<div class="mono" style="margin-top:8px">大型推理模型通过扩展推理时的链式思维实现强大性能，但这种范式存在二次成本、上下文长度限制以及中间丢失效应导致的推理退化问题。迭代推理通过定期总结中间思维来缓解这些问题，但现有方法依赖监督学习或固定启发式规则，无法优化何时总结、保留什么以及如何恢复推理。我们提出InftyThink+，这是一个端到端的强化学习框架，通过模型控制的迭代边界和显式总结来优化整个迭代推理轨迹。InftyThink+采用两阶段训练方案，先进行监督学习冷启动，再进行轨迹级强化学习，使模型能够学习策略性的总结和继续推理决策。在DeepSeek-R1-Distill-Qwen-1.5B上的实验表明，InftyThink+在AIME24上准确率提升了21%，在常规长链式思维强化学习方法上表现显著优于后者，同时在分布外基准测试中也展现出更好的泛化能力。此外，InftyThink+还显著降低了推理延迟并加速了强化学习训练，实现了推理效率的提升和性能的增强。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large reasoning models achieve strong performance by scaling inference-time chain-of-thought, but this paradigm suffers from quadratic cost, context length limits, and degraded reasoning due to lost-in-the-middle effects.</div>
</details>
</div>
<div class="card">
<div class="title">CineScene: Implicit 3D as Effective Scene Representation for Cinematic Video Generation</div>
<div class="meta-line">Authors: Kaiyi Huang, Yukun Huang, Yu Li, Jianhong Bai, Xintao Wang, Zinan Lin, Xuefei Ning, Jiwen Yu, Pengfei Wan, Yu Wang, Xihui Liu</div>
<div class="meta-line">First: 2026-02-06T18:59:24+00:00 · Latest: 2026-02-06T18:59:24+00:00</div>
<div class="meta-line">Comments: Project website: https://karine-huang.github.io/CineScene/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06959v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06959v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://karine-huang.github.io/CineScene/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Cinematic video production requires control over scene-subject composition and camera movement, but live-action shooting remains costly due to the need for constructing physical sets. To address this, we introduce the task of cinematic video generation with decoupled scene context: given multiple images of a static environment, the goal is to synthesize high-quality videos featuring dynamic subject while preserving the underlying scene consistency and following a user-specified camera trajectory. We present CineScene, a framework that leverages implicit 3D-aware scene representation for cinematic video generation. Our key innovation is a novel context conditioning mechanism that injects 3D-aware features in an implicit way: By encoding scene images into visual representations through VGGT, CineScene injects spatial priors into a pretrained text-to-video generation model by additional context concatenation, enabling camera-controlled video synthesis with consistent scenes and dynamic subjects. To further enhance the model&#x27;s robustness, we introduce a simple yet effective random-shuffling strategy for the input scene images during training. To address the lack of training data, we construct a scene-decoupled dataset with Unreal Engine 5, containing paired videos of scenes with and without dynamic subjects, panoramic images representing the underlying static scene, along with their camera trajectories. Experiments show that CineScene achieves state-of-the-art performance in scene-consistent cinematic video generation, handling large camera movements and demonstrating generalization across diverse environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CineScene: 一种隐式3D场景表示用于电影视频生成</div>
<div class="mono" style="margin-top:8px">电影视频制作需要对场景主体的构图和摄像机运动进行控制，但实拍仍然昂贵，因为需要搭建物理布景。为了解决这一问题，我们引入了基于解耦场景上下文的电影视频生成任务：给定多个静态环境的图像，目标是合成高质量的视频，其中包含动态主体，同时保持场景一致性并遵循用户指定的摄像机轨迹。我们提出了CineScene框架，该框架利用隐式3D感知的场景表示进行电影视频生成。我们的关键创新是一种新的上下文条件机制，通过隐式方式注入3D感知特征：通过VGGT将场景图像编码为视觉表示，CineScene通过额外的上下文拼接将空间先验注入到预训练的文本到视频生成模型中，从而实现摄像机控制的视频合成，保持场景一致性和动态主体。为了进一步增强模型的鲁棒性，我们在训练过程中引入了一种简单而有效的输入场景图像随机洗牌策略。为了解决训练数据不足的问题，我们使用Unreal Engine 5构建了一个解耦场景的数据集，其中包含有动态主体和无动态主体的场景视频配对，以及代表底层静态场景的全景图像及其摄像机轨迹。实验表明，CineScene在场景一致的电影视频生成中达到了最先进的性能，能够处理大范围的摄像机运动，并在多种环境中展现出良好的泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Cinematic video production requires control over scene-subject composition and camera movement, but live-action shooting remains costly due to the need for constructing physical sets.</div>
</details>
</div>
<div class="card">
<div class="title">Counting point configurations in projective space</div>
<div class="meta-line">Authors: Alex Fink, Navid Nabijou, Rob Silversmith</div>
<div class="meta-line">First: 2026-01-21T19:32:46+00:00 · Latest: 2026-02-06T18:58:59+00:00</div>
<div class="meta-line">Comments: 27 pages. Includes ancillary Mathematica code. Fixed a typography issue where some references were mislabelled. Comments welcome</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15421v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15421v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate the enumerative geometry of point configurations in projective space. We define &quot;projective configuration counts&quot;: these enumerate configurations of points in projective space such that certain specified subsets are in fixed relative positions. The $\mathbb{P}^1$ case recovers cross-ratio degrees, which arise naturally in numerous contexts. We establish two main results. The first is a combinatorial upper bound given by the number of weighted transversals of a bipartite graph. The second is a recursion that relates counts associated to projective spaces of different dimensions, by projecting away from a given point. Key inputs include the Gelfand-MacPherson correspondence, the Jacobi-Trudi and Thom-Porteous formulae, and the notion of surplus from matching theory of bipartite graphs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>计数射影空间中的点配置</div>
<div class="mono" style="margin-top:8px">我们研究射影空间中点配置的计数几何。我们定义了&quot;射影配置计数&quot;：这些计数枚举射影空间中满足某些指定子集具有固定相对位置的点配置。$\mathbb{P}^1$ 的情况恢复了交叉比次数，这在许多上下文中自然出现。我们建立了两个主要结果。第一个是通过二分图的加权横截线数量给出的组合上界。第二个是一个递归关系，通过从给定点投影，将不同维度射影空间的计数联系起来。关键输入包括 Gelfand-MacPherson 对应关系、Jacobi-Trudi 和 Thom-Porteous 公式，以及二分图匹配理论中的剩余概念。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Credit Card Fraud Detection with an Optimized Explainable Boosting Machine</div>
<div class="meta-line">Authors: Reza E. Fazel, Arash Bakhtiary, Siavash A. Bigdeli</div>
<div class="meta-line">First: 2026-02-06T18:56:17+00:00 · Latest: 2026-02-06T18:56:17+00:00</div>
<div class="meta-line">Comments: 22 pages, 5 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06955v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06955v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Addressing class imbalance is a central challenge in credit card fraud detection, as it directly impacts predictive reliability in real-world financial systems. To overcome this, the study proposes an enhanced workflow based on the Explainable Boosting Machine (EBM)-a transparent, state-of-the-art implementation of the GA2M algorithm-optimized through systematic hyperparameter tuning, feature selection, and preprocessing refinement. Rather than relying on conventional sampling techniques that may introduce bias or cause information loss, the optimized EBM achieves an effective balance between accuracy and interpretability, enabling precise detection of fraudulent transactions while providing actionable insights into feature importance and interaction effects. Furthermore, the Taguchi method is employed to optimize both the sequence of data scalers and model hyperparameters, ensuring robust, reproducible, and systematically validated performance improvements. Experimental evaluation on benchmark credit card data yields an ROC-AUC of 0.983, surpassing prior EBM baselines (0.975) and outperforming Logistic Regression, Random Forest, XGBoost, and Decision Tree models. These results highlight the potential of interpretable machine learning and data-driven optimization for advancing trustworthy fraud analytics in financial systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过优化可解释增强提升机改进信用卡欺诈检测</div>
<div class="mono" style="margin-top:8px">解决类别不平衡问题是信用卡欺诈检测中的核心挑战，因为它直接影响实际金融系统中预测的可靠性。为克服这一问题，本研究提出了一种基于可解释增强提升机（EBM）的增强工作流程——这是一种透明且先进的GA2M算法实现——通过系统的超参数调优、特征选择和预处理优化来提升性能。与依赖传统采样技术可能引入偏差或导致信息丢失的方法不同，优化后的EBM在准确性和可解释性之间实现了有效平衡，从而能够精确检测欺诈交易，同时提供关于特征重要性和交互效应的可操作见解。此外，采用田口方法优化数据缩放器的序列和模型超参数，确保了稳健、可重复且系统验证的性能提升。在基准信用卡数据集上的实验评估显示，ROC-AUC达到0.983，超过了先前的EBM基线（0.975），并优于逻辑回归、随机森林、XGBoost和决策树模型。这些结果突显了可解释机器学习和数据驱动优化在推动金融系统中可信欺诈分析方面的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">Implicit Unitarity Bias in Tensor Factorization: A Theoretical Framework for Symmetry Group Discovery</div>
<div class="meta-line">Authors: Dongsung Huh, Halyun Jeong</div>
<div class="meta-line">First: 2025-11-28T12:58:13+00:00 · Latest: 2026-02-06T18:51:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.23152v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.23152v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While modern neural architectures typically generalize via smooth interpolation, it lacks the inductive biases required to uncover algebraic structures essential for systematic generalization. We present the first theoretical analysis of HyperCube, a differentiable tensor factorization architecture designed to bridge this gap. This work establishes an intrinsic geometric property of the HyperCube formulation: we prove that the architecture mediates a fundamental equivalence between geometric alignment and algebraic structure. Independent of the global optimization landscape, we show that the condition of geometric alignment imposes rigid algebraic constraints, proving that the feasible collinear manifold is non-empty if and only if the target is isotopic to a group. Within this manifold, we characterize the objective as a rank-maximizing potential that unconditionally drives factors toward full-rank, unitary representations. Finally, we propose the Collinearity Dominance mechanism to link these structural results to the global landscape. Supported by empirical scaling laws, we establish that global minima are achieved exclusively by unitary regular representations of group isotopes. This formalizes the HyperCube objective as a differentiable proxy for associativity, demonstrating how rigid geometric constraints enable the discovery of latent algebraic symmetry.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>张量分解中的隐式单位性偏见：对对称群发现的理论框架</div>
<div class="mono" style="margin-top:8px">尽管现代神经架构通常通过平滑插值进行泛化，但其缺乏发现系统泛化所需代数结构的归纳偏见。我们提出了对HyperCube的首个理论分析，HyperCube是一种可微分的张量分解架构，旨在弥合这一差距。本工作确立了HyperCube公式的一个内在几何属性：我们证明该架构在几何对齐与代数结构之间建立了一种基本等价关系。无论全局优化景观如何，我们展示了几何对齐的条件会施加严格的代数约束，证明了目标若同构于一个群，则可行的共线流形非空。在此流形中，我们将目标函数表征为一个最大化秩的势能，无条件地推动因子向全秩、单位性表示收敛。最后，我们提出了共线主导机制，将这些结构结果与全局景观联系起来。通过实证缩放定律的支持，我们证明全局最小值仅由群同构的单位正则表示实现。这将HyperCube目标函数形式化为可微分的结合性代理，展示了严格的几何约束如何实现潜在代数对称性的发现。</div>
</details>
</div>
<div class="card">
<div class="title">DreamDojo: A Generalist Robot World Model from Large-Scale Human Videos</div>
<div class="meta-line">Authors: Shenyuan Gao, William Liang, Kaiyuan Zheng, Ayaan Malik, Seonghyeon Ye, Sihyun Yu, Wei-Cheng Tseng, Yuzhu Dong, Kaichun Mo, Chen-Hsuan Lin, Qianli Ma, Seungjun Nah, Loic Magne, Jiannan Xiang, Yuqi Xie, Ruijie Zheng, Dantong Niu, You Liang Tan, K. R. Zentner, George Kurian, Suneel Indupuru, Pooya Jannaty, Jinwei Gu, Jun Zhang, Jitendra Malik, Pieter Abbeel, Ming-Yu Liu, Yuke Zhu, Joel Jang, Linxi &quot;Jim&quot; Fan</div>
<div class="meta-line">First: 2026-02-06T18:49:43+00:00 · Latest: 2026-02-06T18:49:43+00:00</div>
<div class="meta-line">Comments: Project page: https://dreamdojo-world.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06949v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06949v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://dreamdojo-world.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale. However, modeling these world dynamics, especially for dexterous robotics tasks, poses significant challenges due to limited data coverage and scarce action labels. As an endeavor towards this end, we introduce DreamDojo, a foundation world model that learns diverse interactions and dexterous controls from 44k hours of egocentric human videos. Our data mixture represents the largest video dataset to date for world model pretraining, spanning a wide range of daily scenarios with diverse objects and skills. To address the scarcity of action labels, we introduce continuous latent actions as unified proxy actions, enhancing interaction knowledge transfer from unlabeled videos. After post-training on small-scale target robot data, DreamDojo demonstrates a strong understanding of physics and precise action controllability. We also devise a distillation pipeline that accelerates DreamDojo to a real-time speed of 10.81 FPS and further improves context consistency. Our work enables several important applications based on generative world models, including live teleoperation, policy evaluation, and model-based planning. Systematic evaluation on multiple challenging out-of-distribution (OOD) benchmarks verifies the significance of our method for simulating open-world, contact-rich tasks, paving the way for general-purpose robot world models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DreamDojo：从大规模人类视频中学习通用机器人世界模型</div>
<div class="mono" style="margin-top:8px">能够在不同环境中模拟行为结果将彻底改变通用智能体的大规模开发。然而，建模这些世界动态，尤其是在精细操作任务中，由于数据覆盖有限和动作标签稀缺，面临重大挑战。为此，我们提出了DreamDojo，一个基础世界模型，从44000小时的自拍式人类视频中学习多样化的交互和精细控制。我们的数据混合代表了迄今为止用于世界模型预训练的最大视频数据集，涵盖了广泛日常场景中的多种物体和技能。为了解决动作标签稀缺的问题，我们引入了连续潜在动作作为统一的代理动作，从而增强从无标签视频中转移交互知识的能力。在小型目标机器人数据上进行微调后，DreamDojo展现出对物理的强理解和精确的动作可控性。我们还设计了一个蒸馏流程，将DreamDojo加速至实时速度10.81 FPS，并进一步提升上下文一致性。我们的工作使基于生成式世界模型的多个重要应用成为可能，包括实时远程操作、策略评估和基于模型的规划。在多个具有挑战性的分布外（OOD）基准上的系统性评估验证了我们方法在模拟开放世界、接触密集任务中的重要性，为通用机器人世界模型铺平了道路。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Being able to simulate the outcomes of actions in varied environments will revolutionize the development of generalist agents at scale.</div>
</details>
</div>
<div class="card">
<div class="title">Agentic Uncertainty Reveals Agentic Overconfidence</div>
<div class="meta-line">Authors: Jean Kaddour, Srijan Patel, Gbètondji Dovonon, Leo Richter, Pasquale Minervini, Matt J. Kusner</div>
<div class="meta-line">First: 2026-02-06T18:49:35+00:00 · Latest: 2026-02-06T18:49:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06948v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06948v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Can AI agents predict whether they will succeed at a task? We study agentic uncertainty by eliciting success probability estimates before, during, and after task execution. All results exhibit agentic overconfidence: some agents that succeed only 22% of the time predict 77% success. Counterintuitively, pre-execution assessment with strictly less information tends to yield better discrimination than standard post-execution review, though differences are not always significant. Adversarial prompting reframing assessment as bug-finding achieves the best calibration.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>代理不确定性揭示代理过度自信</div>
<div class="mono" style="margin-top:8px">AI代理能否预测自己是否能成功完成任务？我们通过在任务执行前、执行中和执行后获取成功概率估计来研究代理不确定性。所有结果都显示出代理过度自信：有些仅22%成功率的代理却预测77%的成功率。反直觉的是，执行前的评估虽然信息更少，但往往比标准的执行后回顾具有更好的区分度，尽管差异并不总是显著。对抗性提示将评估重新框架为寻找错误，从而实现最佳校准。</div>
</details>
</div>
<div class="card">
<div class="title">Optimal Derivative Feedback Control for an Active Magnetic Levitation System: An Experimental Study on Data-Driven Approaches</div>
<div class="meta-line">Authors: Saber Omidi, Rene Akupan Ebunle, Se Young Yoon</div>
<div class="meta-line">First: 2026-02-06T18:42:01+00:00 · Latest: 2026-02-06T18:42:01+00:00</div>
<div class="meta-line">Comments: 10 pages, 9 figures. Preprint; manuscript under journal review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06944v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06944v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents the design and implementation of data-driven optimal derivative feedback controllers for an active magnetic levitation system. A direct, model-free control design method based on the reinforcement learning framework is compared with an indirect optimal control design derived from a numerically identified mathematical model of the system. For the direct model-free approach, a policy iteration procedure is proposed, which adds an iteration layer called the epoch loop to gather multiple sets of process data, providing a more diverse dataset and helping reduce learning biases. This direct control design method is evaluated against a comparable optimal control solution designed from a plant model obtained through the combined Dynamic Mode Decomposition with Control (DMDc) and Prediction Error Minimization (PEM) system identification. Results show that while both controllers can stabilize and improve the performance of the magnetic levitation system when compared to controllers designed from a nominal model, the direct model-free approach consistently outperforms the indirect solution when multiple epochs are allowed. The iterative refinement of the optimal control law over the epoch loop provides the direct approach a clear advantage over the indirect method, which relies on a single set of system data to determine the identified model and control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>主动磁悬浮系统的最优导数反馈控制：基于数据驱动方法的实验研究</div>
<div class="mono" style="margin-top:8px">本文提出并实现了针对主动磁悬浮系统的数据驱动最优导数反馈控制器的设计与实施。一种基于强化学习框架的直接、无模型控制设计方法与一种间接最优控制设计方法进行比较，该间接方法来源于系统数值识别得到的数学模型。对于直接无模型方法，提出了一种策略迭代程序，通过在数据收集过程中加入一个称为“周期循环”的迭代层，获取多组过程数据，从而提供更加多样化的数据集，有助于减少学习偏差。该直接控制设计方法与一种基于通过结合动态模态分解与控制（DMDc）和预测误差最小化（PEM）系统辨识获得的被控对象模型设计的最优控制方案进行对比。结果表明，虽然两种控制器在与基于名义模型设计的控制器相比时都能稳定并提升磁悬浮系统的性能，但当允许多个周期时，直接无模型方法始终优于间接方法。在周期循环中对最优控制律的迭代优化使直接方法相较于依赖单一数据集确定识别模型和控制的间接方法具有明显优势。</div>
</details>
</div>
<div class="card">
<div class="title">Optimal Turkish Subword Strategies at Scale: Systematic Evaluation of Data, Vocabulary, Morphology Interplay</div>
<div class="meta-line">Authors: Duygu Altinok</div>
<div class="meta-line">First: 2026-02-06T18:41:14+00:00 · Latest: 2026-02-06T18:41:14+00:00</div>
<div class="meta-line">Comments: Submitted to Cambridge NLP journal, all rights belong to them</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06942v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06942v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tokenization is a pivotal design choice for neural language modeling in morphologically rich languages (MRLs) such as Turkish, where productive agglutination challenges both vocabulary efficiency and morphological fidelity. Prior studies have explored tokenizer families and vocabulary sizes but typically (i) vary vocabulary without systematically controlling the tokenizer&#x27;s training corpus, (ii) provide limited intrinsic diagnostics, and (iii) evaluate a narrow slice of downstream tasks. We present the first comprehensive, principled study of Turkish subword tokenization; a &quot;subwords manifest&quot;, that jointly varies vocabulary size and tokenizer training corpus size (data and vocabulary coupling), compares multiple tokenizer families under matched parameter budgets (WordPiece, morphology level, and character baselines), and evaluates across semantic (NLI, STS, sentiment analysis, NER), syntactic (POS, dependency parsing), and morphology-sensitive probes. To explain why tokenizers succeed or fail, we introduce a morphology-aware diagnostic toolkit that goes beyond coarse aggregates to boundary-level micro/macro F1, decoupled lemma atomicity vs. surface boundary hits, over/under-segmentation indices, character/word edit distances (CER/WER), continuation rates, and affix-type coverage and token-level atomicity. Our contributions are fourfold: (i) a systematic investigation of the vocabulary-corpus-success triad; (ii) a unified, morphology-aware evaluation framework linking intrinsic diagnostics to extrinsic outcomes; (iii) controlled comparisons identifying when character-level and morphology-level tokenization pay off; and (iv) an open-source release of evaluation code, tokenizer pipelines, and models. As the first work of its kind, this &quot;subwords manifest&quot; delivers actionable guidance for building effective tokenizers in MRLs and establishes a reproducible foundation for future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大规模下的最优土耳其语子词策略：数据、词汇量与形态学相互作用的系统性评估</div>
<div class="mono" style="margin-top:8px">分词是神经语言建模中一个关键的设计选择，特别是在像土耳其语这样的形态丰富的语言（MRLs）中，其活跃的黏着构词现象对词汇效率和形态学准确性提出了挑战。以往的研究探讨了分词器家族和词汇量大小，但通常存在以下问题：(i) 在不系统控制分词器训练语料库的情况下变化词汇量；(ii) 提供有限的内在诊断指标；(iii) 仅评估了下游任务的一个狭窄子集。我们提出了首个全面且基于原则的土耳其语子词分词研究，即一份「子词宣言」，该研究同时变化词汇量大小和分词器训练语料库大小（数据与词汇量耦合），在匹配的参数预算下比较了多种分词器家族（WordPiece、形态层级和字符基线），并在语义（自然语言推理、语义相似度、情感分析、命名实体识别）、句法（词性标注、依存句法分析）以及形态学敏感的探针上进行评估。为了解释为何某些分词器成功或失败，我们引入了一种形态学感知的诊断工具包，超越了粗粒度的聚合指标，包括边界级别的微/宏F1值、解耦的词根原子性与表层边界命中率、过分割/欠分割指数、字符/词编辑距离（CER/WER）、延续率以及词缀类型覆盖和词级原子性。我们的贡献包括四个方面：(i) 对词汇量-语料库-成功三元组的系统性研究；(ii) 一种统一的、形态学感知的评估框架，将内在诊断指标与外在结果联系起来；(iii) 受控比较，识别字符级和形态级分词在何时更具优势；(iv) 评估代码、分词器流程和模型的开源发布。作为首篇此类研究，这份「子词宣言」为构建有效的MRLs分词器提供了可操作的指导，并为未来研究奠定了可复现的基础。</div>
</details>
</div>
<div class="card">
<div class="title">Endogenous Resistance to Activation Steering in Language Models</div>
<div class="meta-line">Authors: Alex McKenzie, Keenan Pepper, Stijn Servaes, Martin Leitgab, Murat Cubuktepe, Mike Vaiana, Diogo de Lucena, Judd Rosenblatt, Michael S. A. Graziano</div>
<div class="meta-line">First: 2026-02-06T18:41:12+00:00 · Latest: 2026-02-06T18:41:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06941v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06941v1">PDF</a> · <a href="http://github.com/agencyenterprise/endogenous-steering-resistance">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models can resist task-misaligned activation steering during inference, sometimes recovering mid-generation to produce improved responses even when steering remains active. We term this Endogenous Steering Resistance (ESR). Using sparse autoencoder (SAE) latents to steer model activations, we find that Llama-3.3-70B shows substantial ESR, while smaller models from the Llama-3 and Gemma-2 families exhibit the phenomenon less frequently. We identify 26 SAE latents that activate differentially during off-topic content and are causally linked to ESR in Llama-3.3-70B. Zero-ablating these latents reduces the multi-attempt rate by 25%, providing causal evidence for dedicated internal consistency-checking circuits. We demonstrate that ESR can be deliberately enhanced through both prompting and training: meta-prompts instructing the model to self-monitor increase the multi-attempt rate by 4x for Llama-3.3-70B, and fine-tuning on self-correction examples successfully induces ESR-like behavior in smaller models. These findings have dual implications: ESR could protect against adversarial manipulation but might also interfere with beneficial safety interventions that rely on activation steering. Understanding and controlling these resistance mechanisms is important for developing transparent and controllable AI systems. Code is available at github.com/agencyenterprise/endogenous-steering-resistance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言模型对激活引导的内生性抵抗</div>
<div class="mono" style="margin-top:8px">大型语言模型可以在推理过程中抵抗任务不匹配的激活引导，有时即使引导仍然有效，也能在生成过程中恢复并产生改进的响应。我们将这种现象称为内生性激活引导抵抗（Endogenous Steering Resistance, ESR）。通过使用稀疏自动编码器（SAE）的潜在变量来引导模型激活，我们发现Llama-3.3-70B表现出显著的ESR，而Llama-3和Gemma-2系列的小型模型则较少出现该现象。我们识别出26个在离题内容中激活差异显著的SAE潜在变量，并发现它们与Llama-3.3-70B的ESR存在因果联系。将这些潜在变量零化后，多尝试率降低了25%，提供了专门的内部一致性检查电路的因果证据。我们还证明，ESR可以通过提示和训练有意增强：指导模型进行自我监控的元提示使Llama-3.3-70B的多尝试率提高了4倍，而使用自我纠正示例进行微调则成功在小型模型中诱导出类似ESR的行为。这些发现具有双重意义：ESR可能保护模型免受对抗性操控，但也可能干扰依赖激活引导的有益安全干预。理解并控制这些抵抗机制对于开发透明可控的AI系统至关重要。代码可在github.com/agencyenterprise/endogenous-steering-resistance获取。</div>
</details>
</div>
<div class="card">
<div class="title">From Core to Detail: Unsupervised Disentanglement with Entropy-Ordered Flows</div>
<div class="meta-line">Authors: Daniel Galperin, Ullrich Köthe</div>
<div class="meta-line">First: 2026-02-06T18:41:03+00:00 · Latest: 2026-02-06T18:41:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06940v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06940v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Learning unsupervised representations that are both semantically meaningful and stable across runs remains a central challenge in modern representation learning. We introduce entropy-ordered flows (EOFlows), a normalizing-flow framework that orders latent dimensions by their explained entropy, analogously to PCA&#x27;s explained variance. This ordering enables adaptive injective flows: after training, one may retain only the top C latent variables to form a compact core representation while the remaining variables capture fine-grained detail and noise, with C chosen flexibly at inference time rather than fixed during training. EOFlows build on insights from Independent Mechanism Analysis, Principal Component Flows and Manifold Entropic Metrics. We combine likelihood-based training with local Jacobian regularization and noise augmentation into a method that scales well to high-dimensional data such as images. Experiments on the CelebA dataset show that our method uncovers a rich set of semantically interpretable features, allowing for high compression and strong denoising.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从核心到细节：使用熵序流进行无监督解耦</div>
<div class="mono" style="margin-top:8px">学习既具有语义意义又在多次运行中稳定的无监督表示仍然是现代表示学习中的核心挑战。我们引入了熵序流（EOFlows），这是一种归一化流框架，通过其解释的熵对潜在维度进行排序，类似于PCA的解释方差。这种排序使得可以使用自适应单射流：在训练后，可以仅保留前C个潜在变量以形成紧凑的核心表示，而其余变量则捕捉细粒度的细节和噪声，C在推理时灵活选择，而非在训练时固定。EOFlows借鉴了独立机制分析、主成分流和流形熵度量的见解。我们将基于似然的训练与局部雅可比正则化以及噪声增强相结合，形成一种适用于高维数据（如图像）的方法。在CelebA数据集上的实验表明，我们的方法揭示了一组丰富的语义可解释特征，从而实现了高压缩率和强大的去噪能力。</div>
</details>
</div>
<div class="card">
<div class="title">code_transformed: The Influence of Large Language Models on Code</div>
<div class="meta-line">Authors: Yuliang Xu, Siming Huang, Mingmeng Geng, Yao Wan, Xuanhua Shi, Dongping Chen</div>
<div class="meta-line">First: 2025-06-13T17:59:39+00:00 · Latest: 2026-02-06T18:40:02+00:00</div>
<div class="meta-line">Comments: EACL 2026 Findings</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.12014v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.12014v2">PDF</a> · <a href="https://github.com/ignorancex/LLM_code">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coding remains one of the most fundamental modes of interaction between humans and machines. With the rapid advancement of Large Language Models (LLMs), code generation capabilities have begun to significantly reshape programming practices. This development prompts a central question: Have LLMs transformed code style, and how can such transformation be characterized? In this paper, we present a pioneering study that investigates the impact of LLMs on code style, with a focus on naming conventions, complexity, maintainability, and similarity. By analyzing code from over 20,000 GitHub repositories linked to arXiv papers published between 2020 and 2025, we identify measurable trends in the evolution of coding style that align with characteristics of LLM-generated code. For instance, the proportion of snake_case function names in Python code increased from 40.7% in Q1 2023 to 49.8% in Q3 2025. Furthermore, we investigate how LLMs approach algorithmic problems by examining their reasoning processes. Our experimental results may provide the first large-scale empirical evidence that LLMs affect real-world programming style. We release all the experimental dataset and source code at: https://github.com/ignorancex/LLM_code</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>code_transformed: 大语言模型对代码的影响</div>
<div class="mono" style="margin-top:8px">编程仍然是人与机器之间最基本的交互方式之一。随着大语言模型（LLMs）的快速发展，代码生成能力已经开始显著改变编程实践。这一发展引发了一个核心问题：大语言模型是否改变了代码风格，这种改变如何体现？在本文中，我们提出了一项开创性的研究，探讨大语言模型对代码风格的影响，重点关注命名规范、复杂度、可维护性和相似性。通过分析2020年至2025年间arXiv论文关联的20000多个GitHub仓库中的代码，我们识别出与大语言模型生成代码特征相符的可衡量的代码风格演变趋势。例如，Python代码中使用snake_case命名的函数比例从2023年第一季度的40.7%上升到2025年第三季度的49.8%。此外，我们还通过分析其推理过程，研究大语言模型如何解决算法问题。我们的实验结果可能提供了首个大规模实证证据，表明大语言模型影响了现实世界的编程风格。我们将在以下链接发布所有实验数据集和源代码：https://github.com/ignorancex/LLM_code</div>
</details>
</div>
<div class="card">
<div class="title">Teaching Models to Teach Themselves: Reasoning at the Edge of Learnability</div>
<div class="meta-line">Authors: Shobhita Sundaram, John Quan, Ariel Kwiatkowski, Kartik Ahuja, Yann Ollivier, Julia Kempe</div>
<div class="meta-line">First: 2026-01-26T18:46:56+00:00 · Latest: 2026-02-06T18:38:32+00:00</div>
<div class="meta-line">Comments: Blog post: https://ssundaram21.github.io/soar/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.18778v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.18778v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://ssundaram21.github.io/soar/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Can a model learn to escape its own learning plateau? Reinforcement learning methods for finetuning large reasoning models stall on datasets with low initial success rates, and thus little training signal. We investigate a fundamental question: Can a pretrained LLM leverage latent knowledge to generate an automated curriculum for problems it cannot solve? To explore this, we design SOAR: A self-improvement framework designed to surface these pedagogical signals through meta-RL. A teacher copy of the model proposes synthetic problems for a student copy, and is rewarded with its improvement on a small subset of hard problems. Critically, SOAR grounds the curriculum in measured student progress rather than intrinsic proxy rewards. Our study on the hardest subsets of mathematical benchmarks (0/128 success) reveals three core findings. First, we show that it is possible to realize bi-level meta-RL that unlocks learning under sparse, binary rewards by sharpening a latent capacity of pretrained models to generate useful stepping stones. Second, grounded rewards outperform intrinsic reward schemes used in prior LLM self-play, reliably avoiding the instability and diversity collapse modes they typically exhibit. Third, analyzing the generated questions reveals that structural quality and well-posedness are more critical for learning progress than solution correctness. Our results suggest that the ability to generate useful stepping stones does not require the preexisting ability to actually solve the hard problems, paving a principled path to escape reasoning plateaus without additional curated data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>让模型学会自我教学：在可学习性边缘进行推理</div>
<div class="mono" style="margin-top:8px">一个模型能否学习以突破自身的学习瓶颈？用于微调大型推理模型的强化学习方法在初始成功率低的数据集上会停滞，因此缺乏训练信号。我们探讨了一个基本问题：一个预训练的LLM能否利用其潜在知识，为无法解决的问题生成自动化的教学大纲？为此，我们设计了SOAR：一个自我提升框架，通过元强化学习（meta-RL）来揭示这些教学信号。一个教师模型副本为学生模型副本提出合成问题，并因其在一小部分困难问题上的改进而获得奖励。关键的是，SOAR将教学大纲建立在可衡量的学生进展之上，而非内在的代理奖励。我们对数学基准中最难子集（0/128成功率）的研究揭示了三个核心发现。首先，我们展示了如何通过增强预训练模型生成有用学习步骤的潜在能力，实现双层元强化学习，从而在稀疏的二元奖励下解锁学习。其次，基于实际进展的奖励机制优于之前LLM自玩中使用的内在奖励方案，能够可靠地避免其通常表现出的不稳定性与多样性崩溃问题。第三，分析生成的问题表明，结构性质量和问题的恰当性比解题正确性对学习进展更为关键。我们的结果表明，生成有用学习步骤的能力并不需要预先具备解决困难问题的能力，从而为在无需额外人工整理数据的情况下突破推理瓶颈提供了一条原理性的路径。</div>
</details>
</div>
<div class="card">
<div class="title">Dataset Distillation as Pushforward Optimal Quantization</div>
<div class="meta-line">Authors: Hong Ye Tan, Emma Slade</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-01-13T20:41:52+00:00 · Latest: 2026-02-06T18:38:03+00:00</div>
<div class="meta-line">Comments: ICLR 2026, https://openreview.net/forum?id=FMSp8AUF3m</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.07681v3">Abs</a> · <a href="https://arxiv.org/pdf/2501.07681v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dataset distillation aims to find a synthetic training set such that training on the synthetic data achieves similar performance to training on real data, with orders of magnitude less computational requirements. Existing methods can be broadly categorized as either bi-level optimization problems that have neural network training heuristics as the lower level problem, or disentangled methods that bypass the bi-level optimization by matching distributions of data. The latter method has the major advantages of speed and scalability in terms of size of both training and distilled datasets. We demonstrate that when equipped with an encoder-decoder structure, the empirically successful disentangled methods can be reformulated as an optimal quantization problem, where a finite set of points is found to approximate the underlying probability measure by minimizing the expected projection distance. In particular, we link existing disentangled dataset distillation methods to the classical optimal quantization and Wasserstein barycenter problems, demonstrating consistency of distilled datasets for diffusion-based generative priors. We propose Dataset Distillation by Optimal Quantization, based on clustering in a latent space. Compared to the previous SOTA method D\textsuperscript{4}M, we achieve better performance and inter-model generalization on the ImageNet-1K dataset with trivial additional computation, and SOTA performance in higher image-per-class settings. Using the distilled noise initializations in a stronger diffusion transformer model, we obtain SOTA distillation performance on ImageNet-1K and its subsets, outperforming diffusion guidance methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>数据集蒸馏作为前推最优量化</div>
<div class="mono" style="margin-top:8px">数据集蒸馏旨在找到一个合成训练集，使得在合成数据上训练可以达到与在真实数据上训练相似的性能，同时计算需求减少几个数量级。现有方法大致可分为两类：一类是双层优化问题，其中神经网络训练启发式方法作为下层问题；另一类是解耦方法，通过匹配数据分布绕过双层优化。后者在训练集和蒸馏数据集的规模方面具有速度和可扩展性的主要优势。我们证明，当配备编码器-解码器结构时，经验上成功的解耦方法可以重新表述为一个最优量化问题，即通过最小化期望投影距离，找到一个有限点集来近似底层概率测度。特别地，我们将现有的解耦数据集蒸馏方法与经典的最优量化和Wasserstein均值问题联系起来，展示了基于扩散生成先验的数据集蒸馏的一致性。我们提出了一种基于潜在空间聚类的最优量化数据集蒸馏方法。与之前的SOTA方法D\textsuperscript{4}M相比，我们在ImageNet-1K数据集上实现了更好的性能和跨模型泛化能力，且仅需微不足道的额外计算。在更高图像每类设置下，我们还实现了SOTA性能。使用蒸馏的噪声初始化在一个更强的扩散变换模型中，我们在ImageNet-1K及其子集上获得了SOTA的蒸馏性能，优于扩散引导方法。</div>
</details>
</div>
<div class="card">
<div class="title">Forecast Aware Deep Reinforcement Learning for Efficient Electricity Load Scheduling in Dairy Farms</div>
<div class="meta-line">Authors: Nawazish Ali, Rachael Shaw, Karl Mason</div>
<div class="meta-line">First: 2026-01-12T22:41:26+00:00 · Latest: 2026-02-06T18:36:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08052v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08052v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dairy farming is an energy intensive sector that relies heavily on grid electricity. With increasing renewable energy integration, sustainable energy management has become essential for reducing grid dependence and supporting the United Nations Sustainable Development Goal 7 on affordable and clean energy. However, the intermittent nature of renewables poses challenges in balancing supply and demand in real time. Intelligent load scheduling is therefore crucial to minimize operational costs while maintaining reliability. Reinforcement Learning has shown promise in improving energy efficiency and reducing costs. However, most RL-based scheduling methods assume complete knowledge of future prices or generation, which is unrealistic in dynamic environments. Moreover, standard PPO variants rely on fixed clipping or KL divergence thresholds, often leading to unstable training under variable tariffs. To address these challenges, this study proposes a Deep Reinforcement Learning framework for efficient load scheduling in dairy farms, focusing on battery storage and water heating under realistic operational constraints. The proposed Forecast Aware PPO incorporates short term forecasts of demand and renewable generation using hour of day and month based residual calibration, while the PID KL PPO variant employs a proportional integral derivative controller to regulate KL divergence for stable policy updates adaptively. Trained on real world dairy farm data, the method achieves up to 1% lower electricity cost than PPO, 4.8% than DQN, and 1.5% than SAC. For battery scheduling, PPO reduces grid imports by 13.1%, demonstrating scalability and effectiveness for sustainable energy management in modern dairy farming.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向电力负荷调度的预测感知深度强化学习在乳制品农场中的应用</div>
<div class="mono" style="margin-top:8px">乳制品养殖业是一个高能耗行业，严重依赖电网电力。随着可再生能源整合的增加，可持续能源管理已成为减少电网依赖、支持联合国可持续发展目标7（可负担且清洁能源）的关键。然而，可再生能源的间歇性给实时供需平衡带来了挑战。因此，智能负荷调度对于在保持可靠性的同时最小化运营成本至关重要。强化学习在提高能源效率和降低成本方面展现出潜力。然而，大多数基于强化学习的调度方法假设对未来电价或发电量有完全的了解，这在动态环境中并不现实。此外，标准的PPO变体依赖于固定的剪切或KL散度阈值，这在电价波动的情况下往往导致训练不稳定。为了解决这些问题，本研究提出了一种用于乳制品农场高效负荷调度的深度强化学习框架，重点考虑电池储能和水加热，并在现实操作约束下进行优化。所提出的预测感知PPO（Forecast Aware PPO）利用基于小时和月份的残差校准方法，结合短期需求和可再生能源发电的预测。而PID KL PPO变体则采用比例积分微分控制器来调节KL散度，以实现适应性的稳定策略更新。该方法在真实农场数据上进行训练，相比PPO降低了1%的电力成本，相比DQN降低了4.8%，相比SAC降低了1.5%。对于电池调度，PPO减少了电网电力输入13.1%，展示了其在现代乳制品养殖业可持续能源管理中的可扩展性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Cochain Perspectives on Temporal-Difference Signals for Learning Beyond Markov Dynamics</div>
<div class="meta-line">Authors: Zuyuan Zhang, Sizhe Tang, Tian Lan</div>
<div class="meta-line">First: 2026-02-06T18:35:41+00:00 · Latest: 2026-02-06T18:35:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06939v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06939v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Non-Markovian dynamics are commonly found in real-world environments due to long-range dependencies, partial observability, and memory effects. The Bellman equation that is the central pillar of Reinforcement learning (RL) becomes only approximately valid under Non-Markovian. Existing work often focus on practical algorithm designs and offer limited theoretical treatment to address key questions, such as what dynamics are indeed capturable by the Bellman framework and how to inspire new algorithm classes with optimal approximations. In this paper, we present a novel topological viewpoint on temporal-difference (TD) based RL. We show that TD errors can be viewed as 1-cochain in the topological space of state transitions, while Markov dynamics are then interpreted as topological integrability. This novel view enables us to obtain a Hodge-type decomposition of TD errors into an integrable component and a topological residual, through a Bellman-de Rham projection. We further propose HodgeFlow Policy Search (HFPS) by fitting a potential network to minimize the non-integrable projection residual in RL, achieving stability/sensitivity guarantees. In numerical evaluations, HFPS is shown to significantly improve RL performance under non-Markovian.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于超越马尔可夫动力学的时序差分信号的协链视角</div>
<div class="mono" style="margin-top:8px">非马尔可夫动力学由于长程依赖、部分可观测性和记忆效应，在现实环境中很常见。强化学习（RL）的核心支柱贝尔曼方程在非马尔可夫情况下仅近似有效。现有工作通常集中在实用算法设计上，对关键问题如贝尔曼框架究竟可以捕捉哪些动力学，以及如何通过最优近似启发新算法类别的理论处理有限。本文提出了一种基于拓扑的时序差分（TD）强化学习的新视角。我们表明，TD误差可以被视为状态转移拓扑空间中的1-协链，而马尔可夫动力学则被解释为拓扑可积性。这种新视角使我们能够通过贝尔曼-德拉姆投影，将TD误差分解为可积部分和拓扑残差的霍奇型分解。我们进一步提出霍奇流策略搜索（HFPS），通过拟合势网络以最小化非可积投影残差，从而在强化学习中实现稳定性/敏感性保证。数值评估表明，HFPS在非马尔可夫环境下显著提升了强化学习性能。</div>
</details>
</div>
<div class="card">
<div class="title">Reliable Mislabel Detection for Video Capsule Endoscopy Data</div>
<div class="meta-line">Authors: Julia Werner, Julius Oexle, Oliver Bause, Maxime Le Floch, Franz Brinkmann, Hannah Tolle, Jochen Hampe, Oliver Bringmann</div>
<div class="meta-line">First: 2026-02-06T18:33:12+00:00 · Latest: 2026-02-06T18:33:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06938v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06938v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The classification performance of deep neural networks relies strongly on access to large, accurately annotated datasets. In medical imaging, however, obtaining such datasets is particularly challenging since annotations must be provided by specialized physicians, which severely limits the pool of annotators. Furthermore, class boundaries can often be ambiguous or difficult to define which further complicates machine learning-based classification. In this paper, we want to address this problem and introduce a framework for mislabel detection in medical datasets. This is validated on the two largest, publicly available datasets for Video Capsule Endoscopy, an important imaging procedure for examining the gastrointestinal tract based on a video stream of lowresolution images. In addition, potentially mislabeled samples identified by our pipeline were reviewed and re-annotated by three experienced gastroenterologists. Our results show that the proposed framework successfully detects incorrectly labeled data and results in an improved anomaly detection performance after cleaning the datasets compared to current baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于视频胶囊内镜数据的可靠误标签检测</div>
<div class="mono" style="margin-top:8px">深度神经网络的分类性能高度依赖于大规模且准确标注的数据集。然而，在医学影像领域，获取此类数据集极具挑战性，因为标注必须由专业医生完成，这严重限制了标注者的数量。此外，类别边界往往模糊或难以定义，进一步增加了基于机器学习的分类难度。本文旨在解决这一问题，提出一个用于医学数据集误标签检测的框架。该框架在两个最大的公开可用视频胶囊内镜数据集上进行了验证，这些数据集是基于低分辨率图像视频流的重要胃肠检查影像技术。此外，我们通过管道识别出的潜在误标签样本，由三位经验丰富的胃肠病学家进行了复查和重新标注。实验结果表明，所提出的框架能够成功检测出错误标注的数据，并在清理数据集后，相较于当前基线方法，显著提升了异常检测性能。</div>
</details>
</div>
<div class="card">
<div class="title">Reciprocal Latent Fields for Precomputed Sound Propagation</div>
<div class="meta-line">Authors: Hugo Seuté, Pranai Vasudev, Etienne Richan, Louis-Xavier Buffoni</div>
<div class="meta-line">First: 2026-02-06T18:31:11+00:00 · Latest: 2026-02-06T18:31:11+00:00</div>
<div class="meta-line">Comments: Temporary pre-print, will be updated. In review at a conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06937v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06937v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Realistic sound propagation is essential for immersion in a virtual scene, yet physically accurate wave-based simulations remain computationally prohibitive for real-time applications. Wave coding methods address this limitation by precomputing and compressing impulse responses of a given scene into a set of scalar acoustic parameters, which can reach unmanageable sizes in large environments with many source-receiver pairs. We introduce Reciprocal Latent Fields (RLF), a memory-efficient framework for encoding and predicting these acoustic parameters. The RLF framework employs a volumetric grid of trainable latent embeddings decoded with a symmetric function, ensuring acoustic reciprocity. We study a variety of decoders and show that leveraging Riemannian metric learning leads to a better reproduction of acoustic phenomena in complex scenes. Experimental validation demonstrates that RLF maintains replication quality while reducing the memory footprint by several orders of magnitude. Furthermore, a MUSHRA-like subjective listening test indicates that sound rendered via RLF is perceptually indistinguishable from ground-truth simulations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于预计算声音传播的互易潜在场</div>
<div class="mono" style="margin-top:8px">逼真的声音传播对虚拟场景的沉浸感至关重要，但基于物理的波形模拟在实时应用中仍具有计算上的不可行性。波形编码方法通过预计算和压缩给定场景的脉冲响应，将其转化为一组标量声学参数，但在具有大量声源-接收器对的大环境中，这些参数可能会达到难以管理的规模。我们引入了互易潜在场（RLF），这是一种内存高效的框架，用于编码和预测这些声学参数。RLF框架采用可训练的潜在嵌入体的体积网格，并通过对称函数进行解码，从而确保声学互易性。我们研究了多种解码器，并表明利用黎曼度量学习可以更好地再现复杂场景中的声学现象。实验验证表明，RLF在保持再现质量的同时，将内存占用减少了几个数量级。此外，类似MUSHRA的主观听觉测试表明，通过RLF渲染的声音在感知上与真实模拟无法区分。</div>
</details>
</div>
<div class="card">
<div class="title">On the Efficiency of Sequentially Aware Recommender Systems: Cotten4Rec</div>
<div class="meta-line">Authors: Shankar Veludandi, Gulrukh Kurdistan, Uzma Mushtaque</div>
<div class="meta-line">First: 2026-02-06T18:30:23+00:00 · Latest: 2026-02-06T18:30:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06935v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06935v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sequential recommendation (SR) models predict a user&#x27;s next interaction by modeling their historical behaviors. Transformer-based SR methods, notably BERT4Rec, effectively capture these patterns but incur significant computational overhead due to extensive intermediate computations associated with Softmax-based attention. We propose Cotten4Rec, a novel SR model utilizing linear-time cosine similarity attention, implemented through a single optimized compute unified device architecture (CUDA) kernel. By minimizing intermediate buffers and kernel-launch overhead, Cotten4Rec substantially reduces resource usage compared to BERT4Rec and the linear-attention baseline, LinRec, especially for datasets with moderate sequence lengths and vocabulary sizes. Evaluations across three benchmark datasets confirm that Cotten4Rec achieves considerable reductions in memory and runtime with minimal compromise in recommendation accuracy, demonstrating Cotten4Rec&#x27;s viability as an efficient alternative for practical, large-scale sequential recommendation scenarios where computational resources are critical.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于顺序感知推荐系统效率的研究：Cotten4Rec</div>
<div class="mono" style="margin-top:8px">顺序推荐（SR）模型通过建模用户的历史行为来预测其下一步交互。基于Transformer的SR方法，如BERT4Rec，能够有效捕捉这些模式，但由于涉及基于Softmax的注意力机制的大量中间计算，导致显著的计算开销。我们提出了一种新的SR模型Cotten4Rec，该模型采用线性时间余弦相似度注意力机制，并通过一个优化的计算统一设备架构（CUDA）内核实现。通过减少中间缓冲区和内核启动开销，Cotten4Rec在资源使用方面相比BERT4Rec和线性注意力基线模型LinRec有显著提升，尤其是在序列长度和词汇量适中的数据集上。在三个基准数据集上的评估表明，Cotten4Rec在内存和运行时间方面有显著减少，同时仅对推荐准确性造成极小影响，证明了其作为高效顺序推荐系统替代方案的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">Implementing Grassroots Logic Programs with Multiagent Transition Systems and AI</div>
<div class="meta-line">Authors: Ehud Shapiro</div>
<div class="meta-line">First: 2026-02-06T18:30:11+00:00 · Latest: 2026-02-06T18:30:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06934v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06934v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Grassroots Logic Programs (GLP) is a concurrent logic programming language with variables partitioned into paired \emph{readers} and \emph{writers}, conjuring both linear logic and futures/promises: an assignment is produced at most once via the sole occurrence of a writer (promise) and consumed at most once via the sole occurrence of its paired reader (future), and may contain additional readers and/or writers, enabling the concise expression of rich multidirectional communication modalities.
  GLP was designed as a language for grassroots platforms -- distributed systems with multiple instances that can operate independently of each other and of any global resource, and can coalesce into ever larger instances -- with its target architecture being smartphones communicating peer-to-peer. The operational semantics of Concurrent (single-agent) GLP and of multiagent GLP (maGLP) were defined via transition systems/multiagent transition systems, respectively.
  Here, we describe the mathematics developed to facilitate the workstation- and smartphone-based implementations of GLP by AI in Dart. We developed dGLP -- implementation-ready deterministic operational semantics for single-agent GLP -- and proved it correct with respect to the Concurrent GLP operational semantics; dGLP was used by AI as a formal spec, from which it developed a workstation-based implementation of GLP. We developed madGLP -- an implementation-ready multiagent operational semantics for maGLP -- and proved it correct with respect to the maGLP operational semantics; madGLP is deterministic at the agent level (not at the system level due to communication asynchrony), and is being used by AI as a formal spec from which it develops a smartphone-based implementation of maGLP.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用多智能体转换系统和人工智能实现基层逻辑程序</div>
<div class="mono" style="margin-top:8px">基层逻辑程序（GLP）是一种并发逻辑编程语言，其变量被划分为配对的\emph{读者}和\emph{写入器}，结合了线性逻辑和未来/承诺机制：一个赋值最多由一个写入器（承诺）产生，并最多由其配对的读者（未来）消耗，还可以包含额外的读者和/或写入器，从而能够简洁地表达丰富的多向通信模式。
GLP被设计为一种用于基层平台的语言——这些平台是分布式系统，具有多个可以独立运行的实例，且可以合并为更大的实例，其目标架构是基于点对点通信的智能手机。并发（单智能体）GLP和多智能体GLP（maGLP）的操作语义分别通过转换系统和多智能体转换系统定义。
在此，我们描述了为在Dart中通过人工智能实现基于工作站和智能手机的GLP而开发的数学方法。我们开发了dGLP——一种适用于单智能体GLP的确定性操作语义，并证明其与并发GLP操作语义的正确性；dGLP被人工智能用作形式规范，从而开发出基于工作站的GLP实现。我们还开发了madGLP——一种适用于maGLP的确定性多智能体操作语义，并证明其与maGLP操作语义的正确性；madGLP在智能体层面是确定性的（由于通信异步性，系统层面并非如此），并且被人工智能用作形式规范，从而开发出基于智能手机的maGLP实现。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Grassroots Logic Programs (GLP) is a concurrent logic programming language with variables partitioned into paired \emph{readers} and \emph{writers}, conjuring both linear logic and futures/promises: an assignment is produced at most once via the sole occurrence of a writer (promise) and consumed at most once via the sole occurrence of its paired reader (future), and may contain additional readers and/or writers, enabling the concise expression of rich multidirectional communication modalities.</div>
</details>
</div>
<div class="card">
<div class="title">When RL Meets Adaptive Speculative Training: A Unified Training-Serving System</div>
<div class="meta-line">Authors: Junxiong Wang, Fengxiang Bie, Jisen Li, Zhongzhu Zhou, Zelei Shao, Yubo Wang, Yinghui Liu, Qingyang Wu, Avner May, Sri Yanamandra, Yineng Zhang, Ce Zhang, Tri Dao, Percy Liang, Ben Athiwaratkun, Shuaiwen Leon Song, Chenfeng Xu, Xiaoxia Wu</div>
<div class="meta-line">First: 2026-02-06T18:28:54+00:00 · Latest: 2026-02-06T18:28:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06932v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06932v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Speculative decoding can significantly accelerate LLM serving, yet most deployments today disentangle speculator training from serving, treating speculator training as a standalone offline modeling problem. We show that this decoupled formulation introduces substantial deployment and adaptation lag: (1) high time-to-serve, since a speculator must be trained offline for a considerable period before deployment; (2) delayed utility feedback, since the true end-to-end decoding speedup is only known after training and cannot be inferred reliably from acceptance rate alone due to model-architecture and system-level overheads; and (3) domain-drift degradation, as the target model is repurposed to new domains and the speculator becomes stale and less effective.
  To address these issues, we present Aurora, a unified training-serving system that closes the loop by continuously learning a speculator directly from live inference traces. Aurora reframes online speculator learning as an asynchronous reinforcement-learning problem: accepted tokens provide positive feedback, while rejected speculator proposals provide implicit negative feedback that we exploit to improve sample efficiency. Our design integrates an SGLang-based inference server with an asynchronous training server, enabling hot-swapped speculator updates without service interruption. Crucially, Aurora supports day-0 deployment: a speculator can be served immediately and rapidly adapted to live traffic, improving system performance while providing immediate utility feedback. Across experiments, Aurora achieves a 1.5x day-0 speedup on recently released frontier models (e.g., MiniMax M2.1 229B and Qwen3-Coder-Next 80B). Aurora also adapts effectively to distribution shifts in user traffic, delivering an additional 1.25x speedup over a well-trained but static speculator on widely used models (e.g., Qwen3 and Llama3).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当强化学习遇见自适应推测训练：一种统一的训练-服务系统</div>
<div class="mono" style="margin-top:8px">推测解码可以显著加速大语言模型（LLM）的服务，但目前大多数部署将推测训练与服务分离，将其视为一个独立的离线建模问题。我们表明，这种解耦的设定会导致部署和适应的显著延迟：(1) 服务延迟高，因为推测模型必须在部署前进行长时间的离线训练；(2) 实用性反馈延迟，因为真正的端到端解码加速效果只有在训练完成后才能得知，无法仅通过接受率可靠推断，由于模型架构和系统级开销；(3) 领域漂移退化，当目标模型被重新用于新领域时，推测模型会变得过时且效果下降。
为了解决这些问题，我们提出了Aurora，这是一种统一的训练-服务系统，通过持续从实时推理轨迹中学习推测模型来闭合反馈环。Aurora将在线推测学习重新定义为一个异步强化学习问题：接受的标记提供正反馈，而被拒绝的推测提案则提供隐式的负反馈，我们利用这些反馈来提高样本效率。我们的设计将基于SGLang的推理服务器与异步训练服务器集成，从而实现服务中断的情况下热替换推测模型更新。关键的是，Aurora支持零日部署：推测模型可以立即提供服务，并迅速适应实时流量，从而在提升系统性能的同时提供即时的实用性反馈。在实验中，Aurora在最近发布的前沿模型（如MiniMax M2.1 229B和Qwen3-Coder-Next 80B）上实现了1.5倍的零日加速。此外，Aurora还能有效适应用户流量分布的变化，在广泛使用的模型（如Qwen3和Llama3）上，相较于训练良好但静态的推测模型，额外实现了1.25倍的加速。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Speculative decoding can significantly accelerate LLM serving, yet most deployments today disentangle speculator training from serving, treating speculator training as a standalone offline modeling problem.</div>
</details>
</div>
<div class="card">
<div class="title">RMT-KD: Random Matrix Theoretic Causal Knowledge Distillation</div>
<div class="meta-line">Authors: Davide Ettori, Nastaran Darabi, Sureshkumar Senthilkumar, Amit Ranjan Trivedi</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-09-19T07:53:55+00:00 · Latest: 2026-02-06T18:25:36+00:00</div>
<div class="meta-line">Comments: 5 pages, submitted to ICASSP 2026, September 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.15724v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.15724v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large deep learning models such as BERT and ResNet achieve state-of-the-art performance but are costly to deploy at the edge due to their size and compute demands. We present RMT-KD, a compression method that leverages Random Matrix Theory (RMT) for knowledge distillation to iteratively reduce network size. Instead of pruning or heuristic rank selection, RMT-KD preserves only informative directions identified via the spectral properties of hidden representations. RMT-based causal reduction is applied layer by layer with self-distillation to maintain stability and accuracy. On GLUE and CIFAR-10, RMT-KD achieves up to 80% parameter reduction with only 2% accuracy loss, delivering 2.8x faster inference and nearly halved power consumption. These results establish RMT-KD as a mathematically grounded approach to network distillation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RMT-KD：基于随机矩阵理论的因果知识蒸馏方法</div>
<div class="mono" style="margin-top:8px">大型深度学习模型如BERT和ResNet在性能上处于领先水平，但由于其规模和计算需求，在边缘设备上的部署成本较高。我们提出了RMT-KD，一种利用随机矩阵理论（RMT）进行知识蒸馏的压缩方法，通过迭代方式减少网络规模。与剪枝或启发式排名选择不同，RMT-KD仅保留通过隐藏表示的谱特性识别出的信息性方向。基于RMT的因果降维方法结合自蒸馏逐层应用，以保持稳定性和准确性。在GLUE和CIFAR-10数据集上，RMT-KD实现了高达80%的参数减少，仅损失2%的精度，带来了2.8倍更快的推理速度和近半的功耗降低。这些结果确立了RMT-KD作为一种基于数学理论的网络蒸馏方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large deep learning models such as BERT and ResNet achieve state-of-the-art performance but are costly to deploy at the edge due to their size and compute demands.</div>
</details>
</div>
<div class="card">
<div class="title">Continuous-time reinforcement learning: ellipticity enables model-free value function approximation</div>
<div class="meta-line">Authors: Wenlong Mou</div>
<div class="meta-line">First: 2026-02-06T18:25:33+00:00 · Latest: 2026-02-06T18:25:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06930v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06930v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study off-policy reinforcement learning for controlling continuous-time Markov diffusion processes with discrete-time observations and actions. We consider model-free algorithms with function approximation that learn value and advantage functions directly from data, without unrealistic structural assumptions on the dynamics.
  Leveraging the ellipticity of the diffusions, we establish a new class of Hilbert-space positive definiteness and boundedness properties for the Bellman operators. Based on these properties, we propose the Sobolev-prox fitted $q$-learning algorithm, which learns value and advantage functions by iteratively solving least-squares regression problems. We derive oracle inequalities for the estimation error, governed by (i) the best approximation error of the function classes, (ii) their localized complexity, (iii) exponentially decaying optimization error, and (iv) numerical discretization error. These results identify ellipticity as a key structural property that renders reinforcement learning with function approximation for Markov diffusions no harder than supervised learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>连续时间强化学习：椭圆性使无模型价值函数逼近成为可能</div>
<div class="mono" style="margin-top:8px">我们研究用于控制连续时间马尔可夫扩散过程的无策略强化学习，这些过程具有离散时间观测和动作。我们考虑无模型算法，通过函数逼近直接从数据中学习价值函数和优势函数，而不需要对动态结构做出不现实的假设。
  利用扩散的椭圆性，我们建立了一类新的希尔伯特空间正定性和有界性性质，用于贝尔曼算子。基于这些性质，我们提出了Sobolev- prox fitted $q$-学习算法，该算法通过迭代求解最小二乘回归问题来学习价值函数和优势函数。我们推导了估计误差的Oracle不等式，其误差由以下四个因素控制：(i) 函数类的最佳逼近误差，(ii) 其局部化复杂度，(iii) 指数衰减的优化误差，(iv) 数值离散化误差。这些结果表明，椭圆性是使马尔可夫扩散过程的无模型强化学习与监督学习一样容易的关键结构属性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We study off-policy reinforcement learning for controlling continuous-time Markov diffusion processes with discrete-time observations and actions.</div>
</details>
</div>
<div class="card">
<div class="title">EigenTrack: Spectral Activation Feature Tracking for Hallucination and Out-of-Distribution Detection in LLMs and VLMs</div>
<div class="meta-line">Authors: Davide Ettori, Nastaran Darabi, Sina Tayebati, Ranganath Krishnan, Mahesh Subedar, Omesh Tickoo, Amit Ranjan Trivedi</div>
<div class="meta-line">Venue: ICASSP 2026</div>
<div class="meta-line">First: 2025-09-19T08:05:28+00:00 · Latest: 2026-02-06T18:25:25+00:00</div>
<div class="meta-line">Comments: 5 pages, submitted to ICASSP 2026, September 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.15735v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.15735v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) offer broad utility but remain prone to hallucination and out-of-distribution (OOD) errors. We propose EigenTrack, an interpretable real-time detector that uses the spectral geometry of hidden activations, a compact global signature of model dynamics. By streaming covariance-spectrum statistics such as entropy, eigenvalue gaps, and KL divergence from random baselines into a lightweight recurrent classifier, EigenTrack tracks temporal shifts in representation structure that signal hallucination and OOD drift before surface errors appear. Unlike black- and grey-box methods, it needs only a single forward pass without resampling. Unlike existing white-box detectors, it preserves temporal context, aggregates global signals, and offers interpretable accuracy-latency trade-offs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EigenTrack：用于LLMs和VLMs中幻觉和分布外检测的谱激活特征追踪</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）具有广泛的应用价值，但仍然容易产生幻觉和分布外（OOD）错误。我们提出EigenTrack，这是一种可解释的实时检测器，利用隐藏激活的谱几何特性，作为模型动态的紧凑全局签名。通过将熵、特征值间隔和随机基线的KL散度等协方差谱统计信息流式传输到一个轻量级的循环分类器中，EigenTrack追踪表示结构随时间的变化，这些变化在表面错误出现之前即可指示幻觉和分布外漂移。与黑盒和灰盒方法不同，它仅需一次前向传播而无需重采样。与现有白盒检测器不同，它保留了时间上下文，聚合了全局信号，并提供了可解释的准确率-延迟权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) offer broad utility but remain prone to hallucination and out-of-distribution (OOD) errors.</div>
</details>
</div>
<div class="card">
<div class="title">WAFT: Warping-Alone Field Transforms for Optical Flow</div>
<div class="meta-line">Authors: Yihan Wang, Jia Deng</div>
<div class="meta-line">First: 2025-06-26T17:47:59+00:00 · Latest: 2026-02-06T18:23:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.21526v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.21526v3">PDF</a> · <a href="https://github.com/princeton-vl/WAFT}{https://github.com/princeton-vl/WAFT">Code1</a> · <a href="https://github.com/princeton-vl/WAFT">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Warping-Alone Field Transforms (WAFT), a simple and effective method for optical flow. WAFT is similar to RAFT but replaces cost volume with high-resolution warping, achieving better accuracy with lower memory cost. This design challenges the conventional wisdom that constructing cost volumes is necessary for strong performance. WAFT is a simple and flexible meta-architecture with minimal inductive biases and reliance on custom designs. Compared with existing methods, WAFT ranks 1st on Spring, Sintel, and KITTI benchmarks, achieves the best zero-shot generalization on KITTI, while being 1.3-4.1x faster than existing methods that have competitive accuracy (e.g., 1.3x than Flowformer++, 4.1x than CCMR+). Code and model weights are available at \href{https://github.com/princeton-vl/WAFT}{https://github.com/princeton-vl/WAFT}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WAFT：用于光流的形变独立场变换</div>
<div class="mono" style="margin-top:8px">我们引入了Warping-Alone Field Transforms（WAFT），这是一种简单且有效的光流方法。WAFT类似于RAFT，但用高分辨率形变替代了代价体积，实现了更高的准确性且内存消耗更低。这种设计挑战了传统观念，即构建代价体积对于强性能是必要的。WAFT是一种简单且灵活的元架构，具有最小的归纳偏置和对定制设计的依赖。与现有方法相比，WAFT在Spring、Sintel和KITTI基准测试中排名第一，在KITTI上实现了最佳的零样本泛化能力，同时比具有竞争力准确性的现有方法（例如，比Flowformer++快1.3倍，比CCMR+快4.1倍）快1.3-4.1倍。代码和模型权重可在\href{https://github.com/princeton-vl/WAFT}{https://github.com/princeton-vl/WAFT}获取。</div>
</details>
</div>
<div class="card">
<div class="title">Constrained Group Relative Policy Optimization</div>
<div class="meta-line">Authors: Roger Girgis, Rodrigue de Schaetzen, Luke Rowe, Azalée Robitaille, Christopher Pal, Liam Paull</div>
<div class="meta-line">First: 2026-02-05T16:44:23+00:00 · Latest: 2026-02-06T18:22:59+00:00</div>
<div class="meta-line">Comments: 16 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.05863v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.05863v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While Group Relative Policy Optimization (GRPO) has emerged as a scalable framework for critic-free policy learning, extending it to settings with explicit behavioral constraints remains underexplored. We introduce Constrained GRPO, a Lagrangian-based extension of GRPO for constrained policy optimization. Constraints are specified via indicator cost functions, enabling direct optimization of violation rates through a Lagrangian relaxation. We show that a naive multi-component treatment in advantage estimation can break constrained learning: mismatched component-wise standard deviations distort the relative importance of the different objective terms, which in turn corrupts the Lagrangian signal and prevents meaningful constraint enforcement. We formally derive this effect to motivate our scalarized advantage construction that preserves the intended trade-off between reward and constraint terms. Experiments in a toy gridworld confirm the predicted optimization pathology and demonstrate that scalarizing advantages restores stable constraint control. In addition, we evaluate Constrained GRPO on robotics tasks, where it improves constraint satisfaction while increasing task success, establishing a simple and effective recipe for constrained policy optimization in embodied AI domains that increasingly rely on large multimodal foundation models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>带约束的组相对策略优化</div>
<div class="mono" style="margin-top:8px">尽管组相对策略优化（GRPO）已成为一种可扩展的无批评策略学习框架，但将其扩展到具有显式行为约束的场景仍处于探索阶段。我们引入了带约束的GRPO（Constrained GRPO），这是GRPO的一种基于拉格朗日乘数法的扩展，用于带约束的策略优化。约束通过指示成本函数指定，从而通过拉格朗日松弛直接优化违反率。我们表明，优势估计中的朴素多组件处理可能会破坏带约束的学习：组件间的标准差不匹配会扭曲不同目标项的相对重要性，进而破坏拉格朗日信号并阻碍有效的约束执行。我们正式推导了这一效应，以支持我们提出的标量优势构造方法，该方法保留了奖励项与约束项之间预期的权衡关系。在玩具网格世界中的实验验证了预测的优化病理现象，并展示了标量优势如何恢复稳定的约束控制。此外，我们在机器人任务上评估了带约束的GRPO，结果表明它在提高约束满足度的同时也提升了任务成功率，为日益依赖大型多模态基础模型的具身AI领域提供了一种简单而有效的带约束策略优化方法。</div>
</details>
</div>
<div class="card">
<div class="title">Harnessing the Unseen: The Hidden Influence of Intrinsic Knowledge in Long-Context Language Models</div>
<div class="meta-line">Authors: Yu Fu, Haz Sameen Shahgir, Hui Liu, Xianfeng Tang, Qi He, Yue Dong</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-04-11T02:06:58+00:00 · Latest: 2026-02-06T18:20:22+00:00</div>
<div class="meta-line">Comments: 17 pages,11figures (accepted to AAAI 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2504.08202v2">Abs</a> · <a href="https://arxiv.org/pdf/2504.08202v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in long-context language models (LCLMs), designed to handle extremely long contexts, primarily focus on utilizing external contextual information, often leaving the influence of language models&#x27; parametric knowledge underexplored. In this work, we firstly investigate how this parametric knowledge affects content generation and demonstrate that its impact becomes increasingly pronounced as context length extends. Furthermore, we show that the model&#x27;s ability to utilize parametric knowledge, which we call parametric recall ability, does not improve simultaneously with its ability to leverage contextual knowledge through extrinsic retrieval ability. Moreover, better extrinsic retrieval ability can interfere with the model&#x27;s parametric recall ability, limiting its full potential. To bridge this gap, we design a simple yet effective Hybrid Needle-in-a-Haystack test that evaluates models based on their capabilities across both abilities, rather than solely emphasizing extrinsic retrieval ability. Our experimental results reveal that Qwen-2.5 models significantly outperform Llama-3.1 models, demonstrating superior potential to combine various abilities. Moreover, even the more powerful Llama-3.1-70B-Instruct model fails to exhibit better performance, highlighting the importance of evaluating models from a dual-ability perspective.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用不可见的力量：长上下文语言模型中参数化知识的隐藏影响</div>
<div class="mono" style="margin-top:8px">近期在长上下文语言模型（LCLMs）方面的进展主要集中在利用外部上下文信息上，而对语言模型自身参数化知识的影响研究较少。在本工作中，我们首先探讨了这种参数化知识如何影响内容生成，并证明其影响随着上下文长度的增加而愈发显著。此外，我们还表明，模型利用参数化知识的能力（我们称之为参数回忆能力）并不随着其通过外部检索能力利用上下文知识的能力同步提升。更进一步，更好的外部检索能力可能会干扰模型的参数回忆能力，从而限制其全部潜力。为弥合这一差距，我们设计了一个简单而有效的混合“大海捞针”测试，该测试不仅评估模型的外部检索能力，还评估其参数回忆能力。实验结果表明，Qwen-2.5模型显著优于Llama-3.1模型，展示了其在结合多种能力方面的更大潜力。此外，即使更强大的Llama-3.1-70B-Instruct模型也未能表现出更优的性能，突显了从双能力角度评估模型的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Robustness Beyond Known Groups with Low-rank Adaptation</div>
<div class="meta-line">Authors: Abinitha Gourabathina, Hyewon Jeong, Teya Bergamaschi, Marzyeh Ghassemi, Collin Stultz</div>
<div class="meta-line">First: 2026-02-06T18:18:13+00:00 · Latest: 2026-02-06T18:18:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06924v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06924v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep learning models trained to optimize average accuracy often exhibit systematic failures on particular subpopulations. In real world settings, the subpopulations most affected by such disparities are frequently unlabeled or unknown, thereby motivating the development of methods that are performant on sensitive subgroups without being pre-specified. However, existing group-robust methods typically assume prior knowledge of relevant subgroups, using group annotations for training or model selection. We propose Low-rank Error Informed Adaptation (LEIA), a simple two-stage method that improves group robustness by identifying a low-dimensional subspace in the representation space where model errors concentrate. LEIA restricts adaptation to this error-informed subspace via a low-rank adjustment to the classifier logits, directly targeting latent failure modes without modifying the backbone or requiring group labels. Using five real-world datasets, we analyze group robustness under three settings: (1) truly no knowledge of subgroup relevance, (2) partial knowledge of subgroup relevance, and (3) full knowledge of subgroup relevance. Across all settings, LEIA consistently improves worst-group performance while remaining fast, parameter-efficient, and robust to hyperparameter choice.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越已知群体的鲁棒性：低秩自适应方法</div>
<div class="mono" style="margin-top:8px">深度学习模型在优化平均准确率时，常常在特定子群体上表现出系统性失败。在现实场景中，这些子群体往往未被标记或未知，因此需要开发能够在敏感子群体上表现良好但无需预先指定的方法。然而，现有群体鲁棒性方法通常假设已知相关子群体，利用群体标注进行训练或模型选择。我们提出低秩误差引导自适应（LEIA），这是一种简单的两阶段方法，通过在表示空间中识别出模型误差集中的低维子空间，从而提升群体鲁棒性。LEIA通过对分类器logits进行低秩调整，将自适应限制在该误差引导的子空间中，直接针对潜在的失败模式，而无需修改主干网络或依赖群体标签。我们使用五个真实世界数据集，在三种设置下分析群体鲁棒性：(1) 完全不了解子群体相关性，(2) 部分了解子群体相关性，(3) 全面了解子群体相关性。在所有设置中，LEIA都能持续提升最差群体的性能，同时保持快速、参数高效且对超参数选择不敏感。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260209_0404.html">20260209_0404</a>
<a href="archive/20260208_0353.html">20260208_0353</a>
<a href="archive/20260207_0410.html">20260207_0410</a>
<a href="archive/20260206_0412.html">20260206_0412</a>
<a href="archive/20260205_0417.html">20260205_0417</a>
<a href="archive/20260204_0421.html">20260204_0421</a>
<a href="archive/20260202_0402.html">20260202_0402</a>
<a href="archive/20260201_0354.html">20260201_0354</a>
<a href="archive/20260131_0408.html">20260131_0408</a>
<a href="archive/20260130_0406.html">20260130_0406</a>
<a href="archive/20260129_0407.html">20260129_0407</a>
<a href="archive/20260128_0407.html">20260128_0407</a>
<a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
