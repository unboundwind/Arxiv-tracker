<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-13 03:42</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260113_0342</div>
    <div class="row"><div class="card">
<div class="title">Manifold limit for the training of shallow graph convolutional neural networks</div>
<div class="meta-line">Authors: Johanna Tengler, Christoph Brune, José A. Iglesias</div>
<div class="meta-line">First: 2026-01-09T18:59:20+00:00 · Latest: 2026-01-09T18:59:20+00:00</div>
<div class="meta-line">Comments: 44 pages, 0 figures, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06025v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06025v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the discrete-to-continuum consistency of the training of shallow graph convolutional neural networks (GCNNs) on proximity graphs of sampled point clouds under a manifold assumption. Graph convolution is defined spectrally via the graph Laplacian, whose low-frequency spectrum approximates that of the Laplace-Beltrami operator of the underlying smooth manifold, and shallow GCNNs of possibly infinite width are linear functionals on the space of measures on the parameter space. From this functional-analytic perspective, graph signals are seen as spatial discretizations of functions on the manifold, which leads to a natural notion of training data consistent across graph resolutions. To enable convergence results, the continuum parameter space is chosen as a weakly compact product of unit balls, with Sobolev regularity imposed on the output weight and bias, but not on the convolutional parameter. The corresponding discrete parameter spaces inherit the corresponding spectral decay, and are additionally restricted by a frequency cutoff adapted to the informative spectral window of the graph Laplacians. Under these assumptions, we prove $Γ$-convergence of regularized empirical risk minimization functionals and corresponding convergence of their global minimizers, in the sense of weak convergence of the parameter measures and uniform convergence of the functions over compact sets. This provides a formalization of mesh and sample independence for the training of such networks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>浅层图卷积神经网络训练的流形极限</div>
<div class="mono" style="margin-top:8px">我们研究在流形假设下，对采样点云的邻近图上训练浅层图卷积神经网络（GCNNs）的离散到连续一致性。图卷积通过图拉普拉斯算子谱定义，其低频谱近似于底层光滑流形的拉普拉斯-贝尔特拉米算子的谱。可能无限宽的浅层GCNNs被看作是参数空间上测度空间的线性泛函。从这一泛函分析的角度来看，图信号被视为流形上函数的空间离散化，从而自然地引出了跨不同图分辨率的训练数据一致性概念。为了实现收敛性结果，我们将连续参数空间选择为单位球的弱紧致积，对输出权重和偏置施加索伯列夫正则性，但不对卷积参数施加。相应的离散参数空间继承了相应的谱衰减特性，并进一步受到与图拉普拉斯算子信息谱窗口相适应的频率截断限制。在这些假设下，我们证明了正则化经验风险最小化泛函的Γ收敛性及其全局极小值的相应收敛性，即参数测度的弱收敛和函数在紧集上的一致收敛。这为这类网络训练的网格和采样独立性提供了形式化表述。</div>
</details>
</div>
<div class="card">
<div class="title">AdaFuse: Adaptive Ensemble Decoding with Test-Time Scaling for LLMs</div>
<div class="meta-line">Authors: Chengming Cui, Tianxin Wei, Ziyi Chen, Ruizhong Qiu, Zhichen Zeng, Zhining Liu, Xuying Ning, Duo Zhou, Jingrui He</div>
<div class="meta-line">First: 2026-01-09T18:58:22+00:00 · Latest: 2026-01-09T18:58:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06022v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06022v1">PDF</a> · <a href="https://github.com/CCM0111/AdaFuse">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) exhibit complementary strengths arising from differences in pretraining data, model architectures, and decoding behaviors. Inference-time ensembling provides a practical way to combine these capabilities without retraining. However, existing ensemble approaches suffer from fundamental limitations. Most rely on fixed fusion granularity, which lacks the flexibility required for mid-generation adaptation and fails to adapt to different generation characteristics across tasks. To address these challenges, we propose AdaFuse, an adaptive ensemble decoding framework that dynamically selects semantically appropriate fusion units during generation. Rather than committing to a fixed granularity, AdaFuse adjusts fusion behavior on the fly based on the decoding context, with words serving as basic building blocks for alignment. To be specific, we introduce an uncertainty-based criterion to decide whether to apply ensembling at each decoding step. Under confident decoding states, the model continues generation directly. In less certain states, AdaFuse invokes a diversity-aware scaling strategy to explore alternative candidate continuations and inform ensemble decisions. This design establishes a synergistic interaction between adaptive ensembling and test-time scaling, where ensemble decisions guide targeted exploration, and the resulting diversity in turn strengthens ensemble quality. Experiments on open-domain question answering, arithmetic reasoning, and machine translation demonstrate that AdaFuse consistently outperforms strong ensemble baselines, achieving an average relative improvement of 6.88%. The code is available at https://github.com/CCM0111/AdaFuse.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AdaFuse: 一种用于大语言模型的自适应集成解码框架，具有测试时缩放功能</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）由于预训练数据、模型架构和解码行为的差异，展现出互补的优势。推理时集成提供了一种无需重新训练即可结合这些能力的实用方法。然而，现有的集成方法存在根本性的局限。大多数方法依赖固定的融合粒度，缺乏在生成过程中进行适应的灵活性，且无法适应不同任务的生成特性。为了解决这些问题，我们提出了AdaFuse，这是一种自适应集成解码框架，能够在生成过程中动态选择语义上合适的融合单元。与固定粒度不同，AdaFuse根据解码上下文实时调整融合行为，以词作为对齐的基本构建块。具体而言，我们引入了一种基于不确定性的标准，以决定在每个解码步骤是否应用集成。在自信的解码状态下，模型直接继续生成；在不确定的状态下，AdaFuse调用一种具有多样性感知的缩放策略，以探索替代的候选延续并指导集成决策。这种设计建立了自适应集成与测试时缩放之间的协同作用，其中集成决策引导有针对性的探索，而生成的多样性反过来增强了集成质量。在开放域问答、算术推理和机器翻译任务上的实验表明，AdaFuse在这些任务上持续优于强大的集成基线，平均相对提升达6.88%。代码可在https://github.com/CCM0111/AdaFuse获取。</div>
</details>
</div>
<div class="card">
<div class="title">Probing Cosmic Expansion and Early Universe with Einstein Telescope</div>
<div class="meta-line">Authors: Angelo Ricciardone, Mairi Sakellariadou, Archisman Ghosh, Alessandro Agapito, M. Celeste Artale, Michael Bacchi, Tessa Baker, Marco Baldi, Nicola Bartolo, Andrea Begnoni, Enis Belgacem, Marek Biesiada, Jose J. Blanco-Pillado, Tomasz Bulik, Marica Branchesi, Gianluca Calcagni, Giulia Capurri, Carmelita Carbone, Roberto Casadio, J. A. R. Cembranos, Andrea Cozzumbo, Ivan De Martino, Jose M. Diego, Emanuela Dimastrogiovanni, Guillem Domènech, Ulyana Dupletsa, Hannah Duval, Gabriele Franciolini, Andrea Giusti, Giuseppe Greco, Lavinia Heisenberg, Alexander C. Jenkins, Sumit Kumar, Gaetano Lambiase, Michele Maggiore, Michele Mancarella, Federico Marulli, Sabino Matarrese, Isabela Santiago de Matos, Michele Moresco, Riccardo Murgia, Ilia Musco, Gabriele Perna, Michele Punturo, Diego Rubiera-Garcia, Javier Rubio, Alexander Sevrin, Riccardo Sturani, Matteo Tagliazucchi, Nicola Tamanini, Alessandro Tronconi, Ville Vaskonen, Daniele Vernieri, Stoytcho Yazadjiev, Ivonne Zavala</div>
<div class="meta-line">First: 2026-01-09T18:52:42+00:00 · Latest: 2026-01-09T18:52:42+00:00</div>
<div class="meta-line">Comments: 4 pages. White paper submitted to the ESO Expanding Horizons Call on behalf of ET OSB Div2 - Cosmology</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06017v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06017v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Over the next two decades, gravitational-wave (GW) observations are expected to evolve from a discovery-driven endeavour into a precision tool for astrophysics, cosmology, and fundamental physics. Current second-generation ground-based detectors have established the existence of compact-binary mergers and enabled GW multi-messenger astronomy, but they remain limited in sensitivity, redshift reach, frequency coverage, and duty cycle. These limitations prevent them from addressing many fundamental open questions in cosmology. By the 2040s, wide-field electromagnetic surveys will have mapped the luminous Universe with unprecedented depth and accuracy. Nevertheless, key problems including the nature of dark matter, the physical origin of cosmic acceleration, the properties of gravity on cosmological scales, and the physical conditions of the earliest moments after the Big Bang will remain only partially constrained by electromagnetic observations alone. Progress on these fronts requires access to physical processes and epochs that do not emit light. Gravitational waves provide a unique and complementary observational channel: they propagate over cosmological distances largely unaffected by intervening matter, probe extreme astrophysical environments, and respond directly to the geometry of spacetime. In this context, next-generation GW observatories such as the Einstein Telescope (ET) will be transformative for European astronomy. Operating at sensitivities and frequencies beyond existing detectors, ET will observe binary black holes and neutron stars out to previously inaccessible redshifts, enable continuous high signal-to-noise monitoring of compact sources, and detect gravitational-wave backgrounds of astrophysical and cosmological origin. Together with space-based detectors, ET will play a central role in advancing our understanding of cosmic evolution and fundamental physics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用埃因斯坦望远镜探索宇宙膨胀与早期宇宙</div>
<div class="mono" style="margin-top:8px">在未来二十年中，引力波（GW）观测预计将从以发现为主的研究转变为天体物理、宇宙学和基本物理的高精度工具。目前的第二代地面引力波探测器已经确认了紧凑双星合并的存在，并使多信使引力波天文学成为可能，但它们在灵敏度、红移探测范围、频率覆盖和运行周期方面仍存在局限。这些限制阻碍了它们对许多宇宙学基本问题的解答。到2040年代，广域电磁观测将以前所未有的深度和精度绘制出可观测宇宙的地图。然而，诸如暗物质本质、宇宙加速的物理起源、引力在宇宙尺度上的性质以及大爆炸后最早时刻的物理条件等关键问题，仍无法仅通过电磁观测完全约束。在这些领域取得进展需要研究不发光的物理过程和时期。引力波提供了一种独特且互补的观测手段：它们在宇宙尺度上传播时几乎不受 intervening matter 的影响，能够探测极端天体物理环境，并直接反映时空的几何结构。在此背景下，下一代引力波观测设施如埃因斯坦望远镜（ET）将对欧洲天文学产生变革性影响。ET 将在灵敏度和频率上超越现有探测器，能够探测到此前无法触及的红移范围的双黑洞和中子星系统，实现对紧凑源的持续高信噪比监测，并探测来自天体物理和宇宙学的引力波背景。与空间引力波探测器结合，ET 将在推进我们对宇宙演化和基本物理的理解中发挥核心作用。</div>
</details>
</div>
<div class="card">
<div class="title">LookAroundNet: Extending Temporal Context with Transformers for Clinically Viable EEG Seizure Detection</div>
<div class="meta-line">Authors: Þór Sverrisson, Steinn Guðmundsson</div>
<div class="meta-line">First: 2026-01-09T18:52:24+00:00 · Latest: 2026-01-09T18:52:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06016v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06016v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated seizure detection from electroencephalography (EEG) remains difficult due to the large variability of seizure dynamics across patients, recording conditions, and clinical settings. We introduce LookAroundNet, a transformer-based seizure detector that uses a wider temporal window of EEG data to model seizure activity. The seizure detector incorporates EEG signals before and after the segment of interest, reflecting how clinicians use surrounding context when interpreting EEG recordings. We evaluate the proposed method on multiple EEG datasets spanning diverse clinical environments, patient populations, and recording modalities, including routine clinical EEG and long-term ambulatory recordings, in order to study performance across varying data distributions. The evaluation includes publicly available datasets as well as a large proprietary collection of home EEG recordings, providing complementary views of controlled clinical data and unconstrained home-monitoring conditions. Our results show that LookAroundNet achieves strong performance across datasets, generalizes well to previously unseen recording conditions, and operates with computational costs compatible with real-world clinical deployment. The results indicate that extended temporal context, increased training data diversity, and model ensembling are key factors for improving performance. This work contributes to moving automatic seizure detection models toward clinically viable solutions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LookAroundNet：利用Transformer扩展时间上下文以实现临床可行的EEG癫痫检测</div>
<div class="mono" style="margin-top:8px">由于癫痫发作动态在患者、记录条件和临床环境中存在显著差异，从脑电图（EEG）中实现自动癫痫检测仍然具有挑战性。我们引入了LookAroundNet，这是一种基于Transformer的癫痫检测器，通过使用更宽的时间窗口来建模癫痫活动。该检测器结合了感兴趣段落前后的EEG信号，反映了临床医生在解读EEG记录时如何利用周围上下文。我们评估了所提出的方法在多个涵盖不同临床环境、患者群体和记录模式的EEG数据集上的性能，包括常规临床EEG和长期便携式记录，以研究在不同数据分布下的表现。评估包括公开数据集以及一个大型的自有家庭EEG记录集合，提供了受控临床数据和无约束家庭监测条件的互补视角。我们的结果表明，LookAroundNet在多个数据集上表现出色，能够很好地泛化到之前未见过的记录条件，并且其计算成本适合实际临床部署。结果表明，扩展的时间上下文、增加训练数据的多样性以及模型集成是提升性能的关键因素。本工作有助于推动自动癫痫检测模型向临床可行的解决方案发展。</div>
</details>
</div>
<div class="card">
<div class="title">Simple Mechanisms for Representing, Indexing and Manipulating Concepts</div>
<div class="meta-line">Authors: Yuanzhi Li, Raghu Meka, Rina Panigrahy, Kulin Shah</div>
<div class="meta-line">First: 2023-10-18T17:54:29+00:00 · Latest: 2026-01-09T18:48:43+00:00</div>
<div class="meta-line">Comments: 29 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2310.12143v2">Abs</a> · <a href="https://arxiv.org/pdf/2310.12143v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Supervised and unsupervised learning using deep neural networks typically aims to exploit the underlying structure in the training data; this structure is often explained using a latent generative process that produces the data, and the generative process is often hierarchical, involving latent concepts. Despite the significant work on understanding the learning of the latent structure and underlying concepts using theory and experiments, a framework that mathematically captures the definition of a concept and provides ways to operate on concepts is missing. In this work, we propose to characterize a simple primitive concept by the zero set of a collection of polynomials and use moment statistics of the data to uniquely represent the concepts; we show how this view can be used to obtain a signature of the concept. These signatures can be used to discover a common structure across the set of concepts and could recursively produce the signature of higher-level concepts from the signatures of lower-level concepts. To utilize such desired properties, we propose a method by keeping a dictionary of concepts and show that the proposed method can learn different types of hierarchical structures of the data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>表示、索引和操作概念的简单机制</div>
<div class="mono" style="margin-top:8px">使用深度神经网络进行监督和非监督学习通常旨在利用训练数据中的潜在结构；这种结构通常通过一种潜在的生成过程来解释，而该生成过程往往是分层的，涉及潜在的概念。尽管已有大量工作通过理论和实验来理解潜在结构和潜在概念的学习，但缺乏一个能够数学上捕捉概念定义并提供操作概念方法的框架。在本工作中，我们提出通过一组多项式的零集来表征一个简单的基本概念，并利用数据的矩统计量唯一地表示这些概念；我们展示了这种视角如何用于获取概念的签名。这些签名可用于发现概念集合中的共同结构，并且可以递归地从低层次概念的签名生成高层次概念的签名。为了利用这些期望的特性，我们提出了一种方法，通过维护一个概念字典，并证明该方法能够学习数据的不同类型的分层结构。</div>
</details>
</div>
<div class="card">
<div class="title">Slow mixing and emergent one-form symmetries in three-dimensional $\mathbb{Z}_2$ gauge theory</div>
<div class="meta-line">Authors: Charles Stahl, Benedikt Placke, Vedika Khemani, Yaodong Li</div>
<div class="meta-line">First: 2026-01-09T18:48:39+00:00 · Latest: 2026-01-09T18:48:39+00:00</div>
<div class="meta-line">Comments: 19+4 pages, 10+1 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06010v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06010v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Symmetry-breaking order at low temperatures is often accompanied by slow relaxation dynamics, due to diverging free-energy barriers arising from interfaces between different ordered states. Here, we extend this correspondence to classical topological order, where the ordered states are locally indistinguishable, so there is no notion of interfaces between them. We study the relaxation dynamics of the three-dimensional (3D) classical $\mathbb{Z}_2$ lattice gauge theory (LGT) as a canonical example. We prove a lower bound on the mixing time in the deconfined phase, $t_{\text{mix}} = \exp [Ω(L)]$, where L is the linear system size. This bound applies even in the presence of perturbations that explicitly break the one-form symmetry between different long-lived states. This perturbation destroys the energy barriers between ordered states, but we show that entropic effects nevertheless lead to diverging free-energy barriers at nonzero temperature. Our proof establishes the LGT as a robust finite-temperature classical memory. We further prove that entropic effects lead to an emergent one-form symmetry, via a notion that we make precise. We argue that the exponential mixing time follows from universal properties of the deconfined phase, and numerically corroborate this expectation by exploring mixing time scales at the Higgs and confinement transitions out of the deconfined phase. These transitions are found to exhibit markedly different dynamic scaling, even though both have the static critical exponents of the 3D Ising model. We expect this novel entropic mechanism for memory and emergent symmetry to also bring insight into self-correcting quantum memories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>三维 $\mathbb{Z}_2$ 规范理论中的慢混合与浮现的一形式对称性</div>
<div class="mono" style="margin-top:8px">在低温下，对称性破缺序通常伴随着缓慢的弛豫动力学，这是由于不同有序态之间的界面导致自由能势垒发散。在这里，我们将这种对应关系扩展到经典拓扑序，其中有序态在局域上无法区分，因此不存在它们之间的界面概念。我们以三维经典 $\mathbb{Z}_2$ 晶格规范理论（LGT）作为典型例子，研究其弛豫动力学。我们证明了在无约束相中混合时间的下界为 $t_{\text{mix}} = \exp [Ω(L)]$，其中 L 是线性系统尺寸。这一界限即使在存在显式破坏不同长寿命态之间一形式对称性的扰动下依然成立。这种扰动会破坏有序态之间的能量势垒，但我们证明在非零温度下，熵效应仍然会导致自由能势垒发散。我们的证明确立了LGT作为一种稳健的经典有限温度记忆系统。我们进一步证明，熵效应通过一种我们精确定义的概念，会导致浮现的一形式对称性。我们论证了指数级的混合时间源于无约束相的普适性质，并通过数值研究无约束相中霍奇和约束转变的混合时间尺度来验证这一预期。这些转变表现出显著不同的动态标度，尽管它们的静态临界指数与三维伊辛模型相同。我们预期这种新颖的熵机制对于记忆和浮现对称性的理解也将为自校正量子记忆提供新的见解。</div>
</details>
</div>
<div class="card">
<div class="title">Detecting Stochasticity in Discrete Signals via Nonparametric Excursion Theorem</div>
<div class="meta-line">Authors: Sunia Tanweer, Firas A. Khasawneh</div>
<div class="meta-line">First: 2026-01-09T18:47:57+00:00 · Latest: 2026-01-09T18:47:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06009v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06009v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We develop a practical framework for distinguishing diffusive stochastic processes from deterministic signals using only a single discrete time series. Our approach is based on classical excursion and crossing theorems for continuous semimartingales, which correlates number $N_\varepsilon$ of excursions of magnitude at least $\varepsilon$ with the quadratic variation $[X]_T$ of the process. The scaling law holds universally for all continuous semimartingales with finite quadratic variation, including general Ito diffusions with nonlinear or state-dependent volatility, but fails sharply for deterministic systems -- thereby providing a theoretically-certfied method of distinguishing between these dynamics, as opposed to the subjective entropy or recurrence based state of the art methods. We construct a robust data-driven diffusion test. The method compares the empirical excursion counts against the theoretical expectation. The resulting ratio $K(\varepsilon)=N_{\varepsilon}^{\mathrm{emp}}/N_{\varepsilon}^{\mathrm{theory}}$ is then summarized by a log-log slope deviation measuring the $\varepsilon^{-2}$ law that provides a classification into diffusion-like or not. We demonstrate the method on canonical stochastic systems, some periodic and chaotic maps and systems with additive white noise, as well as the stochastic Duffing system. The approach is nonparametric, model-free, and relies only on the universal small-scale structure of continuous semimartingales.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过非参数游程定理检测离散信号中的随机性</div>
<div class="mono" style="margin-top:8px">我们开发了一个实用框架，仅使用一个离散时间序列即可区分扩散型随机过程与确定性信号。我们的方法基于连续半鞅的经典游程和穿越定理，将幅度至少为 $\varepsilon$ 的游程次数 $N_\varepsilon$ 与过程的二次变差 $[X]_T$ 相关联。该标度律对所有具有有限二次变差的连续半鞅普遍成立，包括具有非线性或状态依赖波动率的一般伊藤扩散过程，但在确定性系统中则明显失效，从而提供了一种理论上认证的方法来区分这些动态，与基于主观熵或递归的现有方法不同。我们构建了一个稳健的数据驱动扩散检验方法。该方法将实测游程次数与理论期望值进行比较。所得比值 $K(\varepsilon)=N_{\varepsilon}^{\mathrm{emp}}/N_{\varepsilon}^{\mathrm{theory}}$ 通过测量 $\varepsilon^{-2}$ 标度律的对数-对数斜率偏差进行总结，从而实现对扩散型或非扩散型的分类。我们在一些典型的随机系统、周期性和混沌映射系统、带有加性白噪声的系统以及随机杜芬系统中展示了该方法。该方法是非参数的、无模型依赖的，仅依赖于连续半鞅的普遍小尺度结构。</div>
</details>
</div>
<div class="card">
<div class="title">From Fact to Judgment: Investigating the Impact of Task Framing on LLM Conviction in Dialogue Systems</div>
<div class="meta-line">Authors: Parisa Rabbani, Nimet Beyza Bozdag, Dilek Hakkani-Tür</div>
<div class="meta-line">First: 2025-11-14T00:55:28+00:00 · Latest: 2026-01-09T18:47:24+00:00</div>
<div class="meta-line">Comments: 11 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.10871v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.10871v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLMs are increasingly employed as judges across a variety of tasks, including those involving everyday social interactions. Yet, it remains unclear whether such LLM-judges can reliably assess tasks that require social or conversational judgment. We investigate how an LLM&#x27;s conviction is changed when a task is reframed from a direct factual query to a Conversational Judgment Task. Our evaluation framework contrasts the model&#x27;s performance on direct factual queries with its assessment of a speaker&#x27;s correctness when the same information is presented within a minimal dialogue, effectively shifting the query from &quot;Is this statement correct?&quot; to &quot;Is this speaker correct?&quot;. Furthermore, we apply pressure in the form of a simple rebuttal (&quot;The previous answer is incorrect.&quot;) to both conditions. This perturbation allows us to measure how firmly the model maintains its position under conversational pressure. Our findings show that while some models like GPT-4o-mini reveal sycophantic tendencies under social framing tasks, others like Llama-8B-Instruct become overly-critical. We observe an average performance change of 9.24% across all models, demonstrating that even minimal dialogue context can significantly alter model judgment, underscoring conversational framing as a key factor in LLM-based evaluation. The proposed framework offers a reproducible methodology for diagnosing model conviction and contributes to the development of more trustworthy dialogue systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从事实到判断：探究任务框架对对话系统中LLM确信度的影响</div>
<div class="mono" style="margin-top:8px">LLM越来越多地被用作各种任务的裁判，包括涉及日常社会互动的任务。然而，尚不清楚这些LLM裁判是否能够可靠地评估需要社会或对话判断的任务。我们研究了当任务从直接事实性查询重新框架为对话判断任务时，LLM的判断确信度会发生怎样的变化。我们的评估框架对比了模型在直接事实性查询上的表现与其在最小化对话中对说话人正确性的评估，实质上将问题从“这个陈述是否正确？”转变为“这个说话人是否正确？”此外，我们还对两种条件施加简单的反驳（“之前的回答是错误的。”）以测试模型在对话压力下的立场稳定性。我们的发现表明，尽管某些模型如GPT-4o-mini在社会框架任务中表现出顺从倾向，而其他模型如Llama-8B-Instruct则变得过于批判。我们观察到所有模型平均性能变化为9.24%，这表明即使是最小的对话上下文也能显著改变模型的判断，突显了对话框架在基于LLM的评估中的关键作用。所提出的框架为诊断模型确信度提供了一种可复现的方法，并有助于构建更可信的对话系统。</div>
</details>
</div>
<div class="card">
<div class="title">Co-Training Vision Language Models for Remote Sensing Multi-task Learning</div>
<div class="meta-line">Authors: Qingyun Li, Shuran Ma, Junwei Luo, Yi Yu, Yue Zhou, Fengxiang Wang, Xudong Lu, Xiaoxing Wang, Xin He, Yushi Chen, Xue Yang</div>
<div class="meta-line">First: 2025-11-26T10:55:07+00:00 · Latest: 2026-01-09T18:43:00+00:00</div>
<div class="meta-line">Comments: 14 pages, 6 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.21272v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.21272v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With Transformers achieving outstanding performance on individual remote sensing (RS) tasks, we are now approaching the realization of a unified model that excels across multiple tasks through multi-task learning (MTL). Compared to single-task approaches, MTL methods offer improved generalization, enhanced scalability, and greater practical applicability. Recently, vision language models (VLMs) have achieved promising results in RS image understanding, grounding, and ultra-high-resolution (UHR) image reasoning, respectively. Moreover, the unified text-based interface demonstrates significant potential for MTL. Hence, in this work, we present RSCoVLM, a simple yet flexible VLM baseline for RS MTL. Firstly, we create the data curation engine, including data acquisition, offline processing and integrating, as well as online loading and weighting. This data engine effectively addresses complex RS data enviroment and generates flexible vision-language conversations. Furthermore, we propose a unified dynamic-resolution strategy to address the diverse image scales inherent in RS imagery. For UHR images, we introduce the Zoom-in Chain mechanism together with its corresponding dataset, LRS-VQA-Zoom. The strategies are flexible and effectively mitigate the computational burdens. Additionally, we significantly enhance the model&#x27;s object detection capability and propose a novel evaluation protocol that ensures fair comparison between VLMs and conventional detection models. Extensive experiments demonstrate that RSCoVLM achieves state-of-the-art performance across diverse tasks, outperforming existing RS VLMs and even rivaling specialized expert models. All the training and evaluating tools, model weights, and datasets have been fully open-sourced to support reproducibility. We expect that this baseline will promote further progress toward general-purpose RS models.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于遥感多任务学习的协同训练视觉语言模型</div>
<div class="mono" style="margin-top:8px">随着Transformer在单个遥感（RS）任务中取得出色表现，我们正逐步接近通过多任务学习（MTL）实现一个在多个任务中均表现优异的统一模型。与单任务方法相比，MTL方法提供了更好的泛化能力、更强的可扩展性和更高的实际应用价值。最近，视觉语言模型（VLMs）在遥感图像理解、图像定位以及超高清分辨率（UHR）图像推理等方面取得了有前景的成果。此外，统一的基于文本的接口在MTL中展现出巨大的潜力。因此，在本工作中，我们提出了RSCoVLM，这是一个简单而灵活的RS MTL视觉语言模型基线。首先，我们构建了数据编排引擎，包括数据采集、离线处理与整合，以及在线加载与加权。该数据引擎有效应对了复杂的遥感数据环境，并生成了灵活的视觉-语言对话。此外，我们提出了一种统一的动态分辨率策略，以解决遥感图像固有的多样化尺度问题。对于超高清图像，我们引入了Zoom-in Chain机制及其对应的LRS-VQA-Zoom数据集。这些策略具有灵活性，并有效缓解了计算负担。同时，我们显著增强了模型的物体检测能力，并提出了一种新的评估协议，确保VLMs与传统检测模型之间的公平比较。大量实验表明，RSCoVLM在多种任务上均取得了最先进的性能，优于现有的RS VLMs，甚至在某些方面可与专门的专家模型相媲美。所有训练和评估工具、模型权重和数据集均已完全开源，以支持可复现性。我们期望这一基线能够推动通用遥感模型的进一步发展。</div>
</details>
</div>
<div class="card">
<div class="title">Don&#x27;t Break the Cache: An Evaluation of Prompt Caching for Long-Horizon Agentic Tasks</div>
<div class="meta-line">Authors: Elias Lumer, Faheem Nizar, Akshaya Jangiti, Kevin Frank, Anmol Gulati, Mandar Phadate, Vamse Kumar Subbiah</div>
<div class="meta-line">First: 2026-01-09T18:41:57+00:00 · Latest: 2026-01-09T18:41:57+00:00</div>
<div class="meta-line">Comments: 15 pages, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06007v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06007v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in Large Language Model (LLM) agents have enabled complex multi-turn agentic tasks requiring extensive tool calling, where conversations can span dozens of API calls with increasingly large context windows. However, although major LLM providers offer prompt caching to reduce cost and latency, its benefits for agentic workloads remain underexplored in the research literature. To our knowledge, no prior work quantifies these cost savings or compares caching strategies for multi-turn agentic tasks. We present a comprehensive evaluation of prompt caching across three major LLM providers (OpenAI, Anthropic, and Google) and compare three caching strategies, including full context caching, system prompt only caching, and caching that excludes dynamic tool results. We evaluate on DeepResearchBench, a multi-turn agentic benchmark where agents autonomously execute real-world web search tool calls to answer complex research questions, measuring both API cost and time to first token (TTFT) across over 500 agent sessions with 10,000-token system prompts. Our results demonstrate that prompt caching reduces API costs by 45-80% and improves time to first token by 13-31% across providers. We find that strategic prompt cache block control, such as placing dynamic content at the end of the system prompt, avoiding dynamic traditional function calling, and excluding dynamic tool results, provides more consistent benefits than naive full-context caching, which can paradoxically increase latency. Our analysis reveals nuanced variations in caching behavior across providers, and we provide practical guidance for implementing prompt caching in production agentic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>不要破坏缓存：对长周期代理任务中提示缓存的评估</div>
<div class="mono" style="margin-top:8px">最近在大型语言模型（LLM）代理领域的进展使得复杂多轮代理任务成为可能，这些任务需要大量的工具调用，对话可能跨越数十次API调用，并且上下文窗口越来越大。然而，尽管主要的LLM提供商提供了提示缓存功能以降低成本和延迟，但其在代理工作负载中的优势在研究文献中仍被较少探讨。据我们所知，目前尚无研究量化这些成本节约，或比较多轮代理任务中的缓存策略。我们对三个主要LLM提供商（OpenAI、Anthropic和Google）的提示缓存进行了全面评估，并比较了三种缓存策略，包括完整上下文缓存、仅系统提示缓存以及排除动态工具结果的缓存。我们在DeepResearchBench上进行了评估，这是一个多轮代理基准测试，代理可以自主执行现实世界的网络搜索工具调用以回答复杂的研究问题，测量了超过500个代理会话中API成本和首次令牌生成时间（TTFT），这些会话使用了10,000个标记的系统提示。我们的结果表明，提示缓存可将API成本降低45-80%，并提高首次令牌生成时间的效率13-31%。我们发现，战略性提示缓存块控制，例如将动态内容放在系统提示的末尾、避免动态传统函数调用以及排除动态工具结果，比简单的完整上下文缓存提供了更一致的收益，而后者甚至可能反常地增加延迟。我们的分析揭示了不同提供商之间缓存行为的细微差异，并为在生产代理系统中实现提示缓存提供了实用的指导。</div>
</details>
</div>
<div class="card">
<div class="title">The Molecular Structure of Thought: Mapping the Topology of Long Chain-of-Thought Reasoning</div>
<div class="meta-line">Authors: Qiguang Chen, Yantao Du, Ziniu Li, Jinhao Liu, Songyao Duan, Jiarui Guo, Minghao Liu, Jiaheng Liu, Tong Yang, Ge Zhang, Libo Qin, Wanxiang Che, Wenhao Huang</div>
<div class="meta-line">First: 2026-01-09T18:39:01+00:00 · Latest: 2026-01-09T18:39:01+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.06002v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.06002v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) often fail to learn effective long chain-of-thought (Long CoT) reasoning from human or non-Long-CoT LLMs imitation. To understand this, we propose that effective and learnable Long CoT trajectories feature stable molecular-like structures in unified view, which are formed by three interaction types: Deep-Reasoning (covalent-like), Self-Reflection (hydrogen-bond-like), and Self-Exploration (van der Waals-like). Analysis of distilled trajectories reveals these structures emerge from Long CoT fine-tuning, not keyword imitation. We introduce Effective Semantic Isomers and show that only bonds promoting fast entropy convergence support stable Long CoT learning, while structural competition impairs training. Drawing on these findings, we present Mole-Syn, a distribution-transfer-graph method that guides synthesis of effective Long CoT structures, boosting performance and RL stability across benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>思维的分子结构：长链式思维的拓扑映射</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）通常无法从人类或非长链式思维（Long CoT）LLMs的模仿中学习到有效的长链式思维推理。为理解这一现象，我们提出有效的、可学习的长链式思维轨迹在统一视角下具有稳定的类似分子的结构，这些结构由三种交互类型构成：深度推理（类似共价键）、自我反思（类似氢键）和自我探索（类似范德华力）。对蒸馏轨迹的分析表明，这些结构源于长链式思维微调，而非关键词模仿。我们引入了有效语义异构体，并展示只有促进快速熵收敛的键才能支持稳定的长链式思维学习，而结构竞争则会损害训练。基于这些发现，我们提出了Mole-Syn，一种分布迁移图方法，用于指导有效长链式思维结构的合成，从而在多个基准测试中提升性能和强化学习的稳定性。</div>
</details>
</div>
<div class="card">
<div class="title">There are no Champions in Supervised Long-Term Time Series Forecasting</div>
<div class="meta-line">Authors: Lorenzo Brigato, Rafael Morand, Knut Strømmen, Maria Panagiotou, Markus Schmidt, Stavroula Mougiakakou</div>
<div class="meta-line">First: 2025-02-19T19:08:37+00:00 · Latest: 2026-01-09T18:37:55+00:00</div>
<div class="meta-line">Comments: Accepted at TMLR</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.14045v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.14045v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in long-term time series forecasting have introduced numerous complex supervised prediction models that consistently outperform previously published architectures. However, this rapid progression raises concerns regarding inconsistent benchmarking and reporting practices, which may undermine the reliability of these comparisons. In this study, we first perform a broad, thorough, and reproducible evaluation of the top-performing supervised models on the most popular benchmark and additional baselines representing the most active architecture families. This extensive evaluation assesses eight models on 14 datasets, encompassing $\sim$5,000 trained networks for the hyperparameter (HP) searches. Then, through a comprehensive analysis, we find that slight changes to experimental setups or current evaluation metrics drastically shift the common belief that newly published results are advancing the state of the art. Our findings emphasize the need to shift focus away from pursuing ever-more complex models, towards enhancing benchmarking practices through rigorous and standardized evaluations that enable more substantiated claims, including reproducible HP setups and statistical testing. We offer recommendations for future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在监督的长期时间序列预测中没有冠军</div>
<div class="mono" style="margin-top:8px">近年来，长期时间序列预测领域取得了诸多进展，引入了大量复杂的监督预测模型，这些模型持续优于之前发表的架构。然而，这种快速的发展引发了对不一致的基准测试和报告实践的担忧，这可能会削弱这些比较的可靠性。在本研究中，我们首先在最流行的数据集和代表当前最活跃架构家族的其他基准上，对表现最好的监督模型进行了广泛、彻底且可复现的评估。这项全面的评估涵盖了14个数据集，对八个模型进行了测试，其中包括约5000个用于超参数搜索的训练网络。通过深入分析，我们发现实验设置或当前评估指标的微小变化会显著改变人们普遍认为新发表结果正在推动领域发展的信念。我们的研究结果强调，应将重点从不断追求更复杂的模型，转向通过严格和标准化的评估来改进基准测试实践，从而支持更有力的结论，包括可复现的超参数设置和统计检验。我们还为未来的研究提供了建议。</div>
</details>
</div>
<div class="card">
<div class="title">ACDZero: MCTS Agent for Mastering Automated Cyber Defense</div>
<div class="meta-line">Authors: Yu Li, Sizhe Tang, Rongqian Chen, Fei Xu Yu, Guangyu Jiang, Mahdi Imani, Nathaniel D. Bastian, Tian Lan</div>
<div class="meta-line">First: 2026-01-05T15:18:54+00:00 · Latest: 2026-01-09T18:28:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02196v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.02196v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated cyber defense (ACD) seeks to protect computer networks with minimal or no human intervention, reacting to intrusions by taking corrective actions such as isolating hosts, resetting services, deploying decoys, or updating access controls. However, existing approaches for ACD, such as deep reinforcement learning (RL), often face difficult exploration in complex networks with large decision/state spaces and thus require an expensive amount of samples. Inspired by the need to learn sample-efficient defense policies, we frame ACD in CAGE Challenge 4 (CAGE-4 / CC4) as a context-based partially observable Markov decision problem and propose a planning-centric defense policy based on Monte Carlo Tree Search (MCTS). It explicitly models the exploration-exploitation tradeoff in ACD and uses statistical sampling to guide exploration and decision making. We make novel use of graph neural networks (GNNs) to embed observations from the network as attributed graphs, to enable permutation-invariant reasoning over hosts and their relationships. To make our solution practical in complex search spaces, we guide MCTS with learned graph embeddings and priors over graph-edit actions, combining model-free generalization and policy distillation with look-ahead planning. We evaluate the resulting agent on CC4 scenarios involving diverse network structures and adversary behaviors, and show that our search-guided, graph-embedding-based planning improves defense reward and robustness relative to state-of-the-art RL baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ACDZero：用于掌握自动化网络防御的MCTS代理</div>
<div class="mono" style="margin-top:8px">自动化网络防御（ACD）旨在通过最少或无需人工干预来保护计算机网络，通过采取隔离主机、重置服务、部署诱饵或更新访问控制等纠正措施来应对入侵。然而，现有的ACD方法，如深度强化学习（RL），在复杂网络中由于决策/状态空间较大，常常面临探索困难，因此需要大量的样本。为了解决学习样本高效防御策略的需求，我们将ACD在CAGE挑战4（CAGE-4 / CC4）中建模为基于上下文的部分可观测马尔可夫决策过程，并提出一种以规划为中心的基于蒙特卡洛树搜索（MCTS）的防御策略。该策略显式地建模了ACD中的探索与利用权衡，并利用统计抽样来指导探索和决策。我们首次将图神经网络（GNNs）用于将网络观测嵌入为带属性的图，从而实现对主机及其关系的排列不变推理。为了使我们的解决方案在复杂的搜索空间中实用，我们通过学习的图嵌入和图编辑操作的先验知识来引导MCTS，结合无模型泛化和策略蒸馏与前瞻规划。我们在涉及多样网络结构和对手行为的CC4场景中评估了所得到的代理，并展示了我们的基于搜索和图嵌入的规划方法在防御奖励和鲁棒性方面优于当前最先进的RL基线。</div>
</details>
</div>
<div class="card">
<div class="title">Low-dimensional semi-supervised latent Bayesian optimization for designing antimicrobial peptides</div>
<div class="meta-line">Authors: Jyler Menard, R. A. Mansbach</div>
<div class="meta-line">First: 2025-10-20T14:20:11+00:00 · Latest: 2026-01-09T18:27:24+00:00</div>
<div class="meta-line">Comments: (Submitted version) v2: 22 pages, 9 figures. Expanded discussion section. Made large figure clearer. Small title and abstract change. Edits to results to make points clearer, but no drastic changes</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.17569v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.17569v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Antimicrobial peptides (AMPs) are a promising class of therapeutics to treat bacterial infections. Discovering and designing such peptides is difficult because of the vast number of possible sequences of amino acids. Deep generative models, such as variational autoencoders, have shown value in peptide design due to their ability to model sequence space with a continuous-valued latent space. Although such models have already been used to great effect in biomolecular design, they still suffer from a lack of interpretability and rigorous quantification of latent space quality as a search space. We investigate (1) whether searching through a dimensionally-reduced variant of the latent design space may facilitate optimization, (2) how organizing latent spaces with physicochemical properties may improve the efficiency of optimizing antimicrobial activity, and (3) the interpretability of the spaces. We find that employing a dimensionally-reduced version of the latent space is more interpretable and can be advantageous, while we can organize the latent space with different physicochemical properties even at different percentages of available labels. This work lays crucial groundwork for biophysically-motivated peptide design procedures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于设计抗菌肽的低维半监督潜在贝叶斯优化</div>
<div class="mono" style="margin-top:8px">抗菌肽（AMPs）是一类有前景的治疗细菌感染的药物。由于氨基酸序列的可能组合数量庞大，发现和设计这类肽具有挑战性。深度生成模型，如变分自编码器，因其能够通过连续值潜在空间建模序列空间而在肽设计中展现出价值。尽管这些模型已经在生物分子设计中取得了显著成效，但它们仍然缺乏可解释性，并且在潜在空间作为搜索空间时，对潜在空间质量的严格量化仍存在问题。我们探讨了以下三个问题：（1）在潜在设计空间的降维版本中进行搜索是否有助于优化；（2）如何通过物理化学性质组织潜在空间以提高抗菌活性优化的效率；（3）这些空间的可解释性。我们发现，使用潜在空间的降维版本更具可解释性且可能具有优势，同时即使在可用标签比例不同的情况下，也可以通过不同的物理化学性质对潜在空间进行组织。这项工作为以生物物理为导向的肽设计流程奠定了关键基础。</div>
</details>
</div>
<div class="card">
<div class="title">Open-Vocabulary 3D Instruction Ambiguity Detection</div>
<div class="meta-line">Authors: Jiayu Ding, Haoran Tang, Ge Li</div>
<div class="meta-line">First: 2026-01-09T18:17:11+00:00 · Latest: 2026-01-09T18:17:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05991v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05991v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://jiayuding031020.github.io/ambi3d/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In safety-critical domains, linguistic ambiguity can have severe consequences; a vague command like &quot;Pass me the vial&quot; in a surgical setting could lead to catastrophic errors. Yet, most embodied AI research overlooks this, assuming instructions are clear and focusing on execution rather than confirmation. To address this critical safety gap, we are the first to define Open-Vocabulary 3D Instruction Ambiguity Detection, a fundamental new task where a model must determine if a command has a single, unambiguous meaning within a given 3D scene. To support this research, we build Ambi3D, the large-scale benchmark for this task, featuring over 700 diverse 3D scenes and around 22k instructions. Our analysis reveals a surprising limitation: state-of-the-art 3D Large Language Models (LLMs) struggle to reliably determine if an instruction is ambiguous. To address this challenge, we propose AmbiVer, a two-stage framework that collects explicit visual evidence from multiple views and uses it to guide an vision-language model (VLM) in judging instruction ambiguity. Extensive experiments demonstrate the challenge of our task and the effectiveness of AmbiVer, paving the way for safer and more trustworthy embodied AI. Code and dataset available at https://jiayuding031020.github.io/ambi3d/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>开放词汇三维指令歧义检测</div>
<div class="mono" style="margin-top:8px">在安全关键领域，语言歧义可能导致严重后果；例如在手术环境中，一个模糊的指令如“给我递那个试管”可能导致灾难性错误。然而，大多数具身AI研究忽略了这一点，假设指令清晰且专注于执行而非确认。为解决这一关键的安全缺口，我们首次定义了开放词汇三维指令歧义检测任务，这是一个基础的新任务，要求模型判断给定三维场景中指令是否具有单一且明确的含义。为支持这一研究，我们构建了Ambi3D，这是该任务的大规模基准数据集，包含超过700个多样化的三维场景和约22000条指令。我们的分析揭示了一个令人惊讶的局限性：最先进的三维大语言模型（LLMs）在可靠判断指令是否歧义方面存在困难。为应对这一挑战，我们提出了AmbiVer，一个两阶段框架，通过多视角收集显式的视觉证据，并利用这些证据引导视觉语言模型（VLM）判断指令歧义。大量实验验证了我们任务的挑战性和AmbiVer的有效性，为更安全、更可信的具身AI铺平了道路。代码和数据集可在https://jiayuding031020.github.io/ambi3d/获取。</div>
</details>
</div>
<div class="card">
<div class="title">From Superradiance to Superabsorption: An Exact Treatment of Non-Markovian Cooperative Radiation</div>
<div class="meta-line">Authors: Ignacio González, Ángel Rivas</div>
<div class="meta-line">First: 2026-01-09T18:16:11+00:00 · Latest: 2026-01-09T18:16:11+00:00</div>
<div class="meta-line">Comments: RevTex4 file, color figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05989v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05989v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate the emergence of cooperative radiation phenomena in ensembles of two-level atoms coupled to a lossy resonant cavity beyond the Markovian and mean-field approximations. By deriving a complete analytical solution for the two-emitter case and employing a numerically exact method for larger ensembles, we characterize the full transition from Markovian to non-Markovian collective dynamics for systems of up to $10^3$ emitters. Our results reveal three distinct regimes: a Markovian phase exhibiting the standard superradiant burst, a non-Markovian phase featuring spontaneous superabsorption of the emitted field, and a critical regime marked by pulsed collective emission. We show that the critical spectral width separating these behaviors increases monotonically with the number of emitters, demonstrating that environmental memory effects can be enhanced by cooperativity. Finally, we find that the superradiant scaling of the peak intensity progressively degrades with increasing system size, approaching a subquadratic law in the limit of a perfect cavity. In this regime, spontaneous superabsorption emerges as a distinct manifestation of non-Markovian cooperativity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从超辐射到超吸收：非马尔可夫合作辐射的精确处理</div>
<div class="mono" style="margin-top:8px">我们研究了在超过马尔可夫近似和平均场近似的情况下，耦合到有损耗谐振腔的两能级原子集合中合作辐射现象的出现。通过推导两个发射体情况下的完整解析解，并采用数值精确方法处理更大的发射体集合，我们刻画了最多 $10^3$ 个发射体系统的从马尔可夫到非马尔可夫集体动力学的完整转变。我们的结果揭示了三种不同的行为区域：表现出标准超辐射爆发的马尔可夫相、具有自发超吸收特性的非马尔可夫相，以及以脉冲集体发射为特征的临界区域。我们发现，区分这些行为的临界谱宽随着发射体数量的增加而单调增加，这表明合作性可以增强环境记忆效应。最后，我们发现超辐射峰值强度的标度随着系统尺寸的增加而逐渐退化，趋近于一个次二次律。在这一区域，自发超吸收成为非马尔可夫合作性的独特表现。</div>
</details>
</div>
<div class="card">
<div class="title">CyberGFM: Graph Foundation Models for Lateral Movement Detection in Enterprise Networks</div>
<div class="meta-line">Authors: Isaiah J. King, Bernardo Trindade, Benjamin Bowman, H. Howie Huang</div>
<div class="meta-line">First: 2026-01-09T18:08:47+00:00 · Latest: 2026-01-09T18:08:47+00:00</div>
<div class="meta-line">Comments: 17 pages; 11 figures; 8 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05988v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05988v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Representing networks as a graph and training a link prediction model using benign connections is an effective method of anomaly-based intrusion detection. Existing works using this technique have shown great success using temporal graph neural networks and skip-gram-based approaches on random walks. However, random walk-based approaches are unable to incorporate rich edge data, while the GNN-based approaches require large amounts of memory to train. In this work, we propose extending the original insight from random walk-based skip-grams--that random walks through a graph are analogous to sentences in a corpus--to the more modern transformer-based foundation models. Using language models that take advantage of GPU optimizations, we can quickly train a graph foundation model to predict missing tokens in random walks through a network of computers. The graph foundation model is then finetuned for link prediction and used as a network anomaly detector. This new approach allows us to combine the efficiency of random walk-based methods and the rich semantic representation of deep learning methods. This system, which we call CyberGFM, achieved state-of-the-art results on three widely used network anomaly detection datasets, delivering a up to 2$\times$ improvement in average precision. We found that CyberGFM outperforms all prior works in unsupervised link prediction for network anomaly detection, using the same number of parameters, and with equal or better efficiency than the previous best approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CyberGFM：用于企业网络横向移动检测的图基础模型</div>
<div class="mono" style="margin-top:8px">将网络表示为图，并利用良性连接训练链接预测模型是基于异常的入侵检测的有效方法。现有工作使用该技术在随机游走上采用时间图神经网络和基于skip-gram的方法取得了巨大成功。然而，基于随机游走的方法无法利用丰富的边数据，而基于GNN的方法需要大量内存进行训练。在本工作中，我们提出将基于随机游走的skip-gram的原始洞察——即图中的随机游走类似于语料库中的句子——扩展到更现代的基于Transformer的基础模型。利用能够利用GPU优化的语言模型，我们可以快速训练图基础模型以预测计算机网络中随机游走的缺失标记。然后，该图基础模型被微调用于链接预测，并作为网络异常检测器使用。这种新方法使我们能够结合基于随机游走方法的效率和深度学习方法的丰富语义表示。我们称之为CyberGFM的系统在三个广泛使用的网络异常检测数据集上取得了最先进的结果，平均精度提高了高达2倍。我们发现，在使用相同数量的参数的情况下，CyberGFM在无监督链接预测中优于所有先前的工作，并且其效率与之前最佳方法相当或更高。</div>
</details>
</div>
<div class="card">
<div class="title">Deepfake detectors are DUMB: A benchmark to assess adversarial training robustness under transferability constraints</div>
<div class="meta-line">Authors: Adrian Serrano, Erwan Umlil, Ronan Thomas</div>
<div class="meta-line">First: 2026-01-09T18:06:19+00:00 · Latest: 2026-01-09T18:06:19+00:00</div>
<div class="meta-line">Comments: 10 pages, four tables, one figure</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05986v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05986v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deepfake detection systems deployed in real-world environments are subject to adversaries capable of crafting imperceptible perturbations that degrade model performance. While adversarial training is a widely adopted defense, its effectiveness under realistic conditions -- where attackers operate with limited knowledge and mismatched data distributions - remains underexplored. In this work, we extend the DUMB -- Dataset soUrces, Model architecture and Balance - and DUMBer methodology to deepfake detection. We evaluate detectors robustness against adversarial attacks under transferability constraints and cross-dataset configuration to extract real-world insights. Our study spans five state-of-the-art detectors (RECCE, SRM, XCeption, UCF, SPSL), three attacks (PGD, FGSM, FPBA), and two datasets (FaceForensics++ and Celeb-DF-V2). We analyze both attacker and defender perspectives mapping results to mismatch scenarios. Experiments show that adversarial training strategies reinforce robustness in the in-distribution cases but can also degrade it under cross-dataset configuration depending on the strategy adopted. These findings highlight the need for case-aware defense strategies in real-world applications exposed to adversarial attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>深度伪造检测器是DUMB：在可迁移性约束下评估对抗训练的鲁棒性基准</div>
<div class="mono" style="margin-top:8px">部署在现实环境中的深度伪造检测系统会受到攻击者的影响，这些攻击者能够制造难以察觉的扰动以降低模型性能。尽管对抗训练是一种广泛采用的防御方法，但在现实条件下（攻击者知识有限且数据分布不匹配）其有效性仍缺乏深入研究。在本工作中，我们扩展了DUMB（数据源、模型架构和平衡）和DUMBer方法论到深度伪造检测领域。我们评估了检测器在可迁移性约束和跨数据集配置下的鲁棒性，以提取现实世界的洞察。本研究涵盖了五个最先进的检测器（RECCE、SRM、XCeption、UCF、SPSL）、三种攻击方式（PGD、FGSM、FPBA）和两个数据集（FaceForensics++ 和 Celeb-DF-V2）。我们从攻击者和防御者的角度分析结果，并映射到数据分布不匹配的场景。实验表明，对抗训练策略在分布内情况下能增强鲁棒性，但在跨数据集配置下，根据所采用的策略可能会降低鲁棒性。这些发现强调了在现实应用中，针对不同情况设计防御策略的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Community-Based Model Sharing and Generalisation: Anomaly Detection in IoT Temperature Sensor Networks</div>
<div class="meta-line">Authors: Sahibzada Saadoon Hammad, Joaquín Huerta Guijarro, Francisco Ramos, Michael Gould Carlson, Sergio Trilles Oliver</div>
<div class="meta-line">First: 2026-01-09T18:05:57+00:00 · Latest: 2026-01-09T18:05:57+00:00</div>
<div class="meta-line">Comments: 20 pages, 9 figures, Journal submission</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05984v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05984v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid deployment of Internet of Things (IoT) devices has led to large-scale sensor networks that monitor environmental and urban phenomena in real time. Communities of Interest (CoIs) provide a promising paradigm for organising heterogeneous IoT sensor networks by grouping devices with similar operational and environmental characteristics. This work presents an anomaly detection framework based on the CoI paradigm by grouping sensors into communities using a fused similarity matrix that incorporates temporal correlations via Spearman coefficients, spatial proximity using Gaussian distance decay, and elevation similarities. For each community, representative stations based on the best silhouette are selected and three autoencoder architectures (BiLSTM, LSTM, and MLP) are trained using Bayesian hyperparameter optimization with expanding window cross-validation and tested on stations from the same cluster and the best representative stations of other clusters. The models are trained on normal temperature patterns of the data and anomalies are detected through reconstruction error analysis. Experimental results show a robust within-community performance across the evaluated configurations, while variations across communities are observed. Overall, the results support the applicability of community-based model sharing in reducing computational overhead and to analyse model generalisability across IoT sensor networks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于社区的模型共享与泛化：物联网温度传感器网络中的异常检测</div>
<div class="mono" style="margin-top:8px">物联网（IoT）设备的快速部署导致了大规模的传感器网络，用于实时监测环境和城市现象。兴趣社区（CoIs）为组织异构的IoT传感器网络提供了一个有前景的范式，通过将具有相似运行和环境特性的设备分组实现。本文提出了一种基于CoI范式的异常检测框架，利用融合相似性矩阵对传感器进行分组，该矩阵结合了通过斯皮尔曼系数体现的时间相关性、通过高斯距离衰减体现的空间邻近性以及高度相似性。对于每个社区，基于最佳轮廓值选择代表性站点，并使用贝叶斯超参数优化和扩展窗口交叉验证训练三种自编码器架构（BiLSTM、LSTM和MLP），并在同一聚类中的站点以及其它聚类的最佳代表性站点上进行测试。模型在正常温度模式的数据上进行训练，通过重构误差分析检测异常。实验结果表明，在评估的配置中，社区内部的性能具有鲁棒性，同时观察到不同社区之间的差异。总体而言，结果支持了基于社区的模型共享在减少计算开销和分析模型泛化能力方面的适用性。</div>
</details>
</div>
<div class="card">
<div class="title">Adaptive Conditional Contrast-Agnostic Deformable Image Registration with Uncertainty Estimation</div>
<div class="meta-line">Authors: Yinsong Wang, Xinzhe Luo, Siyi Du, Chen Qin</div>
<div class="meta-line">First: 2026-01-09T18:00:49+00:00 · Latest: 2026-01-09T18:00:49+00:00</div>
<div class="meta-line">Comments: Accepted by ieee transactions on Medical Imaging</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05981v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05981v1">PDF</a> · <a href="https://github.com/Yinsong0510/AC-CAR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deformable multi-contrast image registration is a challenging yet crucial task due to the complex, non-linear intensity relationships across different imaging contrasts. Conventional registration methods typically rely on iterative optimization of the deformation field, which is time-consuming. Although recent learning-based approaches enable fast and accurate registration during inference, their generalizability remains limited to the specific contrasts observed during training. In this work, we propose an adaptive conditional contrast-agnostic deformable image registration framework (AC-CAR) based on a random convolution-based contrast augmentation scheme. AC-CAR can generalize to arbitrary imaging contrasts without observing them during training. To encourage contrast-invariant feature learning, we propose an adaptive conditional feature modulator (ACFM) that adaptively modulates the features and the contrast-invariant latent regularization to enforce the consistency of the learned feature across different imaging contrasts. Additionally, we enable our framework to provide contrast-agnostic registration uncertainty by integrating a variance network that leverages the contrast-agnostic registration encoder to improve the trustworthiness and reliability of AC-CAR. Experimental results demonstrate that AC-CAR outperforms baseline methods in registration accuracy and exhibits superior generalization to unseen imaging contrasts. Code is available at https://github.com/Yinsong0510/AC-CAR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于随机卷积对比增强方案的自适应条件对比无关可变形图像配准与不确定性估计</div>
<div class="mono" style="margin-top:8px">由于不同成像对比度之间复杂的非线性强度关系，可变形多对比度图像配准是一项具有挑战性但至关重要的任务。传统配准方法通常依赖于对形变场的迭代优化，这会耗费大量时间。尽管最近基于学习的方法在推理过程中能够实现快速且准确的配准，但其泛化能力仍受限于训练时所观察到的特定对比度。在本文中，我们提出了一种基于随机卷积对比增强方案的自适应条件对比无关可变形图像配准框架（AC-CAR），该框架能够在不观察特定对比度的情况下泛化到任意成像对比度。为了促进对比无关的特征学习，我们提出了一种自适应条件特征调制器（ACFM），该模块能够自适应地调制特征，并结合对比无关的潜在正则化以确保在不同成像对比度下学习到的特征保持一致。此外，我们通过集成一个方差网络，使框架能够提供对比无关的配准不确定性，该网络利用对比无关的配准编码器来提升AC-CAR的可信度和可靠性。实验结果表明，AC-CAR在配准精度上优于基线方法，并且在面对未见过的成像对比度时表现出更强的泛化能力。代码可在 https://github.com/Yinsong0510/AC-CAR 获取。</div>
</details>
</div>
<div class="card">
<div class="title">Spectral Masking and Interpolation Attack (SMIA): A Black-box Adversarial Attack against Voice Authentication and Anti-Spoofing Systems</div>
<div class="meta-line">Authors: Kamel Kamel, Hridoy Sankar Dutta, Keshav Sood, Sunil Aryal</div>
<div class="meta-line">First: 2025-09-09T12:43:59+00:00 · Latest: 2026-01-09T17:56:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.07677v4">Abs</a> · <a href="https://arxiv.org/pdf/2509.07677v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Voice Authentication Systems (VAS) use unique vocal characteristics for verification. They are increasingly integrated into high-security sectors such as banking and healthcare. Despite their improvements using deep learning, they face severe vulnerabilities from sophisticated threats like deepfakes and adversarial attacks. The emergence of realistic voice cloning complicates detection, as systems struggle to distinguish authentic from synthetic audio. While anti-spoofing countermeasures (CMs) exist to mitigate these risks, many rely on static detection models that can be bypassed by novel adversarial methods, leaving a critical security gap. To demonstrate this vulnerability, we propose the Spectral Masking and Interpolation Attack (SMIA), a novel method that strategically manipulates inaudible frequency regions of AI-generated audio. By altering the voice in imperceptible zones to the human ear, SMIA creates adversarial samples that sound authentic while deceiving CMs. We conducted a comprehensive evaluation of our attack against state-of-the-art (SOTA) models across multiple tasks, under simulated real-world conditions. SMIA achieved a strong attack success rate (ASR) of at least 82% against combined VAS/CM systems, at least 97.5% against standalone speaker verification systems, and 100% against countermeasures. These findings conclusively demonstrate that current security postures are insufficient against adaptive adversarial attacks. This work highlights the urgent need for a paradigm shift toward next-generation defenses that employ dynamic, context-aware frameworks capable of evolving with the threat landscape.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>频谱掩码与插值攻击（SMIA）：一种针对语音认证和反合成语音系统的黑盒对抗攻击</div>
<div class="mono" style="margin-top:8px">语音认证系统（VAS）利用独特的语音特征进行身份验证。它们越来越多地被集成到银行和医疗等高安全领域。尽管使用深度学习技术提升了性能，但它们仍面临来自深度伪造和对抗攻击等复杂威胁的严重漏洞。随着逼真的语音克隆技术的出现，检测变得更加困难，因为系统难以区分真实语音和合成语音。虽然存在反合成语音的防御措施（CMs）来缓解这些风险，但许多防御措施依赖静态检测模型，容易被新的对抗方法绕过，从而留下关键的安全漏洞。为展示这一漏洞，我们提出了频谱掩码与插值攻击（SMIA），一种新颖的方法，通过战略性地操控AI生成语音中人耳无法察觉的频率区域。SMIA通过在人耳无法察觉的区域修改语音，创建出听起来真实但能欺骗CMs的对抗样本。我们在模拟真实世界条件下的多个任务中，对我们的攻击方法进行了全面评估。SMIA在针对结合的VAS/CM系统中达到了至少82%的攻击成功率（ASR），在独立的说话人验证系统中达到了至少97.5%，在反制措施中达到了100%。这些结果明确表明，当前的安全措施不足以应对适应性对抗攻击。本工作突显了向下一代防御机制转变的迫切需求，这些防御机制应采用动态、上下文感知的框架，以适应不断变化的威胁环境。</div>
</details>
</div>
<div class="card">
<div class="title">AWaRe-SAC: Proactive Slice Admission Control under Weather-Induced Capacity Uncertainty</div>
<div class="meta-line">Authors: Dror Jacoby, Yanzhi Li, Shuyue Yu, Nicola Di Cicco, Hagit Messer, Gil Zussman, Igor Kadota</div>
<div class="meta-line">First: 2026-01-09T17:53:09+00:00 · Latest: 2026-01-09T17:53:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05978v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05978v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As emerging applications demand higher throughput and lower latencies, operators are increasingly deploying millimeter-wave (mmWave) links within x-haul transport networks, spanning fronthaul, midhaul, and backhaul segments. However, the inherent susceptibility of mmWave frequencies to weather-related attenuation, particularly rain fading, complicates the maintenance of stringent Quality of Service (QoS) requirements. This creates a critical challenge: making admission decisions under uncertainty regarding future network capacity. To address this, we develop a proactive slice admission control framework for mmWave x-haul networks subject to rain-induced fluctuations. Our objective is to improve network performance, ensure QoS, and optimize revenue, thereby surpassing the limitations of standard reactive approaches. The proposed framework integrates a deep learning predictor of future network conditions with a proactive Q-learning-based slice admission control mechanism. We validate our solution using real-world data from a mmWave x-haul deployment in a dense urban area, incorporating realistic models of link capacity attenuation and dynamic slice demands. Extensive evaluations demonstrate that our proactive solution achieves 2-3x higher long-term average revenue under dynamic link conditions, providing a scalable and resilient framework for adaptive admission control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AWaRe-SAC: 面向毫米波xhaul网络的天气诱导容量不确定性下的主动切片准入控制</div>
<div class="mono" style="margin-top:8px">随着新兴应用对高吞吐量和低延迟的需求增加，运营商越来越多地在xhaul传输网络中部署毫米波（mmWave）链路，涵盖前传、中传和回传段。然而，毫米波频段对天气相关衰减（尤其是降雨衰减）的固有敏感性，使得维持严格的服务质量（QoS）要求变得复杂。这带来了关键挑战：在对未来网络容量不确定的情况下做出准入决策。为解决这一问题，我们提出了一种针对受降雨影响的毫米波xhaul网络的主动切片准入控制框架。我们的目标是提升网络性能，确保QoS，并优化收益，从而超越标准被动方法的局限性。所提出的框架集成了对未来网络状况的深度学习预测器与基于Q学习的主动切片准入控制机制。我们利用一个密集城区中毫米波xhaul部署的真实数据进行验证，结合了链路容量衰减的现实模型和动态切片需求。广泛评估表明，我们的主动方案在动态链路条件下实现了2-3倍的长期平均收益，提供了一个可扩展且具有韧性的框架，用于自适应准入控制。</div>
</details>
</div>
<div class="card">
<div class="title">Monadic Context Engineering</div>
<div class="meta-line">Authors: Yifan Zhang, Yang Yuan, Mengdi Wang, Andrew Chi-Chih Yao</div>
<div class="meta-line">First: 2025-12-27T01:52:06+00:00 · Latest: 2026-01-09T17:48:20+00:00</div>
<div class="meta-line">Comments: The authors have decided to withdraw this manuscript, as the ideas presented in the paper are not yet sufficiently mature and require further development and refinement</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.22431v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.22431v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The proliferation of Large Language Models (LLMs) has catalyzed a shift towards autonomous agents capable of complex reasoning and tool use. However, current agent architectures are frequently constructed using imperative, ad hoc patterns. This results in brittle systems plagued by difficulties in state management, error handling, and concurrency. This paper introduces Monadic Context Engineering (MCE), a novel architectural paradigm leveraging the algebraic structures of Functors, Applicative Functors, and Monads to provide a formal foundation for agent design. MCE treats agent workflows as computational contexts where cross-cutting concerns, such as state propagation, short-circuiting error handling, and asynchronous execution, are managed intrinsically by the algebraic properties of the abstraction. We demonstrate how Monads enable robust sequential composition, how Applicatives provide a principled structure for parallel execution, and crucially, how Monad Transformers allow for the systematic composition of these capabilities. This layered approach enables developers to construct complex, resilient, and efficient AI agents from simple, independently verifiable components. We further extend this framework to describe Meta-Agents, which leverage MCE for generative orchestration, dynamically creating and managing sub-agent workflows through metaprogramming.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>单子上下文工程</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）的普及推动了向能够进行复杂推理和工具使用的自主代理的转变。然而，当前的代理架构通常采用命令式、临时性的模式构建，导致系统脆弱，状态管理、错误处理和并发控制等方面存在困难。本文引入了一种新颖的架构范式——单子上下文工程（MCE），利用函子、应用函子和单子的代数结构为代理设计提供形式化基础。MCE将代理工作流视为计算上下文，其中诸如状态传播、短路错误处理和异步执行等横切关注点，通过抽象的代数特性内在地进行管理。我们展示了单子如何实现稳健的顺序组合，应用函子如何为并行执行提供原理性的结构，以及关键的是，单子转换器如何系统地组合这些能力。这种分层方法使开发者能够从简单、可独立验证的组件构建出复杂、健壮且高效的AI代理。我们进一步扩展了这一框架，用于描述元代理（Meta-Agents），它们利用MCE进行生成性编排，通过元编程动态创建和管理子代理工作流。</div>
</details>
</div>
<div class="card">
<div class="title">QueryGym: Step-by-Step Interaction with Relational Databases</div>
<div class="meta-line">Authors: Haritha Ananthakrishnan, Harsha Kokel, Kelsey Sikes, Debarun Bhattacharjya, Michael Katz, Shirin Sohrabi, Kavitha Srinivas</div>
<div class="meta-line">First: 2025-09-25T22:48:49+00:00 · Latest: 2026-01-09T17:48:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.21674v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.21674v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce QueryGym, an interactive environment for building, testing, and evaluating LLM-based query planning agents. Existing frameworks often tie agents to specific query language dialects or obscure their reasoning; QueryGym instead requires agents to construct explicit sequences of relational algebra operations, ensuring engine-agnostic evaluation and transparent step-by-step planning. The environment is implemented as a Gymnasium interface that supplies observations -- including schema details, intermediate results, and execution feedback -- and receives actions that represent database exploration (e.g., previewing tables, sampling column values, retrieving unique values) as well as relational algebra operations (e.g., filter, project, join). We detail the motivation and the design of the environment. In the demo, we showcase the utility of the environment by contrasting it with contemporary LLMs that query databases. QueryGym serves as a practical testbed for research in error remediation, transparency, and reinforcement learning for query generation. For the associated demo, see https://ibm.biz/QueryGym.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>QueryGym：与关系型数据库的逐步交互</div>
<div class="mono" style="margin-top:8px">我们介绍了QueryGym，一个用于构建、测试和评估基于LLM的查询规划代理的交互环境。现有框架通常将代理绑定到特定的查询语言方言，或隐藏其推理过程；QueryGym则要求代理构建显式的、由关系代数操作组成的序列，从而确保与数据库引擎无关的评估和透明的逐步规划。该环境被实现为一个Gymnasium接口，提供包括模式细节、中间结果和执行反馈在内的观察信息，并接收代表数据库探索（如预览表、采样列值、获取唯一值）以及关系代数操作（如过滤、投影、连接）的动作。我们详细阐述了该环境的动机和设计。在演示中，我们通过与当前LLM查询数据库的方法进行对比，展示了该环境的实用性。QueryGym为查询生成中的错误修复、透明度和强化学习研究提供了一个实际的测试平台。相关演示请参见 https://ibm.biz/QueryGym。</div>
</details>
</div>
<div class="card">
<div class="title">DeePM: Regime-Robust Deep Learning for Systematic Macro Portfolio Management</div>
<div class="meta-line">Authors: Kieran Wood, Stephen J. Roberts, Stefan Zohren</div>
<div class="meta-line">First: 2026-01-09T17:47:32+00:00 · Latest: 2026-01-09T17:47:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05975v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05975v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose DeePM (Deep Portfolio Manager), a structured deep-learning macro portfolio manager trained end-to-end to maximize a robust, risk-adjusted utility. DeePM addresses three fundamental challenges in financial learning: (1) it resolves the asynchronous &quot;ragged filtration&quot; problem via a Directed Delay (Causal Sieve) mechanism that prioritizes causal impulse-response learning over information freshness; (2) it combats low signal-to-noise ratios via a Macroeconomic Graph Prior, regularizing cross-asset dependence according to economic first principles; and (3) it optimizes a distributionally robust objective where a smooth worst-window penalty serves as a differentiable proxy for Entropic Value-at-Risk (EVaR) - a window-robust utility encouraging strong performance in the most adverse historical subperiods. In large-scale backtests from 2010-2025 on 50 diversified futures with highly realistic transaction costs, DeePM attains net risk-adjusted returns that are roughly twice those of classical trend-following strategies and passive benchmarks, solely using daily closing prices. Furthermore, DeePM improves upon the state-of-the-art Momentum Transformer architecture by roughly fifty percent. The model demonstrates structural resilience across the 2010s &quot;CTA (Commodity Trading Advisor) Winter&quot; and the post-2020 volatility regime shift, maintaining consistent performance through the pandemic, inflation shocks, and the subsequent higher-for-longer environment. Ablation studies confirm that strictly lagged cross-sectional attention, graph prior, principled treatment of transaction costs, and robust minimax optimization are the primary drivers of this generalization capability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DeePM：用于系统性宏观投资组合管理的制度稳健深度学习</div>
<div class="mono" style="margin-top:8px">我们提出DeePM（深度投资组合管理器），这是一种结构化的深度学习宏观投资组合管理模型，通过端到端训练最大化一种稳健、风险调整后的效用。DeePM解决了金融学习中的三个根本性挑战：(1) 通过有向延迟（因果筛）机制解决异步&quot;不规则过滤&quot;问题，优先进行因果脉冲响应学习而非信息新鲜度；(2) 通过宏观经济学图先验来对抗低信噪比，根据经济第一原理对跨资产依赖进行正则化；(3) 优化一个分布稳健的目标，其中平滑的最坏窗口惩罚作为可微分的Entropic Value-at-Risk (EVaR) 的代理，这是一种鼓励在最不利历史子周期中表现强劲的窗口稳健效用。在2010-2025年间对50种高度现实交易成本的多样化期货进行的大规模回测中，DeePM获得的净风险调整后收益大约是经典趋势跟踪策略和被动基准的两倍，仅使用每日收盘价。此外，DeePM在Momentum Transformer架构上提升了约50%的性能。该模型在2010年代的&quot;CTA（商品交易顾问）寒冬&quot;以及2020年后的波动率制度转变中展现出结构稳健性，通过疫情、通胀冲击以及随后的长期高波动环境，保持了稳定的表现。消融研究确认，严格滞后的跨截面注意力、图先验、对交易成本的原理性处理以及稳健的最小最大优化是其泛化能力的主要驱动因素。</div>
</details>
</div>
<div class="card">
<div class="title">Controlled Automatic Task-Specific Synthetic Data Generation for Hallucination Detection</div>
<div class="meta-line">Authors: Yong Xie, Karan Aggarwal, Aitzaz Ahmad, Stephen Lau</div>
<div class="meta-line">Venue: KDD 2024</div>
<div class="meta-line">First: 2024-10-16T06:31:59+00:00 · Latest: 2026-01-09T17:41:42+00:00</div>
<div class="meta-line">Comments: 30th ACM SIGKDD Conference on Knowledge Discovery and Data Mining (ACM KDD 2024). Accepted by Workshop on Evaluation and Trustworthiness of Generative AI Models</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.12278v2">Abs</a> · <a href="https://arxiv.org/pdf/2410.12278v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a novel approach to automatically generate non-trivial task-specific synthetic datasets for hallucination detection. Our approach features a two-step generation-selection pipeline, using hallucination pattern guidance and a language style alignment during generation. Hallucination pattern guidance leverages the most important task-specific hallucination patterns while language style alignment aligns the style of the synthetic dataset with benchmark text. To obtain robust supervised detectors from synthetic datasets, we also adopt a data mixture strategy to improve performance robustness and generalization. Our results on three datasets show that our generated hallucination text is more closely aligned with non-hallucinated text versus baselines, to train hallucination detectors with better generalization. Our hallucination detectors trained on synthetic datasets outperform in-context-learning (ICL)-based detectors by a large margin of 32%. Our extensive experiments confirm the benefits of our approach with cross-task and cross-generator generalization. Our data-mixture-based training further improves the generalization and robustness of hallucination detection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于幻觉检测的受控自动任务特定合成数据生成</div>
<div class="mono" style="margin-top:8px">我们提出了一种新颖的方法，用于自动生成非平凡的任务特定合成数据集以用于幻觉检测。我们的方法包含一个两步生成-选择流程，在生成过程中使用幻觉模式引导和语言风格对齐。幻觉模式引导利用了最重要的任务特定幻觉模式，而语言风格对齐则使合成数据集的风格与基准文本保持一致。为了从合成数据集中获得稳健的监督检测器，我们还采用了一种数据混合策略以提高性能的稳健性和泛化能力。我们在三个数据集上的结果表明，我们生成的幻觉文本与非幻觉文本的匹配度高于基线方法，从而可以训练具有更好泛化能力的幻觉检测器。我们的基于合成数据集的幻觉检测器在性能上比基于上下文学习（ICL）的检测器高出32%。我们广泛的实验验证了该方法在跨任务和跨生成器泛化方面的优势。基于数据混合的训练进一步提升了幻觉检测的泛化能力和稳健性。</div>
</details>
</div>
<div class="card">
<div class="title">Towards AI-Native Software Engineering (SE 3.0): A Vision and a Challenge Roadmap</div>
<div class="meta-line">Authors: Ahmed E. Hassan, Gustavo A. Oliva, Dayi Lin, Boyuan Chen, Zhen Ming, Jiang</div>
<div class="meta-line">First: 2024-10-08T15:04:07+00:00 · Latest: 2026-01-09T17:38:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.06107v2">Abs</a> · <a href="https://arxiv.org/pdf/2410.06107v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rise of AI-assisted software engineering (SE 2.0), powered by Foundation Models (FMs) and FM-powered coding assistants, has shown promise in improving developer productivity. However, it has also exposed inherent limitations, such as cognitive overload on developers and inefficiencies. We propose a shift towards Software Engineering 3.0 (SE 3.0), an AI-native approach characterized by intent-centric, conversation-oriented development between human developers and AI teammates. SE 3.0 envisions AI systems evolving beyond task-driven copilots into intelligent collaborators, capable of deeply understanding and reasoning about software engineering principles and intents. We outline the key components of the SE 3.0 technology stack, which includes Teammate.next for adaptive and personalized AI partnership, IDE.next for intent-centric conversation-oriented development, Compiler.next for multi-objective code synthesis, and Runtime.next for SLA-aware execution with edge-computing support. Our vision addresses the inefficiencies and cognitive strain of SE 2.0 by fostering a symbiotic relationship between human developers and AI, maximizing their complementary strengths. We also present a roadmap of challenges that must be overcome to realize our vision of SE 3.0. This paper lays the foundation for future discussions on the role of AI in the next era of software engineering.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向AI原生软件工程（SE 3.0）：愿景与挑战路线图</div>
<div class="mono" style="margin-top:8px">AI辅助软件工程（SE 2.0）的兴起，依托基础模型（FMs）和基于FMs的编码助手，展现出提升开发者生产力的潜力。然而，它也暴露出一些固有局限，如开发者认知过载和效率低下。我们提出向软件工程3.0（SE 3.0）转变，这是一种以意图为中心、以对话为导向的人机协作开发方式。SE 3.0设想AI系统超越任务驱动的协作者，成为能够深入理解并推理软件工程原则和意图的智能合作者。我们概述了SE 3.0技术栈的关键组成部分，包括Teammate.next用于自适应和个性化的AI协作、IDE.next用于以意图为中心的对话式开发、Compiler.next用于多目标代码合成，以及Runtime.next用于具有服务等级协议（SLA）意识的执行并支持边缘计算。我们的愿景通过促进人类开发者与AI之间的共生关系，最大化其互补优势，来解决SE 2.0的效率低下和认知负担问题。我们还提出了实现SE 3.0愿景所需克服的挑战路线图。本文为未来关于AI在软件工程下一阶段角色的讨论奠定了基础。</div>
</details>
</div>
<div class="card">
<div class="title">Phased-Array Laser Power Beaming from Cislunar Space to the Lunar Surface</div>
<div class="meta-line">Authors: Slava G. Turyshev</div>
<div class="meta-line">First: 2025-08-14T17:26:08+00:00 · Latest: 2026-01-09T17:38:15+00:00</div>
<div class="meta-line">Comments: 49 pages, 5 figures and 30 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.10855v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.10855v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a time-dependent, end-to-end framework for laser power beaming from cislunar orbits to the lunar surface. The model links on-orbit generation (solar arrays and wall-plug to optical), terrain-masked visibility and range, beam propagation with realistic divergence and jitter, and surface conversion with thermal and dust limits, returning delivered daily energy. Baseline loads for early polar activities (habitat survival, mobility, comm/nav, pilot ISRU) set target Wh\,day$^{-1}$ and are used consistently in scaling laws and design maps. A near-rectilinear halo orbit (NRHO) to a Shackleton-rim site provides a worked example: for a 2\,m-class phased array at 1064\,nm the reference geometry yields $\sim$0.6--0.8\,kWh\,day$^{-1}$ to a 1\,m$^2$ receiver (about 28\,W averaged over the day). We place this result in context by comparing on the same daily-energy metric to surface photovoltaics (PV) with storage and to compact fission, and by showing how delivered energy scales nearly linearly with transmit power and as $D_{\rm eff}^{2}$ via encircled-energy capture, with a multiplicative gain from visibility (constellations). The same framework indicates practical regimes already within reach: e.g., a 10\,m effective-aperture optical phased array at $P_{\rm tx}=100$\,kW delivers $\sim$30--50\,kWh\,day$^{-1}$ at polar sites with typical single-orbiter visibility, as quantified by the delivered-energy and sizing maps. Thus, laser beaming is mass-competitive where darkness or permanent shadow forces deep storage for PV, or where distributed and duty-cycled users can amortize a shared transmitter; compact fission retains advantage for continuous multi-kW baseload at fixed sites.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从地月轨道向月球表面进行相控阵激光能量传输</div>
<div class="mono" style="margin-top:8px">我们提出了一种时间依赖的端到端激光能量传输框架，从地月轨道向月球表面进行能量传输。该模型将轨道上发电（太阳能电池板和电光转换）、地形遮挡可见性与距离、具有现实离散度和抖动的光束传播，以及受热和尘埃限制的表面能量转换联系起来，最终返回每日输送的能量。早期极地活动的基准负载（栖息地生存、移动性、通信/导航、试点ISRU）设定了目标Wh\,day$^{-1}$，并在缩放定律和设计图中一致使用。一个近矩形晕轨道（NRHO）到Shackleton边缘的示例表明：对于一个2\,m级的1064\,nm相控阵，参考几何结构可向1\,m$^2$接收器输送$\sim$0.6--0.8\,kWh\,day$^{-1}$（约28\,W的日均平均功率）。我们通过与具有储能的表面光伏（PV）和紧凑型裂变系统在相同每日能量指标上的比较，以及展示输送能量如何几乎与发射功率线性相关，并通过包围能量捕获与$D_{\rm eff}^{2}$成比例，从而将这一结果置于背景中。可见性（星座）的乘法增益也有所体现。该框架还表明，实际应用的运行模式已经可以实现：例如，一个10\,m有效孔径的光学相控阵在$P_{\rm tx}=100$\,kW时，可在具有典型单轨道器可见性的极地站点输送$\sim$30--50\,kWh\,day$^{-1}$，如输送能量和尺寸图所量化。因此，在黑暗或永久阴影迫使光伏系统进行深度储能的区域，或在分布式、间歇性用户可以分摊共享发射器的区域，激光能量传输具有质量竞争力；而在固定站点需要连续多千瓦基载的场景中，紧凑型裂变系统仍具有优势。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present a time-dependent, end-to-end framework for laser power beaming from cislunar orbits to the lunar surface.</div>
</details>
</div>
<div class="card">
<div class="title">VideoAR: Autoregressive Video Generation via Next-Frame &amp; Scale Prediction</div>
<div class="meta-line">Authors: Longbin Ji, Xiaoxiong Liu, Junyuan Shang, Shuohuan Wang, Yu Sun, Hua Wu, Haifeng Wang</div>
<div class="meta-line">First: 2026-01-09T17:34:59+00:00 · Latest: 2026-01-09T17:34:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.05966v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.05966v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in video generation have been dominated by diffusion and flow-matching models, which produce high-quality results but remain computationally intensive and difficult to scale. In this work, we introduce VideoAR, the first large-scale Visual Autoregressive (VAR) framework for video generation that combines multi-scale next-frame prediction with autoregressive modeling. VideoAR disentangles spatial and temporal dependencies by integrating intra-frame VAR modeling with causal next-frame prediction, supported by a 3D multi-scale tokenizer that efficiently encodes spatio-temporal dynamics. To improve long-term consistency, we propose Multi-scale Temporal RoPE, Cross-Frame Error Correction, and Random Frame Mask, which collectively mitigate error propagation and stabilize temporal coherence. Our multi-stage pretraining pipeline progressively aligns spatial and temporal learning across increasing resolutions and durations. Empirically, VideoAR achieves new state-of-the-art results among autoregressive models, improving FVD on UCF-101 from 99.5 to 88.6 while reducing inference steps by over 10x, and reaching a VBench score of 81.74-competitive with diffusion-based models an order of magnitude larger. These results demonstrate that VideoAR narrows the performance gap between autoregressive and diffusion paradigms, offering a scalable, efficient, and temporally consistent foundation for future video generation research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VideoAR：通过下一帧与尺度预测的自回归视频生成</div>
<div class="mono" style="margin-top:8px">近期视频生成领域的进展主要由扩散模型和流匹配模型主导，这些模型虽然能生成高质量结果，但计算成本高且难以扩展。本文提出VideoAR，这是首个结合多尺度下一帧预测与自回归建模的大规模视觉自回归（VAR）框架。VideoAR通过整合帧内VAR建模与因果下一帧预测，分离了空间与时间依赖性，并借助3D多尺度分词器高效编码时空动态。为提升长期一致性，我们提出了多尺度时间RoPE、跨帧误差校正和随机帧掩码，共同抑制误差传播并稳定时间一致性。我们的多阶段预训练流程逐步在更高分辨率和更长时序上对齐空间与时间学习。实验表明，VideoAR在自回归模型中取得了新的最先进结果，在UCF-101数据集上将FVD从99.5提升至88.6，推理步骤减少超过10倍，并达到VBench评分81.74，与基于扩散的模型相比具有数量级的优势。这些结果表明，VideoAR缩小了自回归与扩散范式之间的性能差距，为未来视频生成研究提供了一个可扩展、高效且时间一致的基础。</div>
</details>
</div>
<div class="card">
<div class="title">Efficient Bayesian Computation Using Plug-and-Play Priors for Poisson Inverse Problems</div>
<div class="meta-line">Authors: Teresa Klatzer, Savvas Melidonis, Marcelo Pereyra, Konstantinos C. Zygalakis</div>
<div class="meta-line">First: 2025-03-20T15:17:05+00:00 · Latest: 2026-01-09T17:33:02+00:00</div>
<div class="meta-line">Comments: 35 pages, 19 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.16222v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.16222v2">PDF</a> · <a href="https://github.com/freyyia/pnp-langevin-poisson">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper studies plug-and-play (PnP) Langevin sampling strategies for Bayesian inference in low-photon Poisson imaging problems, a challenging class of problems with significant applications in astronomy, medicine, and biology. PnP Langevin sampling offers a powerful framework for Bayesian image restoration, enabling accurate point estimation as well as advanced inference tasks, including uncertainty quantification and visualization analyses, and empirical Bayesian inference for automatic model parameter tuning. Herein, we leverage and adapt recent developments in this framework to tackle challenging imaging problems involving weakly informative Poisson data. Existing PnP Langevin algorithms are not well-suited for low-photon Poisson imaging due to high solution uncertainty and poor regularity properties, such as exploding gradients and non-negativity constraints. To address these challenges, we explore two strategies for extending Langevin PnP sampling to Poisson imaging models: (i) an accelerated PnP Langevin method that incorporates boundary reflections and a Poisson likelihood approximation and (ii) a mirror sampling algorithm that leverages a Riemannian geometry to handle the constraints and the poor regularity of the likelihood without approximations. The effectiveness of these approaches is evaluated and contrasted through extensive numerical experiments and comparisons with state-of-the-art methods. The source code accompanying this paper is available at https://github.com/freyyia/pnp-langevin-poisson.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用插件式先验用于泊松逆问题的高效贝叶斯计算</div>
<div class="mono" style="margin-top:8px">本文研究了用于低光子泊松成像问题的插件式（PnP）朗之万采样策略，这是一个具有广泛应用的具有挑战性的问题类别，涵盖天文学、医学和生物学等领域。PnP朗之万采样为贝叶斯图像恢复提供了一个强大的框架，能够实现精确的点估计以及高级推理任务，包括不确定性量化、可视化分析和经验贝叶斯推理以实现自动模型参数调优。本文利用并改进了该框架中的最新发展，以解决涉及弱信息泊松数据的复杂成像问题。现有的PnP朗之万算法由于高解不确定性及较差的正则性属性（如梯度爆炸和非负性约束）而不适合低光子泊松成像。为应对这些挑战，我们探索了两种扩展朗之万PnP采样到泊松成像模型的策略：(i) 一种加速的PnP朗之万方法，结合了边界反射和泊松似然近似；(ii) 一种镜像采样算法，利用黎曼几何处理约束和似然的不良正则性，无需近似。通过大量数值实验和与现有先进方法的比较，评估了这些方法的有效性。本文的源代码可在 https://github.com/freyyia/pnp-langevin-poisson 获取。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
