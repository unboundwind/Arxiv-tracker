<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-07 03:55</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260107_0355</div>
    <div class="row"><div class="card">
<div class="title">Heterogeneous Low-Bandwidth Pre-Training of LLMs</div>
<div class="meta-line">Authors: Yazan Obeidi, Amir Sarfi, Joel Lidin, Paul Janson, Eugene Belilovsky</div>
<div class="meta-line">First: 2026-01-05T18:59:57+00:00 · Latest: 2026-01-05T18:59:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02360v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02360v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-training large language models (LLMs) increasingly requires distributed compute, yet bandwidth constraints make it difficult to scale beyond well-provisioned datacenters-especially when model parallelism forces frequent, large inter-device communications. We study whether SparseLoCo, a low-communication data parallel method based on infrequent synchronization and sparse pseudo-gradient exchange, can be combined with low-bandwidth pipeline model parallelism via activation and activation-gradient compression. We introduce a heterogeneous distributed training framework where some participants host full replicas on high-bandwidth interconnects, while resource-limited participants are grouped to jointly instantiate a replica using pipeline parallelism with subspace-projected inter-stage communication. To make the recently introduced subspace pipeline compression compatible with SparseLoCo, we study a number of adaptations. Across large-scale language modeling experiments (178M-1B parameters) on standard pretraining corpora, we find that activation compression composes with SparseLoCo at modest cost, while selective (heterogeneous) compression consistently improves the loss-communication tradeoff relative to compressing all replicas-especially at aggressive compression ratios. These results suggest a practical path to incorporating low-bandwidth model parallelism and heterogeneous participants into LLM pre-training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>异构低带宽LLMs预训练</div>
<div class="mono" style="margin-top:8px">预训练大型语言模型（LLMs）日益需要分布式计算，但带宽限制使得难以扩展到资源充足的数据中心，尤其是在模型并行性要求频繁且大量的设备间通信时。我们研究了SparseLoCo，一种基于不频繁同步和稀疏伪梯度交换的低通信量数据并行方法，是否可以通过激活和激活梯度压缩与低带宽流水线模型并行性相结合。我们引入了一个异构分布式训练框架，其中一些参与者在高带宽互连上托管完整副本，而资源受限的参与者则被分组，通过流水线并行性联合实例化副本，并使用子空间投影的阶段间通信。为了使最近提出的子空间流水线压缩与SparseLoCo兼容，我们研究了多种适应方法。在标准预训练语料库上的大规模语言建模实验（1.78亿至10亿参数）中，我们发现激活压缩在较低成本下可以与SparseLoCo结合，而选择性（异构）压缩在压缩比较高时，相对于压缩所有副本，能更有效地改善损失与通信量之间的权衡。这些结果表明了一条将低带宽模型并行性和异构参与者纳入LLMs预训练的实用路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Pre-training large language models (LLMs) increasingly requires distributed compute, yet bandwidth constraints make it difficult to scale beyond well-provisioned datacenters-especially when model parallelism forces frequent, large inter-device communications.</div>
</details>
</div>
<div class="card">
<div class="title">ExposeAnyone: Personalized Audio-to-Expression Diffusion Models Are Robust Zero-Shot Face Forgery Detectors</div>
<div class="meta-line">Authors: Kaede Shiohara, Toshihiko Yamasaki, Vladislav Golyanik</div>
<div class="meta-line">First: 2026-01-05T18:59:54+00:00 · Latest: 2026-01-05T18:59:54+00:00</div>
<div class="meta-line">Comments: 17 pages, 8 figures, 11 tables; project page: https://mapooon.github.io/ExposeAnyonePage/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02359v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02359v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://mapooon.github.io/ExposeAnyonePage/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Detecting unknown deepfake manipulations remains one of the most challenging problems in face forgery detection. Current state-of-the-art approaches fail to generalize to unseen manipulations, as they primarily rely on supervised training with existing deepfakes or pseudo-fakes, which leads to overfitting to specific forgery patterns. In contrast, self-supervised methods offer greater potential for generalization, but existing work struggles to learn discriminative representations only from self-supervision. In this paper, we propose ExposeAnyone, a fully self-supervised approach based on a diffusion model that generates expression sequences from audio. The key idea is, once the model is personalized to specific subjects using reference sets, it can compute the identity distances between suspected videos and personalized subjects via diffusion reconstruction errors, enabling person-of-interest face forgery detection. Extensive experiments demonstrate that 1) our method outperforms the previous state-of-the-art method by 4.22 percentage points in the average AUC on DF-TIMIT, DFDCP, KoDF, and IDForge datasets, 2) our model is also capable of detecting Sora2-generated videos, where the previous approaches perform poorly, and 3) our method is highly robust to corruptions such as blur and compression, highlighting the applicability in real-world face forgery detection.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ExposeAnyone: 基于扩散模型的个性化音频到表情生成模型在零样本人脸伪造检测中表现稳健</div>
<div class="mono" style="margin-top:8px">检测未知的深度伪造操作仍然是人脸伪造检测中最具有挑战性的问题之一。当前最先进的方法由于主要依赖于现有深度伪造或伪伪造数据的监督训练，无法泛化到未见过的伪造操作，导致对特定伪造模式过拟合。相比之下，自监督方法在泛化方面具有更大的潜力，但现有工作难以仅通过自监督学习来获取具有判别性的表示。本文提出ExposeAnyone，这是一种完全自监督的方法，基于扩散模型，从音频生成表情序列。其核心思想是，一旦模型通过参考集个性化到特定对象，就可以通过扩散重建误差计算疑似视频与个性化对象之间的身份距离，从而实现对目标人物的人脸伪造检测。大量实验表明，1) 我们的方法在DF-TIMIT、DFDCP、KoDF和IDForge数据集上的平均AUC比之前最先进的方法高出4.22个百分点；2) 我们模型还能够检测Sora2生成的视频，而之前的方法在这些视频上表现不佳；3) 我们的方法对模糊和压缩等破坏具有高度鲁棒性，突显了其在现实世界人脸伪造检测中的适用性。</div>
</details>
</div>
<div class="card">
<div class="title">EmoNet-Voice: A Fine-Grained, Expert-Verified Benchmark for Speech Emotion Detection</div>
<div class="meta-line">Authors: Christoph Schuhmann, Robert Kaczmarczyk, Gollam Rabby, Felix Friedrich, Maurice Kraus, Kourosh Nadi, Huu Nguyen, Kristian Kersting, Sören Auer</div>
<div class="meta-line">First: 2025-06-11T15:06:59+00:00 · Latest: 2026-01-05T18:59:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.09827v3">Abs</a> · <a href="https://arxiv.org/pdf/2506.09827v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Speech emotion recognition (SER) systems are constrained by existing datasets that typically cover only 6-10 basic emotions, lack scale and diversity, and face ethical challenges when collecting sensitive emotional states. We introduce EMONET-VOICE, a comprehensive resource addressing these limitations through two components: (1) EmoNet-Voice Big, a 5,000-hour multilingual pre-training dataset spanning 40 fine-grained emotion categories across 11 voices and 4 languages, and (2) EmoNet-Voice Bench, a rigorously validated benchmark of 4,7k samples with unanimous expert consensus on emotion presence and intensity levels. Using state-of-the-art synthetic voice generation, our privacy-preserving approach enables ethical inclusion of sensitive emotions (e.g., pain, shame) while maintaining controlled experimental conditions. Each sample underwent validation by three psychology experts. We demonstrate that our Empathic Insight models trained on our synthetic data achieve strong real-world dataset generalization, as tested on EmoDB and RAVDESS. Furthermore, our comprehensive evaluation reveals that while high-arousal emotions (e.g., anger: 95% accuracy) are readily detected, the benchmark successfully exposes the difficulty of distinguishing perceptually similar emotions (e.g., sadness vs. distress: 63% discrimination), providing quantifiable metrics for advancing nuanced emotion AI. EMONET-VOICE establishes a new paradigm for large-scale, ethically-sourced, fine-grained SER research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EmoNet-Voice：一种用于语音情感检测的细粒度、专家验证基准</div>
<div class="mono" style="margin-top:8px">现有的语音情感识别（SER）系统受到数据集的限制，这些数据集通常仅涵盖6-10种基本情感，缺乏规模和多样性，并且在收集敏感情感状态时面临伦理挑战。我们引入了EMONET-VOICE，通过两个组成部分解决这些限制：(1) EmoNet-Voice Big，一个包含5000小时多语言预训练数据集，覆盖11种语音和4种语言的40种细粒度情感类别；(2) EmoNet-Voice Bench，一个经过严格验证的基准，包含4,7k个样本，所有样本在情感存在和强度等级上均获得专家一致共识。通过使用最先进的合成语音生成技术，我们的隐私保护方法能够在保持受控实验条件的同时，伦理地包含敏感情感（如疼痛、羞愧）。每个样本都由三位心理学专家进行了验证。我们证明，基于我们合成数据训练的Empathic Insight模型在实际数据集上具有良好的泛化能力，如在EmoDB和RAVDESS上测试的结果所示。此外，我们的全面评估表明，尽管高唤醒情感（如愤怒：95%准确率）易于检测，但该基准成功揭示了区分感知相似情感（如悲伤与痛苦：63%区分度）的难度，为推进细致情感AI提供了可量化的指标。EMONET-VOICE为大规模、伦理来源的细粒度SER研究建立了一个新的范式。</div>
</details>
</div>
<div class="card">
<div class="title">LIMOncello: Iterated Error-State Kalman Filter on the SGal(3) Manifold for Fast LiDAR-Inertial Odometry</div>
<div class="meta-line">Authors: Carlos Pérez-Ruiz, Joan Solà</div>
<div class="meta-line">First: 2025-12-22T16:50:10+00:00 · Latest: 2026-01-05T18:58:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.19567v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.19567v2">PDF</a> · <a href="https://github.com/CPerezRuiz335/LIMOncello">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work introduces LIMOncello, a tightly coupled LiDAR-Inertial Odometry system that models 6-DoF motion on the $\mathrm{SGal}(3)$ manifold within an iterated error-state Kalman filter backend. Compared to state representations defined on $\mathrm{SO}(3)\times\mathbb{R}^6$, the use of $\mathrm{SGal}(3)$ provides a coherent and numerically stable discrete-time propagation model that helps limit drift in low-observability conditions.
  LIMOncello also includes a lightweight incremental i-Octree mapping backend that enables faster updates and substantially lower memory usage than incremental kd-tree style map structures, without relying on locality-restricted search heuristics. Experiments on multiple real-world datasets show that LIMOncello achieves competitive accuracy while improving robustness in geometrically sparse environments. The system maintains real-time performance with stable memory growth and is released as an extensible open-source implementation at https://github.com/CPerezRuiz335/LIMOncello.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LIMOncello：基于SGal(3)流形的快速激光雷达惯性里程计的迭代误差状态卡尔曼滤波器</div>
<div class="mono" style="margin-top:8px">本工作引入了LIMOncello，这是一个紧密耦合的激光雷达惯性里程计系统，它在一个迭代误差状态卡尔曼滤波器后端中，将6自由度运动建模为$\mathrm{SGal}(3)$流形。与在$\mathrm{SO}(3)\times\mathbb{R}^6$上定义的状态表示相比，使用$\mathrm{SGal}(3)$提供了一种连贯且数值稳定的离散时间传播模型，有助于在观测性较低的条件下限制漂移。
LIMOncello还包括一个轻量级的增量i-Octree映射后端，使得更新速度更快，内存使用显著降低，而无需依赖局部性受限的搜索启发式方法。在多个真实世界数据集上的实验表明，LIMOncello在几何稀疏环境中实现了竞争力的精度并提高了鲁棒性。该系统保持实时性能，内存增长稳定，并在https://github.com/CPerezRuiz335/LIMOncello上作为可扩展的开源实现发布。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This work introduces LIMOncello, a tightly coupled LiDAR-Inertial Odometry system that models 6-DoF motion on the $\mathrm{SGal}(3)$ manifold within an iterated error-state Kalman filter backend.</div>
</details>
</div>
<div class="card">
<div class="title">VINO: A Unified Visual Generator with Interleaved OmniModal Context</div>
<div class="meta-line">Authors: Junyi Chen, Tong He, Zhoujie Fu, Pengfei Wan, Kun Gai, Weicai Ye</div>
<div class="meta-line">First: 2026-01-05T18:56:34+00:00 · Latest: 2026-01-05T18:56:34+00:00</div>
<div class="meta-line">Comments: Project page: https://sotamak1r.github.io/VINO-web/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02358v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02358v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sotamak1r.github.io/VINO-web/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present VINO, a unified visual generator that performs image and video generation and editing within a single framework. Instead of relying on task-specific models or independent modules for each modality, VINO uses a shared diffusion backbone that conditions on text, images and videos, enabling a broad range of visual creation and editing tasks under one model. Specifically, VINO couples a vision-language model (VLM) with a Multimodal Diffusion Transformer (MMDiT), where multimodal inputs are encoded as interleaved conditioning tokens, and then used to guide the diffusion process. This design supports multi-reference grounding, long-form instruction following, and coherent identity preservation across static and dynamic content, while avoiding modality-specific architectural components. To train such a unified system, we introduce a multi-stage training pipeline that progressively expands a video generation base model into a unified, multi-task generator capable of both image and video input and output. Across diverse generation and editing benchmarks, VINO demonstrates strong visual quality, faithful instruction following, improved reference and attribute preservation, and more controllable multi-identity edits. Our results highlight a practical path toward scalable unified visual generation, and the promise of interleaved, in-context computation as a foundation for general-purpose visual creation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VINO：一种结合多模态上下文的统一视觉生成器</div>
<div class="mono" style="margin-top:8px">我们提出了VINO，这是一种统一的视觉生成器，能够在单一框架内执行图像和视频的生成与编辑。与依赖任务特定模型或独立模块处理每种模态不同，VINO使用一个共享的扩散主干，通过文本、图像和视频进行条件建模，从而在一个模型下支持广泛的视觉创作和编辑任务。具体而言，VINO将视觉语言模型（VLM）与多模态扩散Transformer（MMDiT）结合，其中多模态输入被编码为交错的条件标记，然后用于指导扩散过程。这种设计支持多参考定位、长文本指令遵循以及静态和动态内容之间的连贯身份保持，同时避免了特定模态的架构组件。为了训练这种统一系统，我们引入了一个多阶段训练流程，逐步将视频生成基础模型扩展为能够处理图像和视频输入输出的统一多任务生成器。在多种生成和编辑基准测试中，VINO展示了出色的视觉质量、忠实的指令遵循、改进的参考和属性保持，以及更可控的多身份编辑能力。我们的结果表明了一条实现可扩展统一视觉生成的实用路径，并突显了交错上下文计算作为通用视觉创作基础的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">SpatialBench: Can Agents Analyze Real-World Spatial Biology Data?</div>
<div class="meta-line">Authors: Kenny Workman, Zhen Yang, Harihara Muralidharan, Hannah Le</div>
<div class="meta-line">Venue: NeurIPS 2024</div>
<div class="meta-line">First: 2025-12-26T07:40:11+00:00 · Latest: 2026-01-05T18:55:51+00:00</div>
<div class="meta-line">Comments: 10 pages, 9 figures, 4 tables; NeurIPS 2024 format</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.21907v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.21907v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spatial transcriptomics assays are rapidly increasing in scale and complexity, making computational analysis a major bottleneck in biological discovery. Although frontier AI agents have improved dramatically at software engineering and general data analysis, it remains unclear whether they can extract biological insight from messy, real-world spatial datasets. We introduce SpatialBench, a benchmark of 146 verifiable problems derived from practical spatial analysis workflows spanning five spatial technologies and seven task categories. Each problem provides a snapshot of experimental data immediately prior to an analysis step and a deterministic grader that evaluates recovery of a key biological result. Benchmark data on frontier models shows that base model accuracy remains low (20-38% across model families), with strong model-task and model-platform interactions. Harness design has a large empirical effect on performance, indicating that tools, prompts, control flow, and execution environment should be evaluated and improved as first-class objects. SpatialBench serves both as a measurement tool and a diagnostic lens for developing agents that can interact with real spatial datasets faithfully, transparently, and reproducibly.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpatialBench：智能体能否分析真实世界的空间生物学数据？</div>
<div class="mono" style="margin-top:8px">空间转录组学检测技术在规模和复杂性上迅速增长，使得计算分析成为生物发现中的主要瓶颈。尽管前沿AI智能体在软件工程和通用数据分析方面取得了显著进步，但尚不清楚它们是否能够从杂乱的真实世界空间数据集中提取生物学见解。我们引入了SpatialBench，这是一个由五个空间技术及七个任务类别中实际空间分析工作流程衍生出的146个可验证问题的基准测试集。每个问题都提供分析步骤前的实验数据快照，并包含一个确定性评分器，用于评估关键生物学结果的恢复情况。对前沿模型的基准测试数据表明，基础模型的准确性仍然较低（在不同模型家族中为20-38%），且模型与任务、平台之间存在显著的交互作用。工具设计对性能有显著的实证影响，表明工具、提示、控制流和执行环境应作为首要对象进行评估和改进。SpatialBench不仅是一个测量工具，还为开发能够真实、透明且可重复地与真实空间数据集交互的智能体提供了一个诊断视角。</div>
</details>
</div>
<div class="card">
<div class="title">DARC: Drum accompaniment generation with fine-grained rhythm control</div>
<div class="meta-line">Authors: Trey Brosnan</div>
<div class="meta-line">First: 2026-01-05T18:55:43+00:00 · Latest: 2026-01-05T18:55:43+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02357v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02357v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In music creation, rapid prototyping is essential for exploring and refining ideas, yet existing generative tools often fall short when users require both structural control and stylistic flexibility. Prior approaches in stem-to-stem generation can condition on other musical stems but offer limited control over rhythm, and timbre-transfer methods allow users to specify specific rhythms, but cannot condition on musical context. We introduce DARC, a generative drum accompaniment model that conditions both on musical context from other stems and explicit rhythm prompts such as beatboxing or tapping tracks. Using parameter-efficient fine-tuning, we augment STAGE, a state-of-the-art drum stem generator, with fine-grained rhythm control while maintaining musical context awareness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DARC：通过细粒度节奏控制生成鼓伴奏</div>
<div class="mono" style="margin-top:8px">在音乐创作中，快速原型设计对于探索和打磨创意至关重要，但现有的生成工具在用户需要结构控制和风格灵活性时往往表现不足。以往的音轨到音轨生成方法可以基于其他音乐音轨进行条件生成，但对节奏的控制有限；而音色迁移方法允许用户指定特定节奏，但无法基于音乐上下文进行条件生成。我们引入DARC，这是一种能够同时基于其他音轨的音乐上下文和显式的节奏提示（如打拍子或打拍子音轨）生成鼓伴奏的生成模型。通过参数高效的微调，我们在STAGE（当前最先进的鼓音轨生成器）中加入了细粒度的节奏控制，同时保持对音乐上下文的感知能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In music creation, rapid prototyping is essential for exploring and refining ideas, yet existing generative tools often fall short when users require both structural control and stylistic flexibility.</div>
</details>
</div>
<div class="card">
<div class="title">Talk2Move: Reinforcement Learning for Text-Instructed Object-Level Geometric Transformation in Scenes</div>
<div class="meta-line">Authors: Jing Tan, Zhaoyang Zhang, Yantao Shen, Jiarui Cai, Shuo Yang, Jiajun Wu, Wei Xia, Zhuowen Tu, Stefano Soatto</div>
<div class="meta-line">First: 2026-01-05T18:55:32+00:00 · Latest: 2026-01-05T18:55:32+00:00</div>
<div class="meta-line">Comments: Project page: https://sparkstj.github.io/talk2move</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02356v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02356v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sparkstj.github.io/talk2move">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Talk2Move, a reinforcement learning (RL) based diffusion framework for text-instructed spatial transformation of objects within scenes. Spatially manipulating objects in a scene through natural language poses a challenge for multimodal generation systems. While existing text-based manipulation methods can adjust appearance or style, they struggle to perform object-level geometric transformations-such as translating, rotating, or resizing objects-due to scarce paired supervision and pixel-level optimization limits. Talk2Move employs Group Relative Policy Optimization (GRPO) to explore geometric actions through diverse rollouts generated from input images and lightweight textual variations, removing the need for costly paired data. A spatial reward guided model aligns geometric transformations with linguistic description, while off-policy step evaluation and active step sampling improve learning efficiency by focusing on informative transformation stages. Furthermore, we design object-centric spatial rewards that evaluate displacement, rotation, and scaling behaviors directly, enabling interpretable and coherent transformations. Experiments on curated benchmarks demonstrate that Talk2Move achieves precise, consistent, and semantically faithful object transformations, outperforming existing text-guided editing approaches in both spatial accuracy and scene coherence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Talk2Move：用于场景中文本指令对象级几何变换的强化学习</div>
<div class="mono" style="margin-top:8px">我们引入Talk2Move，这是一个基于强化学习（RL）的扩散框架，用于场景中对象的空间变换。通过自然语言对场景中的对象进行空间操作，对多模态生成系统提出了挑战。尽管现有的基于文本的操作方法可以调整外观或风格，但由于配对监督稀缺和像素级优化的限制，它们难以执行对象级的几何变换，如平移、旋转或缩放。Talk2Move采用组相对策略优化（GRPO）方法，通过输入图像和轻量级文本变体生成多样化的轨迹，从而探索几何动作，无需昂贵的配对数据。一个由空间奖励引导的模型将几何变换与语言描述对齐，而离策略步骤评估和主动步骤采样则通过关注信息量大的变换阶段来提高学习效率。此外，我们设计了以对象为中心的空间奖励，直接评估位移、旋转和缩放行为，从而实现可解释且连贯的变换。在精心挑选的基准测试中，实验表明Talk2Move实现了精确、一致且语义忠实的对象变换，在空间精度和场景连贯性方面均优于现有的文本引导编辑方法。</div>
</details>
</div>
<div class="card">
<div class="title">Meta-Learning Guided Pruning for Few-Shot Plant Pathology on Edge Devices</div>
<div class="meta-line">Authors: Shahnawaz Alam, Mohammed Mudassir Uddin, Mohammed Kaif Pasha</div>
<div class="meta-line">First: 2026-01-05T18:55:05+00:00 · Latest: 2026-01-05T18:55:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02353v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02353v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Farmers in remote areas need quick and reliable methods for identifying plant diseases, yet they often lack access to laboratories or high-performance computing resources. Deep learning models can detect diseases from leaf images with high accuracy, but these models are typically too large and computationally expensive to run on low-cost edge devices such as Raspberry Pi. Furthermore, collecting thousands of labeled disease images for training is both expensive and time-consuming. This paper addresses both challenges by combining neural network pruning -- removing unnecessary parts of the model -- with few-shot learning, which enables the model to learn from limited examples. This paper proposes Disease-Aware Channel Importance Scoring (DACIS), a method that identifies which parts of the neural network are most important for distinguishing between different plant diseases, integrated into a three-stage Prune-then-Meta-Learn-then-Prune (PMP) pipeline. Experiments on PlantVillage and PlantDoc datasets demonstrate that the proposed approach reduces model size by 78\% while maintaining 92.3\% of the original accuracy, with the compressed model running at 7 frames per second on a Raspberry Pi 4, making real-time field diagnosis practical for smallholder farmers.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于元学习的边缘设备少样本植物病理学剪枝方法</div>
<div class="mono" style="margin-top:8px">偏远地区的农民需要快速可靠的方法来识别植物病害，但他们通常无法获得实验室或高性能计算资源。深度学习模型可以从叶片图像中以高精度检测病害，但这些模型通常太大且计算成本过高，难以在低成本的边缘设备（如Raspberry Pi）上运行。此外，收集数千张带标签的病害图像进行训练既昂贵又耗时。本文通过结合神经网络剪枝（移除模型中不必要的部分）和少样本学习（使模型能够从有限示例中学习），解决了这两个挑战。本文提出了一种Disease-Aware Channel Importance Scoring（DACIS）方法，用于识别神经网络中对区分不同植物病害最为重要的部分，并将其集成到一个三阶段的Prune-then-Meta-Learn-then-Prune（PMP）流程中。在PlantVillage和PlantDoc数据集上的实验表明，所提出的方法将模型大小减少了78%，同时保持了92.3%的原始精度，压缩后的模型在Raspberry Pi 4上以每秒7帧的速度运行，使小农户在田间进行实时诊断成为可能。</div>
</details>
</div>
<div class="card">
<div class="title">An information-theoretic bound on cosmic coherence in finite-volume simulations</div>
<div class="meta-line">Authors: Biswajit Pandey, Anindita Nandi</div>
<div class="meta-line">First: 2026-01-05T18:53:14+00:00 · Latest: 2026-01-05T18:53:14+00:00</div>
<div class="meta-line">Comments: 7 pages, 4 figures, comments are welcome</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02351v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02351v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We quantify the physical memory of the cosmic density field using mutual information between $N$-body snapshots at different redshifts, removing a random baseline to isolate gravitational correlations. The shared mutual information rises with scale, peaks near $\simeq L/8$ (where $L$ is the simulation box size), and declines thereafter. This behaviour is robust to box size and discretization, and identifies the largest coherently retained modes unaffected by missing long-wavelength power, establishing a finite-volume limit on the coherence of cosmic structure with direct implications for homogeneity studies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>有限体积模拟中宇宙相干性的信息论界限</div>
<div class="mono" style="margin-top:8px">我们利用不同红移下的N体快照之间的互信息来量化宇宙密度场的物理记忆，通过去除随机基线以分离引力相关性。共享的互信息随尺度增加而上升，在约L/8（其中L为模拟盒子大小）附近达到峰值，之后逐渐下降。这种行为对盒子大小和离散化具有鲁棒性，识别出不受缺失长波长功率影响的最大相干保留模式，确立了宇宙结构相干性的有限体积限制，并对均匀性研究有直接的启示。</div>
</details>
</div>
<div class="card">
<div class="title">Explainable AI Technique in Lung Cancer Detection Using Convolutional Neural Networks</div>
<div class="meta-line">Authors: Nishan Rai, Sujan Khatri, Devendra Risal</div>
<div class="meta-line">First: 2025-08-13T21:02:38+00:00 · Latest: 2026-01-05T18:51:53+00:00</div>
<div class="meta-line">Comments: 11 pages, 9 figures, 4 tables. Undergraduate research project report</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.10196v2">Abs</a> · <a href="https://arxiv.org/pdf/2508.10196v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Early detection of lung cancer is critical to improving survival outcomes. We present a deep learning framework for automated lung cancer screening from chest computed tomography (CT) images with integrated explainability. Using the IQ-OTH/NCCD dataset (1,197 scans across Normal, Benign, and Malignant classes), we evaluate a custom convolutional neural network (CNN) and three fine-tuned transfer learning backbones: DenseNet121, ResNet152, and VGG19. Models are trained with cost-sensitive learning to mitigate class imbalance and evaluated via accuracy, precision, recall, F1-score, and ROC-AUC. While ResNet152 achieved the highest accuracy (97.3%), DenseNet121 provided the best overall balance in precision, recall, and F1 (up to 92%, 90%, 91%, respectively). We further apply Shapley Additive Explanations (SHAP) to visualize evidence contributing to predictions, improving clinical transparency. Results indicate that CNN-based approaches augmented with explainability can provide fast, accurate, and interpretable support for lung cancer screening, particularly in resource-limited settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于卷积神经网络的肺癌检测可解释性AI技术</div>
<div class="mono" style="margin-top:8px">肺癌的早期检测对于改善生存率至关重要。我们提出了一种深度学习框架，用于从胸部计算机断层扫描（CT）图像中进行自动化肺癌筛查，并集成了可解释性功能。使用IQ-OTH/NCCD数据集（包含正常、良性及恶性三类共1,197次扫描），我们评估了一个自定义卷积神经网络（CNN）以及三个微调的迁移学习模型：DenseNet121、ResNet152和VGG19。模型采用代价敏感学习进行训练以缓解类别不平衡问题，并通过准确率、精确率、召回率、F1分数和ROC-AUC进行评估。尽管ResNet152实现了最高的准确率（97.3%），但DenseNet121在精确率、召回率和F1分数上提供了最佳的整体平衡（分别达到92%、90%和91%）。我们进一步应用Shapley加法解释（SHAP）来可视化对预测有贡献的证据，从而提高临床透明度。结果表明，结合可解释性的基于CNN的方法可以为肺癌筛查提供快速、准确且可解释的支持，尤其是在资源有限的环境中。</div>
</details>
</div>
<div class="card">
<div class="title">PRIMAD-LID: A Developed Framework for Computational Reproducibility</div>
<div class="meta-line">Authors: Meznah Aloqalaa, Stian Soiland-Reyes, Carole Goble</div>
<div class="meta-line">First: 2026-01-05T18:47:48+00:00 · Latest: 2026-01-05T18:47:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02349v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02349v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Over the past decade alongside increased focus on computational reproducibility significant efforts have been made to define reproducibility. However, these definitions provide a textual description rather than a framework. The community has sought conceptual frameworks that identify all factors that must be controlled and described for credible computational reproducibility. The PRIMAD model was initially introduced to address inconsistencies in terminology surrounding computational reproducibility by outlining six key factors: P (Platforms), R (Research objective), I (Implementations), M (Methods), A (Actors), and D (Data). Subsequently various studies across different fields adopted the model and proposed extensions. However, these contributions remain fragmented and require systematic integration and cross-disciplinary validation. To bridge this gap and recognising that PRIMAD provides a broadly applicable framework for reproducibility in computational science, this work undertakes a focused investigation of the PRIMAD model. It combines the models previous extensions into a unified framework suitable for diverse research contexts. The result is PRIMAD-LID, a discipline-diagnostic reproducibility framework that retains the original six PRIMAD dimensions and enhances each with three overarching modifiers: Lifespan (temporal qualifier), Interpretation (contextual reasoning) and Depth (necessary granularity), thereby establishing a more cohesive and robust foundation for computational reproducibility practices.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PRIMAD-LID：计算可重复性的一个发展框架</div>
<div class="mono" style="margin-top:8px">过去十年间，随着对计算可重复性关注的增加，人们已做出大量努力来定义可重复性。然而，这些定义仅提供了文本描述，而非框架。学术界一直在寻求能够识别所有必须被控制和描述的因素的概念框架，以实现可信的计算可重复性。PRIMAD模型最初被提出，旨在通过明确六个关键因素（P：平台，R：研究目标，I：实现，M：方法，A：参与者，D：数据）来解决计算可重复性相关术语不一致的问题。随后，不同领域的多项研究采用该模型并提出了扩展。然而，这些贡献仍较为分散，需要系统整合和跨学科验证。为弥合这一差距，并认识到PRIMAD为计算科学中的可重复性提供了一个广泛适用的框架，本文对PRIMAD模型进行了有针对性的研究。它将模型先前的扩展整合为一个统一的框架，适用于多样化的研究情境。最终成果是PRIMAD-LID，一个学科诊断性的可重复性框架，保留了原始的六个PRIMAD维度，并为每个维度增加了三个主要修饰词：生命周期（时间限定词）、解释（情境推理）和深度（必要的粒度），从而为计算可重复性实践建立了一个更加协调和稳固的基础。</div>
</details>
</div>
<div class="card">
<div class="title">Scaling Open-Ended Reasoning to Predict the Future</div>
<div class="meta-line">Authors: Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping</div>
<div class="meta-line">First: 2025-12-31T18:59:51+00:00 · Latest: 2026-01-05T18:45:47+00:00</div>
<div class="meta-line">Comments: 45 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.25070v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.25070v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-stakes decision making involves reasoning under uncertainty about the future. In this work, we train language models to make predictions on open-ended forecasting questions. To scale up training data, we synthesize novel forecasting questions from global events reported in daily news, using a fully automated, careful curation recipe. We train the Qwen3 thinking models on our dataset, OpenForesight. To prevent leakage of future information during training and evaluation, we use an offline news corpus, both for data generation and retrieval in our forecasting system. Guided by a small validation set, we show the benefits of retrieval, and an improved reward function for reinforcement learning (RL). Once we obtain our final forecasting system, we perform held-out testing between May to August 2025. Our specialized model, OpenForecaster 8B, matches much larger proprietary models, with our training improving the accuracy, calibration, and consistency of predictions. We find calibration improvements from forecasting training generalize across popular benchmarks. We open-source all our models, code, and data to make research on language model forecasting broadly accessible.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>将开放式推理扩展到预测未来</div>
<div class="mono" style="margin-top:8px">高风险决策涉及对未来不确定性的推理。在本工作中，我们训练语言模型对开放式预测问题进行预测。为了扩展训练数据，我们通过每日新闻中报道的全球事件，使用一个全自动且细致的筛选方法合成新的预测问题。我们在我们的数据集OpenForesight上训练Qwen3思考模型。为了防止训练和评估过程中未来信息的泄露，我们在预测系统的数据生成和检索中均使用离线新闻语料库。在小型验证集的指导下，我们展示了检索的优势以及用于强化学习（RL）的改进奖励函数。在获得最终预测系统后，我们在2025年5月至8月期间进行保留测试。我们的专用模型OpenForecaster 8B在性能上与许多更大的专有模型相当，我们的训练提高了预测的准确性、校准度和一致性。我们发现预测训练带来的校准度提升可以推广到流行的基准测试中。我们开源了所有模型、代码和数据，以使语言模型预测研究更广泛地被访问。</div>
</details>
</div>
<div class="card">
<div class="title">Falcon-H1R: Pushing the Reasoning Frontiers with a Hybrid Model for Efficient Test-Time Scaling</div>
<div class="meta-line">Authors: Falcon LLM Team, Iheb Chaabane, Puneesh Khanna, Suhail Mohmad, Slim Frikha, Shi Hu, Abdalgader Abubaker, Reda Alami, Mikhail Lubinets, Mohamed El Amine Seddik, Hakim Hacid</div>
<div class="meta-line">First: 2026-01-05T18:44:27+00:00 · Latest: 2026-01-05T18:44:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02346v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02346v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This work introduces Falcon-H1R, a 7B-parameter reasoning-optimized model that establishes the feasibility of achieving competitive reasoning performance with small language models (SLMs). Falcon-H1R stands out for its parameter efficiency, consistently matching or outperforming SOTA reasoning models that are $2\times$ to $7\times$ larger across a variety of reasoning-intensive benchmarks. These results underscore the importance of careful data curation and targeted training strategies (via both efficient SFT and RL scaling) in delivering significant performance gains without increasing model size. Furthermore, Falcon-H1R advances the 3D limits of reasoning efficiency by combining faster inference (through its hybrid-parallel architecture design), token efficiency, and higher accuracy. This unique blend makes Falcon-H1R-7B a practical backbone for scaling advanced reasoning systems, particularly in scenarios requiring extensive chain-of-thoughts generation and parallel test-time scaling. Leveraging the recently introduced DeepConf approach, Falcon-H1R achieves state-of-the-art test-time scaling efficiency, offering substantial improvements in both accuracy and computational cost. As a result, Falcon-H1R demonstrates that compact models, through targeted model training and architectural choices, can deliver robust and scalable reasoning performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Falcon-H1R：通过混合模型实现高效推理扩展，推动推理前沿</div>
<div class="mono" style="margin-top:8px">本工作引入了Falcon-H1R，这是一个7B参数的推理优化模型，验证了使用小型语言模型（SLMs）实现具有竞争力的推理性能的可行性。Falcon-H1R因其参数效率而突出，其表现与参数规模为2到7倍的SOTA推理模型相当，甚至在多种推理密集型基准测试中表现更优。这些结果突显了在不增加模型规模的情况下，通过精心的数据筛选和有针对性的训练策略（包括高效的SFT和RL扩展）实现显著性能提升的重要性。此外，Falcon-H1R通过结合更快的推理速度（通过其混合并行架构设计）、更高的令牌效率和更高的准确性，进一步拓展了推理效率的三维边界。这种独特的组合使Falcon-H1R-7B成为扩展高级推理系统的一个实用基础，特别是在需要大量链式推理生成和并行推理扩展的场景中。借助最近提出的DeepConf方法，Falcon-H1R实现了最先进的推理扩展效率，显著提升了准确性和计算成本。因此，Falcon-H1R证明了通过有针对性的模型训练和架构选择，紧凑型模型可以提供稳健且可扩展的推理性能。</div>
</details>
</div>
<div class="card">
<div class="title">Question Answering for Multi-Release Systems: A Case Study at Ciena</div>
<div class="meta-line">Authors: Parham Khamsepour, Mark Cole, Ish Ashraf, Sandeep Puri, Mehrdad Sabetzadeh, Shiva Nejati</div>
<div class="meta-line">First: 2026-01-05T18:44:26+00:00 · Latest: 2026-01-05T18:44:26+00:00</div>
<div class="meta-line">Comments: Accepted for publication in SANER 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02345v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02345v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Companies regularly have to contend with multi-release systems, where several versions of the same software are in operation simultaneously. Question answering over documents from multi-release systems poses challenges because different releases have distinct yet overlapping documentation. Motivated by the observed inaccuracy of state-of-the-art question-answering techniques on multi-release system documents, we propose QAMR, a chatbot designed to answer questions across multi-release system documentation. QAMR enhances traditional retrieval-augmented generation (RAG) to ensure accuracy in the face of highly similar yet distinct documentation for different releases. It achieves this through a novel combination of pre-processing, query rewriting, and context selection. In addition, QAMR employs a dual-chunking strategy to enable separately tuned chunk sizes for retrieval and answer generation, improving overall question-answering accuracy. We evaluate QAMR using a public software-engineering benchmark as well as a collection of real-world, multi-release system documents from our industry partner, Ciena. Our evaluation yields five main findings: (1) QAMR outperforms a baseline RAG-based chatbot, achieving an average answer correctness of 88.5% and an average retrieval accuracy of 90%, which correspond to improvements of 16.5% and 12%, respectively. (2) An ablation study shows that QAMR&#x27;s mechanisms for handling multi-release documents directly improve answer accuracy. (3) Compared to its component-ablated variants, QAMR achieves a 19.6% average gain in answer correctness and a 14.0% average gain in retrieval accuracy over the best ablation. (4) QAMR reduces response time by 8% on average relative to the baseline. (5) The automatically computed accuracy metrics used in our evaluation strongly correlate with expert human assessments, validating the reliability of our methodology.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多版本系统中的问答：思科案例研究</div>
<div class="mono" style="margin-top:8px">企业经常需要应对多版本系统，其中同一软件的多个版本同时运行。在多版本系统的文档上进行问答面临挑战，因为不同版本的文档具有相似但不同的内容。鉴于现有最先进的问答技术在多版本系统文档上的准确性不足，我们提出了QAMR，一个旨在跨多版本系统文档回答问题的聊天机器人。QAMR通过预处理、查询重写和上下文选择的创新组合，增强了传统的检索增强生成（RAG）方法，以确保在面对高度相似但不同的版本文档时的准确性。此外，QAMR采用双块策略，使检索和答案生成可以分别调整块大小，从而提高整体问答准确性。我们使用公开的软件工程基准以及来自行业合作伙伴思科的现实世界多版本系统文档对QAMR进行了评估。我们的评估得出五个主要发现：(1) QAMR优于基于RAG的基线聊天机器人，平均答案正确率为88.5%，平均检索准确率为90%，分别提高了16.5%和12%。 (2) 消融研究显示，QAMR处理多版本文档的机制直接提高了答案准确性。 (3) 与组件消融变体相比，QAMR在最佳消融基础上平均答案正确率提高了19.6%，平均检索准确率提高了14.0%。 (4) QAMR的响应时间比基线平均减少了8%。 (5) 我们评估中使用的自动计算准确性指标与专家人工评估高度相关，验证了我们方法的可靠性。</div>
</details>
</div>
<div class="card">
<div class="title">Improving Action Smoothness for a Cascaded Online Learning Flight Control System</div>
<div class="meta-line">Authors: Yifei Li, Erik-jan van Kampen</div>
<div class="meta-line">First: 2025-07-06T11:19:34+00:00 · Latest: 2026-01-05T18:39:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.04346v6">Abs</a> · <a href="https://arxiv.org/pdf/2507.04346v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper aims to improve the action smoothness of a cascaded online learning flight control system. Although the cascaded structure is widely used in flight control design, its stability can be compromised by oscillatory control actions, which poses challenges for practical engineering applications. To address this issue, we introduce an online temporal smoothness technique and a low-pass filter to reduce the amplitude and frequency of the control actions. Fast Fourier Transform (FFT) is used to analyze policy performance in the frequency domain. Simulation results demonstrate the improvements achieved by the two proposed techniques.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>改进级联在线学习飞行控制系统中的动作平滑性</div>
<div class="mono" style="margin-top:8px">本文旨在改进级联在线学习飞行控制系统中的动作平滑性。尽管级联结构在飞行控制设计中被广泛使用，但其稳定性可能受到振荡控制动作的影响，这对实际工程应用提出了挑战。为了解决这一问题，我们引入了一种在线时间平滑技术以及低通滤波器来降低控制动作的幅值和频率。利用快速傅里叶变换（FFT）在频域中分析策略性能。仿真结果展示了这两种提出技术所带来的改进。</div>
</details>
</div>
<div class="card">
<div class="title">Topological Magnons and Giant Orbital Nernst Effect in a Zigzag Kitaev Antiferromagnet</div>
<div class="meta-line">Authors: Shreya Debnath, Saurabh Basu</div>
<div class="meta-line">First: 2026-01-05T18:37:20+00:00 · Latest: 2026-01-05T18:37:20+00:00</div>
<div class="meta-line">Comments: 15 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02342v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02342v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The exploration of topological and transport properties of collinear antiferromagnets and the role of Kitaev interactions in realising topological states therein have rarely been systematically addressed in literature. In this context, we consider a zigzag-ordered antiferromagnet with both extended Kitaev and Dzyaloshinskii-Moriya interactions (DMI) in presence of an external magnetic field to focus on the topological phases demonstrated by the magnon band structure and validated by the transport properties. The hybridization between the up- and down-spin sectors carries evidences of opening bulk gaps in the magnon band structure, giving rise to nontrivial topological phases characterized by finite Chern numbers, chiral edge modes, and a nonzero thermal Hall conductivity. Furthermore, generally speaking, a finite magnon orbital moment can exist and contribute to the Nernst response even when the net spin moment vanishes owing to the fundamental independence of the spin and orbital magnetizations. This motivates us to investigate the magnon orbital moment, orbital Berry curvature, and the resulting orbital Nernst conductivity associated with the magnon bands. We find that a giant orbital Nernst conductivity emerges even in the absence of an external magnetic field. Moreover, the distinction between different topological phases is more lucidly manifested via the orbital Nernst conductivity, thereby highlighting an enhanced sensitivity of the orbital transport to the underlying band topology. For completeness, we briefly discuss the scenario corresponding to a Néel-ordered spin alignment, which leads to a vanishing Chern number and consequently suppressed thermal Hall and orbital Nernst conductivities compared to the zigzag-ordered case, even in the presence of DMI and Kitaev interactions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>锯齿形Kitaev反铁磁中的拓扑磁振子与巨轨道Nernst效应</div>
<div class="mono" style="margin-top:8px">关于直列反铁磁体的拓扑和输运性质的研究，以及Kitaev相互作用在其中实现拓扑态的作用，很少在文献中被系统地探讨。在此背景下，我们考虑在外部磁场作用下具有扩展Kitaev相互作用和Dzyaloshinskii-Moriya相互作用（DMI）的锯齿形有序反铁磁体，以关注磁振子能带结构所展示的拓扑相，并通过输运性质加以验证。上旋和下旋部分之间的杂化显示出磁振子能带结构中打开体能隙的证据，从而产生具有有限陈数、手性边缘模式和非零热霍尔电导率的非平凡拓扑相。此外，一般来说，即使净自旋矩由于自旋与轨道磁化的基本独立性而消失，有限的磁振子轨道矩仍可存在并贡献于Nernst响应。这促使我们研究磁振子的轨道矩、轨道Berry曲率以及与磁振子能带相关的轨道Nernst电导率。我们发现，即使在没有外部磁场的情况下，也会出现巨大的轨道Nernst电导率。此外，通过轨道Nernst电导率可以更清晰地区分不同的拓扑相，从而突显轨道输运对底层能带拓扑结构的更高敏感性。为完整性，我们简要讨论了对应于Néel有序自旋排列的情形，该情形导致陈数为零，并且即使存在DMI和Kitaev相互作用，其热霍尔电导率和轨道Nernst电导率也比锯齿形有序情形有所抑制。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The exploration of topological and transport properties of collinear antiferromagnets and the role of Kitaev interactions in realising topological states therein have rarely been systematically addressed in literature.</div>
</details>
</div>
<div class="card">
<div class="title">Causal Multi-fidelity Surrogate Forward and Inverse Models for ICF Implosions</div>
<div class="meta-line">Authors: Tyler E. Maltba, Ben S. Southworth, Jeffrey R. Haack, Marc L. Klasky</div>
<div class="meta-line">First: 2025-09-05T21:39:53+00:00 · Latest: 2026-01-05T18:34:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.05510v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.05510v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continued progress in inertial confinement fusion (ICF) requires solving inverse problems relating experimental observations to simulation input parameters, followed by design optimization. However, such high-dimensional dynamic PDE-constrained optimization problems are extremely challenging or even intractable. It has been recently shown that inverse problems can be solved by only considering certain robust features. Here we consider the ICF capsule&#x27;s deuterium-tritium (DT) interface, and construct a causal, dynamic, multifidelity reduced-order surrogate that maps from a time-dependent radiation temperature drive to the interface&#x27;s radius and velocity dynamics. The surrogate targets an ODE embedding of DT interface dynamics, and is constructed by learning a controller for a base analytical model using low- and high-fidelity simulation training data with respect to radiation energy group structure. After demonstrating excellent accuracy of the surrogate interface model, we use machine learning (ML) models with surrogate-generated data to solve inverse problems optimizing radiation temperature drive to reproduce observed interface dynamics. For sparse snapshots in time, the ML model further characterizes the most informative times at which to sample dynamics. Altogether we demonstrate how operator learning, causal architectures, and physical inductive bias can be integrated to accelerate discovery, design, and diagnostics in high-energy-density systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于惯性约束聚变压缩的因果多保真度替代前向与逆模型</div>
<div class="mono" style="margin-top:8px">惯性约束聚变（ICF）的持续进展需要解决将实验观测与模拟输入参数关联的逆问题，并进行设计优化。然而，这类高维动态PDE约束优化问题极具挑战性，甚至难以处理。最近研究表明，仅考虑某些鲁棒特征即可解决逆问题。本文考虑ICF胶囊中的氘-氚（DT）界面，并构建了一个因果、动态、多保真度的降阶替代模型，该模型将随时间变化的辐射温度驱动映射到界面的半径和速度动态。该替代模型针对DT界面动态的ODE嵌入进行设计，并通过使用低保真度和高保真度模拟训练数据来学习基础解析模型的控制器，从而构建。在展示替代界面模型的高精度后，我们使用替代生成数据的机器学习（ML）模型来解决逆问题，优化辐射温度驱动以重现观测到的界面动态。对于时间稀疏的快照，ML模型进一步表征了采样动态的最有信息时间点。总体而言，我们展示了如何将算子学习、因果架构和物理归纳偏置整合起来，以加速高能密度系统中的发现、设计和诊断过程。</div>
</details>
</div>
<div class="card">
<div class="title">SteganoBackdoor: Stealthy and Data-Efficient Backdoor Attacks on Language Models</div>
<div class="meta-line">Authors: Eric Xue, Ruiyi Zhang, Pengtao Xie</div>
<div class="meta-line">First: 2025-11-18T09:56:16+00:00 · Latest: 2026-01-05T18:33:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.14301v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.14301v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modern language models remain vulnerable to backdoor attacks via poisoned data, where training inputs containing a trigger are paired with a target output, causing the model to reproduce that behavior whenever the trigger appears at inference time. Recent work has emphasized stealthy attacks that stress-test data-curation defenses using stylized artifacts or token-level perturbations as triggers, but this focus leaves a more practically relevant threat model underexplored: backdoors tied to naturally occurring semantic concepts. We introduce SteganoBackdoor, an optimization-based framework that constructs SteganoPoisons, steganographic poisoned training examples in which a backdoor payload is distributed across a fluent sentence while exhibiting no representational overlap with the inference-time semantic trigger. Across diverse model architectures, SteganoBackdoor achieves high attack success under constrained poisoning budgets and remains effective under conservative data-level filtering, highlighting a blind spot in existing data-curation defenses.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SteganoBackdoor：语言模型上隐蔽且数据高效的后门攻击</div>
<div class="mono" style="margin-top:8px">现代语言模型仍易受通过污染数据进行的后门攻击影响，其中包含触发器的训练输入与目标输出配对，导致模型在推理时出现触发器时重现该行为。近期的研究强调了隐蔽性攻击，利用风格化人工制品或词级扰动作为触发器来测试数据整理防御，但这种关注忽略了更具实际意义的威胁模型：与自然语义概念相关的后门。我们引入了SteganoBackdoor，这是一种基于优化的框架，用于构建SteganoPoisons，即在流畅句子中分布后门负载的隐写术式污染训练样本，且在推理时的语义触发器上没有表示重叠。在多种模型架构上，SteganoBackdoor在受限的污染预算下实现了高攻击成功率，并在保守的数据级过滤下仍保持有效性，突显了现有数据整理防御中的盲点。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Modern language models remain vulnerable to backdoor attacks via poisoned data, where training inputs containing a trigger are paired with a target output, causing the model to reproduce that behavior whenever the trigger appears at inference time.</div>
</details>
</div>
<div class="card">
<div class="title">Joint Semantic and Rendering Enhancements in 3D Gaussian Modeling with Anisotropic Local Encoding</div>
<div class="meta-line">Authors: Jingming He, Chongyi Li, Shiqi Wang, Sam Kwong</div>
<div class="meta-line">Venue: ICCV 2025</div>
<div class="meta-line">First: 2026-01-05T18:33:50+00:00 · Latest: 2026-01-05T18:33:50+00:00</div>
<div class="meta-line">Comments: Accepted by ICCV 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02339v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02339v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent works propose extending 3DGS with semantic feature vectors for simultaneous semantic segmentation and image rendering. However, these methods often treat the semantic and rendering branches separately, relying solely on 2D supervision while ignoring the 3D Gaussian geometry. Moreover, current adaptive strategies adapt the Gaussian set depending solely on rendering gradients, which can be insufficient in subtle or textureless regions. In this work, we propose a joint enhancement framework for 3D semantic Gaussian modeling that synergizes both semantic and rendering branches. Firstly, unlike conventional point cloud shape encoding, we introduce an anisotropic 3D Gaussian Chebyshev descriptor using the Laplace-Beltrami operator to capture fine-grained 3D shape details, thereby distinguishing objects with similar appearances and reducing reliance on potentially noisy 2D guidance. In addition, without relying solely on rendering gradient, we adaptively adjust Gaussian allocation and spherical harmonics with local semantic and shape signals, enhancing rendering efficiency through selective resource allocation. Finally, we employ a cross-scene knowledge transfer module to continuously update learned shape patterns, enabling faster convergence and robust representations without relearning shape information from scratch for each new scene. Experiments on multiple datasets demonstrate improvements in segmentation accuracy and rendering quality while maintaining high rendering frame rates.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于各向异性局部编码的3D高斯建模中的语义与渲染联合增强</div>
<div class="mono" style="margin-top:8px">近期工作提出通过扩展3DGS以包含语义特征向量，实现语义分割与图像渲染的同时进行。然而，这些方法通常将语义和渲染分支分开处理，仅依赖2D监督信号而忽略了3D高斯几何。此外，当前的自适应策略仅根据渲染梯度调整高斯集，这在细节微妙或无纹理区域可能不够充分。本文提出了一种联合增强框架，用于3D语义高斯建模，使语义和渲染分支协同工作。首先，不同于传统的点云形状编码，我们引入了基于Laplace-Beltrami算子的各向异性3D高斯切比雪夫描述符，以捕捉细粒度的3D形状细节，从而区分外观相似的对象，并减少对可能噪声较大的2D引导的依赖。其次，不依赖于渲染梯度，我们利用局部语义和形状信号自适应地调整高斯分配和球面调和，通过选择性资源分配提升渲染效率。最后，我们采用跨场景知识迁移模块，持续更新学习到的形状模式，实现更快的收敛和鲁棒的表示，而无需为每个新场景重新学习形状信息。在多个数据集上的实验表明，该方法在分割精度和渲染质量方面有所提升，同时保持了较高的渲染帧率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent works propose extending 3DGS with semantic feature vectors for simultaneous semantic segmentation and image rendering.</div>
</details>
</div>
<div class="card">
<div class="title">Robust Persona-Aware Toxicity Detection with Prompt Optimization and Learned Ensembling</div>
<div class="meta-line">Authors: Berk Atil, Rebecca J. Passonneau, Ninareh Mehrabi</div>
<div class="meta-line">First: 2026-01-05T18:32:45+00:00 · Latest: 2026-01-05T18:32:45+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02337v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02337v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Toxicity detection is inherently subjective, shaped by the diverse perspectives and social priors of different demographic groups. While ``pluralistic&#x27;&#x27; modeling as used in economics and the social sciences aims to capture perspective differences across contexts, current Large Language Model (LLM) prompting techniques have different results across different personas and base models. In this work, we conduct a systematic evaluation of persona-aware toxicity detection, showing that no single prompting method, including our proposed automated prompt optimization strategy, uniformly dominates across all model-persona pairs. To exploit complementary errors, we explore ensembling four prompting variants and propose a lightweight meta-ensemble: an SVM over the 4-bit vector of prompt predictions. Our results demonstrate that the proposed SVM ensemble consistently outperforms individual prompting methods and traditional majority-voting techniques, achieving the strongest overall performance across diverse personas. This work provides one of the first systematic comparisons of persona-conditioned prompting for toxicity detection and offers a robust method for pluralistic evaluation in subjective NLP tasks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于提示优化和学习集成的鲁棒人格感知毒性检测</div>
<div class="mono" style="margin-top:8px">毒性检测本质上是主观的，受到不同人口群体的多样化视角和社会先验的影响。尽管在经济学和社会科学中使用的『多元主义』建模方法旨在捕捉不同情境下的视角差异，但当前大型语言模型（LLM）的提示技术在不同人格和基础模型之间会产生不同的结果。在本工作中，我们系统评估了人格感知的毒性检测，表明没有单一的提示方法，包括我们提出的自动化提示优化策略，能在所有模型-人格对中统一占优。为了利用互补性错误，我们探索了四种提示变体的集成，并提出了一种轻量级的元集成方法：在提示预测的4位向量上使用支持向量机（SVM）。我们的结果表明，所提出的SVM集成方法在所有模型-人格对中均优于单个提示方法和传统多数投票技术，实现了在多样化人格下的最强整体性能。本工作提供了毒性检测中人格条件提示方法的首次系统比较，并为主观NLP任务中的多元性评估提供了一种稳健的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Vision-Language Reasoning for Geolocalization: A Reinforcement Learning Approach</div>
<div class="meta-line">Authors: Biao Wu, Meng Fang, Ling Chen, Ke Xu, Tao Cheng, Jun Wang</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2026-01-01T16:51:41+00:00 · Latest: 2026-01-05T18:27:19+00:00</div>
<div class="meta-line">Comments: Accepted to AAAI 2026. Project Page: https://github.com/aialt/geo-r</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00388v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.00388v2">PDF</a> · <a href="https://github.com/aialt/geo-r">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in vision-language models have opened up new possibilities for reasoning-driven image geolocalization. However, existing approaches often rely on synthetic reasoning annotations or external image retrieval, which can limit interpretability and generalizability. In this paper, we present Geo-R, a retrieval-free framework that uncovers structured reasoning paths from existing ground-truth coordinates and optimizes geolocation accuracy via reinforcement learning. We propose the Chain of Region, a rule-based hierarchical reasoning paradigm that generates precise, interpretable supervision by mapping GPS coordinates to geographic entities (e.g., country, province, city) without relying on model-generated or synthetic labels. Building on this, we introduce a lightweight reinforcement learning strategy with coordinate-aligned rewards based on Haversine distance, enabling the model to refine predictions through spatially meaningful feedback. Our approach bridges structured geographic reasoning with direct spatial supervision, yielding improved localization accuracy, stronger generalization, and more transparent inference. Experimental results across multiple benchmarks confirm the effectiveness of Geo-R, establishing a new retrieval-free paradigm for scalable and interpretable image geolocalization. To facilitate further research and ensure reproducibility, both the model and code will be made publicly available.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向地理定位的视觉-语言推理：一种强化学习方法</div>
<div class="mono" style="margin-top:8px">近年来，视觉-语言模型的进步为基于推理的图像地理定位带来了新的可能性。然而，现有方法通常依赖于合成推理标注或外部图像检索，这可能会限制其可解释性和泛化能力。本文提出Geo-R，一种无需检索的框架，该框架通过现有的真实坐标揭示结构化的推理路径，并利用强化学习优化地理定位的准确性。我们提出了基于规则的层次化推理范式Chain of Region，通过将GPS坐标映射到地理实体（如国家、省份、城市）来生成精确且可解释的监督信号，而无需依赖模型生成或合成标签。在此基础上，我们引入了一种轻量级的强化学习策略，基于Haversine距离对齐的奖励机制，使模型能够通过具有空间意义的反馈来优化预测。我们的方法将结构化的地理推理与直接的空间监督相结合，从而提升了定位准确性、增强了泛化能力并提高了推理的透明度。在多个基准数据集上的实验结果验证了Geo-R的有效性，确立了一种新的无需检索的范式，用于可扩展且可解释的图像地理定位。为促进进一步研究并确保可复现性，我们将模型和代码公开。</div>
</details>
</div>
<div class="card">
<div class="title">BEDS: Bayesian Emergent Dissipative Structures</div>
<div class="meta-line">Authors: Laurent Caraffa</div>
<div class="meta-line">First: 2026-01-05T18:21:02+00:00 · Latest: 2026-01-05T18:21:02+00:00</div>
<div class="meta-line">Comments: 19 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02329v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02329v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present BEDS (Bayesian Emergent Dissipative Structures), a theoretical framework that unifies concepts from non-equilibrium thermodynamics, Bayesian inference, information geometry, and machine learning. The central thesis proposes that learning, across physical, biological, and computational systems, fundamentally constitutes the conversion of flux into structure through entropy export. Building on Prigogine&#x27;s theory of dissipative structures, we establish a formal isomorphism between thermodynamic processes and Bayesian updating, demonstrating that sustainable learning systems must follow dissipative patterns where crystallized posteriors become priors for subsequent levels of emergence.
  We derive fundamental mathematical constants (e, π, φ) as fixed points of Bayesian inference under minimal axioms, suggesting these constants emerge necessarily from any system capable of representing and updating uncertainty. Furthermore, we propose a conjecture linking Gödel&#x27;s incompleteness theorems to thermodynamic constraints, hypothesizing that pathologies of formal systems (incompleteness, undecidability) are structurally analogous to dissipation deficits in physical systems.
  As practical validation, we present a peer-to-peer network architecture implementing BEDS principles, achieving six orders of magnitude improvement in energy efficiency compared to existing distributed consensus systems while enabling continuous learning. This work bridges fundamental physics, mathematical logic, and practical system design, offering both theoretical insights into the nature of learning and computation, and a concrete pathway toward sustainable artificial intelligence.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BEDS：贝叶斯涌现耗散结构</div>
<div class="mono" style="margin-top:8px">我们提出了BEDS（贝叶斯涌现耗散结构），这是一个统一非平衡热力学、贝叶斯推断、信息几何和机器学习概念的理论框架。核心论点认为，学习在物理、生物和计算系统中本质上是通过熵输出将流转化为结构的过程。基于普里戈金的耗散结构理论，我们建立了热力学过程与贝叶斯更新之间的形式同构，表明可持续的学习系统必须遵循耗散模式，其中结晶化的后验分布成为后续层次涌现的先验。
我们从最小公理出发，推导出基本数学常数（e, π, φ）作为贝叶斯推断的不动点，表明这些常数必然从任何能够表示和更新不确定性的系统中涌现。此外，我们提出一个将哥德尔不完备定理与热力学约束联系起来的猜想，假设形式系统的病态（不完备性、不可判定性）在结构上与物理系统中的耗散不足具有类比关系。
作为实际验证，我们提出了一种实现BEDS原理的点对点网络架构，其能源效率比现有的分布式共识系统提高了六个数量级，同时支持持续学习。这项工作连接了基础物理学、数学逻辑和实际系统设计，既提供了对学习和计算本质的理论见解，也提供了一条实现可持续人工智能的具体路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present BEDS (Bayesian Emergent Dissipative Structures), a theoretical framework that unifies concepts from non-equilibrium thermodynamics, Bayesian inference, information geometry, and machine learning.</div>
</details>
</div>
<div class="card">
<div class="title">Diminishing Returns in Self-Supervised Learning</div>
<div class="meta-line">Authors: Oli Bridge, Huey Sun, Botond Branyicskai-Nagy, Charles D&#x27;Ornano, Shomit Basu</div>
<div class="meta-line">First: 2025-12-03T15:11:44+00:00 · Latest: 2026-01-05T18:17:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.03862v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.03862v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Transformer-based architectures have become a dominant paradigm in vision and language, but their success is often attributed to large model capacity and massive training data. In this work, we examine how self-supervised pre-training, intermediate fine-tuning, and downstream fine-tuning interact in a low-capacity regime, using a 5M-parameter Vision Transformer for semantic segmentation. Across multiple data scales, we find that masked image modeling pre-training and downstream fine-tuning reliably improve performance, but with clear diminishing returns as supervision increases. In contrast, inserting an intermediate classification fine-tuning stage consistently degrades downstream performance, with the largest drops occurring precisely where pre-training is most effective. Through an analysis of patch-level representation geometry, we show that classification-based intermediate supervision actively interferes with representations learned during pre-training by collapsing spatial structure critical for dense prediction. These results indicate that, in small models, the geometry of supervision matters more than the number of training stages: misaligned intermediate objectives can negate the benefits of pre-training rather than amplify them.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自监督学习中的边际效益递减</div>
<div class="mono" style="margin-top:8px">基于Transformer的架构在视觉和语言领域已成为主流范式，但其成功通常归功于大模型容量和海量训练数据。在本工作中，我们研究了在低容量情况下，自监督预训练、中间微调和下游微调之间的相互作用，使用一个500万参数的Vision Transformer进行语义分割。在多个数据规模下，我们发现掩码图像建模预训练和下游微调能可靠地提升性能，但随着监督程度的增加，边际效益递减。相反，插入中间分类微调阶段会持续降低下游性能，尤其是在预训练效果最佳的区域，性能下降最为显著。通过分析patch级别的表示几何结构，我们表明基于分类的中间监督会主动干扰预训练期间学习到的表示，导致对密集预测至关重要的空间结构崩溃。这些结果表明，在小模型中，监督的几何结构比训练阶段的数量更为重要：不匹配的中间目标可能会抵消预训练的好处，而非增强其效果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Transformer-based architectures have become a dominant paradigm in vision and language, but their success is often attributed to large model capacity and massive training data.</div>
</details>
</div>
<div class="card">
<div class="title">Another look at regularity in transport-commutator estimates</div>
<div class="meta-line">Authors: Elias Hess-Childs, Matthew Rosenzweig, Sylvia Serfaty</div>
<div class="meta-line">First: 2026-01-05T18:17:38+00:00 · Latest: 2026-01-05T18:17:38+00:00</div>
<div class="meta-line">Comments: 42 pages. This article is dedicated to the memory of Haïm Brezis whose life and work were a great source of inspiration</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02326v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02326v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We are interested in how regular a transport velocity field must be in order to control Riesz-type commutators. Estimates for these commutators play a central role in the analysis of the mean-field limit and fluctuations for systems of particles with pairwise Riesz interactions, which we start by reviewing. Our first new result shows that the usual $L^\infty$ assumption on the gradient of the velocity field cannot, in general, be relaxed to a BMO assumption. We construct counterexamples in all dimensions and all Riesz singularities $-2&lt; s&lt;d$, except for the one-dimensional logarithmic endpoint $s=0$. At this exceptional endpoint, such a relaxation is possible, a fact related to the classical Coifman-Rochberg-Weiss commutator bound for the Hilbert transform. Our second result identifies a trade-off between the singularity of the interaction potential and the required regularity of the velocity field. Roughly speaking, smoother (less singular) interactions require stronger velocity control if one wants a commutator estimate in the natural energy seminorm determined by the potential. We formulate this principle for a broad class of potentials and show that, in the sub-Coulomb Riesz regime, the velocity regularity appearing in the known commutator inequality is sharp. Despite these negative findings, we show as our third result that a defective commutator estimate holds for almost-Lipschitz transport fields. Such a defective estimate, which is a consequence of the celebrated Brezis-Wainger-Hansson inequality, allows us to prove rates of convergence when the mean-field density belongs to the scaling-critical Sobolev space.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>再看运输速度场在运输-交换估计中的规律性</div>
<div class="mono" style="margin-top:8px">我们关注运输速度场需要具备多大的规律性才能控制Riesz型交换算子。这些交换算子的估计在分析具有成对Riesz相互作用的粒子系统的平均场极限和波动时起着核心作用，我们首先回顾了相关背景。我们的第一个新结果表明，通常对速度场梯度的$L^\infty$假设一般不能被减弱为BMO假设。我们在所有维度和所有Riesz奇异性$-2&lt; s&lt;d$的情况下构造了反例，除了在一维情况下的对数端点$s=0$。在这一特殊端点，这种假设可以被放宽，这一事实与经典的Coifman-Rochberg-Weiss交换算子对希尔伯特变换的界有关。我们的第二个结果识别了相互作用势的奇异性与所需速度场规律性之间的权衡。简而言之，更光滑（更不奇异）的相互作用需要更强的速度控制，如果希望在由势决定的自然能量半范数中获得交换算子估计。我们为广泛的一类势函数提出了这一原则，并证明在子库仑Riesz范围内，已知交换算子不等式中的速度场规律性是精确的。尽管有这些负面结果，我们作为第三个结果展示了对于几乎利普希茨运输速度场，一个有缺陷的交换算子估计仍然成立。这种有缺陷的估计是著名的Brezis-Wainger-Hansson不等式的结果，使我们能够证明当平均场密度属于缩放临界Sobolev空间时的收敛速率。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We are interested in how regular a transport velocity field must be in order to control Riesz-type commutators.</div>
</details>
</div>
<div class="card">
<div class="title">Hunting for &quot;Oddballs&quot; with Machine Learning: Detecting Anomalous Exoplanets Using a Deep-Learned Low-Dimensional Representation of Transit Spectra with Autoencoders</div>
<div class="meta-line">Authors: Alexander Roman, Emilie Panek, Roy T. Forestano, Eyup B. Unlu, Katia Matcheva, Konstantin T. Matchev</div>
<div class="meta-line">First: 2026-01-05T18:15:53+00:00 · Latest: 2026-01-05T18:15:53+00:00</div>
<div class="meta-line">Comments: 14 pages, 12 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02324v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02324v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study explores the application of autoencoder-based machine learning techniques for anomaly detection to identify exoplanet atmospheres with unconventional chemical signatures using a low-dimensional data representation. We use the Atmospheric Big Challenge (ABC) database, a publicly available dataset with over 100,000 simulated exoplanet spectra, to construct an anomaly detection scenario by defining CO2-rich atmospheres as anomalies and CO2-poor atmospheres as the normal class. We benchmarked four different anomaly detection strategies: Autoencoder Reconstruction Loss, One-Class Support Vector Machine (1 class-SVM), K-means Clustering, and Local Outlier Factor (LOF). Each method was evaluated in both the original spectral space and the autoencoder&#x27;s latent space using Receiver Operating Characteristic (ROC) curves and Area Under the Curve (AUC) metrics. To test the performance of the different methods under realistic conditions, we introduced Gaussian noise levels ranging from 10 to 50 ppm. Our results indicate that anomaly detection is consistently more effective when performed within the latent space across all noise levels. Specifically, K-means clustering in the latent space emerged as a stable and high-performing method. We demonstrate that this anomaly detection approach is robust to noise levels up to 30 ppm (consistent with realistic space-based observations) and remains viable even at 50 ppm when leveraging latent space representations. On the other hand, the performance of the anomaly detection methods applied directly in the raw spectral space degrades significantly with increasing the level of noise. This suggests that autoencoder-driven dimensionality reduction offers a robust methodology for flagging chemically anomalous targets in large-scale surveys where exhaustive retrievals are computationally prohibitive.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用机器学习寻找&quot;异类&quot;：通过自编码器对凌日光谱的低维表示检测具有非常规化学特征的系外行星</div>
<div class="mono" style="margin-top:8px">本研究探讨了基于自编码器的机器学习技术在异常检测中的应用，以识别具有非常规化学特征的系外行星大气。我们使用了大气大挑战（ABC）数据库，这是一个包含超过100,000条模拟系外行星光谱的公开数据集，通过将CO2丰富的大气定义为异常，CO2贫乏的大气定义为正常类，构建了一个异常检测场景。我们对四种不同的异常检测策略进行了基准测试：自编码器重构损失、一类支持向量机（1 class-SVM）、K均值聚类和局部离群因子（LOF）。每种方法均在原始光谱空间和自编码器的潜在空间中使用接收者操作特征（ROC）曲线和曲线下面积（AUC）指标进行评估。为了在现实条件下测试不同方法的性能，我们引入了从10到50 ppm的高斯噪声。我们的结果表明，在所有噪声水平下，异常检测在潜在空间中始终更为有效。具体而言，潜在空间中的K均值聚类方法表现出稳定且高性能的特性。我们证明了该异常检测方法在高达30 ppm的噪声水平下（与现实中的空间观测一致）具有鲁棒性，并且在利用潜在空间表示的情况下，即使在50 ppm噪声水平下仍具有可行性。另一方面，直接在原始光谱空间中应用的异常检测方法在噪声水平增加时性能显著下降。这表明，由自编码器驱动的降维方法为在计算上不可行的大规模调查中标记化学异常目标提供了一种稳健的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study explores the application of autoencoder-based machine learning techniques for anomaly detection to identify exoplanet atmospheres with unconventional chemical signatures using a low-dimensional data representation.</div>
</details>
</div>
<div class="card">
<div class="title">A Categorical Generalization of Counterpoint</div>
<div class="meta-line">Authors: Octavio A. Agustín-Aquino, Juan Sebastián Arias, Enrique Ruiz Hernández</div>
<div class="meta-line">First: 2018-09-30T04:09:34+00:00 · Latest: 2026-01-05T18:13:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/1810.00312v3">Abs</a> · <a href="https://arxiv.org/pdf/1810.00312v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We extend Mazzola&#x27;s counterpoint model using category theory, generalizing from the category $\mathbf{Set}$ to other topoi with suitable properties. This generalization suggests that counterpoint&#x27;s essential structure depends on specific categorical conditions rather than classical set-theoretic reasoning.
  A key contribution is identifying the minimal requirements for counterpoint theory: the topos satisfying some version of Zorn&#x27;s Lemma (ZL) and being two-valued with split supports (NS).
  Within this framework, we introduce (weak) quasidichotomies alongside the classical notion of dichotomy. These structures capture varying degrees of oppositional structure between consonance and dissonance, with weak quasidichotomies preserving the non-Boolean flexibility essential to musical practice while quasidichotomies represent maximal opposition short of complete partition.
  We prove a generalized counterpoint theorem: sequences of admitted successors exist in any topos satisfying our conditions. The framework naturally accommodates counterpoint with sets instead of pure pitches, relaxing the &quot;yes/no&quot; character of classical consonance definitions and emphasizing context-dependence.
  Mazzola&#x27;s model allows a Kuratowski closure operator induced by a polarity, which defines an internal topology enabling algebraic-topological analysis of counterpoint structure. We close proving this construction generalizes to involutive morphisms. This categorical approach provides foundations for understanding both the historical evolution of contrapuntal practice and cross-cultural divergences in interval organization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>对对位法的范畴论推广</div>
<div class="mono" style="margin-top:8px">我们利用范畴论扩展了Mazzola的对位法模型，从范畴$\mathbf{Set}$推广到具有适当性质的其他拓扑斯。这种推广表明，对位法的本质结构依赖于特定的范畴条件，而非传统的集合论推理。
  一个关键贡献是确定了对位法理论的最小要求：满足某种形式的佐恩引理（ZL）且为二值拓扑斯（NS）。
  在此框架下，我们引入了（弱）拟二分结构，与传统的二分概念并列。这些结构捕捉了和声与不协和音之间不同层次的对立结构，其中弱拟二分结构保留了非布尔灵活性，这是音乐实践所必需的，而拟二分结构则代表了在完全分割之前的最大对立。
  我们证明了一个推广的对位法定理：在满足我们条件的任何拓扑斯中，都存在允许的后续序列。该框架自然地容纳了以集合而非纯音高为基础的对位法，放松了传统和声定义中的&quot;是/否&quot;特性，强调了情境依赖性。
  Mazzola的模型允许由极性诱导的Kuratowski闭包算子，该算子定义了一个内部拓扑，从而能够对对位结构进行代数拓扑分析。我们最终证明了这一构造可以推广到反演态射。这种范畴论方法为理解对位实践的历史演变以及不同文化中音程组织的差异提供了基础。</div>
</details>
</div>
<div class="card">
<div class="title">Environment-Adaptive Covariate Selection: Learning When to Use Spurious Correlations for Out-of-Distribution Prediction</div>
<div class="meta-line">Authors: Shuozhi Zuo, Yixin Wang</div>
<div class="meta-line">First: 2026-01-05T18:13:02+00:00 · Latest: 2026-01-05T18:13:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02322v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02322v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Out-of-distribution (OOD) prediction is often approached by restricting models to causal or invariant covariates, avoiding non-causal spurious associations that may be unstable across environments. Despite its theoretical appeal, this strategy frequently underperforms empirical risk minimization (ERM) in practice. We investigate the source of this gap and show that such failures naturally arise when only a subset of the true causes of the outcome is observed. In these settings, non-causal spurious covariates can serve as informative proxies for unobserved causes and substantially improve prediction, except under distribution shifts that break these proxy relationships. Consequently, the optimal set of predictive covariates is neither universal nor necessarily exhibits invariant relationships with the outcome across all environments, but instead depends on the specific type of shift encountered. Crucially, we observe that different covariate shifts induce distinct, observable signatures in the covariate distribution itself. Moreover, these signatures can be extracted from unlabeled data in the target OOD environment and used to assess when proxy covariates remain reliable and when they fail. Building on this observation, we propose an environment-adaptive covariate selection (EACS) algorithm that maps environment-level covariate summaries to environment-specific covariate sets, while allowing the incorporation of prior causal knowledge as constraints. Across simulations and applied datasets, EACS consistently outperforms static causal, invariant, and ERM-based predictors under diverse distribution shifts.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>环境自适应协变量选择：学习何时利用虚假相关性进行分布外预测</div>
<div class="mono" style="margin-top:8px">分布外（OOD）预测通常通过限制模型使用因果或不变的协变量来实现，避免那些可能在不同环境中不稳定而非因果的虚假关联。尽管这种方法在理论上具有吸引力，但在实践中经常不如经验风险最小化（ERM）策略表现好。我们研究了这种差距的来源，并表明当仅观察到结果的真正原因子集时，这种失败自然会发生。在这种情况下，非因果的虚假协变量可以作为未观察原因的有用代理，并显著提高预测性能，除非分布变化破坏了这些代理关系。因此，最优的预测协变量集合既不是普遍适用的，也不一定在所有环境中与结果具有不变关系，而是取决于遇到的具体分布变化类型。关键的是，我们观察到不同的协变量变化会在协变量分布中产生不同的、可观察的特征。此外，这些特征可以从目标OOD环境的无标签数据中提取，并用于判断代理协变量何时可靠、何时失效。基于这一观察，我们提出了一种环境自适应协变量选择（EACS）算法，该算法将环境级别的协变量摘要映射到环境特定的协变量集合，同时允许将先验因果知识作为约束条件纳入其中。在模拟和实际数据集上，EACS在多种分布变化下始终优于静态因果、不变和基于ERM的预测器。</div>
</details>
</div>
<div class="card">
<div class="title">Fusion2Print: Deep Flash-Non-Flash Fusion for Contactless Fingerprint Matching</div>
<div class="meta-line">Authors: Roja Sahoo, Anoop Namboodiri</div>
<div class="meta-line">First: 2026-01-05T18:09:27+00:00 · Latest: 2026-01-05T18:09:27+00:00</div>
<div class="meta-line">Comments: 15 pages, 8 figures, 5 tables. Submitted to ICPR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.02318v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.02318v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Contactless fingerprint recognition offers a hygienic and convenient alternative to contact-based systems, enabling rapid acquisition without latent prints, pressure artifacts, or hygiene risks. However, contactless images often show degraded ridge clarity due to illumination variation, subcutaneous skin discoloration, and specular reflections. Flash captures preserve ridge detail but introduce noise, whereas non-flash captures reduce noise but lower ridge contrast. We propose Fusion2Print (F2P), the first framework to systematically capture and fuse paired flash-non-flash contactless fingerprints. We construct a custom paired dataset, FNF Database, and perform manual flash-non-flash subtraction to isolate ridge-preserving signals. A lightweight attention-based fusion network also integrates both modalities, emphasizing informative channels and suppressing noise, and then a U-Net enhancement module produces an optimally weighted grayscale image. Finally, a deep embedding model with cross-domain compatibility, generates discriminative and robust representations in a unified embedding space compatible with both contactless and contact-based fingerprints for verification. F2P enhances ridge clarity and achieves superior recognition performance (AUC=0.999, EER=1.12%) over single-capture baselines (Verifinger, DeepPrint).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Fusion2Print：面向非接触式指纹匹配的深度闪光-非闪光融合</div>
<div class="mono" style="margin-top:8px">非接触式指纹识别为接触式系统提供了一种更卫生且便捷的替代方案，能够快速采集指纹图像而无需留下潜伏指纹、压力痕迹或卫生风险。然而，非接触式图像常因光照变化、皮下皮肤变色和镜面反射导致纹线清晰度下降。闪光采集能保留纹线细节但引入噪声，而非闪光采集则减少噪声但降低纹线对比度。我们提出Fusion2Print（F2P），这是首个系统性采集并融合配对闪光与非闪光非接触式指纹的框架。我们构建了一个定制的配对数据集FNF Database，并通过手动闪光-非闪光减法来分离保留纹线的信号。一个轻量级的基于注意力的融合网络也整合了两种模态，强调信息性通道并抑制噪声，随后一个U-Net增强模块生成最优加权灰度图像。最后，一个具有跨域兼容性的深度嵌入模型，在统一的嵌入空间中生成具有判别性和鲁棒性的表示，适用于非接触式和接触式指纹的验证。F2P提升了纹线清晰度，并在单次采集基线（Verifinger, DeepPrint）之上实现了优越的识别性能（AUC=0.999，EER=1.12%）。</div>
</details>
</div>
<div class="card">
<div class="title">BitDecoding: Unlocking Tensor Cores for Long-Context LLMs with Low-Bit KV Cache</div>
<div class="meta-line">Authors: Dayou Du, Shijie Cao, Jianyi Cheng, Luo Mai, Ting Cao, Mao Yang</div>
<div class="meta-line">First: 2025-03-24T15:22:41+00:00 · Latest: 2026-01-05T18:08:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.18773v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.18773v3">PDF</a> · <a href="https://github.com/OpenBitSys/BitDecoding">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The growth of long-context Large Language Models (LLMs) significantly increases memory and bandwidth pressure during autoregressive decoding due to the expanding Key-Value (KV) cache. While accuracy-preserving KV-cache quantization (e.g., 4-bit or 2-bit) reduces memory footprint, existing systems decode inefficiently by relying solely on CUDA cores, underutilizing Tensor Cores-the dominant compute resource on GPUs.
  We present BitDecoding, the first inference system to efficiently decode low-bit KV caches by cooperatively leveraging CUDA cores and Tensor Cores. BitDecoding smartly induces Tensor-Core-friendly layouts, introduces warp-level dequantization parallelism, and provides unified system support through query transformation, high-performance tensor- and channel-wise quantization, and a software-pipelined dequantization kernel enabling mixed-precision execution. Architecture-aware optimizations further leverage Hopper&#x27;s warpgroup tensor instructions and Blackwell&#x27;s NVFP4 (MXFP4) tensor formats.
  Evaluated on Blackwell, Hopper, and Ampere GPUs, BitDecoding achieves an average 7.5x decoding speedup over FP16 FlashDecoding-v2, up to 8.6x on Blackwell with NVFP4, and up to 4.3x over state-of-the-art approaches. On LLaMA-3.1-8B with a 128K context, BitDecoding reduces single-batch decoding latency by 3x. BitDecoding is open-sourced at https://github.com/OpenBitSys/BitDecoding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BitDecoding: 通过低比特KV缓存解锁Tensor Core用于长上下文LLM</div>
<div class="mono" style="margin-top:8px">长上下文大语言模型（LLMs）的发展显著增加了自回归解码过程中的内存和带宽压力，这是由于Key-Value（KV）缓存的扩展。虽然保持精度的KV缓存量化（如4位或2位）可以减少内存占用，但现有系统仅依赖CUDA核心进行解码，未能充分利用GPU上占主导地位的Tensor Core计算资源。
我们提出了BitDecoding，这是首个通过协同利用CUDA核心和Tensor Core高效解码低比特KV缓存的推理系统。BitDecoding智能地诱导Tensor Core友好的布局，引入了warp级去量化并行性，并通过查询转换、高性能张量和通道级量化以及软件流水线去量化内核，提供统一的系统支持，从而实现混合精度执行。架构感知的优化进一步利用了Hopper的warpgroup张量指令和Blackwell的NVFP4（MXFP4）张量格式。
在Blackwell、Hopper和Ampere GPU上评估，BitDecoding在FP16 FlashDecoding-v2基础上平均实现7.5倍的解码加速，在Blackwell上使用NVFP4可达到8.6倍，相比最先进的方法可达到4.3倍加速。在LLaMA-3.1-8B模型上，使用128K上下文时，BitDecoding将单批次解码延迟降低了3倍。BitDecoding已在https://github.com/OpenBitSys/BitDecoding开源。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The growth of long-context Large Language Models (LLMs) significantly increases memory and bandwidth pressure during autoregressive decoding due to the expanding Key-Value (KV) cache.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
