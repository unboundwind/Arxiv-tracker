<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-24 04:35</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260224_0435</div>
    <div class="row"><div class="card">
<div class="title">Assigning Confidence: K-partition Ensembles</div>
<div class="meta-line">Authors: Aggelos Semoglou, John Pavlopoulos</div>
<div class="meta-line">First: 2026-02-20T18:59:53+00:00 · Latest: 2026-02-20T18:59:53+00:00</div>
<div class="meta-line">Comments: 31 pages including appendix</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18435v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18435v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Clustering is widely used for unsupervised structure discovery, yet it offers limited insight into how reliable each individual assignment is. Diagnostics, such as convergence behavior or objective values, may reflect global quality, but they do not indicate whether particular instances are assigned confidently, especially for initialization-sensitive algorithms like k-means. This assignment-level instability can undermine both accuracy and robustness. Ensemble approaches improve global consistency by aggregating multiple runs, but they typically lack tools for quantifying pointwise confidence in a way that combines cross-run agreement with geometric support from the learned cluster structure. We introduce CAKE (Confidence in Assignments via K-partition Ensembles), a framework that evaluates each point using two complementary statistics computed over a clustering ensemble: assignment stability and consistency of local geometric fit. These are combined into a single, interpretable score in [0,1]. Our theoretical analysis shows that CAKE remains effective under noise and separates stable from unstable points. Experiments on synthetic and real-world datasets indicate that CAKE effectively highlights ambiguous points and stable core members, providing a confidence ranking that can guide filtering or prioritization to improve clustering quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>赋予置信度：K划分集成</div>
<div class="mono" style="margin-top:8px">聚类广泛用于无监督结构发现，但其提供的个体分配可靠性信息有限。诸如收敛行为或目标函数值等诊断方法可能反映整体质量，但无法表明特定实例是否被自信地分配，尤其是对于像k-means这样的初始化敏感算法。这种分配级别的不稳定性可能影响准确性和鲁棒性。集成方法通过聚合多个运行来提高全局一致性，但通常缺乏能够结合跨运行一致性与学习到的聚类结构几何支持来量化点级置信度的工具。我们引入了CAKE（通过K划分集成赋予分配置信度），一个框架，它通过在聚类集成上计算的两个互补统计量来评估每个点：分配稳定性与局部几何拟合的一致性。这两个统计量被结合成一个[0,1]范围内的可解释得分。我们的理论分析表明，CAKE在噪声环境下仍有效，并能区分稳定点和不稳定点。在合成数据集和真实数据集上的实验表明，CAKE能够有效突出模糊点和稳定的内核成员，提供一个置信度排名，可用于过滤或优先级排序以提高聚类质量。</div>
</details>
</div>
<div class="card">
<div class="title">Going Down Memory Lane: Scaling Tokens for Video Stream Understanding with Dynamic KV-Cache Memory</div>
<div class="meta-line">Authors: Vatsal Agarwal, Saksham Suri, Matthew Gwilliam, Pulkit Kumar, Abhinav Shrivastava</div>
<div class="meta-line">First: 2026-02-20T18:59:50+00:00 · Latest: 2026-02-20T18:59:50+00:00</div>
<div class="meta-line">Comments: Project page: see https://vatsalag99.github.io/memstream/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18434v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18434v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vatsalag99.github.io/memstream/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Streaming video understanding requires models to robustly encode, store, and retrieve information from a continuous video stream to support accurate video question answering (VQA). Existing state-of-the-art approaches rely on key-value caching to accumulate frame-level information over time, but use a limited number of tokens per frame, leading to the loss of fine-grained visual details. In this work, we propose scaling the token budget to enable more granular spatiotemporal understanding and reasoning. First, we find that current methods are ill-equipped to handle dense streams: their feature encoding causes query-frame similarity scores to increase over time, biasing retrieval toward later frames. To address this, we introduce an adaptive selection strategy that reduces token redundancy while preserving local spatiotemporal information. We further propose a training-free retrieval mixture-of-experts that leverages external models to better identify relevant frames. Our method, MemStream, achieves +8.0% on CG-Bench, +8.5% on LVBench, and +2.4% on VideoMME (Long) over ReKV with Qwen2.5-VL-7B.</div></details>
</div>
<div class="card">
<div class="title">SARAH: Spatially Aware Real-time Agentic Humans</div>
<div class="meta-line">Authors: Evonne Ng, Siwei Zhang, Zhang Chen, Michael Zollhoefer, Alexander Richard</div>
<div class="meta-line">First: 2026-02-20T18:59:35+00:00 · Latest: 2026-02-20T18:59:35+00:00</div>
<div class="meta-line">Comments: Project page: https://evonneng.github.io/sarah/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18432v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18432v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://evonneng.github.io/sarah/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As embodied agents become central to VR, telepresence, and digital human applications, their motion must go beyond speech-aligned gestures: agents should turn toward users, respond to their movement, and maintain natural gaze. Current methods lack this spatial awareness. We close this gap with the first real-time, fully causal method for spatially-aware conversational motion, deployable on a streaming VR headset. Given a user&#x27;s position and dyadic audio, our approach produces full-body motion that aligns gestures with speech while orienting the agent according to the user. Our architecture combines a causal transformer-based VAE with interleaved latent tokens for streaming inference and a flow matching model conditioned on user trajectory and audio. To support varying gaze preferences, we introduce a gaze scoring mechanism with classifier-free guidance to decouple learning from control: the model captures natural spatial alignment from data, while users can adjust eye contact intensity at inference time. On the Embody 3D dataset, our method achieves state-of-the-art motion quality at over 300 FPS -- 3x faster than non-causal baselines -- while capturing the subtle spatial dynamics of natural conversation. We validate our approach on a live VR system, bringing spatially-aware conversational agents to real-time deployment. Please see https://evonneng.github.io/sarah/ for details.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SARAH：具有空间感知能力的实时代理人类</div>
<div class="mono" style="margin-top:8px">随着具身代理在VR、远程临场感和数字人类应用中变得越来越重要，其运动必须超越与语音对齐的手势：代理应朝向用户，响应其移动，并保持自然的视线。当前方法缺乏这种空间感知能力。我们提出了首个适用于流式VR头显的实时、完全因果的空间感知对话运动方法。基于用户位置和双人语音，我们的方法生成全身运动，使手势与语音对齐，同时根据用户位置调整代理方向。我们的架构结合了一个基于因果Transformer的VAE和用于流式推理的交错潜在标记，以及一个基于用户轨迹和语音的流匹配模型。为了支持不同的视线偏好，我们引入了一种视线评分机制，结合无分类器引导，将学习与控制解耦：模型从数据中捕捉自然的空间对齐，而用户可以在推理时调整眼神接触强度。在Embody 3D数据集上，我们的方法实现了超过300 FPS的最先进运动质量，比非因果基线快3倍，同时捕捉自然对话中的微妙空间动态。我们在一个实时VR系统中验证了我们的方法，实现了空间感知对话代理的实时部署。详情请参见 https://evonneng.github.io/sarah/。</div>
</details>
</div>
<div class="card">
<div class="title">Online Smoothed Demand Management</div>
<div class="meta-line">Authors: Adam Lechowicz, Nicolas Christianson, Mohammad Hajiesmaili, Adam Wierman, Prashant Shenoy</div>
<div class="meta-line">First: 2025-11-23T17:59:51+00:00 · Latest: 2026-02-20T18:59:32+00:00</div>
<div class="meta-line">Comments: Accepted to SIGMETRICS &#x27;26. 65 pages, 11 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.18554v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.18554v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce and study a class of online problems called online smoothed demand management $(\texttt{OSDM})$, motivated by paradigm shifts in grid integration and energy storage for large energy consumers such as data centers. In $\texttt{OSDM}$, an operator makes two decisions at each time step: an amount of energy to be purchased, and an amount of energy to be delivered (i.e., used for computation). The difference between these decisions charges (or discharges) the operator&#x27;s energy storage (e.g., a battery). Two types of demand arrive online: base demand, which must be covered at the current time, and flexible demand, which can be satisfied at any time before a demand-specific deadline $Δ_t$. The operator&#x27;s goal is to minimize a cost (subject to above constraints) that combines a cost of purchasing energy, a cost for delivering energy (if applicable), and smoothness penalties on the purchasing and delivery rates to discourage fluctuations and encourage ``grid healthy&#x27;&#x27; decisions. $\texttt{OSDM}$ generalizes several problems in the online algorithms literature while being the first to fully model applications of interest. We propose a competitive algorithm for $\texttt{OSDM}$ called $\texttt{PAAD}$ (partitioned accounting &amp; aggregated decisions) and show it achieves the optimal competitive ratio. To overcome the pessimism typical of worst-case analysis, we also propose a novel learning framework that provides guarantees on the worst-case competitive ratio (i.e., to provide robustness against nonstationarity) while allowing end-to-end differentiable learning of the best algorithm on historical instances of the problem. We evaluate our algorithms in a case study of a grid-integrated data center with battery storage, showing that $\texttt{PAAD}$ effectively solves the problem and end-to-end learning achieves substantial performance improvements compared to $\texttt{PAAD}$.</div></details>
</div>
<div class="card">
<div class="title">CMind: An AI Agent for Localizing C Memory Bugs</div>
<div class="meta-line">Authors: Chia-Yi Su, Collin McMillan</div>
<div class="meta-line">First: 2026-01-20T19:43:11+00:00 · Latest: 2026-02-20T18:56:02+00:00</div>
<div class="meta-line">Comments: 4 pages, 2 figures. To be published in 34th IEEE/ACM International Conference on Program Comprehension (ICPC 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.14434v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.14434v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This demonstration paper presents CMind, an artificial intelligence agent for localizing C memory bugs. The novel aspect to CMind is that it follows steps that we observed human programmers perform during empirical study of those programmers finding memory bugs in C programs. The input to the tool is a C program&#x27;s source code and a bug report describing the problem. The output is the tool&#x27;s hypothesis about the reason for the bug and its location. CMind reads the bug report to find potential entry points to the program, then navigates the program&#x27;s source code, analyzes that source code, and generates a hypothesis location and rationale that fit a template. The tool combines large language model reasoning with guided decision making we encoded to mimic human behavior. The video demonstration is available at https://youtu.be/_vVd0LRvVHI.</div></details>
</div>
<div class="card">
<div class="title">VIRAASAT: Traversing Novel Paths for Indian Cultural Reasoning</div>
<div class="meta-line">Authors: Harshul Raj Surana, Arijit Maji, Aryan Vats, Akash Ghosh, Sriparna Saha, Amit Sheth</div>
<div class="meta-line">First: 2026-02-20T18:53:07+00:00 · Latest: 2026-02-20T18:53:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18429v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18429v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding. However, their performance deteriorates in tasks requiring rich socio-cultural knowledge and diverse local contexts, particularly those involving Indian Culture. Existing Cultural benchmarks are (i) Manually crafted, (ii) contain single-hop questions testing factual recall, and (iii) prohibitively costly to scale, leaving this deficiency largely unmeasured. To address this, we introduce VIRAASAT, a novel, semi-automated multi-hop approach for generating cultural specific multi-hop Question-Answering dataset for Indian culture. VIRAASAT leverages a Knowledge Graph comprising more than 700 expert-curated cultural artifacts, covering 13 key attributes of Indian culture (history, festivals, etc). VIRAASAT spans all 28 states and 8 Union Territories, yielding more than 3,200 multi-hop questions that necessitate chained cultural reasoning. We evaluate current State-of-the-Art (SOTA) LLMs on VIRAASAT and identify key limitations in reasoning wherein fine-tuning on Chain-of-Thought(CoT) traces fails to ground and synthesize low-probability facts. To bridge this gap, we propose a novel framework named Symbolic Chain-of-Manipulation (SCoM). Adapting the Chain-of-Manipulation paradigm, we train the model to simulate atomic Knowledge Graph manipulations internally. SCoM teaches the model to reliably traverse the topological structure of the graph. Experiments on Supervised Fine-Tuning (SFT) demonstrate that SCoM outperforms standard CoT baselines by up to 20%. We release the VIRAASAT dataset along with our findings, laying a strong foundation towards building Culturally Aware Reasoning Models.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Models (LLMs) have made significant progress in reasoning tasks across various domains such as mathematics and coding.</div>
</details>
</div>
<div class="card">
<div class="title">The Geometry of Noise: Why Diffusion Models Don&#x27;t Need Noise Conditioning</div>
<div class="meta-line">Authors: Mojtaba Sahraee-Ardakan, Mauricio Delbracio, Peyman Milanfar</div>
<div class="meta-line">First: 2026-02-20T18:49:00+00:00 · Latest: 2026-02-20T18:49:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18428v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18428v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous (noise-agnostic) generative models, such as Equilibrium Matching and blind diffusion, challenge the standard paradigm by learning a single, time-invariant vector field that operates without explicit noise-level conditioning. While recent work suggests that high-dimensional concentration allows these models to implicitly estimate noise levels from corrupted observations, a fundamental paradox remains: what is the underlying landscape being optimized when the noise level is treated as a random variable, and how can a bounded, noise-agnostic network remain stable near the data manifold where gradients typically diverge? We resolve this paradox by formalizing Marginal Energy, $E_{\text{marg}}(\mathbf{u}) = -\log p(\mathbf{u})$, where $p(\mathbf{u}) = \int p(\mathbf{u}|t)p(t)dt$ is the marginal density of the noisy data integrated over a prior distribution of unknown noise levels. We prove that generation using autonomous models is not merely blind denoising, but a specific form of Riemannian gradient flow on this Marginal Energy. Through a novel relative energy decomposition, we demonstrate that while the raw Marginal Energy landscape possesses a $1/t^p$ singularity normal to the data manifold, the learned time-invariant field implicitly incorporates a local conformal metric that perfectly counteracts the geometric singularity, converting an infinitely deep potential well into a stable attractor. We also establish the structural stability conditions for sampling with autonomous models. We identify a ``Jensen Gap&#x27;&#x27; in noise-prediction parameterizations that acts as a high-gain amplifier for estimation errors, explaining the catastrophic failure observed in deterministic blind models. Conversely, we prove that velocity-based parameterizations are inherently stable because they satisfy a bounded-gain condition that absorbs posterior uncertainty into a smooth geometric drift.</div></details>
</div>
<div class="card">
<div class="title">Spatio-Spectroscopic Representation Learning using Unsupervised Convolutional Long-Short Term Memory Networks</div>
<div class="meta-line">Authors: Kameswara Bharadwaj Mantha, Lucy Fortson, Ramanakumar Sankar, Claudia Scarlata, Chris Lintott, Sandor Kruk, Mike Walmsley, Hugh Dickinson, Karen Masters, Brooke Simmons, Rebecca Smethurst</div>
<div class="meta-line">Venue: ICML</div>
<div class="meta-line">First: 2026-02-20T18:48:36+00:00 · Latest: 2026-02-20T18:48:36+00:00</div>
<div class="meta-line">Comments: This manuscript was previously submitted to ICML for peer review. Reviewers noted that while the underlying VAE-based architecture builds on established methods, its application to spatially-resolved IFS data is promising for unsupervised representation learning in astronomy. This version is released for community visibility. Reviewer decisions: Weak accept and Weak reject (Final: Reject)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18426v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18426v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Integral Field Spectroscopy (IFS) surveys offer a unique new landscape in which to learn in both spatial and spectroscopic dimensions and could help uncover previously unknown insights into galaxy evolution. In this work, we demonstrate a new unsupervised deep learning framework using Convolutional Long-Short Term Memory Network Autoencoders to encode generalized feature representations across both spatial and spectroscopic dimensions spanning $19$ optical emission lines (3800A $&lt; λ&lt;$ 8000A) among a sample of $\sim 9000$ galaxies from the MaNGA IFS survey. As a demonstrative exercise, we assess our model on a sample of $290$ Active Galactic Nuclei (AGN) and highlight scientifically interesting characteristics of some highly anomalous AGN.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用无监督卷积长短期记忆网络进行空间-光谱表示学习</div>
<div class="mono" style="margin-top:8px">积分场光谱（IFS）调查提供了一个独特的全新视角，使我们能够在空间和光谱维度上进行学习，有助于揭示银河系演化的先前未知见解。在本工作中，我们展示了一种新的无监督深度学习框架，利用卷积长短期记忆网络自编码器对来自MaNGA IFS调查的约9000个星系样本中的19条光学发射线（3800A &lt; λ &lt; 8000A）进行编码，以获得跨空间和光谱维度的通用特征表示。作为示范性练习，我们对290个活动星系核（AGN）样本评估了我们的模型，并突出了某些高度异常AGN的科学有趣的特性。</div>
</details>
</div>
<div class="card">
<div class="title">Some Remarks on Positive/Negative Feedback</div>
<div class="meta-line">Authors: Thomas Berger, Achim Ilchmann, Eugene P. Ryan</div>
<div class="meta-line">First: 2025-12-10T09:51:11+00:00 · Latest: 2026-02-20T18:47:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.09474v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.09474v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the context of linear control systems, a commonly-held intuition is that negative and positive feedback cannot both be stability enhancing. The canonical linear prototype is the scalar system $\dot x=u$ which, under negative linear feedback $u=-kx$ ($k &gt;0$) is exponentially stable for all $k &gt;0 $, whereas the lack of exponential instability of the (marginally stable) uncontrolled system is amplified by positive feedback $u=kx$ ($k &gt;0)$. By contrast, for nonlinear systems it is shown, by example, that this intuitive dichotomy may fail to hold.</div></details>
</div>
<div class="card">
<div class="title">CapNav: Benchmarking Vision Language Models on Capability-conditioned Indoor Navigation</div>
<div class="meta-line">Authors: Xia Su, Ruiqi Chen, Benlin Liu, Jingwei Ma, Zonglin Di, Ranjay Krishna, Jon Froehlich</div>
<div class="meta-line">First: 2026-02-20T18:46:27+00:00 · Latest: 2026-02-20T18:46:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18424v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18424v1">PDF</a> · <a href="https://github.com/makeabilitylab/CapNav">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) have shown remarkable progress in Vision-Language Navigation (VLN), offering new possibilities for navigation decision-making that could benefit both robotic platforms and human users. However, real-world navigation is inherently conditioned by the agent&#x27;s mobility constraints. For example, a sweeping robot cannot traverse stairs, while a quadruped can. We introduce Capability-Conditioned Navigation (CapNav), a benchmark designed to evaluate how well VLMs can navigate complex indoor spaces given an agent&#x27;s specific physical and operational capabilities. CapNav defines five representative human and robot agents, each described with physical dimensions, mobility capabilities, and environmental interaction abilities. CapNav provides 45 real-world indoor scenes, 473 navigation tasks, and 2365 QA pairs to test if VLMs can traverse indoor environments based on agent capabilities. We evaluate 13 modern VLMs and find that current VLM&#x27;s navigation performance drops sharply as mobility constraints tighten, and that even state-of-the-art models struggle with obstacle types that require reasoning on spatial dimensions. We conclude by discussing the implications for capability-aware navigation and the opportunities for advancing embodied spatial reasoning in future VLMs. The benchmark is available at https://github.com/makeabilitylab/CapNav</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CapNav：基于能力条件的视觉语言模型室内导航基准测试</div>
<div class="mono" style="margin-top:8px">视觉语言模型（VLMs）在视觉语言导航（VLN）领域取得了显著进展，为导航决策提供了新的可能性，这对机器人平台和人类用户都有潜在益处。然而，现实世界的导航本质上受到代理移动能力的限制。例如，扫地机器人无法跨越楼梯，而四足机器人可以。我们引入了能力条件导航（CapNav），这是一个基准测试，用于评估VLMs在给定代理特定物理和操作能力的情况下，能否有效导航复杂的室内环境。CapNav定义了五种代表性的代理，包括人类和机器人，每个代理都描述了其物理尺寸、移动能力和环境交互能力。CapNav提供了45个现实世界的室内场景、473个导航任务和2365个问答对，以测试VLMs是否能够基于代理的能力在室内环境中进行导航。我们评估了13种现代VLMs，并发现当前VLMs的导航性能随着移动能力的限制而急剧下降，即使是最先进的模型也难以处理需要空间推理的障碍物类型。最后，我们讨论了能力感知导航的意义以及未来VLMs在提升具身空间推理方面的机遇。该基准测试可在https://github.com/makeabilitylab/CapNav获取。</div>
</details>
</div>
<div class="card">
<div class="title">Generated Reality: Human-centric World Simulation using Interactive Video Generation with Hand and Camera Control</div>
<div class="meta-line">Authors: Linxi Xie, Lisong C. Sun, Ashley Neall, Tong Wu, Shengqu Cai, Gordon Wetzstein</div>
<div class="meta-line">First: 2026-02-20T18:45:29+00:00 · Latest: 2026-02-20T18:45:29+00:00</div>
<div class="meta-line">Comments: Project page here: https://codeysun.github.io/generated-reality</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18422v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18422v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://codeysun.github.io/generated-reality">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Extended reality (XR) demands generative models that respond to users&#x27; tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction. We introduce a human-centric video world model that is conditioned on both tracked head pose and joint-level hand poses. For this purpose, we evaluate existing diffusion transformer conditioning strategies and propose an effective mechanism for 3D head and hand control, enabling dexterous hand--object interactions. We train a bidirectional video diffusion model teacher using this strategy and distill it into a causal, interactive system that generates egocentric virtual environments. We evaluate this generated reality system with human subjects and demonstrate improved task performance as well as a significantly higher level of perceived amount of control over the performed actions compared with relevant baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成现实：基于手部与摄像机控制的交互式视频生成的人本世界模拟</div>
<div class="mono" style="margin-top:8px">扩展现实（XR）需要能够响应用户真实世界运动轨迹的生成模型，但当前的视频世界模型仅接受粗粒度控制信号，如文本或键盘输入，这限制了其在具身交互中的实用性。我们提出了一种以人类为中心的视频世界模型，该模型同时依赖于追踪的头部姿态和关节级手部姿态。为此，我们评估了现有的扩散变压器条件化策略，并提出了一种有效的三维头部和手部控制机制，从而实现灵活的手部-物体交互。我们使用该策略训练了一个双向视频扩散模型教师，并将其蒸馏为一个因果的、交互式的系统，用于生成以第一人称视角为主的虚拟环境。我们通过人类受试者评估了该生成现实系统，并展示了其在任务表现上的提升以及相较于相关基线方法在感知控制程度上的显著提高。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Extended reality (XR) demands generative models that respond to users&#x27; tracked real-world motion, yet current video world models accept only coarse control signals such as text or keyboard input, limiting their utility for embodied interaction.</div>
</details>
</div>
<div class="card">
<div class="title">SPQ: An Ensemble Technique for Large Language Model Compression</div>
<div class="meta-line">Authors: Jiamin Yao, Eren Gultepe</div>
<div class="meta-line">First: 2026-02-20T18:44:16+00:00 · Latest: 2026-02-20T18:44:16+00:00</div>
<div class="meta-line">Comments: Accepted to LREC 2026 Main Conference</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18420v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18420v1">PDF</a> · <a href="https://github.com/JiaminYao/SPQ_LLM_Compression/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD), activation-based pruning, and post-training linear quantization. Each component targets a different source of inefficiency: i) pruning removes redundant neurons in MLP layers, ii) SVD reduces attention projections into compact low-rank factors, iii) and 8-bit quantization uniformly compresses all linear layers. At matched compression ratios, SPQ outperforms individual methods (SVD-only, pruning-only, or quantization-only) in perplexity, demonstrating the benefit of combining complementary techniques. Applied to LLaMA-2-7B, SPQ achieves up to 75% memory reduction while maintaining or improving perplexity (e.g., WikiText-2 5.47 to 4.91) and preserving accuracy on downstream benchmarks such as C4, TruthfulQA, and GSM8K. Compared to strong baselines like GPTQ and SparseGPT, SPQ offers competitive perplexity and accuracy while using less memory (6.86 GB vs. 7.16 GB for GPTQ). Moreover, SPQ improves inference throughput over GPTQ, achieving up to a 1.9x speedup, which further enhances its practicality for real-world deployment. The effectiveness of SPQ&#x27;s robust compression through layer-aware and complementary compression techniques may provide practical deployment of LLMs in memory-constrained environments. Code is available at: https://github.com/JiaminYao/SPQ_LLM_Compression/</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study presents an ensemble technique, SPQ (SVD-Pruning-Quantization), for large language model (LLM) compression that combines variance-retained singular value decomposition (SVD), activation-based pruning, and post-training linear quantization.</div>
</details>
</div>
<div class="card">
<div class="title">Benchmarking Graph Neural Networks in Solving Hard Constraint Satisfaction Problems</div>
<div class="meta-line">Authors: Geri Skenderi, Lorenzo Buffoni, Francesco D&#x27;Amico, David Machado, Raffaele Marino, Matteo Negri, Federico Ricci-Tersenghi, Carlo Lucibello, Maria Chiara Angelini</div>
<div class="meta-line">First: 2026-02-20T18:41:48+00:00 · Latest: 2026-02-20T18:41:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18419v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18419v1">PDF</a> · <a href="https://github.com/ArtLabBocconi/RandCSPBench">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Graph neural networks (GNNs) are increasingly applied to hard optimization problems, often claiming superiority over classical heuristics. However, such claims risk being unsolid due to a lack of standard benchmarks on truly hard instances. From a statistical physics perspective, we propose new hard benchmarks based on random problems. We provide these benchmarks, along with performance results from both classical heuristics and GNNs. Our fair comparison shows that classical algorithms still outperform GNNs. We discuss the challenges for neural networks in this domain. Future claims of superiority can be made more robust using our benchmarks, available at https://github.com/ArtLabBocconi/RandCSPBench.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>图神经网络在解决硬约束满足问题中的基准测试</div>
<div class="mono" style="margin-top:8px">图神经网络（GNNs）越来越多地被应用于硬优化问题，常常声称优于经典启发式算法。然而，由于缺乏对真正困难实例的标准基准，这些主张可能缺乏说服力。从统计物理学的角度出发，我们提出了基于随机问题的新硬基准。我们提供了这些基准，以及经典启发式算法和GNNs的性能结果。我们的公平比较表明，经典算法仍然优于GNNs。我们讨论了神经网络在此领域面临的挑战。未来关于优越性的主张可以借助我们的基准更加稳健，该基准可在 https://github.com/ArtLabBocconi/RandCSPBench 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Graph neural networks (GNNs) are increasingly applied to hard optimization problems, often claiming superiority over classical heuristics.</div>
</details>
</div>
<div class="card">
<div class="title">Subgroups of $U(d)$ Induce Natural RNN and Transformer Architectures</div>
<div class="meta-line">Authors: Joshua Nunley</div>
<div class="meta-line">First: 2026-02-20T18:35:43+00:00 · Latest: 2026-02-20T18:35:43+00:00</div>
<div class="meta-line">Comments: 12 pages, 3 figures, 8 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18417v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18417v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a direct framework for sequence models with hidden states on closed subgroups of U(d). We use a minimal axiomatic setup and derive recurrent and transformer templates from a shared skeleton in which subgroup choice acts as a drop-in replacement for state space, tangent projection, and update map. We then specialize to O(d) and evaluate orthogonal-state RNN and transformer models on Tiny Shakespeare and Penn Treebank under parameter-matched settings. We also report a general linear-mixing extension in tangent space, which applies across subgroup choices and improves finite-budget performance in the current O(d) experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>U(d) 子群诱导自然的 RNN 和 Transformer 架构</div>
<div class="mono" style="margin-top:8px">本文提出了一种针对 U(d) 闭子群上具有隐藏状态的序列模型的直接框架。我们采用最小的公理化设定，并从一个共享的骨架中推导出循环和 Transformer 模板，其中子群的选择可作为状态空间、切线投影和更新映射的直接替换。随后我们专门研究 O(d) 情况，并在参数匹配的设置下在 Tiny Shakespeare 和 Penn Treebank 数据集上评估正交状态的 RNN 和 Transformer 模型。我们还报告了一种通用的线性混合扩展方法，该方法适用于所有子群选择，并在当前的 O(d) 实验中提升了有限预算下的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Deep Generative model that uses physical quantities to generate and retrieve solar magnetic active regions</div>
<div class="meta-line">Authors: Subhamoy Chatterjee, Andres Munoz-Jaramillo, Anna Malanushenko</div>
<div class="meta-line">First: 2025-02-07T21:44:01+00:00 · Latest: 2026-02-20T18:35:16+00:00</div>
<div class="meta-line">Comments: 14 pages, 9 figures, accepted for publication in ApJS</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.05351v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.05351v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deep generative models have shown immense potential in generating unseen data that has properties of real data. These models learn complex data-generating distributions starting from a smaller set of latent dimensions. However, generative models have encountered great skepticism in scientific domains due to the disconnection between generative latent vectors and scientifically relevant quantities. In this study, we integrate three types of machine learning models to generate solar magnetic patches in a physically interpretable manner and use those as a query to find matching patches in real observations. We use the magnetic field measurements from Space-weather HMI Active Region Patches (SHARPs) to train a Generative Adversarial Network (GAN). We connect the physical properties of GAN-generated images with their latent vectors to train Support Vector Machines (SVMs) that do mapping between physical and latent spaces. These produce directions in the GAN latent space along which known physical parameters of the SHARPs change. We train a self-supervised learner (SSL) to make queries with generated images and find matches from real data. We find that the GAN-SVM combination enables users to produce high-quality patches that change smoothly only with a prescribed physical quantity, making generative models physically interpretable. We also show that GAN outputs can be used to retrieve real data that shares the same physical properties as the generated query. This elevates Generative Artificial Intelligence (AI) from a means-to-produce artificial data to a novel tool for scientific data interrogation, supporting its applicability beyond the domain of heliophysics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用物理量生成和检索太阳磁活动区的深度生成模型</div>
<div class="mono" style="margin-top:8px">深度生成模型在生成具有真实数据属性的未见过的数据方面展现出巨大潜力。这些模型从较小的潜在维度集开始，学习复杂的数据生成分布。然而，由于生成潜在向量与科学相关量之间的脱节，生成模型在科学领域遭遇了极大的怀疑。在本研究中，我们整合了三种类型的机器学习模型，以一种物理可解释的方式生成太阳磁斑，并利用这些生成的磁斑作为查询，在真实观测数据中寻找匹配的磁斑。我们使用空间天气HMI活动区磁斑（SHARPs）的磁场测量数据来训练生成对抗网络（GAN）。我们将GAN生成图像的物理属性与其潜在向量联系起来，训练支持向量机（SVMs）以在物理空间和潜在空间之间进行映射。这些方法在GAN潜在空间中产生方向，沿着这些方向，SHARPs的已知物理参数发生变化。我们还训练了一个自监督学习器（SSL），以使用生成的图像进行查询，并从真实数据中找到匹配项。我们发现，GAN-SVM的结合使用户仅通过指定的物理量即可生成高质量且平滑变化的磁斑，从而使生成模型具备物理可解释性。此外，我们还展示了GAN输出可用于检索与生成查询具有相同物理特性的真实数据。这将生成人工智能（AI）从一种生成人工数据的手段提升为一种科学数据查询的新工具，支持其在日球物理学以外领域的应用。</div>
</details>
</div>
<div class="card">
<div class="title">AI-Wrapped: Participatory, Privacy-Preserving Measurement of Longitudinal LLM Use In-the-Wild</div>
<div class="meta-line">Authors: Cathy Mengying Fang, Sheer Karny, Chayapatr Archiwaranguprok, Yasith Samaradivakara, Pat Pataranutaporn, Pattie Maes</div>
<div class="meta-line">First: 2026-02-20T18:34:23+00:00 · Latest: 2026-02-20T18:34:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18415v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18415v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Alignment research on large language models (LLMs) increasingly depends on understanding how these systems are used in everyday contexts. yet naturalistic interaction data is difficult to access due to privacy constraints and platform control. We present AI-Wrapped, a prototype workflow for collecting naturalistic LLM usage data while providing participants with an immediate ``wrapped&#x27;&#x27;-style report on their usage statistics, top topics, and safety-relevant behavioral patterns. We report findings from an initial deployment with 82 U.S.-based adults across 48,495 conversations from their 2025 histories. Participants used LLMs for both instrumental and reflective purposes, including creative work, professional tasks, and emotional or existential themes. Some usage patterns were consistent with potential over-reliance or perfectionistic refinement, while heavier users showed comparatively more reflective exchanges than primarily transactional ones. Methodologically, even with zero data retention and PII removal, participants may remain hesitant to share chat data due to perceived privacy and judgment risks, underscoring the importance of trust, agency, and transparent design when building measurement infrastructure for alignment research.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Alignment research on large language models (LLMs) increasingly depends on understanding how these systems are used in everyday contexts.</div>
</details>
</div>
<div class="card">
<div class="title">Learning Performance Maximizing Ensembles with Explainability Guarantees</div>
<div class="meta-line">Authors: Vincent Pisztora, Jia Li</div>
<div class="meta-line">First: 2023-12-20T02:21:26+00:00 · Latest: 2026-02-20T18:33:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2312.12715v3">Abs</a> · <a href="https://arxiv.org/pdf/2312.12715v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this paper we propose a method for the optimal allocation of observations between an intrinsically explainable glass box model and a black box model. An optimal allocation being defined as one which, for any given explainability level (i.e. the proportion of observations for which the explainable model is the prediction function), maximizes the performance of the ensemble on the underlying task, and maximizes performance of the explainable model on the observations allocated to it, subject to the maximal ensemble performance condition. The proposed method is shown to produce such explainability optimal allocations on a benchmark suite of tabular datasets across a variety of explainable and black box model types. These learned allocations are found to consistently maintain ensemble performance at very high explainability levels (explaining $74\%$ of observations on average), and in some cases even outperforming both the component explainable and black box models while improving explainability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>具有可解释性保证的学习性能最大化集成</div>
<div class="mono" style="margin-top:8px">本文提出了一种方法，用于在本质上可解释的玻璃盒模型和黑盒模型之间最优分配观测数据。最优分配被定义为在任何给定的可解释性水平（即可解释模型作为预测函数的观测比例）下，最大化集成在基础任务上的性能，同时在分配给可解释模型的观测上最大化其性能，前提是集成性能达到最大。所提出的方法在多种可解释和黑盒模型类型下的基准表格数据集上被证明能够产生这样的可解释性最优分配。这些学习到的分配在非常高的可解释性水平下保持集成性能，并且在某些情况下甚至优于组件模型，同时提高了可解释性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In this paper we propose a method for the optimal allocation of observations between an intrinsically explainable glass box model and a black box model.</div>
</details>
</div>
<div class="card">
<div class="title">Pole-Expansion of the T-Matrix Based on a Matrix-Valued AAA-Algorithm</div>
<div class="meta-line">Authors: Jan David Fischbach, Fridtjof Betz, Lukas Rebholz, Puneet Garg, Kristina Frizyuk, Felix Binkowski, Sven Burger, Martin Hammerschmidt, Carsten Rockstuhl</div>
<div class="meta-line">First: 2026-02-20T18:31:07+00:00 · Latest: 2026-02-20T18:31:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18414v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18414v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The transition matrix (T-matrix) is a complete description of an object&#x27;s linear scattering response. As such, it has found wide adoption for the theoretical and computational description of multiple-scattering phenomena. In its original form, the T-matrix describes the interaction of a scatterer with a monochromatic source. In practice, however, information about the T-matrix is usually needed in an extended spectral domain. To access the frequency-dispersion, one might naively sample T-matrices over a finely resolved set of discrete frequencies and store one T-matrix per frequency. This approach has multiple drawbacks: it is computationally expensive, requires excessive memory, and it disregards the physical origin of the spectral features, weakening physical interpretability. To overcome these major limitations, we leverage a pole-expansion technique to represent the T-matrix with arbitrary frequency resolution within a selected frequency domain via a set of resonant contributions. A matrix-valued variant of the recently established adaptive Antoulas-Anderson (AAA) algorithm for rational approximation enables us to compute the pole-expansion at minimal computational cost using only a small number of direct evaluations. We demonstrate the benefits of such a representation with examples ranging from semi-analytically accessible scatterers to quasi-dual bound states in the continuum. To allow the wider community to capitalize on these findings, we provide open-source tools to perform the presented pole-expansion of the T-matrix.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于矩阵值AAA算法的T矩阵极点展开</div>
<div class="mono" style="margin-top:8px">转移矩阵（T矩阵）是对物体线性散射响应的完整描述。因此，它在多散射现象的理论和计算描述中得到了广泛应用。在原始形式中，T矩阵描述了散射体与单色源之间的相互作用。然而在实际应用中，通常需要在扩展的频谱域中获取T矩阵的信息。一种直观的方法是通过精细分辨的离散频率集合采样T矩阵，并为每个频率存储一个T矩阵。这种方法存在多个缺点：计算成本高、需要大量内存，并且忽略了频谱特征的物理来源，削弱了物理可解释性。为克服这些主要限制，我们利用极点展开技术，通过一组共振贡献，在选定的频谱域内以任意频率分辨率表示T矩阵。我们采用一种矩阵值的自适应Antoulas-Anderson（AAA）算法的变体，用于有理逼近，从而仅通过少量直接计算即可以最小的计算成本完成极点展开。我们通过从半解析可访问的散射体到准双连续态的示例，展示了这种表示方法的优势。为了使更广泛的社区能够利用这些发现，我们提供了开源工具来执行T矩阵的极点展开。</div>
</details>
</div>
<div class="card">
<div class="title">Expressiveness of Multi-Neuron Convex Relaxations in Neural Network Certification</div>
<div class="meta-line">Authors: Yuhao Mao, Yani Zhang, Martin Vechev</div>
<div class="meta-line">Venue: ICLR</div>
<div class="meta-line">First: 2024-10-09T12:14:24+00:00 · Latest: 2026-02-20T18:28:54+00:00</div>
<div class="meta-line">Comments: ICLR&#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2410.06816v4">Abs</a> · <a href="https://arxiv.org/pdf/2410.06816v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural network certification methods heavily rely on convex relaxations to provide robustness guarantees. However, these relaxations are often imprecise: even the most accurate single-neuron relaxation is incomplete for general ReLU networks, a limitation known as the *single-neuron convex barrier*. While multi-neuron relaxations have been heuristically applied to address this issue, two central questions arise: (i) whether they overcome the convex barrier, and if not, (ii) whether they offer theoretical capabilities beyond those of single-neuron relaxations. In this work, we present the first rigorous analysis of the expressiveness of multi-neuron relaxations. Perhaps surprisingly, we show that they are inherently incomplete, even when allocated sufficient resources to capture finitely many neurons and layers optimally. This result extends the single-neuron barrier to a *universal convex barrier* for neural network certification. On the positive side, we show that completeness can be achieved by either (i) augmenting the network with a polynomial number of carefully designed ReLU neurons or (ii) partitioning the input domain into convex sub-polytopes, thereby distinguishing multi-neuron relaxations from single-neuron ones which are unable to realize the former and have worse partition complexity for the latter. Our findings establish a foundation for multi-neuron relaxations and point to new directions for certified robustness, including training methods tailored to multi-neuron relaxations and verification methods with multi-neuron relaxations as the main subroutine.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多神经元凸松弛在神经网络认证中的表达能力</div>
<div class="mono" style="margin-top:8px">神经网络认证方法严重依赖凸松弛来提供鲁棒性保证。然而，这些松弛方法往往不够精确：即使是最准确的单神经元松弛，对于一般的ReLU网络来说也是不完整的，这种局限性被称为*单神经元凸障碍*。虽然多神经元松弛已被启发式地应用于解决这一问题，但两个核心问题随之而来：(i) 它们是否克服了凸障碍？如果没有，(ii) 它们是否在理论上提供了超越单神经元松弛的能力？在这项工作中，我们首次对多神经元松弛的表达能力进行了严格的分析。也许令人惊讶的是，我们证明即使分配足够的资源以最优方式捕捉有限数量的神经元和层，它们本质上仍然是不完整的。这一结果将单神经元障碍扩展为*通用凸障碍*，适用于神经网络认证。积极的一面是，我们展示了可以通过以下两种方式实现完备性：(i) 在网络中添加多项式数量的精心设计的ReLU神经元，或 (ii) 将输入域划分为凸子多面体，从而区分多神经元松弛与单神经元松弛，后者无法实现前者，并且对于后者划分复杂度更高。我们的发现为多神经元松弛奠定了基础，并指出了新的认证鲁棒性方向，包括针对多神经元松弛的训练方法和以多神经元松弛为主要子程序的验证方法。</div>
</details>
</div>
<div class="card">
<div class="title">SpecTUS: Spectral Translator for Unknown Structures annotation from EI-MS spectra</div>
<div class="meta-line">Authors: Adam Hájek, Michal Starý, Elliott Price, Filip Jozefov, Helge Hecht, Aleš Křenek</div>
<div class="meta-line">First: 2025-02-07T17:36:13+00:00 · Latest: 2026-02-20T18:18:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.05114v2">Abs</a> · <a href="https://arxiv.org/pdf/2502.05114v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Compound identification and structure annotation from mass spectra is a well-established task widely applied in drug detection, criminal forensics, small molecule biomarker discovery and chemical engineering.
  We propose SpecTUS: Spectral Translator for Unknown Structures, a deep neural model that addresses the task of structural annotation of small molecules from low-resolution gas chromatography electron ionization mass spectra (GC-EI-MS). Our model analyzes the spectra in \textit{de novo} manner -- a direct translation from the spectra into 2D-structural representation. Our approach is particularly useful for analyzing compounds unavailable in spectral libraries.
  In a rigorous evaluation of our model on the novel structure annotation task across different libraries, we outperformed standard database search techniques by a wide margin. On a held-out testing set, including \numprint{28267} spectra from the NIST database, we show that our model&#x27;s single suggestion perfectly reconstructs 43\% of the subset&#x27;s compounds. This single suggestion is strictly better than the candidate of the database hybrid search (common method among practitioners)
  in 76\% of cases. In a~still affordable scenario of~10 suggestions, perfect reconstruction is achieved in 65\%, and 84\% are better than the hybrid search.</div></details>
</div>
<div class="card">
<div class="title">Unifying approach to uniform expressivity of graph neural networks</div>
<div class="meta-line">Authors: Huan Luo, Jonni Virtema</div>
<div class="meta-line">First: 2026-02-20T18:18:48+00:00 · Latest: 2026-02-20T18:18:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18409v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18409v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The expressive power of Graph Neural Networks (GNNs) is often analysed via correspondence to the Weisfeiler-Leman (WL) algorithm and fragments of first-order logic. Standard GNNs are limited to performing aggregation over immediate neighbourhoods or over global read-outs. To increase their expressivity, recent attempts have been made to incorporate substructural information (e.g. cycle counts and subgraph properties). In this paper, we formalize this architectural trend by introducing Template GNNs (T-GNNs), a generalized framework where node features are updated by aggregating over valid template embeddings from a specified set of graph templates. We propose a corresponding logic, Graded template modal logic (GML(T)), and generalized notions of template-based bisimulation and WL algorithm. We establish an equivalence between the expressive power of T-GNNs and GML(T), and provide a unifying approach for analysing GNN expressivity: we show how standard AC-GNNs and its recent variants can be interpreted as instantiations of T-GNNs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>图神经网络统一表达力的方法</div>
<div class="mono" style="margin-top:8px">图神经网络（GNNs）的表达能力通常通过与Weisfeiler-Leman（WL）算法和一阶逻辑片段的对应关系进行分析。标准的GNNs仅能对直接邻域或全局读取进行聚合。为了提高其表达能力，最近的研究尝试引入子结构信息（如环数和子图属性）。本文通过引入模板图神经网络（T-GNNs），形式化这一架构趋势，提出一个通用框架，其中节点特征通过从指定图模板集合中聚合有效的模板嵌入进行更新。我们还提出了相应的逻辑——分级模板模态逻辑（GML(T)），以及基于模板的广义双向模拟和WL算法。我们建立了T-GNNs与GML(T)表达能力之间的等价性，并提供了一种统一的分析GNN表达能力的方法：我们展示了标准AC-GNNs及其最近变体如何可以被解释为T-GNNs的具体实例。</div>
</details>
</div>
<div class="card">
<div class="title">Investigating Writing Professionals&#x27; Relationships with Generative AI: How Combined Perceptions of Rivalry and Collaboration Shape Work Practices and Outcomes</div>
<div class="meta-line">Authors: Rama Adithya Varanasi, Oded Nov, Batia Mishan Wiesenfeld</div>
<div class="meta-line">First: 2026-02-09T03:01:21+00:00 · Latest: 2026-02-20T18:18:05+00:00</div>
<div class="meta-line">Comments: CHI&#x27;2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.08227v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.08227v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This study investigates how professional writers&#x27; complex relationship with GenAI shapes their work practices and outcomes. Through a cross-sectional survey with writing professionals (n=403) in diverse roles, we show that collaboration and rivalry orientation are associated with differences in work practices and outcomes. Rivalry is primarily associated with relational crafting and skill maintenance. Collaboration is primarily associated with task crafting, productivity, and satisfaction, at the cost of long-term skill deterioration. Combination of the orientations (high rivalry and high collaboration) reconciles these differences, while boosting the association with the outcomes. Our findings argue for a balanced approach where high levels of rivalry and collaboration are essential to shape work practices and generate outcomes aimed at the long-term success of the job. We present key design implications on how to increase friction (rivalry) and reduce over-reliance (collaboration) to achieve a more balanced relationship with GenAI.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探究写作专业人士与生成式人工智能的关系：竞争与合作的综合认知如何塑造工作实践与成果</div>
<div class="mono" style="margin-top:8px">本研究探讨专业作家与生成式人工智能（GenAI）复杂关系如何影响其工作实践与成果。通过对不同角色的写作专业人士（n=403）进行横断面调查，我们发现竞争导向与合作导向与工作实践和成果存在差异。竞争主要与关系构建和技能维持相关，而合作则主要与任务构建、生产力和满意度相关，但以长期技能退化为代价。竞争与合作导向的结合缓解了这些差异，并增强了与成果的关联。我们的研究结果支持一种平衡的方法，即高水平的竞争与合作对于塑造工作实践和产生有利于长期职业成功的成果至关重要。我们提出了关键的设计启示，说明如何增加摩擦（竞争）并减少过度依赖（合作），以实现与GenAI更为平衡的关系。</div>
</details>
</div>
<div class="card">
<div class="title">Latent Equivariant Operators for Robust Object Recognition: Promise and Challenges</div>
<div class="meta-line">Authors: Minh Dinh, Stéphane Deny</div>
<div class="meta-line">First: 2026-02-20T18:14:05+00:00 · Latest: 2026-02-20T18:14:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18406v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18406v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Despite the successes of deep learning in computer vision, difficulties persist in recognizing objects that have undergone group-symmetric transformations rarely seen during training-for example objects seen in unusual poses, scales, positions, or combinations thereof. Equivariant neural networks are a solution to the problem of generalizing across symmetric transformations, but require knowledge of transformations a priori. An alternative family of architectures proposes to earn equivariant operators in a latent space from examples of symmetric transformations. Here, using simple datasets of rotated and translated noisy MNIST, we illustrate how such architectures can successfully be harnessed for out-of-distribution classification, thus overcoming the limitations of both traditional and equivariant networks. While conceptually enticing, we discuss challenges ahead on the path of scaling these architectures to more complex datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>鲁棒物体识别的潜在等变算子：前景与挑战</div>
<div class="mono" style="margin-top:8px">尽管深度学习在计算机视觉领域取得了成功，但在识别训练中很少见的群对称变换下的物体（例如以不寻常姿态、尺度、位置或其组合出现的物体）仍存在困难。等变神经网络是解决在对称变换下泛化问题的一种方法，但需要事先知道变换类型。另一种架构家族则提出通过在潜在空间中从对称变换的示例中学习等变算子。本文使用旋转和位移的噪声MNIST简单数据集，展示了如何成功利用此类架构进行分布外分类，从而克服传统和等变网络的局限性。虽然这一概念具有吸引力，但我们讨论了在将这些架构扩展到更复杂数据集的过程中面临的挑战。</div>
</details>
</div>
<div class="card">
<div class="title">Wink: Recovering from Misbehaviors in Coding Agents</div>
<div class="meta-line">Authors: Rahul Nanda, Chandra Maddila, Smriti Jha, Euna Mehnaz Khan, Matteo Paltenghi, Satish Chandra</div>
<div class="meta-line">First: 2026-02-19T03:15:00+00:00 · Latest: 2026-02-20T18:13:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.17037v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.17037v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autonomous coding agents, powered by large language models (LLMs), are increasingly being adopted in the software industry to automate complex engineering tasks. However, these agents are prone to a wide range of misbehaviors, such as deviating from the user&#x27;s instructions, getting stuck in repetitive loops, or failing to use tools correctly. These failures disrupt the development workflow and often require resource-intensive manual intervention. In this paper, we present a system for automatically recovering from agentic misbehaviors at scale. We first introduce a taxonomy of misbehaviors grounded in an analysis of production traffic, identifying three primary categories: Specification Drift, Reasoning Problems, and Tool Call Failures, which we find occur in about 30% of all agent trajectories.
  To address these issues, we developed a lightweight, asynchronous self-intervention system named Wink. Wink observes agent trajectories and provides targeted course-correction guidance to nudge the agent back to a productive path. We evaluated our system on over 10,000 real world agent trajectories and found that it successfully resolves 90% of the misbehaviors that require a single intervention. Furthermore, a live A/B test in our production environment demonstrated that our system leads to a statistically significant reduction in Tool Call Failures, Tokens per Session and Engineer Interventions per Session. We present our experience designing and deploying this system, offering insights into the challenges of building resilient agentic systems at scale.</div></details>
</div>
<div class="card">
<div class="title">Well-posedness and time stepping adaptivity for a class of collocation discretisations of time-fractional subdiffusion equations</div>
<div class="meta-line">Authors: Sebastian Franz, Natalia Kopteva</div>
<div class="meta-line">First: 2026-02-20T18:12:35+00:00 · Latest: 2026-02-20T18:12:35+00:00</div>
<div class="meta-line">Comments: 23 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18404v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18404v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time-fractional parabolic equations with a Caputo time derivative of order $α\in(0,1)$ are discretised in time using collocation methods, which assume that the Caputo derivative of the computed solution is piecewise-polynomial. For such discretisations of any order $m\ge 0$, with any choice of collocation points, we give sufficient conditions for existence and uniqueness of collocation solutions. Furthermore, we investigate the applicability and performance of such schemes in the context of the a-posteriori error estimation and adaptive time stepping algorithms.</div></details>
</div>
<div class="card">
<div class="title">xLSTM Scaling Laws: Competitive Performance with Linear Time-Complexity</div>
<div class="meta-line">Authors: Maximilian Beck, Kajetan Schweighofer, Sebastian Böck, Sebastian Lehner, Sepp Hochreiter</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2025-10-02T17:14:34+00:00 · Latest: 2026-02-20T18:12:16+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026. Code and data available at https://github.com/NX-AI/xlstm_scaling_laws</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.02228v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.02228v2">PDF</a> · <a href="https://github.com/NX-AI/xlstm_scaling_laws">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling laws play a central role in the success of Large Language Models (LLMs), enabling the prediction of model performance relative to compute budgets prior to training. While Transformers have been the dominant architecture, recent alternatives such as xLSTM offer linear complexity with respect to context length while remaining competitive in the billion-parameter regime. We conduct a comparative investigation on the scaling behavior of Transformers and xLSTM along the following lines, providing insights to guide future model design and deployment. First, we study the scaling behavior for xLSTM in compute-optimal and over-training regimes using both IsoFLOP and parametric fit approaches on a wide range of model sizes (80M-7B) and number of training tokens (2B-2T). Second, we examine the dependence of optimal model sizes on context length, a pivotal aspect that was largely ignored in previous work. Finally, we analyze inference-time scaling characteristics. Our findings reveal that in typical LLM training and inference scenarios, xLSTM scales favorably compared to Transformers. Notably, xLSTM models consistently Pareto-dominate Transformer models, delivering lower cross-entropy loss for the same compute budget.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>xLSTM扩展定律：与线性时间复杂度相比具有竞争力的性能</div>
<div class="mono" style="margin-top:8px">扩展定律在大型语言模型（LLMs）的成功中起着核心作用，使人们能够在训练前预测模型性能与计算预算之间的关系。尽管Transformer架构一直是主流，但最近的替代方案如xLSTM在保持与Transformer相当的十亿参数规模性能的同时，实现了相对于上下文长度的线性复杂度。我们对Transformer和xLSTM的扩展行为进行了比较研究，为未来模型的设计和部署提供见解。首先，我们在计算最优和过训练状态下，通过IsoFLOP和参数拟合方法，研究了xLSTM在多种模型规模（80M-7B）和训练标记数量（2B-2T）下的扩展行为。其次，我们探讨了最优模型规模与上下文长度之间的依赖关系，这是之前研究中被忽视的关键方面。最后，我们分析了推理时间的扩展特性。我们的研究结果表明，在典型的LLM训练和推理场景中，xLSTM相比Transformer具有更有利的扩展性。值得注意的是，xLSTM模型在相同计算预算下，始终优于Transformer模型，表现出更低的交叉熵损失。</div>
</details>
</div>
<div class="card">
<div class="title">Scientific Knowledge-Guided Machine Learning for Vessel Power Prediction: A Comparative Study</div>
<div class="meta-line">Authors: Orfeas Bourchas, George Papalambrou</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2026-02-20T18:12:14+00:00 · Latest: 2026-02-20T18:12:14+00:00</div>
<div class="meta-line">Comments: Accepted to the KGML Bridge at AAAI 2026 (non-archival)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18403v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18403v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate prediction of main engine power is essential for vessel performance optimization, fuel efficiency, and compliance with emission regulations. Conventional machine learning approaches, such as Support Vector Machines, variants of Artificial Neural Networks (ANNs), and tree-based methods like Random Forests, Extra Tree Regressors, and XGBoost, can capture nonlinearities but often struggle to respect the fundamental propeller law relationship between power and speed, resulting in poor extrapolation outside the training envelope. This study introduces a hybrid modeling framework that integrates physics-based knowledge from sea trials with data-driven residual learning. The baseline component, derived from calm-water power curves of the form $P = cV^n$, captures the dominant power-speed dependence, while another, nonlinear, regressor is then trained to predict the residual power, representing deviations caused by environmental and operational conditions. By constraining the machine learning task to residual corrections, the hybrid model simplifies learning, improves generalization, and ensures consistency with the underlying physics. In this study, an XGBoost, a simple Neural Network, and a Physics-Informed Neural Network (PINN) coupled with the baseline component were compared to identical models without the baseline component. Validation on in-service data demonstrates that the hybrid model consistently outperformed a pure data-driven baseline in sparse data regions while maintaining similar performance in populated ones. The proposed framework provides a practical and computationally efficient tool for vessel performance monitoring, with applications in weather routing, trim optimization, and energy efficiency planning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于科学知识引导的机器学习船舶功率预测：比较研究</div>
<div class="mono" style="margin-top:8px">准确预测主发动机功率对于船舶性能优化、燃油效率以及符合排放法规至关重要。传统的机器学习方法，如支持向量机、人工神经网络（ANN）的不同变体以及基于树的方法（如随机森林、极端树回归器和XGBoost），虽然可以捕捉非线性关系，但往往难以遵循功率与速度之间的基本螺旋桨定律关系，导致在训练范围之外的外推效果不佳。本研究引入了一种混合建模框架，将来自海试的物理知识与数据驱动的残差学习相结合。基准部分基于形式为 $P = cV^n$ 的静水功率曲线，捕捉主要的功率-速度依赖关系，而另一个非线性回归器则用于预测残差功率，表示由环境和操作条件引起的偏差。通过将机器学习任务限制在残差校正上，混合模型简化了学习过程，提高了泛化能力，并确保与底层物理规律的一致性。在本研究中，将XGBoost、一个简单的神经网络以及与基准部分结合的物理信息神经网络（PINN）与不包含基准部分的相同模型进行了比较。在服务数据上的验证表明，混合模型在稀疏数据区域始终优于纯数据驱动的基准模型，而在数据密集区域则保持相似的性能。所提出的框架为船舶性能监测提供了一个实用且计算高效的工具，可应用于天气航线规划、船舶适航优化和能源效率规划等领域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Accurate prediction of main engine power is essential for vessel performance optimization, fuel efficiency, and compliance with emission regulations.</div>
</details>
</div>
<div class="card">
<div class="title">Leakage and Second-Order Dynamics Improve Hippocampal RNN Replay</div>
<div class="meta-line">Authors: Josue Casco-Rodriguez, Nanda H. Krishna, Richard G. Baraniuk</div>
<div class="meta-line">First: 2026-02-20T18:07:09+00:00 · Latest: 2026-02-20T18:07:09+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18401v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18401v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Biological neural networks (like the hippocampus) can internally generate &quot;replay&quot; resembling stimulus-driven activity. Recent computational models of replay use noisy recurrent neural networks (RNNs) trained to path-integrate. Replay in these networks has been described as Langevin sampling, but new modifiers of noisy RNN replay have surpassed this description. We re-examine noisy RNN replay as sampling to understand or improve it in three ways: (1) Under simple assumptions, we prove that the gradients replay activity should follow are time-varying and difficult to estimate, but readily motivate the use of hidden state leakage in RNNs for replay. (2) We confirm that hidden state adaptation (negative feedback) encourages exploration in replay, but show that it incurs non-Markov sampling that also slows replay. (3) We propose the first model of temporally compressed replay in noisy path-integrating RNNs through hidden state momentum, connect it to underdamped Langevin sampling, and show that, together with adaptation, it counters slowness while maintaining exploration. We verify our findings via path-integration of 2D triangular and T-maze paths and of high-dimensional paths of synthetic rat place cell activity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>泄漏和二阶动力学改进海马体RNN回放</div>
<div class="mono" style="margin-top:8px">生物神经网络（如海马体）可以内部生成类似于刺激驱动活动的&quot;回放&quot;。最近的回放计算模型使用带有噪声的递归神经网络（RNNs）进行路径积分训练。这些网络中的回放被描述为朗之万采样，但新的噪声RNN回放修饰方法超越了这一描述。我们通过三种方式重新审视噪声RNN回放作为采样：(1) 在简单假设下，我们证明回放活动应遵循的梯度是随时间变化且难以估计的，但可以很容易地通过RNN中的隐藏状态泄漏来实现回放。 (2) 我们确认隐藏状态适应（负反馈）鼓励回放中的探索，但同时也导致非马尔可夫采样，从而减缓回放。 (3) 我们通过隐藏状态动量提出了第一个噪声路径积分RNN中时间压缩回放的模型，将其与欠阻尼朗之万采样联系起来，并表明，与适应相结合，它可以在保持探索的同时抵消回放的缓慢性。我们通过二维三角形和T迷宫路径以及合成老鼠位置细胞活动的高维路径的路径积分验证了我们的发现。</div>
</details>
</div>
<div class="card">
<div class="title">Exploiting Completeness Perception with Diffusion Transformer for Unified 3D MRI Synthesis</div>
<div class="meta-line">Authors: Junkai Liu, Nay Aung, Theodoros N. Arvanitis, Joao A. C. Lima, Steffen E. Petersen, Daniel C. Alexander, Le Zhang</div>
<div class="meta-line">First: 2026-02-20T18:05:39+00:00 · Latest: 2026-02-20T18:05:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18400v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18400v1">PDF</a> · <a href="https://github.com/JK-Liu7/CoPeDiT">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Missing data problems, such as missing modalities in multi-modal brain MRI and missing slices in cardiac MRI, pose significant challenges in clinical practice. Existing methods rely on external guidance to supply detailed missing state for instructing generative models to synthesize missing MRIs. However, manual indicators are not always available or reliable in real-world scenarios due to the unpredictable nature of clinical environments. Moreover, these explicit masks are not informative enough to provide guidance for improving semantic consistency. In this work, we argue that generative models should infer and recognize missing states in a self-perceptive manner, enabling them to better capture subtle anatomical and pathological variations. Towards this goal, we propose CoPeDiT, a general-purpose latent diffusion model equipped with completeness perception for unified synthesis of 3D MRIs. Specifically, we incorporate dedicated pretext tasks into our tokenizer, CoPeVAE, empowering it to learn completeness-aware discriminative prompts, and design MDiT3D, a specialized diffusion transformer architecture for 3D MRI synthesis, that effectively uses the learned prompts as guidance to enhance semantic consistency in 3D space. Comprehensive evaluations on three large-scale MRI datasets demonstrate that CoPeDiT significantly outperforms state-of-the-art methods, achieving superior robustness, generalizability, and flexibility. The code is available at https://github.com/JK-Liu7/CoPeDiT .</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Missing data problems, such as missing modalities in multi-modal brain MRI and missing slices in cardiac MRI, pose significant challenges in clinical practice.</div>
</details>
</div>
<div class="card">
<div class="title">How Fast Can I Run My VLA? Demystifying VLA Inference Performance with VLA-Perf</div>
<div class="meta-line">Authors: Wenqi Jiang, Jason Clemons, Karu Sankaralingam, Christos Kozyrakis</div>
<div class="meta-line">First: 2026-02-20T18:02:28+00:00 · Latest: 2026-02-20T18:02:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.18397v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.18397v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models have recently demonstrated impressive capabilities across various embodied AI tasks. While deploying VLA models on real-world robots imposes strict real-time inference constraints, the inference performance landscape of VLA remains poorly understood due to the large combinatorial space of model architectures and inference systems. In this paper, we ask a fundamental research question: How should we design future VLA models and systems to support real-time inference? To address this question, we first introduce VLA-Perf, an analytical performance model that can analyze inference performance for arbitrary combinations of VLA models and inference systems. Using VLA-Perf, we conduct the first systematic study of the VLA inference performance landscape. From a model-design perspective, we examine how inference performance is affected by model scaling, model architectural choices, long-context video inputs, asynchronous inference, and dual-system model pipelines. From the deployment perspective, we analyze where VLA inference should be executed -- on-device, on edge servers, or in the cloud -- and how hardware capability and network performance jointly determine end-to-end latency. By distilling 15 key takeaways from our comprehensive evaluation, we hope this work can provide practical guidance for the design of future VLA models and inference systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>我的VLA能跑多快？通过VLA-Perf揭开VLA推理性能之谜</div>
<div class="mono" style="margin-top:8px">最近，视觉-语言-动作（VLA）模型在各种具身人工智能任务中展示了令人印象深刻的能力。然而，由于VLA模型架构和推理系统的庞大组合空间，将VLA模型部署在现实机器人上所面临的严格实时推理约束，使得VLA推理性能的现状仍不明确。本文提出一个基本研究问题：我们应如何设计未来的VLA模型和系统以支持实时推理？为了解决这个问题，我们首先引入了VLA-Perf，这是一个分析性能模型，可以分析任意VLA模型和推理系统的推理性能。利用VLA-Perf，我们进行了首次系统性的VLA推理性能研究。从模型设计的角度，我们探讨了模型扩展、模型架构选择、长上下文视频输入、异步推理以及双系统模型流水线对推理性能的影响。从部署的角度，我们分析了VLA推理应在设备端、边缘服务器还是云端执行，并研究了硬件能力和网络性能如何共同决定端到端延迟。通过提炼我们全面评估的15个关键结论，我们希望这项工作能为未来VLA模型和推理系统的构建提供实用指导。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision-Language-Action (VLA) models have recently demonstrated impressive capabilities across various embodied AI tasks.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260223_0401.html">20260223_0401</a>
<a href="archive/20260222_0402.html">20260222_0402</a>
<a href="archive/20260221_0415.html">20260221_0415</a>
<a href="archive/20260220_0410.html">20260220_0410</a>
<a href="archive/20260219_0419.html">20260219_0419</a>
<a href="archive/20260218_0416.html">20260218_0416</a>
<a href="archive/20260217_0410.html">20260217_0410</a>
<a href="archive/20260216_0403.html">20260216_0403</a>
<a href="archive/20260215_0401.html">20260215_0401</a>
<a href="archive/20260213_0421.html">20260213_0421</a>
<a href="archive/20260212_0425.html">20260212_0425</a>
<a href="archive/20260210_0435.html">20260210_0435</a>
<a href="archive/20260209_0404.html">20260209_0404</a>
<a href="archive/20260208_0353.html">20260208_0353</a>
<a href="archive/20260207_0410.html">20260207_0410</a>
<a href="archive/20260206_0412.html">20260206_0412</a>
<a href="archive/20260205_0417.html">20260205_0417</a>
<a href="archive/20260204_0421.html">20260204_0421</a>
<a href="archive/20260202_0402.html">20260202_0402</a>
<a href="archive/20260201_0354.html">20260201_0354</a>
<a href="archive/20260131_0408.html">20260131_0408</a>
<a href="archive/20260130_0406.html">20260130_0406</a>
<a href="archive/20260129_0407.html">20260129_0407</a>
<a href="archive/20260128_0407.html">20260128_0407</a>
<a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
