<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-06 04:12</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260206_0412</div>
    <div class="row"><div class="card">
<div class="title">Reinforced Attention Learning</div>
<div class="meta-line">Authors: Bangzheng Li, Jianmo Ni, Chen Qu, Ian Miao, Liu Yang, Xingyu Fu, Muhao Chen, Derek Zhiyuan Cheng</div>
<div class="meta-line">First: 2026-02-04T18:59:52+00:00 · Latest: 2026-02-04T18:59:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04884v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04884v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Post-training with Reinforcement Learning (RL) has substantially improved reasoning in Large Language Models (LLMs) via test-time scaling. However, extending this paradigm to Multimodal LLMs (MLLMs) through verbose rationales yields limited gains for perception and can even degrade performance.
  We propose Reinforced Attention Learning (RAL), a policy-gradient framework that directly optimizes internal attention distributions rather than output token sequences. By shifting optimization from what to generate to where to attend, RAL promotes effective information allocation and improved grounding in complex multimodal inputs. Experiments across diverse image and video benchmarks show consistent gains over GRPO and other baselines. We further introduce On-Policy Attention Distillation, demonstrating that transferring latent attention behaviors yields stronger cross-modal alignment than standard knowledge distillation. Our results position attention policies as a principled and general alternative for multimodal post-training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>强化注意力学习</div>
<div class="mono" style="margin-top:8px">通过测试时的扩展训练，基于强化学习（RL）的后训练显著提升了大型语言模型（LLMs）的推理能力。然而，通过冗长的推理过程将这一范式扩展到多模态语言模型（MLLMs）时，对感知能力的提升有限，甚至可能降低性能。我们提出了一种名为强化注意力学习（RAL）的策略梯度框架，该框架直接优化内部注意力分布，而非输出标记序列。通过将优化目标从生成什么内容转移到关注哪些部分，RAL促进了复杂多模态输入中有效信息分配和更准确的语义对齐。我们在多种图像和视频基准数据集上的实验结果表明，RAL在多个指标上优于GRPO和其他基线方法。我们进一步引入了基于策略的注意力蒸馏方法，证明了将潜在的注意力行为进行迁移，能够比标准的知识蒸馏方法实现更强的跨模态对齐。我们的结果表明，注意力策略可以作为多模态后训练的一种原理性和通用性替代方案。</div>
</details>
</div>
<div class="card">
<div class="title">Protein Autoregressive Modeling via Multiscale Structure Generation</div>
<div class="meta-line">Authors: Yanru Qu, Cheng-Yen Hsieh, Zaixiang Zheng, Ge Liu, Quanquan Gu</div>
<div class="meta-line">First: 2026-02-04T18:59:49+00:00 · Latest: 2026-02-04T18:59:49+00:00</div>
<div class="meta-line">Comments: ByteDance Seed Tech Report; Page: https://par-protein.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04883v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04883v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://par-protein.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present protein autoregressive modeling (PAR), the first multi-scale autoregressive framework for protein backbone generation via coarse-to-fine next-scale prediction. Using the hierarchical nature of proteins, PAR generates structures that mimic sculpting a statue, forming a coarse topology and refining structural details over scales. To achieve this, PAR consists of three key components: (i) multi-scale downsampling operations that represent protein structures across multiple scales during training; (ii) an autoregressive transformer that encodes multi-scale information and produces conditional embeddings to guide structure generation; (iii) a flow-based backbone decoder that generates backbone atoms conditioned on these embeddings. Moreover, autoregressive models suffer from exposure bias, caused by the training and the generation procedure mismatch, and substantially degrades structure generation quality. We effectively alleviate this issue by adopting noisy context learning and scheduled sampling, enabling robust backbone generation. Notably, PAR exhibits strong zero-shot generalization, supporting flexible human-prompted conditional generation and motif scaffolding without requiring fine-tuning. On the unconditional generation benchmark, PAR effectively learns protein distributions and produces backbones of high design quality, and exhibits favorable scaling behavior. Together, these properties establish PAR as a promising framework for protein structure generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过多尺度结构生成实现蛋白质自回归建模</div>
<div class="mono" style="margin-top:8px">我们提出了蛋白质自回归建模（PAR），这是首个通过从粗到细的下一级预测来进行蛋白质主干生成的多尺度自回归框架。利用蛋白质的层次特性，PAR在不同尺度上生成结构，模拟雕塑雕像的过程，先形成粗略拓扑，再逐步细化结构细节。为实现这一目标，PAR包含三个关键组件：(i) 多尺度下采样操作，在训练过程中表示蛋白质结构在多个尺度上的信息；(ii) 一个自回归Transformer，编码多尺度信息并生成条件嵌入以指导结构生成；(iii) 基于流的主干解码器，根据这些嵌入生成主干原子。此外，自回归模型会受到暴露偏差的影响，这是由于训练过程与生成过程不匹配所导致的，会显著降低结构生成质量。我们通过采用带噪声上下文学习和计划采样有效缓解了这一问题，从而实现稳健的主干生成。值得注意的是，PAR展现出强大的零样本泛化能力，支持灵活的人类提示条件生成和模因支架构建，而无需微调。在无条件生成基准测试中，PAR能够有效学习蛋白质分布并生成高质量的主干结构，同时表现出良好的可扩展性。综上所述，这些特性使PAR成为蛋白质结构生成的有前景框架。</div>
</details>
</div>
<div class="card">
<div class="title">Contrastive Continual Learning for Model Adaptability in Internet of Things</div>
<div class="meta-line">Authors: Ajesh Koyatan Chathoth</div>
<div class="meta-line">First: 2026-02-04T18:59:14+00:00 · Latest: 2026-02-04T18:59:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04881v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04881v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Internet of Things (IoT) deployments operate in nonstationary, dynamic environments where factors such as sensor drift, evolving user behavior, and heterogeneous user privacy requirements can affect application utility. Continual learning (CL) addresses this by adapting models over time without catastrophic forgetting. Meanwhile, contrastive learning has emerged as a powerful representation-learning paradigm that improves robustness and sample efficiency in a self-supervised manner. This paper reviews the usage of \emph{contrastive continual learning} (CCL) for IoT, connecting algorithmic design (replay, regularization, distillation, prompts) with IoT system realities (TinyML constraints, intermittent connectivity, privacy). We present a unifying problem formulation, derive common objectives that blend contrastive and distillation losses, propose an IoT-oriented reference architecture for on-device, edge, and cloud-based CCL, and provide guidance on evaluation protocols and metrics. Finally, we highlight open unique challenges with respect to the IoT domain, such as spanning tabular and streaming IoT data, concept drift, federated settings, and energy-aware training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于物联网中模型适应性的对比持续学习</div>
<div class="mono" style="margin-top:8px">物联网（IoT）部署运行在非平稳、动态的环境中，传感器漂移、用户行为演变以及异构用户隐私需求等因素会影响应用的效用。持续学习（CL）通过在不发生灾难性遗忘的情况下随时间适应模型来应对这一挑战。同时，对比学习作为一种强大的表示学习范式，通过自监督方式提高了鲁棒性和样本效率。本文回顾了\emph{对比持续学习}（CCL）在物联网中的应用，将算法设计（重放、正则化、蒸馏、提示）与物联网系统现实（TinyML约束、间歇性连接、隐私）联系起来。我们提出了统一的问题建模，推导了结合对比损失和蒸馏损失的通用目标，提出了面向物联网的参考架构，用于设备端、边缘和云端的CCL，并提供了评估协议和指标的指导。最后，我们突出了物联网领域特有的开放挑战，如跨表格和流式物联网数据、概念漂移、联邦设置以及能耗感知的训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Internet of Things (IoT) deployments operate in nonstationary, dynamic environments where factors such as sensor drift, evolving user behavior, and heterogeneous user privacy requirements can affect application utility.</div>
</details>
</div>
<div class="card">
<div class="title">Robust inverse material design with physical guarantees using the Voigt-Reuss Net</div>
<div class="meta-line">Authors: Sanath Keshav, Felix Fritzen</div>
<div class="meta-line">First: 2025-11-14T15:17:37+00:00 · Latest: 2026-02-04T18:59:08+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.11388v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.11388v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We propose a spectrally normalized surrogate for forward and inverse mechanical homogenization with hard physical guarantees. Leveraging the Voigt-Reuss bounds, we factor their difference via a Cholesky-like operator and learn a dimensionless, symmetric positive semi-definite representation with eigenvalues in $[0,1]$; the inverse map returns symmetric positive-definite predictions that lie between the bounds in the Löwner sense. In 3D linear elasticity on an open dataset of stochastic biphasic microstructures, a fully connected Voigt-Reuss net trained on $&gt;\!7.5\times 10^{5}$ FFT-based labels with 236 isotropy-invariant descriptors and three contrast parameters recovers the isotropic projection with near-perfect fidelity (isotropy-related entries: $R^2 \ge 0.998$), while anisotropy-revealing couplings are unidentifiable from $SO(3)$-invariant inputs. Tensor-level relative Frobenius errors have median $\approx 1.7\%$ and mean $\approx 3.4\%$ across splits. For 2D plane strain on thresholded trigonometric microstructures, coupling spectral normalization with a differentiable renderer and a CNN yields $R^2&gt;0.99$ on all components, subpercent normalized losses, accurate tracking of percolation-induced eigenvalue jumps, and robust generalization to out-of-distribution images. Treating the parametric microstructure as design variables, batched first-order optimization with a single surrogate matches target tensors within a few percent and returns diverse near-optimal designs. Overall, the Voigt-Reuss net unifies accurate, physically admissible forward prediction with large-batch, constraint-consistent inverse design, and is generic to elliptic operators and coupled-physics settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用Voigt-Reuss网络进行具有物理保证的鲁棒逆材料设计</div>
<div class="mono" style="margin-top:8px">我们提出了一种频谱归一化的代理模型，用于具有硬性物理保证的正向和逆向机械均质化。通过利用Voigt-Reuss界限，我们借助类似Cholesky的操作来分解其差异，并学习一个无量纲、对称正半定的表示，其特征值位于$[0,1]$区间内；逆映射返回的对称正定预测在Löwner意义下位于界限之间。在开放数据集的随机双相微结构上进行3D线弹性分析，一个全连接的Voigt-Reuss网络在$&gt;\!7.5\times 10^{5}$个基于FFT的标签上训练，使用236个各向同性不变描述符和三个对比参数，能够以接近完美的保真度恢复各向同性投影（各向同性相关条目：$R^2 \ge 0.998$），而各向异性揭示的耦合则无法从$SO(3)$不变输入中识别。在不同分割下，张量级别的相对Frobenius误差中位数约为1.7%，平均约为3.4%。对于阈值化三角微结构上的2D平面应变，将频谱归一化与可微渲染器和CNN结合，可在所有组件上实现$R^2&gt;0.99$，归一化损失低于百分之一，准确跟踪渗流诱导的特征值跳跃，并且能够稳健地泛化到分布外图像。将参数化微结构视为设计变量，使用单个代理进行批量一阶优化可在几百分比内匹配目标张量，并返回多样化的近最优设计。总体而言，Voigt-Reuss网络统一了准确且符合物理的正向预测与大规模、约束一致的逆向设计，并适用于椭圆算子和耦合物理场景。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We propose a spectrally normalized surrogate for forward and inverse mechanical homogenization with hard physical guarantees.</div>
</details>
</div>
<div class="card">
<div class="title">Rethinking the Trust Region in LLM Reinforcement Learning</div>
<div class="meta-line">Authors: Penghui Qi, Xiangxin Zhou, Zichen Liu, Tianyu Pang, Chao Du, Min Lin, Wee Sun Lee</div>
<div class="meta-line">First: 2026-02-04T18:59:04+00:00 · Latest: 2026-02-04T18:59:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04879v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04879v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reinforcement learning (RL) has become a cornerstone for fine-tuning Large Language Models (LLMs), with Proximal Policy Optimization (PPO) serving as the de facto standard algorithm. Despite its ubiquity, we argue that the core ratio clipping mechanism in PPO is structurally ill-suited for the large vocabularies inherent to LLMs. PPO constrains policy updates based on the probability ratio of sampled tokens, which serves as a noisy single-sample Monte Carlo estimate of the true policy divergence. This creates a sub-optimal learning dynamic: updates to low-probability tokens are aggressively over-penalized, while potentially catastrophic shifts in high-probability tokens are under-constrained, leading to training inefficiency and instability. To address this, we propose Divergence Proximal Policy Optimization (DPPO), which substitutes heuristic clipping with a more principled constraint based on a direct estimate of policy divergence (e.g., Total Variation or KL). To avoid huge memory footprint, we introduce the efficient Binary and Top-K approximations to capture the essential divergence with negligible overhead. Extensive empirical evaluations demonstrate that DPPO achieves superior training stability and efficiency compared to existing methods, offering a more robust foundation for RL-based LLM fine-tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新思考LLM强化学习中的信任区域</div>
<div class="mono" style="margin-top:8px">强化学习（RL）已成为微调大型语言模型（LLMs）的核心方法，其中近端策略优化（PPO）是事实上的标准算法。尽管PPO被广泛使用，但我们认为其核心的比率裁剪机制在结构上并不适合LLMs固有的大词汇量。PPO基于采样标记的概率比率来约束策略更新，这相当于对真实策略差异的噪声单样本蒙特卡洛估计。这导致了次优的学习动态：低概率标记的更新被过度惩罚，而高概率标记的潜在灾难性变化则受到约束不足，从而造成训练效率低下和不稳定。为了解决这一问题，我们提出了基于策略差异直接估计的分歧近端策略优化（DPPO），用更合理的约束机制替代启发式裁剪。为了减少巨大的内存开销，我们引入了高效的二进制和Top-K近似方法，以几乎无额外开销的方式捕捉关键的策略差异。大量实证评估表明，DPPO在训练稳定性和效率方面均优于现有方法，为基于强化学习的LLM微调提供了更稳固的基础。</div>
</details>
</div>
<div class="card">
<div class="title">CoWTracker: Tracking by Warping instead of Correlation</div>
<div class="meta-line">Authors: Zihang Lai, Eldar Insafutdinov, Edgar Sucar, Andrea Vedaldi</div>
<div class="meta-line">First: 2026-02-04T18:58:59+00:00 · Latest: 2026-02-04T18:58:59+00:00</div>
<div class="meta-line">Comments: Project website: cowtracker.github.io</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04877v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04877v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Dense point tracking is a fundamental problem in computer vision, with applications ranging from video analysis to robotic manipulation. State-of-the-art trackers typically rely on cost volumes to match features across frames, but this approach incurs quadratic complexity in spatial resolution, limiting scalability and efficiency. In this paper, we propose \method, a novel dense point tracker that eschews cost volumes in favor of warping. Inspired by recent advances in optical flow, our approach iteratively refines track estimates by warping features from the target frame to the query frame based on the current estimate. Combined with a transformer architecture that performs joint spatiotemporal reasoning across all tracks, our design establishes long-range correspondences without computing feature correlations. Our model is simple and achieves state-of-the-art performance on standard dense point tracking benchmarks, including TAP-Vid-DAVIS, TAP-Vid-Kinetics, and Robo-TAP. Remarkably, the model also excels at optical flow, sometimes outperforming specialized methods on the Sintel, KITTI, and Spring benchmarks. These results suggest that warping-based architectures can unify dense point tracking and optical flow estimation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CoWTracker：通过形变追踪而非相关性</div>
<div class="mono" style="margin-top:8px">密集点追踪是计算机视觉中的基础问题，应用范围从视频分析到机器人操作。最先进的追踪器通常依赖于代价体来匹配帧间的特征，但这种方法在空间分辨率上具有二次复杂度，限制了可扩展性和效率。本文提出\method，一种新颖的密集点追踪方法，摒弃代价体，转而采用形变。受近期光流研究进展的启发，我们的方法通过将目标帧的特征形变到查询帧，基于当前估计迭代地优化追踪结果。结合一种执行联合时空推理的Transformer架构，我们的设计能够在不计算特征相关性的前提下建立长距离对应关系。我们的模型结构简单，在标准的密集点追踪基准上（包括TAP-Vid-DAVIS、TAP-Vid-Kinetics和Robo-TAP）取得了最先进的性能。值得注意的是，该模型在光流任务上也表现出色，有时甚至在Sintel、KITTI和Spring基准上超越专门方法。这些结果表明，基于形变的架构可以统一密集点追踪和光流估计。</div>
</details>
</div>
<div class="card">
<div class="title">PerpetualWonder: Long-Horizon Action-Conditioned 4D Scene Generation</div>
<div class="meta-line">Authors: Jiahao Zhan, Zizhang Li, Hong-Xing Yu, Jiajun Wu</div>
<div class="meta-line">First: 2026-02-04T18:58:55+00:00 · Latest: 2026-02-04T18:58:55+00:00</div>
<div class="meta-line">Comments: Project website: https://johnzhan2023.github.io/PerpetualWonder/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04876v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04876v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://johnzhan2023.github.io/PerpetualWonder/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce PerpetualWonder, a hybrid generative simulator that enables long-horizon, action-conditioned 4D scene generation from a single image. Current works fail at this task because their physical state is decoupled from their visual representation, which prevents generative refinements to update the underlying physics for subsequent interactions. PerpetualWonder solves this by introducing the first true closed-loop system. It features a novel unified representation that creates a bidirectional link between the physical state and visual primitives, allowing generative refinements to correct both the dynamics and appearance. It also introduces a robust update mechanism that gathers supervision from multiple viewpoints to resolve optimization ambiguity. Experiments demonstrate that from a single image, PerpetualWonder can successfully simulate complex, multi-step interactions from long-horizon actions, maintaining physical plausibility and visual consistency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PerpetualWonder：长时域动作条件下的4D场景生成</div>
<div class="mono" style="margin-top:8px">我们引入了PerpetualWonder，这是一个混合生成式模拟器，能够从单张图像中实现长时域、动作条件下的4D场景生成。当前方法在这一任务上失败，因为它们的物理状态与视觉表示解耦，这阻碍了生成式细化以更新后续交互的底层物理。PerpetualWonder通过引入首个真正的闭环系统解决了这一问题。它采用了一种新颖的统一表示方法，在物理状态和视觉基元之间建立双向联系，使生成式细化能够同时校正动态和外观。此外，它还引入了一种鲁棒的更新机制，通过多视角监督来解决优化歧义。实验表明，PerpetualWonder可以从单张图像成功模拟复杂、多步骤的长时域动作交互，保持物理合理性和视觉一致性。</div>
</details>
</div>
<div class="card">
<div class="title">Laminating Representation Autoencoders for Efficient Diffusion</div>
<div class="meta-line">Authors: Ramón Calvo-González, François Fleuret</div>
<div class="meta-line">First: 2026-02-04T18:57:33+00:00 · Latest: 2026-02-04T18:57:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04873v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04873v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent work has shown that diffusion models can generate high-quality images by operating directly on SSL patch features rather than pixel-space latents. However, the dense patch grids from encoders like DINOv2 contain significant redundancy, making diffusion needlessly expensive. We introduce FlatDINO, a variational autoencoder that compresses this representation into a one-dimensional sequence of just 32 continuous tokens -an 8x reduction in sequence length and 48x compression in total dimensionality. On ImageNet 256x256, a DiT-XL trained on FlatDINO latents achieves a gFID of 1.80 with classifier-free guidance while requiring 8x fewer FLOPs per forward pass and up to 4.5x fewer FLOPs per training step compared to diffusion on uncompressed DINOv2 features. These are preliminary results and this work is in progress.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于高效扩散的层叠表示自编码器</div>
<div class="mono" style="margin-top:8px">近期研究表明，扩散模型可以通过直接在SSL补丁特征上操作来生成高质量图像，而不是在像素空间的潜在表示上。然而，像DINOv2这样的编码器产生的密集补丁网格包含大量冗余，导致扩散过程不必要的昂贵。我们引入FlatDINO，这是一种变分自编码器，将这种表示压缩为一个仅包含32个连续标记的一维序列——序列长度减少了8倍，总维度压缩了48倍。在ImageNet 256x256上，使用FlatDINO潜在表示训练的DiT-XL模型，在无分类器引导下实现了1.80的gFID，同时每个前向传递所需的FLOPs减少了8倍，每个训练步骤所需的FLOPs最多减少了4.5倍。这些是初步结果，本工作仍在进行中。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-layer Cross-Attention is Provably Optimal for Multi-modal In-context Learning</div>
<div class="meta-line">Authors: Nicholas Barnfield, Subhabrata Sen, Pragya Sur</div>
<div class="meta-line">First: 2026-02-04T18:57:30+00:00 · Latest: 2026-02-04T18:57:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04872v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04872v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress has rapidly advanced our understanding of the mechanisms underlying in-context learning in modern attention-based neural networks. However, existing results focus exclusively on unimodal data; in contrast, the theoretical underpinnings of in-context learning for multi-modal data remain poorly understood. We introduce a mathematically tractable framework for studying multi-modal learning and explore when transformer-like architectures can recover Bayes-optimal performance in-context. To model multi-modal problems, we assume the observed data arises from a latent factor model. Our first result comprises a negative take on expressibility: we prove that single-layer, linear self-attention fails to recover the Bayes-optimal predictor uniformly over the task distribution. To address this limitation, we introduce a novel, linearized cross-attention mechanism, which we study in the regime where both the number of cross-attention layers and the context length are large. We show that this cross-attention mechanism is provably Bayes optimal when optimized using gradient flow. Our results underscore the benefits of depth for in-context learning and establish the provable utility of cross-attention for multi-modal distributions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多层交叉注意力在多模态上下文学习中是可证明最优的</div>
<div class="mono" style="margin-top:8px">近年来，多模态上下文学习在现代基于注意力的神经网络中的机制理解取得了迅速进展。然而，现有结果仅专注于单模态数据；相比之下，多模态数据的上下文学习理论基础仍不明确。我们引入了一个数学上可处理的框架来研究多模态学习，并探讨类似Transformer的架构在上下文学习中何时能够恢复贝叶斯最优性能。为建模多模态问题，我们假设观测数据来源于一个潜在因子模型。我们的第一个结果是一个负面结论：我们证明了单层线性自注意力无法在任务分布下统一恢复贝叶斯最优预测器。为解决这一局限性，我们引入了一种新颖的线性化交叉注意力机制，并在交叉注意力层数和上下文长度都较大的情况下进行研究。我们展示了当使用梯度流优化时，这种交叉注意力机制是可证明的贝叶斯最优。我们的结果突显了深度对上下文学习的优势，并确立了交叉注意力在多模态分布中的可证明效用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent progress has rapidly advanced our understanding of the mechanisms underlying in-context learning in modern attention-based neural networks.</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Head LatentMoE and Head Parallel: Communication-Efficient and Deterministic MoE Parallelism</div>
<div class="meta-line">Authors: Chenwei Cui, Rockwell Jackson, Benjamin Joseph Herrera, Ana María Tárano, Hannah Kerner</div>
<div class="meta-line">First: 2026-02-04T18:57:19+00:00 · Latest: 2026-02-04T18:57:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04870v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04870v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models have transformed many applications but remain expensive to train. Sparse Mixture of Experts (MoE) addresses this through conditional computation, with Expert Parallel (EP) as the standard distributed training method. However, EP has three limitations: communication cost grows linearly with the number of activated experts $k$, load imbalance affects latency and memory usage, and data-dependent communication requires metadata exchange. We propose Multi-Head LatentMoE and Head Parallel (HP), a new architecture and parallelism achieving $O(1)$ communication cost regardless of $k$, completely balanced traffic, and deterministic communication, all while remaining compatible with EP. To accelerate Multi-Head LatentMoE, we propose IO-aware routing and expert computation. Compared to MoE with EP, Multi-Head LatentMoE with HP trains up to $1.61\times$ faster while having identical performance. With doubled granularity, it achieves higher overall performance while still being $1.11\times$ faster. Our method makes multi-billion-parameter foundation model research more accessible.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多头潜在MoE与头并行：通信高效且确定性的MoE并行方法</div>
<div class="mono" style="margin-top:8px">大语言模型已改变了诸多应用，但训练成本依然高昂。稀疏混合专家（MoE）通过条件计算解决了这一问题，其中专家并行（EP）是标准的分布式训练方法。然而，EP存在三个限制：通信成本随激活专家数量 $k$ 线性增长，负载不平衡影响延迟和内存使用，且数据依赖的通信需要元数据交换。我们提出多头潜在MoE与头并行（HP）这一新架构和并行方法，无论 $k$ 如何，通信成本均为 $O(1)$，实现完全平衡的通信流量和确定性通信，同时保持与EP的兼容性。为加速多头潜在MoE，我们提出IO感知路由和专家计算。与MoE结合EP相比，多头潜在MoE结合HP的训练速度提高了高达 $1.61\times$，同时保持相同性能。在粒度加倍的情况下，其整体性能更高，同时仍比EP快 $1.11\times$。我们的方法使多十亿参数基础模型研究更加可行。</div>
</details>
</div>
<div class="card">
<div class="title">Requirements for Teleportation in an Intercity Quantum Network</div>
<div class="meta-line">Authors: Soubhadra Maiti, Guus Avis, Sounak Kar, Stephanie Wehner</div>
<div class="meta-line">First: 2026-02-04T18:56:48+00:00 · Latest: 2026-02-04T18:56:48+00:00</div>
<div class="meta-line">Comments: 72 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04869v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04869v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We investigate the hardware requirements for quantum teleportation in an intercity-scale network topology consisting of two metropolitan-scale networks connected via a long-distance backbone link. Specifically, we identify the minimal improvements required beyond the state-of-the-art to achieve an end-to-end expected teleportation fidelity of $2/3$, which represents the classical limit. To this end, we formulate the hardware requirements computation as optimisation problems, where the hardware parameters representing the underlying device capabilities serve as decision variables. Assuming a simplified noise model, we derive closed-form analytical expressions for the teleportation fidelity and rate when the network is realised using heterogeneous quantum hardware, including a quantum repeater chain with a memory cut-off. Our derivations are based on events defined by the order statistics of link generation durations in both the metropolitan networks and the backbone, and the resulting expressions are validated through simulations on the NetSquid platform. The analytical expressions facilitate efficient exploration of the optimisation parameter space without resorting to computationally intensive simulations. We then apply this framework to a representative realisation in which the metropolitan nodes are based on trapped-ion processors and the backbone is composed of ensemble-based quantum memories. Our results suggest that teleportation across metropolitan distances is already achievable with state-of-the-art hardware when the data qubit is prepared after end-to-end entanglement has already been established, whereas extending teleportation to intercity scales requires additional, though plausibly achievable, improvements in hardware performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>城际量子网络中量子隐形传态的要求</div>
<div class="mono" style="margin-top:8px">我们研究了在由两个城域网络通过长距离骨干链路连接而成的城际规模网络拓扑中实现量子隐形传态所需的硬件要求。具体而言，我们确定了在现有技术水平基础上实现端到端预期隐形传态保真度为2/3（即经典极限）所需的最小改进。为此，我们将硬件要求的计算建模为优化问题，其中代表底层设备能力的硬件参数作为决策变量。假设一个简化的噪声模型，我们推导了在使用异构量子硬件（包括具有存储截止时间的量子中继链）实现网络时，隐形传态保真度和速率的闭式解析表达式。我们的推导基于城域网络和骨干链路中链路生成持续时间的顺序统计事件，所得到的表达式通过NetSquid平台的仿真进行了验证。这些解析表达式使得无需进行计算密集型仿真即可高效地探索优化参数空间。随后，我们将该框架应用于一个代表性实现，其中城域节点基于囚禁离子处理器，骨干链路由基于集合的量子存储器组成。我们的结果表明，当数据量子比特在端到端纠缠建立之后准备时，实现城域距离的隐形传态已经可以使用现有硬件，而将隐形传态扩展到城际规模则需要额外的、尽管可能实现的硬件性能改进。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We investigate the hardware requirements for quantum teleportation in an intercity-scale network topology consisting of two metropolitan-scale networks connected via a long-distance backbone link.</div>
</details>
</div>
<div class="card">
<div class="title">CRoSS: A Continual Robotic Simulation Suite for Scalable Reinforcement Learning with High Task Diversity and Realistic Physics Simulation</div>
<div class="meta-line">Authors: Yannick Denker, Alexander Gepperth</div>
<div class="meta-line">First: 2026-02-04T18:54:26+00:00 · Latest: 2026-02-04T18:54:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04868v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04868v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Continual reinforcement learning (CRL) requires agents to learn from a sequence of tasks without forgetting previously acquired policies. In this work, we introduce a novel benchmark suite for CRL based on realistically simulated robots in the Gazebo simulator. Our Continual Robotic Simulation Suite (CRoSS) benchmarks rely on two robotic platforms: a two-wheeled differential-drive robot with lidar, camera and bumper sensor, and a robotic arm with seven joints. The former represent an agent in line-following and object-pushing scenarios, where variation of visual and structural parameters yields a large number of distinct tasks, whereas the latter is used in two goal-reaching scenarios with high-level cartesian hand position control (modeled after the Continual World benchmark), and low-level control based on joint angles. For the robotic arm benchmarks, we provide additional kinematics-only variants that bypass the need for physical simulation (as long as no sensor readings are required), and which can be run two orders of magnitude faster. CRoSS is designed to be easily extensible and enables controlled studies of continual reinforcement learning in robotic settings with high physical realism, and in particular allow the use of almost arbitrary simulated sensors. To ensure reproducibility and ease of use, we provide a containerized setup (Apptainer) that runs out-of-the-box, and report performances of standard RL algorithms, including Deep Q-Networks (DQN) and policy gradient methods. This highlights the suitability as a scalable and reproducible benchmark for CRL research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CRoSS：一个用于可扩展强化学习的持续机器人模拟套件，具有高任务多样性和逼真的物理模拟</div>
<div class="mono" style="margin-top:8px">持续强化学习（CRL）要求智能体在不遗忘先前学到策略的情况下，从一系列任务中学习。在本工作中，我们基于Gazebo模拟器中真实感的机器人，引入了一个新的CRL基准套件。我们的持续机器人模拟套件（CRoSS）基准依赖于两个机器人平台：一个配备激光雷达、摄像头和碰撞传感器的双轮差速驱动机器人，以及一个具有七个关节的机械臂。前者用于路径跟随和物体推动场景，其中视觉和结构参数的变化产生大量不同的任务；后者则用于两个目标到达场景，采用高级的笛卡尔手部位置控制（参考Continual World基准）和基于关节角度的低级控制。对于机械臂基准，我们还提供了仅依赖运动学的变体，这些变体无需物理模拟（只要不需要传感器读数），并且运行速度可以快两个数量级。CRoSS设计为易于扩展，使得在具有高物理真实性的机器人环境中，可以进行受控的持续强化学习研究，特别是允许使用几乎任意的模拟传感器。为确保可重复性和易用性，我们提供了一个容器化设置（Apptainer），可直接运行，并报告了标准强化学习算法（包括深度Q网络（DQN）和策略梯度方法）的性能。这突显了其作为可扩展且可重复的CRL研究基准的适用性。</div>
</details>
</div>
<div class="card">
<div class="title">When LLaVA Meets Objects: Token Composition for Vision-Language-Models</div>
<div class="meta-line">Authors: Soumya Jahagirdar, Walid Bousselham, Anna Kukleva, Hilde Kuehne</div>
<div class="meta-line">First: 2026-02-04T18:50:46+00:00 · Latest: 2026-02-04T18:50:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04864v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04864v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current autoregressive Vision Language Models (VLMs) usually rely on a large number of visual tokens to represent images, resulting in a need for more compute especially at inference time. To address this problem, we propose Mask-LLaVA, a framework that leverages different levels of visual features to create a compact yet information-rich visual representation for autoregressive VLMs. Namely, we combine mask-based object representations together with global tokens and local patch tokens. While all tokens are used during training, it shows that the resulting model can flexibly drop especially the number of mask-based object-tokens at test time, allowing to adapt the number of tokens during inference without the need to retrain the model and without a significant drop in performance. We evaluate the proposed approach on a suite of standard benchmarks showing results competitive to current token efficient methods and comparable to the original LLaVA baseline using only a fraction of visual tokens. Our analysis demonstrates that combining multi-level features enables efficient learning with fewer tokens while allowing dynamic token selection at test time for good performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>当LLaVA遇见物体：面向视觉语言模型的标记组合</div>
<div class="mono" style="margin-top:8px">当前的自回归视觉语言模型（VLMs）通常依赖大量视觉标记来表示图像，导致推理时需要更多的计算资源。为了解决这一问题，我们提出了Mask-LLaVA框架，利用不同层次的视觉特征为自回归VLMs创建一个紧凑且信息丰富的视觉表示。具体而言，我们结合了基于掩码的物体表示、全局标记和局部图像块标记。虽然所有标记在训练过程中都会被使用，但实验表明，所得到的模型在测试时可以灵活地减少特别是基于掩码的物体标记数量，从而在推理过程中无需重新训练模型且不显著降低性能即可调整标记数量。我们在一系列标准基准上评估了该方法，结果显示其与当前的标记高效方法具有竞争力，并且仅使用少量视觉标记即可达到与原始LLaVA基线相当的性能。我们的分析表明，结合多级特征可以实现更高效的标记学习，同时在测试时允许动态选择标记以获得良好的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Subliminal Effects in Your Data: A General Mechanism via Log-Linearity</div>
<div class="meta-line">Authors: Ishaq Aden-Ali, Noah Golowich, Allen Liu, Abhishek Shetty, Ankur Moitra, Nika Haghtalab</div>
<div class="meta-line">First: 2026-02-04T18:50:46+00:00 · Latest: 2026-02-04T18:50:46+00:00</div>
<div class="meta-line">Comments: Code available at https://github.com/ishaqadenali/logit-linear-selection</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04863v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04863v1">PDF</a> · <a href="https://github.com/ishaqadenali/logit-linear-selection">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training modern large language models (LLMs) has become a veritable smorgasbord of algorithms and datasets designed to elicit particular behaviors, making it critical to develop techniques to understand the effects of datasets on the model&#x27;s properties. This is exacerbated by recent experiments that show datasets can transmit signals that are not directly observable from individual datapoints, posing a conceptual challenge for dataset-centric understandings of LLM training and suggesting a missing fundamental account of such phenomena. Towards understanding such effects, inspired by recent work on the linear structure of LLMs, we uncover a general mechanism through which hidden subtexts can arise in generic datasets.
  We introduce Logit-Linear-Selection (LLS), a method that prescribes how to select subsets of a generic preference dataset to elicit a wide range of hidden effects. We apply LLS to discover subsets of real-world datasets so that models trained on them exhibit behaviors ranging from having specific preferences, to responding to prompts in a different language not present in the dataset, to taking on a different persona. Crucially, the effect persists for the selected subset, across models with varying architectures, supporting its generality and universality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>你的数据中的潜意识效应：通过对数线性性的一般机制</div>
<div class="mono" style="margin-top:8px">训练现代大型语言模型（LLMs）已成为一种包含多种算法和数据集的盛宴，旨在激发特定行为，因此开发理解数据集如何影响模型属性的技术至关重要。最近的实验表明，数据集可以传递个体数据点无法直接观察到的信号，这给以数据集为中心的LLM训练理解带来了概念上的挑战，并暗示了对这类现象的基本解释尚有缺失。为了理解这些效应，受近期关于LLM线性结构的研究启发，我们揭示了一种通用机制，使得通用数据集中可能产生隐藏的潜意识内容。我们引入了Logit-Linear-Selection（LLS）方法，该方法规定了如何从通用偏好数据集中选择子集，以激发各种隐藏效应。我们将LLS应用于现实数据集，发现训练这些子集的模型可以表现出从特定偏好到对数据集中未包含语言的提示做出反应，再到扮演不同角色等行为。关键的是，这种效应在所选子集上持续存在，并且适用于不同架构的模型，支持其普遍性和广泛适用性。</div>
</details>
</div>
<div class="card">
<div class="title">From Evaluation to Design: Using Potential Energy Surface Smoothness Metrics to Guide Machine Learning Interatomic Potential Architectures</div>
<div class="meta-line">Authors: Ryan Liu, Eric Qu, Tobias Kreiman, Samuel M. Blau, Aditi S. Krishnapriyan</div>
<div class="meta-line">First: 2026-02-04T18:50:10+00:00 · Latest: 2026-02-04T18:50:10+00:00</div>
<div class="meta-line">Comments: 13 pages main text, 10 pages reference &amp; appendix, 8 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04861v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04861v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine Learning Interatomic Potentials (MLIPs) sometimes fail to reproduce the physical smoothness of the quantum potential energy surface (PES), leading to erroneous behavior in downstream simulations that standard energy and force regression evaluations can miss. Existing evaluations, such as microcanonical molecular dynamics (MD), are computationally expensive and primarily probe near-equilibrium states. To improve evaluation metrics for MLIPs, we introduce the Bond Smoothness Characterization Test (BSCT). This efficient benchmark probes the PES via controlled bond deformations and detects non-smoothness, including discontinuities, artificial minima, and spurious forces, both near and far from equilibrium. We show that BSCT correlates strongly with MD stability while requiring a fraction of the cost of MD. To demonstrate how BSCT can guide iterative model design, we utilize an unconstrained Transformer backbone as a testbed, illustrating how refinements such as a new differentiable $k$-nearest neighbors algorithm and temperature-controlled attention reduce artifacts identified by our metric. By optimizing model design systematically based on BSCT, the resulting MLIP simultaneously achieves a low conventional E/F regression error, stable MD simulations, and robust atomistic property predictions. Our results establish BSCT as both a validation metric and as an &quot;in-the-loop&quot; model design proxy that alerts MLIP developers to physical challenges that cannot be efficiently evaluated by current MLIP benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从评估到设计：利用势能面平滑性度量指导机器学习原子间势的架构</div>
<div class="mono" style="margin-top:8px">机器学习原子间势（MLIPs）有时无法再现量子势能面（PES）的物理平滑性，导致下游模拟中出现错误行为，而标准的能量和力回归评估可能无法检测到这些问题。现有的评估方法，如微正则系综分子动力学（MD），计算成本高且主要探测近平衡态。为改进MLIPs的评估指标，我们引入了键平滑性特征测试（BSCT）。该高效基准通过受控的键变形探测PES，并检测非平滑性，包括不连续性、人工极小值和虚假力，无论是在平衡态附近还是远离平衡态的区域。我们展示了BSCT与MD稳定性有很强的相关性，同时计算成本仅为MD的几分之一。为了说明BSCT如何指导模型的迭代设计，我们利用了一个无约束的Transformer主干作为测试平台，展示了诸如新的可微分$k$-最近邻算法和温度控制注意力等改进如何减少我们度量所识别的伪影。通过基于BSCT系统地优化模型设计，所得到的MLIP同时实现了较低的传统能量/力回归误差、稳定的MD模拟以及稳健的原子尺度性质预测。我们的结果确立了BSCT作为验证指标和一种“在环”模型设计代理，能够提醒MLIP开发者注意当前MLIP基准无法高效评估的物理挑战。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Machine Learning Interatomic Potentials (MLIPs) sometimes fail to reproduce the physical smoothness of the quantum potential energy surface (PES), leading to erroneous behavior in downstream simulations that standard energy and force regression evaluations can miss.</div>
</details>
</div>
<div class="card">
<div class="title">Evaluating Prediction-based Interventions with Human Decision Makers In Mind</div>
<div class="meta-line">Authors: Inioluwa Deborah Raji, Lydia Liu</div>
<div class="meta-line">First: 2025-02-12T20:35:52+00:00 · Latest: 2026-02-04T18:48:35+00:00</div>
<div class="meta-line">Comments: To be presented at AISTATS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.05704v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.05704v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated decision systems (ADS) are broadly deployed to inform and support human decision-making across a wide range of consequential settings. However, various context-specific details complicate the goal of establishing meaningful experimental evaluations for prediction-based interventions. Notably, current experiment designs rely on simplifying assumptions about human decision making in order to derive causal estimates. In reality, specific experimental design decisions may induce cognitive biases in human decision makers, which could then significantly alter the observed effect sizes of the prediction intervention. In this paper, we formalize and investigate various models of human decision-making in the presence of a predictive model aid. We show that each of these behavioural models produces dependencies across decision subjects and results in the violation of existing assumptions, with consequences for treatment effect estimation. This work aims to further advance the scientific validity of intervention-based evaluation schemes for the assessment of ADS deployments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>以人类决策者为视角评估基于预测的干预措施</div>
<div class="mono" style="margin-top:8px">自动决策系统（ADS）被广泛部署，以辅助和指导人类在各种具有重大影响的场景中的决策。然而，各种情境相关的细节使得建立有意义的基于预测的干预措施实验评估变得复杂。值得注意的是，当前的实验设计依赖于对人类决策过程的简化假设，以推导因果估计。实际上，特定的实验设计决策可能会在人类决策者中引发认知偏差，从而显著改变预测干预措施的观察效应大小。本文我们形式化并研究了在存在预测模型辅助的情况下，各种人类决策模型。我们表明，这些行为模型会导致决策主体之间的依赖关系，并违反现有假设，对处理效应估计产生影响。本工作旨在进一步提升基于干预的评估方案的科学有效性，以评估ADS的部署。</div>
</details>
</div>
<div class="card">
<div class="title">Digital signatures with classical shadows on near-term quantum computers</div>
<div class="meta-line">Authors: Pradeep Niroula, Minzhao Liu, Sivaprasad Omanakuttan, David Amaro, Shouvanik Chakrabarti, Soumik Ghosh, Zichang He, Yuwei Jin, Fatih Kaleoglu, Steven Kordonowy, Rohan Kumar, Michael A. Perlin, Akshay Seshadri, Matthew Steinberg, Joseph Sullivan, Jacob Watkins, Henry Yuen, Ruslan Shaydulin</div>
<div class="meta-line">First: 2026-02-04T18:48:12+00:00 · Latest: 2026-02-04T18:48:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04859v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04859v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quantum mechanics provides cryptographic primitives whose security is grounded in hardness assumptions independent of those underlying classical cryptography. However, existing proposals require low-noise quantum communication and long-lived quantum memory, capabilities which remain challenging to realize in practice. In this work, we introduce a quantum digital signature scheme that operates with only classical communication, using the classical shadows of states produced by random circuits as public keys. We provide theoretical and numerical evidence supporting the conjectured hardness of learning the private key (the circuit) from the public key (the shadow). A key technical ingredient enabling our scheme is an improved state-certification primitive that achieves higher noise tolerance and lower sample complexity than prior methods. We realize this certification by designing a high-rate error-detecting code tailored to our random-circuit ensemble and experimentally generating shadows for 32-qubit states using circuits with $\geq 80$ logical ($\geq 582$ physical) two-qubit gates, attaining 0.90 $\pm$ 0.01 fidelity. With increased number of measurement samples, our hardware-demonstrated primitives realize a proof-of-principle quantum digital signature, demonstrating the near-term feasibility of our scheme.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于经典阴影的近期量子计算机上的数字签名</div>
<div class="mono" style="margin-top:8px">量子力学提供了其安全性基于与经典密码学不同的困难假设的密码学原语。然而，现有方案需要低噪声的量子通信和长寿命的量子存储，这些能力在实践中仍具有挑战性。在本文中，我们提出了一种仅使用经典通信的量子数字签名方案，利用随机电路产生的状态的经典阴影作为公钥。我们提供了理论和数值证据，支持从公钥（阴影）学习私钥（电路）的困难性这一猜想。使我们的方案成为可能的一个关键技术要素是一种改进的状态认证原语，其具有更高的噪声容忍度和更低的样本复杂度。我们通过设计一种针对我们随机电路集合的高率错误检测码，并实验性地生成32量子比特状态的阴影（使用至少80个逻辑（至少582个物理）两量子比特门），实现了0.90 ± 0.01的保真度。随着测量样本数量的增加，我们通过硬件演示的原语实现了原理性验证的量子数字签名，展示了我们方案在近期的可行性。</div>
</details>
</div>
<div class="card">
<div class="title">Combining Residual U-Net and Data Augmentation for Dense Temporal Segmentation of Spike Wave Discharges in Single-Channel EEG</div>
<div class="meta-line">Authors: Saurav Sengupta, Scott Kilianski, Suchetha Sharma, Sakina Lashkeri, Ashley McHugh, Mark Beenhakker, Donald E. Brown</div>
<div class="meta-line">First: 2026-01-01T19:58:20+00:00 · Latest: 2026-02-04T18:43:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.00459v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.00459v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Manual annotation of spike-wave discharges (SWDs), the electrographic hallmark of absence seizures, is labor-intensive for long-term electroencephalography (EEG) monitoring studies. While machine learning approaches show promise for automated detection, they often struggle with cross-subject generalization due to high inter-individual variability in seizure morphology and signal characteristics. In this study we compare the performance of 15 machine learning classifiers on our own manually annotated dataset of 961 hours of EEG recordings from C3H/HeJ mice, including 22,637 labeled SWDs and find that a 1D U-Net performs the best. We then improve its performance by employing residual connections and data augmentation strategies combining amplitude scaling, Gaussian noise injection, and signal inversion during training to enhance cross-subject generalization. We also compare our method, named AugUNet1D, to a recently published time- and frequency-based algorithmic approach called &quot;Twin Peaks&quot; and show that AugUNet1D performs better on our dataset. AugUNet1D, pretrained on our manually annotated data or untrained, is made public for other users.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结合残差U-Net和数据增强的单通道脑电图中尖波-慢波发放的密集时序分割</div>
<div class="mono" style="margin-top:8px">尖波-慢波发放（SWDs）是失神发作的脑电图特征，其手动标注对于长期脑电图（EEG）监测研究来说是一项繁重的工作。尽管机器学习方法在自动检测方面显示出潜力，但由于癫痫发作形态和信号特征的高个体间变异性，它们通常在跨受试者泛化方面存在困难。在本研究中，我们在自己手动标注的961小时C3H/HeJ小鼠脑电记录数据集上比较了15种机器学习分类器的性能，其中包括22,637个标记的SWDs，并发现1D U-Net表现最佳。随后，我们通过在训练过程中采用残差连接和结合幅度缩放、高斯噪声注入和信号反转的数据增强策略来提升其性能，以增强跨受试者泛化能力。我们还将我们的方法AugUNet1D与最近发表的基于时间和频率的算法方法&quot;Twin Peaks&quot;进行了比较，并在我们的数据集上证明AugUNet1D表现更优。AugUNet1D（基于我们手动标注数据预训练或未预训练）已向其他用户公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Manual annotation of spike-wave discharges (SWDs), the electrographic hallmark of absence seizures, is labor-intensive for long-term electroencephalography (EEG) monitoring studies.</div>
</details>
</div>
<div class="card">
<div class="title">Comparing statistical and deep learning techniques for parameter estimation of continuous-time stochastic differentiable equations</div>
<div class="meta-line">Authors: Aroon Sankoh, Victor Wickerhauser</div>
<div class="meta-line">First: 2025-05-06T21:07:53+00:00 · Latest: 2026-02-04T18:42:42+00:00</div>
<div class="meta-line">Comments: 6 pages, 2 figures, 2 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.03980v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.03980v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Stochastic differential equations such as the Ornstein-Uhlenbeck process have long been used to model realworld probablistic events such as stock prices and temperature fluctuations. While statistical methods such as Maximum Likelihood Estimation (MLE), Kalman Filtering, Inverse Variable Method, and more have historically been used to estimate the parameters of stochastic differential equations, the recent explosion of deep learning technology suggests that models such as a Recurrent Neural Network (RNN) could produce more precise estimators. We present a series of experiments that compare the estimation accuracy and computational expensiveness of a statistical method (MLE) with a deep learning model (RNN) for the parameters of the Ornstein-Uhlenbeck process.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>比较统计方法和深度学习技术在连续时间随机微分方程参数估计中的应用</div>
<div class="mono" style="margin-top:8px">随机微分方程（如Ornstein-Uhlenbeck过程）长期以来被用于建模现实世界中的概率事件，如股票价格和温度波动。虽然最大似然估计（MLE）、卡尔曼滤波、逆变量法等统计方法曾被用于估计随机微分方程的参数，但深度学习技术的迅速发展表明，如循环神经网络（RNN）等模型可能提供更精确的估计器。我们提出一系列实验，比较统计方法（MLE）与深度学习模型（RNN）在Ornstein-Uhlenbeck过程参数估计中的准确性与计算成本。</div>
</details>
</div>
<div class="card">
<div class="title">Mechanically concealed holes</div>
<div class="meta-line">Authors: Kanka Ghosh, Andreas M. Menzel</div>
<div class="meta-line">First: 2025-10-31T12:55:42+00:00 · Latest: 2026-02-04T18:42:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00135v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.00135v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">When a hole is introduced into an elastic material, it will usually act to reduce the overall mechanical stiffness. A general ambition is to investigate whether a stiff shell around the hole can act to maintain the overall mechanical properties. We consider this effect from a macroscopic continuum perspective down to atomistic scales. For this purpose, we focus on the basic continuum example situation of an isotropic, homogeneous, linearly elastic material loaded uniformly under compressive plane strain for low concentrations of holes. As we demonstrate, the thickness of the shell can be adjusted in a way to maintain the overall stiffness of the system. We derive a corresponding mathematical expression for the thickness of the shell that conceals the hole. Thus, one can work with given materials to mask the presence of the holes simply by adjusting the thickness of the surrounding shells, with no need to change the materials. Our predictions from linear elasticity continuum theory are extended to atomistic levels using molecular dynamics simulations of a model Lennard-Jones solid. These extensions attest the robustness of our predictions down to atomistic scales. Thus, they open a straightforward possibility to adjust the strategy of mechanical cloaking via atomistic manipulations. From both perspectives, the underlying concept is important in the context of light-weight construction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>机械隐藏孔洞</div>
<div class="mono" style="margin-top:8px">当孔洞被引入弹性材料时，通常会降低整体的机械刚度。我们旨在探讨是否可以通过在孔洞周围设置一个刚性壳层来维持整体的机械性能。我们从宏观连续介质的角度出发，研究这一效应直至原子尺度。为此，我们专注于一个基本的连续介质情况：一种各向同性、均匀、线弹性材料在低孔洞浓度下受到均匀压缩平面应变作用。我们证明，壳层的厚度可以调整以保持系统的整体刚度。我们推导出一个相应的数学表达式，用于描述能够隐藏孔洞的壳层厚度。因此，只需通过调整周围壳层的厚度，就可以在不改变材料的情况下，利用现有材料来掩盖孔洞的存在。我们利用模型的Lennard-Jones固体的分子动力学模拟，将线弹性连续介质理论的预测扩展到原子尺度。这些扩展验证了我们的预测在原子尺度上的鲁棒性。因此，它们为通过原子尺度操作调整机械隐身策略提供了直接的可能性。从这两个角度来看，这一基本概念在轻量化结构的背景下具有重要意义。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Fixed Frames: Dynamic Character-Aligned Speech Tokenization</div>
<div class="meta-line">Authors: Luca Della Libera, Cem Subakan, Mirco Ravanelli</div>
<div class="meta-line">First: 2026-01-30T16:58:40+00:00 · Latest: 2026-02-04T18:42:12+00:00</div>
<div class="meta-line">Comments: 18 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.23174v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.23174v2">PDF</a> · <a href="https://github.com/lucadellalib/dycast">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural audio codecs are at the core of modern conversational speech technologies, converting continuous speech into sequences of discrete tokens that can be processed by LLMs. However, existing codecs typically operate at fixed frame rates, allocating tokens uniformly in time and producing unnecessarily long sequences. In this work, we introduce DyCAST, a Dynamic Character-Aligned Speech Tokenizer that enables variable-frame-rate tokenization through soft character-level alignment and explicit duration modeling. DyCAST learns to associate tokens with character-level linguistic units during training and supports alignment-free inference with direct control over token durations at decoding time. To improve speech resynthesis quality at low frame rates, we further introduce a retrieval-augmented decoding mechanism that enhances reconstruction fidelity without increasing bitrate. Experiments show that DyCAST achieves competitive speech resynthesis quality and downstream performance while using significantly fewer tokens than fixed-frame-rate codecs. Code and checkpoints will be released publicly at https://github.com/lucadellalib/dycast.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越固定帧：动态字符对齐语音分词</div>
<div class="mono" style="margin-top:8px">神经音频编解码器是现代对话语音技术的核心，将连续语音转换为可以被大语言模型处理的离散标记序列。然而，现有的编解码器通常以固定帧率运行，按时间均匀分配标记，导致生成的序列过长。在本工作中，我们引入了DyCAST，一种动态字符对齐语音分词器，通过软字符级对齐和显式时长建模实现可变帧率分词。DyCAST在训练过程中学习将标记与字符级语言单元相关联，并支持无需对齐的推理，同时在解码时直接控制标记时长。为了在低帧率下提高语音重合成质量，我们进一步引入了检索增强的解码机制，该机制在不增加比特率的情况下提升了重建保真度。实验表明，DyCAST在使用比固定帧率编解码器显著更少的标记时，实现了具有竞争力的语音重合成质量和下游任务性能。代码和检查点将在https://github.com/lucadellalib/dycast上公开发布。</div>
</details>
</div>
<div class="card">
<div class="title">The Key to State Reduction in Linear Attention: A Rank-based Perspective</div>
<div class="meta-line">Authors: Philipp Nazari, T. Konstantin Rusch</div>
<div class="meta-line">First: 2026-02-04T18:39:38+00:00 · Latest: 2026-02-04T18:39:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04852v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04852v1">PDF</a> · <a href="https://github.com/camail-official/LinearAttentionPruning">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Linear attention offers a computationally efficient yet expressive alternative to softmax attention. However, recent empirical results indicate that the state of trained linear attention models often exhibits a low-rank structure, suggesting that these models underexploit their capacity in practice. To illuminate this phenomenon, we provide a theoretical analysis of the role of rank in linear attention, revealing that low effective rank can affect retrieval error by amplifying query noise. In addition to these theoretical insights, we conjecture that the low-rank states can be substantially reduced post-training with only minimal performance degradation, yielding faster and more memory-efficient models. To this end, we propose a novel hardware-aware approach that structurally prunes key and query matrices, reducing the state size while retaining compatibility with existing CUDA kernels. We adapt several existing pruning strategies to fit our framework and, building on our theoretical analysis, propose a novel structured pruning method based on a rank-revealing QR decomposition. Our empirical results, evaluated across models of varying sizes and on various downstream tasks, demonstrate the effectiveness of our state reduction framework. We highlight that our framework enables the removal of 50% of the query and key channels at only a marginal increase in perplexity. The code for this project can be found at https://github.com/camail-official/LinearAttentionPruning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>线性注意力中状态缩减的关键：基于秩的视角</div>
<div class="mono" style="margin-top:8px">线性注意力为softmax注意力提供了一种计算高效且具有表达力的替代方案。然而，最近的实证结果表明，训练后的线性注意力模型的状态通常表现出低秩结构，这表明这些模型在实践中未能充分挖掘其容量。为阐明这一现象，我们从理论上分析了秩在线性注意力中的作用，揭示了低有效秩可能通过放大查询噪声来影响检索误差。除了这些理论见解，我们推测，训练后可以通过仅轻微性能下降的方式显著缩减低秩状态，从而得到更快且更节省内存的模型。为此，我们提出了一种新颖的硬件感知方法，通过结构化剪枝键和查询矩阵，减少状态规模同时保持与现有CUDA内核的兼容性。我们调整了多种现有的剪枝策略以适应我们的框架，并基于我们的理论分析，提出了一种基于秩揭示QR分解的新颖结构化剪枝方法。我们的实证结果在不同规模的模型和多种下游任务上进行了评估，展示了我们状态缩减框架的有效性。我们强调，我们的框架能够在仅轻微增加困惑度的情况下移除查询和键通道的50%。该项目的代码可在https://github.com/camail-official/LinearAttentionPruning上找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Linear attention offers a computationally efficient yet expressive alternative to softmax attention.</div>
</details>
</div>
<div class="card">
<div class="title">PDF-HR: Pose Distance Fields for Humanoid Robots</div>
<div class="meta-line">Authors: Yi Gu, Yukang Gao, Yangchen Zhou, Xingyu Chen, Yixiao Feng, Mingle Zhao, Yunyang Mo, Zhaorui Wang, Lixin Xu, Renjing Xu</div>
<div class="meta-line">First: 2026-02-04T18:38:51+00:00 · Latest: 2026-02-04T18:38:51+00:00</div>
<div class="meta-line">Comments: \href{https://gaoyukang33.github.io/PDF-HR/}{Project page}</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04851v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04851v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://gaoyukang33.github.io/PDF-HR/}{Project">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pose and motion priors play a crucial role in humanoid robotics. Although such priors have been widely studied in human motion recovery (HMR) domain with a range of models, their adoption for humanoid robots remains limited, largely due to the scarcity of high-quality humanoid motion data. In this work, we introduce Pose Distance Fields for Humanoid Robots (PDF-HR), a lightweight prior that represents the robot pose distribution as a continuous and differentiable manifold. Given an arbitrary pose, PDF-HR predicts its distance to a large corpus of retargeted robot poses, yielding a smooth measure of pose plausibility that is well suited for optimization and control. PDF-HR can be integrated as a reward shaping term, a regularizer, or a standalone plausibility scorer across diverse pipelines. We evaluate PDF-HR on various humanoid tasks, including single-trajectory motion tracking, general motion tracking, style-based motion mimicry, and general motion retargeting. Experiments show that this plug-and-play prior consistently and substantially strengthens strong baselines. Code and models will be released.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PDF-HR：用于人形机器人的姿态距离场</div>
<div class="mono" style="margin-top:8px">姿态和运动先验在人形机器人领域起着至关重要的作用。尽管在人类运动恢复（HMR）领域已有多种模型广泛研究这些先验，但它们在人形机器人中的应用仍受到高质量人形运动数据稀缺的限制。在本文中，我们引入了用于人形机器人的姿态距离场（PDF-HR），这是一种轻量级的先验方法，将机器人姿态分布表示为连续且可微分的流形。给定任意姿态，PDF-HR可以预测其与大量重定向机器人姿态数据集之间的距离，从而得到一种适合优化和控制的平滑姿态合理性度量。PDF-HR可以作为奖励塑造项、正则化项或独立的姿态合理性评分器，适用于各种不同的流程。我们在多种人形任务上评估了PDF-HR，包括单轨迹运动跟踪、通用运动跟踪、基于风格的运动模仿以及通用运动重定向。实验表明，这种即插即用的先验方法能够持续且显著地增强现有强基线。代码和模型将被发布。</div>
</details>
</div>
<div class="card">
<div class="title">El Agente Quntur: A research collaborator agent for quantum chemistry</div>
<div class="meta-line">Authors: Juan B. Pérez-Sánchez, Yunheng Zou, Jorge A. Campos-Gonzalez-Angulo, Marcel Müller, Ignacio Gustin, Andrew Wang, Han Hao, Tsz Wai Ko, Changhyeok Choi, Eric S. Isbrandt, Mohammad Ghazi Vakili, Hanyong Xu, Chris Crebolder, Varinia Bernales, Alán Aspuru-Guzik</div>
<div class="meta-line">First: 2026-02-04T18:38:50+00:00 · Latest: 2026-02-04T18:38:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04850v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04850v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Quantum chemistry is a foundational enabling tool for the fields of chemistry, materials science, computational biology and others. Despite of its power, the practical application of quantum chemistry simulations remains in the hands of qualified experts due to methodological complexity, software heterogeneity, and the need for informed interpretation of results. To bridge the accessibility gap for these tools and expand their reach to chemists with broader backgrounds, we introduce El Agente Quntur, a hierarchical, multi-agent AI system designed to operate not merely as an automation tool but as a research collaborator for computational quantum chemistry. Quntur was designed following three main strategies: i) elimination of hard-coded procedural policies in favour of reasoning-driven decisions, ii) construction of general and composable actions that facilitate generalization and efficiency, and iii) implementation of guided deep research to integrate abstract quantum-chemical reasoning across subdisciplines and a detailed understanding of the software&#x27;s internal logic and syntax. Although instantiated in ORCA, these design principles are applicable to research agents more generally and easily expandable to additional quantum chemistry packages and beyond. Quntur supports the full range of calculations available in ORCA 6.0 and reasons over software documentation and scientific literature to plan, execute, adapt, and analyze in silico chemistry experiments following best practices. We discuss the advances and current bottlenecks in agentic systems operating at the research level in computational chemistry, and outline a roadmap toward a fully autonomous end-to-end computational chemistry research agent.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>El Agente Quntur：用于量子化学的科研合作者代理</div>
<div class="mono" style="margin-top:8px">量子化学是化学、材料科学、计算生物学等领域的基础使能工具。尽管其功能强大，但由于方法论的复杂性、软件的异构性以及对结果的有根据解释的需求，其实际应用仍掌握在合格专家手中。为弥合这些工具的可访问性差距，并将其扩展到具有更广泛背景的化学家，我们引入了El Agente Quntur，这是一个分层的多代理AI系统，旨在不仅作为自动化工具，更作为计算量子化学的科研合作者。Quntur的设计遵循三个主要策略：i）用推理驱动的决策替代硬编码的程序策略；ii）构建通用且可组合的操作，以促进泛化和效率；iii）实施引导式深度研究，以在子学科间整合抽象的量子化学推理，并深入理解软件的内部逻辑和语法。尽管Quntur是在ORCA中实现的，但这些设计原则适用于更广泛的科研代理，并且易于扩展到其他量子化学软件包。Quntur支持ORCA 6.0中所有可用的计算类型，并通过软件文档和科学文献进行推理，以遵循最佳实践规划、执行、适应和分析体外化学实验。我们讨论了计算化学中科研级代理系统的进展和当前瓶颈，并概述了向完全自主的端到端计算化学科研代理发展的路线图。</div>
</details>
</div>
<div class="card">
<div class="title">El Agente Estructural: An Artificially Intelligent Molecular Editor</div>
<div class="meta-line">Authors: Changhyeok Choi, Yunheng Zou, Marcel Müller, Han Hao, Yeonghun Kang, Juan B. Pérez-Sánchez, Ignacio Gustin, Hanyong Xu, Mohammad Ghazi Vakili, Chris Crebolder, Alán Aspuru-Guzik, Varinia Bernales</div>
<div class="meta-line">First: 2026-02-04T18:38:48+00:00 · Latest: 2026-02-04T18:38:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04849v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04849v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present El Agente Estructural, a multimodal, natural-language-driven geometry-generation and manipulation agent for autonomous chemistry and molecular modelling. Unlike molecular generation or editing via generative models, Estructural mimics how human experts directly manipulate molecular systems in three dimensions by integrating a comprehensive set of domain-informed tools and vision-language models. This design enables precise control over atomic or functional group replacements, atomic connectivity, and stereochemistry without the need to rebuild extensive core molecular frameworks. Through a series of representative case studies, we demonstrate that Estructural enables chemically meaningful geometry manipulation across a wide range of real-world scenarios. These include site-selective functionalization, ligand binding, ligand exchange, stereochemically controlled structure construction, isomer interconversion, fragment-level structural analysis, image-guided generation of structures from schematic reaction mechanisms, and mechanism-driven geometry generation and modification. These examples illustrate how multimodal reasoning, when combined with specialized geometry-aware tools, supports interactive and context-aware molecular modelling beyond structure generation. Looking forward, the integration of Estructural into El Agente Quntur, an autonomous multi-agent quantum chemistry platform, enhances its capabilities by adding sophisticated tools for the generation and editing of three-dimensional structures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>结构代理：一种基于人工智能的分子编辑器</div>
<div class="mono" style="margin-top:8px">我们介绍了El Agente Estructural，这是一种用于自主化学和分子建模的多模态、自然语言驱动的几何生成与操作代理。与通过生成模型进行分子生成或编辑不同，Estructural通过整合一套全面的领域知识工具和视觉-语言模型，模拟人类专家在三维空间中直接操作分子系统的方式。这种设计使得无需重建复杂的分子框架，即可对原子或功能基团替换、原子连接性和立体化学进行精确控制。通过一系列代表性案例研究，我们展示了Estructural在多种现实场景中能够实现具有化学意义的几何操作。这些场景包括位点选择性功能化、配体结合、配体交换、立体化学控制的结构构建、异构体互变、片段级结构分析、基于示意图反应机理的图像引导结构生成，以及基于机理的几何生成与修改。这些示例说明了多模态推理结合专门的几何感知工具，如何支持超越结构生成的交互式和上下文感知的分子建模。展望未来，将Estructural集成到El Agente Quntur（一个自主的多代理量子化学平台）中，通过添加用于三维结构生成和编辑的高级工具，增强了其功能。</div>
</details>
</div>
<div class="card">
<div class="title">Some Remarks on Positive/Negative Feedback</div>
<div class="meta-line">Authors: Thomas Berger, Achim Ilchmann, Eugene P. Ryan</div>
<div class="meta-line">First: 2025-12-10T09:51:11+00:00 · Latest: 2026-02-04T18:34:53+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.09474v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.09474v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In the context of linear control systems, a commonly-held intuition is that negative and positive feedback cannot both be stability enhancing. The canonical linear prototype is the scalar system $\dot x=u$ which, under negative linear feedback $u=-kx$ ($k &gt;0$) is exponentially stable for all $k &gt;0 $, whereas the lack of exponential instability of the (marginally stable) uncontrolled system is amplified by positive feedback $u=kx$ ($k &gt;0)$. By contrast, for nonlinear systems it is shown, by example, that this intuitive dichotomy may fail to hold.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于正反馈与负反馈的一些评论</div>
<div class="mono" style="margin-top:8px">在线性控制系统背景下，一个普遍存在的直觉是负反馈和正反馈不能同时起到增强稳定性的作用。典型的线性原型系统是标量系统 $\dot x=u$，在负线性反馈 $u=-kx$（$k &gt;0$）下，该系统对于所有 $k &gt;0$ 都是指数稳定的；而正反馈 $u=kx$（$k &gt;0$）则会放大未受控的（临界稳定）系统的非指数不稳定性。相比之下，通过例子说明，在非线性系统中，这种直观的二分法可能不成立。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">In the context of linear control systems, a commonly-held intuition is that negative and positive feedback cannot both be stability enhancing.</div>
</details>
</div>
<div class="card">
<div class="title">Fluid Representations in Reasoning Models</div>
<div class="meta-line">Authors: Dmitrii Kharlapenko, Alessandro Stolfo, Arthur Conmy, Mrinmaya Sachan, Zhijing Jin</div>
<div class="meta-line">First: 2026-02-04T18:34:50+00:00 · Latest: 2026-02-04T18:34:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04843v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04843v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reasoning language models, which generate long chains of thought, dramatically outperform non-reasoning language models on abstract problems. However, the internal model mechanisms that allow this superior performance remain poorly understood. We present a mechanistic analysis of how QwQ-32B - a model specifically trained to produce extensive reasoning traces - process abstract structural information. On Mystery Blocksworld - a semantically obfuscated planning domain - we find that QwQ-32B gradually improves its internal representation of actions and concepts during reasoning. The model develops abstract encodings that focus on structure rather than specific action names. Through steering experiments, we establish causal evidence that these adaptations improve problem solving: injecting refined representations from successful traces boosts accuracy, while symbolic representations can replace many obfuscated encodings with minimal performance loss. We find that one of the factors driving reasoning model performance is in-context refinement of token representations, which we dub Fluid Reasoning Representations.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推理模型中的流体表示</div>
<div class="mono" style="margin-top:8px">能够生成长推理链的推理语言模型在抽象问题上显著优于非推理语言模型。然而，实现这种卓越表现的内部模型机制仍不明确。我们对专门训练以生成详细推理轨迹的QwQ-32B模型进行机制分析，研究其如何处理抽象结构信息。在Mystery Blocks World（语义模糊的规划领域）中，我们发现QwQ-32B在推理过程中逐步优化其对动作和概念的内部表示。该模型发展出关注结构而非具体动作名称的抽象编码。通过引导实验，我们建立了因果证据，表明这些适应性提高了问题解决能力：注入成功轨迹中的精细表示可提升准确性，而符号表示可以在性能损失极小的情况下替代许多模糊编码。我们发现，推动推理模型表现的一个关键因素是上下文中的表示细化，我们将这种现象称为流体推理表示。</div>
</details>
</div>
<div class="card">
<div class="title">LitS: A novel Neighborhood Descriptor for Point Clouds</div>
<div class="meta-line">Authors: Jonatan B. Bastos, Francisco F. Rivera, Oscar G. Lorenzo, David L. Vilariño, José C. Cabaleiro, Alberto M. Esmorís, Tomás F. Pena</div>
<div class="meta-line">First: 2026-02-04T18:31:02+00:00 · Latest: 2026-02-04T18:31:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04838v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04838v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the advancement of 3D scanning technologies, point clouds have become fundamental for representing 3D spatial data, with applications that span across various scientific and technological fields. Practical analysis of this data depends crucially on available neighborhood descriptors to accurately characterize the local geometries of the point cloud. This paper introduces LitS, a novel neighborhood descriptor for 2D and 3D point clouds. LitS are piecewise constant functions on the unit circle that allow points to keep track of their surroundings. Each element in LitS&#x27; domain represents a direction with respect to a local reference system. Once constructed, evaluating LitS at any given direction gives us information about the number of neighbors in a cone-like region centered around that same direction. Thus, LitS conveys a lot of information about the local neighborhood of a point, which can be leveraged to gain global structural understanding by analyzing how LitS changes between close points. In addition, LitS comes in two versions (&#x27;regular&#x27; and &#x27;cumulative&#x27;) and has two parameters, allowing them to adapt to various contexts and types of point clouds. Overall, they are a versatile neighborhood descriptor, capable of capturing the nuances of local point arrangements and resilient to common point cloud data issues such as variable density and noise.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LitS：一种用于点云的新邻域描述符</div>
<div class="mono" style="margin-top:8px">随着3D扫描技术的进步，点云已成为表示三维空间数据的基础工具，其应用涵盖多个科学和技术领域。对这类数据的实用分析在很大程度上依赖于可用的邻域描述符，以准确描述点云的局部几何结构。本文提出了一种新颖的邻域描述符LitS，适用于二维和三维点云。LitS是在单位圆上的分段常数函数，使点能够跟踪其周围环境。LitS的每个元素代表相对于局部参考系的一个方向。一旦构建完成，评估LitS在任意方向上的值可以提供关于该方向为中心的锥形区域内的邻居数量的信息。因此，LitS能够传达大量关于点局部邻域的信息，通过分析相邻点之间的LitS变化，可以获取全局结构的理解。此外，LitS有两种版本（&#x27;常规&#x27;和&#x27;累积&#x27;）和两个参数，使其能够适应各种应用场景和类型的点云。总体而言，LitS是一种多功能的邻域描述符，能够捕捉局部点排列的细节，并且对点云数据中常见的密度变化和噪声具有鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Personalized Image Generation via Human-in-the-loop Bayesian Optimization</div>
<div class="meta-line">Authors: Rajalaxmi Rajagopalan, Debottam Dutta, Yu-Lin Wei, Romit Roy Choudhury</div>
<div class="meta-line">First: 2026-02-02T17:51:30+00:00 · Latest: 2026-02-04T18:30:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02388v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.02388v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Imagine Alice has a specific image $x^\ast$ in her mind, say, the view of the street in which she grew up during her childhood. To generate that exact image, she guides a generative model with multiple rounds of prompting and arrives at an image $x^{p*}$. Although $x^{p*}$ is reasonably close to $x^\ast$, Alice finds it difficult to close that gap using language prompts. This paper aims to narrow this gap by observing that even after language has reached its limits, humans can still tell when a new image $x^+$ is closer to $x^\ast$ than $x^{p*}$. Leveraging this observation, we develop MultiBO (Multi-Choice Preferential Bayesian Optimization) that carefully generates $K$ new images as a function of $x^{p*}$, gets preferential feedback from the user, uses the feedback to guide the diffusion model, and ultimately generates a new set of $K$ images. We show that within $B$ rounds of user feedback, it is possible to arrive much closer to $x^\ast$, even though the generative model has no information about $x^\ast$. Qualitative scores from $30$ users, combined with quantitative metrics compared across $5$ baselines, show promising results, suggesting that multi-choice feedback from humans can be effectively harnessed for personalized image generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过人参与贝叶斯优化实现个性化图像生成</div>
<div class="mono" style="margin-top:8px">想象Alice心中有一个特定的图像$x^\ast$，比如她童年时成长的街道的景象。为了生成这张精确的图像，她通过多轮提示引导生成模型，最终得到一个图像$x^{p*}$。尽管$x^{p*}$与$x^\ast$相当接近，但Alice发现仅靠语言提示难以进一步缩小差距。本文通过观察到即使语言提示达到极限，人类仍能判断新生成的图像$x^+$是否比$x^{p*}$更接近$x^\ast$，从而开发出MultiBO（多选项偏好贝叶斯优化）方法。该方法根据$x^{p*}$生成$K$张新图像，获取用户的偏好反馈，利用反馈引导扩散模型，最终生成新的$K$张图像。我们证明，在$B$轮用户反馈内，即使生成模型对$x^\ast$一无所知，也能够显著接近$x^\ast$。结合30名用户的定性评分以及与5个基线模型的定量指标比较，结果显示出良好的前景，表明人类的多选项反馈可以有效用于个性化图像生成。</div>
</details>
</div>
<div class="card">
<div class="title">OverThink: Slowdown Attacks on Reasoning LLMs</div>
<div class="meta-line">Authors: Abhinav Kumar, Jaechul Roh, Ali Naseh, Marzena Karpinska, Mohit Iyyer, Amir Houmansadr, Eugene Bagdasarian</div>
<div class="meta-line">First: 2025-02-04T18:12:41+00:00 · Latest: 2026-02-04T18:30:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.02542v4">Abs</a> · <a href="https://arxiv.org/pdf/2502.02542v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Most flagship language models generate explicit reasoning chains, enabling inference-time scaling. However, producing these reasoning chains increases token usage (i.e., reasoning tokens), which in turn increases latency and costs. Our OverThink attack increases overhead for applications that rely on reasoning language models (RLMs) and external context by forcing them to spend substantially more reasoning tokens while still producing contextually correct answers. An adversary mounts an attack by injecting decoy reasoning problems into public content that is consumed by RLM at inference time. Because our decoys (e.g., Markov decision processes, Sudokus, etc.) are benign, they evade safety filters. We evaluate OverThink on both closed-source and open-source reasoning models across the FreshQA, SQuAD, and MuSR datasets. We also explore the attack in multi-modal settings by creating images that cause excessive reasoning. We show that the resulting slowdown transfers across models. Finally, we explore both LLM-based and systems-level defenses, and discuss the societal, financial, and energy implications of the OverThink attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>OverThink：对推理大语言模型的延迟攻击</div>
<div class="mono" style="margin-top:8px">大多数旗舰语言模型会生成显式的推理链，从而实现推理时间的扩展。然而，生成这些推理链会增加令牌使用量（即推理令牌），进而增加延迟和成本。我们的OverThink攻击通过迫使依赖推理语言模型（RLMs）和外部上下文的应用程序消耗大量推理令牌，而仍然生成符合上下文的正确答案，从而增加额外开销。攻击者通过在推理时被RLMs消费的公共内容中注入虚假推理问题来发动攻击。由于我们的诱饵（例如马尔可夫决策过程、数独等）是无害的，因此可以规避安全过滤器。我们在FreshQA、SQuAD和MuSR数据集上对闭源和开源推理模型进行了OverThink攻击评估。我们还通过创建导致过度推理的图像，探索了多模态环境下的攻击方式。我们展示了由此产生的延迟在不同模型间具有可迁移性。最后，我们探讨了基于LLM和系统级别的防御方法，并讨论了OverThink攻击在社会、财务和能源方面的影响。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260205_0417.html">20260205_0417</a>
<a href="archive/20260204_0421.html">20260204_0421</a>
<a href="archive/20260202_0402.html">20260202_0402</a>
<a href="archive/20260201_0354.html">20260201_0354</a>
<a href="archive/20260131_0408.html">20260131_0408</a>
<a href="archive/20260130_0406.html">20260130_0406</a>
<a href="archive/20260129_0407.html">20260129_0407</a>
<a href="archive/20260128_0407.html">20260128_0407</a>
<a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
