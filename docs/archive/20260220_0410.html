<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-20 04:10</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260220_0410</div>
    <div class="row"><div class="card">
<div class="title">TeCoNeRV: Leveraging Temporal Coherence for Compressible Neural Representations for Videos</div>
<div class="meta-line">Authors: Namitha Padmanabhan, Matthew Gwilliam, Abhinav Shrivastava</div>
<div class="meta-line">First: 2026-02-18T18:59:55+00:00 · Latest: 2026-02-18T18:59:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16711v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16711v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://namithap10.github.io/teconerv/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Implicit Neural Representations (INRs) have recently demonstrated impressive performance for video compression. However, since a separate INR must be overfit for each video, scaling to high-resolution videos while maintaining encoding efficiency remains a significant challenge. Hypernetwork-based approaches predict INR weights (hyponetworks) for unseen videos at high speeds, but with low quality, large compressed size, and prohibitive memory needs at higher resolutions. We address these fundamental limitations through three key contributions: (1) an approach that decomposes the weight prediction task spatially and temporally, by breaking short video segments into patch tubelets, to reduce the pretraining memory overhead by 20$\times$; (2) a residual-based storage scheme that captures only differences between consecutive segment representations, significantly reducing bitstream size; and (3) a temporal coherence regularization framework that encourages changes in the weight space to be correlated with video content. Our proposed method, TeCoNeRV, achieves substantial improvements of 2.47dB and 5.35dB PSNR over the baseline at 480p and 720p on UVG, with 36% lower bitrates and 1.5-3$\times$ faster encoding speeds. With our low memory usage, we are the first hypernetwork approach to demonstrate results at 480p, 720p and 1080p on UVG, HEVC and MCL-JCV. Our project page is available at https://namithap10.github.io/teconerv/ .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>TeCoNeRV：利用时间一致性实现可压缩的视频神经表示</div>
<div class="mono" style="margin-top:8px">隐式神经表示（INRs）最近在视频压缩中表现出色。然而，由于每个视频都需要单独过拟合INR，将其扩展到高分辨率视频同时保持编码效率仍然是一个重大挑战。基于超网络的方法可以高速预测未见过视频的INR权重（超网络），但在更高分辨率下会导致质量低、压缩体积大和内存需求高。我们通过三个关键贡献解决了这些基本限制：(1) 一种将权重预测任务分解为空间和时间的方法，通过将短视频片段分割为管状补丁（patch tubelets）来减少预训练的内存开销，降低20倍；(2) 一种基于残差的存储方案，仅记录连续段表示之间的差异，显著减少比特流大小；(3) 一种时间一致性正则化框架，鼓励权重空间中的变化与视频内容相关联。我们的方法TeCoNeRV在UVG数据集上，于480p和720p分辨率下分别比基线提升了2.47dB和5.35dB的PSNR，同时降低了36%的比特率，编码速度提高了1.5-3倍。凭借低内存使用，我们是首个在UVG、HEVC和MCL-JCV数据集上展示480p、720p和1080p结果的超网络方法。我们的项目页面可在https://namithap10.github.io/teconerv/ 访问。</div>
</details>
</div>
<div class="card">
<div class="title">Semantic Chunking and the Entropy of Natural Language</div>
<div class="meta-line">Authors: Weishun Zhong, Doron Sivan, Tankut Can, Mikhail Katkov, Misha Tsodyks</div>
<div class="meta-line">First: 2026-02-13T18:58:10+00:00 · Latest: 2026-02-18T18:59:22+00:00</div>
<div class="meta-line">Comments: 29 pages, 9 figures; typos fixed</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.13194v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.13194v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The entropy rate of printed English is famously estimated to be about one bit per character, a benchmark that modern large language models (LLMs) have only recently approached. This entropy rate implies that English contains nearly 80 percent redundancy relative to the five bits per character expected for random text. We introduce a statistical model that attempts to capture the intricate multi-scale structure of natural language, providing a first-principles account of this redundancy level. Our model describes a procedure of self-similarly segmenting text into semantically coherent chunks down to the single-word level. The semantic structure of the text can then be hierarchically decomposed, allowing for analytical treatment. Numerical experiments with modern LLMs and open datasets suggest that our model quantitatively captures the structure of real texts at different levels of the semantic hierarchy. The entropy rate predicted by our model agrees with the estimated entropy rate of printed English. Moreover, our theory further reveals that the entropy rate of natural language is not fixed but should increase systematically with the semantic complexity of corpora, which are captured by the only free parameter in our model.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语义分块与自然语言的熵</div>
<div class="mono" style="margin-top:8px">印刷英语的熵率著名地估计为每字符约1比特，这一基准直到最近现代大语言模型（LLMs）才接近。这一熵率意味着英语相对于随机文本每字符预期的5比特而言，具有近80%的冗余性。我们引入了一个统计模型，旨在捕捉自然语言的复杂多尺度结构，为这种冗余水平提供第一性原理的解释。我们的模型描述了一种自相似地将文本分割为语义连贯块的过程，直至单个单词级别。随后，文本的语义结构可以被分层分解，从而便于分析。使用现代LLMs和公开数据集进行的数值实验表明，我们的模型在不同语义层级上定量地捕捉了真实文本的结构。我们模型预测的熵率与印刷英语的估计熵率一致。此外，我们的理论进一步揭示，自然语言的熵率并非固定，而是应随着语料库的语义复杂性系统性地增加，这一复杂性由我们模型中唯一的自由参数所捕捉。</div>
</details>
</div>
<div class="card">
<div class="title">Knowledge-Embedded Latent Projection for Robust Representation Learning</div>
<div class="meta-line">Authors: Weijing Tang, Ming Yuan, Zongqi Xia, Tianxi Cai</div>
<div class="meta-line">First: 2026-02-18T18:58:16+00:00 · Latest: 2026-02-18T18:58:16+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16709v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16709v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Latent space models are widely used for analyzing high-dimensional discrete data matrices, such as patient-feature matrices in electronic health records (EHRs), by capturing complex dependence structures through low-dimensional embeddings. However, estimation becomes challenging in the imbalanced regime, where one matrix dimension is much larger than the other. In EHR applications, cohort sizes are often limited by disease prevalence or data availability, whereas the feature space remains extremely large due to the breadth of medical coding system. Motivated by the increasing availability of external semantic embeddings, such as pre-trained embeddings of clinical concepts in EHRs, we propose a knowledge-embedded latent projection model that leverages semantic side information to regularize representation learning. Specifically, we model column embeddings as smooth functions of semantic embeddings via a mapping in a reproducing kernel Hilbert space. We develop a computationally efficient two-step estimation procedure that combines semantically guided subspace construction via kernel principal component analysis with scalable projected gradient descent. We establish estimation error bounds that characterize the trade-off between statistical error and approximation error induced by the kernel projection. Furthermore, we provide local convergence guarantees for our non-convex optimization procedure. Extensive simulation studies and a real-world EHR application demonstrate the effectiveness of the proposed method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>知识嵌入的潜在投影用于鲁棒表示学习</div>
<div class="mono" style="margin-top:8px">潜在空间模型被广泛用于分析高维离散数据矩阵，例如电子健康记录（EHRs）中的患者特征矩阵，通过低维嵌入捕捉复杂的依赖结构。然而，在不平衡情况下，即其中一个矩阵维度远大于另一个时，估计变得具有挑战性。在EHR应用中，队列规模通常受限于疾病患病率或数据可用性，而特征空间由于医学编码系统的广泛性仍然非常庞大。受外部语义嵌入日益增多的启发，例如EHR中临床概念的预训练嵌入，我们提出了一种知识嵌入的潜在投影模型，利用语义侧信息来正则化表示学习。具体而言，我们通过一个再生核希尔伯特空间中的映射，将列嵌入建模为语义嵌入的平滑函数。我们开发了一种计算高效的两步估计过程，结合了基于语义的子空间构建（通过核主成分分析）和可扩展的投影梯度下降。我们建立了估计误差界，以刻画由核投影引起的统计误差与近似误差之间的权衡。此外，我们为非凸优化过程提供了局部收敛保证。广泛的模拟研究和一个真实世界的EHR应用展示了所提方法的有效性。</div>
</details>
</div>
<div class="card">
<div class="title">Policy Compiler for Secure Agentic Systems</div>
<div class="meta-line">Authors: Nils Palumbo, Sarthak Choudhary, Jihye Choi, Prasad Chalasani, Mihai Christodorescu, Somesh Jha</div>
<div class="meta-line">First: 2026-02-18T18:57:12+00:00 · Latest: 2026-02-18T18:57:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16708v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16708v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based agents are increasingly being deployed in contexts requiring complex authorization policies: customer service protocols, approval workflows, data access restrictions, and regulatory compliance. Embedding these policies in prompts provides no enforcement guarantees. We present PCAS, a Policy Compiler for Agentic Systems that provides deterministic policy enforcement.
  Enforcing such policies requires tracking information flow across agents, which linear message histories cannot capture. Instead, PCAS models the agentic system state as a dependency graph capturing causal relationships among events such as tool calls, tool results, and messages. Policies are expressed in a Datalog-derived language, as declarative rules that account for transitive information flow and cross-agent provenance. A reference monitor intercepts all actions and blocks violations before execution, providing deterministic enforcement independent of model reasoning.
  PCAS takes an existing agent implementation and a policy specification, and compiles them into an instrumented system that is policy-compliant by construction, with no security-specific restructuring required. We evaluate PCAS on three case studies: information flow policies for prompt injection defense, approval workflows in a multi-agent pharmacovigilance system, and organizational policies for customer service. On customer service tasks, PCAS improves policy compliance from 48% to 93% across frontier models, with zero policy violations in instrumented runs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于安全代理系统的策略编译器</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的代理越来越多地部署在需要复杂授权策略的场景中：客户服务协议、审批工作流、数据访问限制和监管合规。将这些策略嵌入提示中无法提供执行保障。我们提出了PCAS，一种用于代理系统的策略编译器，能够提供确定性的策略执行。
  执行此类策略需要跟踪代理之间的信息流，而线性消息历史无法捕捉这种信息流。相反，PCAS将代理系统状态建模为依赖图，以捕捉诸如工具调用、工具结果和消息等事件之间的因果关系。策略使用一种源自Datalog的语言表达，作为声明性规则，考虑了传递信息流和跨代理溯源。参考监控器拦截所有操作并在执行前阻止违规行为，从而提供与模型推理无关的确定性执行。
  PCAS接受现有的代理实现和策略规范，并将其编译为一个符合策略的仪器化系统，无需进行任何与安全相关的重构。我们在三个案例研究中评估了PCAS：用于提示注入防御的信息流策略、多代理药物警戒系统中的审批工作流，以及用于客户服务的组织策略。在客户服务任务中，PCAS将前沿模型的策略合规性从48%提升至93%，在仪器化运行中没有出现任何策略违规。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Humanoid End-Effector Control for Open-Vocabulary Visual Loco-Manipulation</div>
<div class="meta-line">Authors: Runpei Dong, Ziyan Li, Xialin He, Saurabh Gupta</div>
<div class="meta-line">First: 2026-02-18T18:55:02+00:00 · Latest: 2026-02-18T18:55:02+00:00</div>
<div class="meta-line">Comments: Project page: https://hero-humanoid.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16705v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16705v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://hero-humanoid.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual loco-manipulation of arbitrary objects in the wild with humanoid robots requires accurate end-effector (EE) control and a generalizable understanding of the scene via visual inputs (e.g., RGB-D images). Existing approaches are based on real-world imitation learning and exhibit limited generalization due to the difficulty in collecting large-scale training datasets. This paper presents a new paradigm, HERO, for object loco-manipulation with humanoid robots that combines the strong generalization and open-vocabulary understanding of large vision models with strong control performance from simulated training. We achieve this by designing an accurate residual-aware EE tracking policy. This EE tracking policy combines classical robotics with machine learning. It uses a) inverse kinematics to convert residual end-effector targets into reference trajectories, b) a learned neural forward model for accurate forward kinematics, c) goal adjustment, and d) replanning. Together, these innovations help us cut down the end-effector tracking error by 3.2x. We use this accurate end-effector tracker to build a modular system for loco-manipulation, where we use open-vocabulary large vision models for strong visual generalization. Our system is able to operate in diverse real-world environments, from offices to coffee shops, where the robot is able to reliably manipulate various everyday objects (e.g., mugs, apples, toys) on surfaces ranging from 43cm to 92cm in height. Systematic modular and end-to-end tests in simulation and the real world demonstrate the effectiveness of our proposed design. We believe the advances in this paper can open up new ways of training humanoid robots to interact with daily objects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向开放词汇视觉移动操作的人形末端执行器控制学习</div>
<div class="mono" style="margin-top:8px">在野外对任意物体进行视觉移动操作需要人形机器人具备精确的末端执行器（EE）控制和通过视觉输入（如RGB-D图像）获得的可泛化的场景理解。现有方法基于现实世界模仿学习，但由于难以收集大规模训练数据集，其泛化能力有限。本文提出了一种新的范式HERO，结合了大型视觉模型的强泛化能力和开放词汇理解，以及通过模拟训练获得的强控制性能。我们通过设计一种精确的残差感知末端执行器跟踪策略实现了这一目标。该策略结合了经典机器人学与机器学习，包括：a) 逆运动学将残差末端执行器目标转换为参考轨迹；b) 学习的神经前向模型用于精确的前向运动学；c) 目标调整；d) 重新规划。这些创新共同帮助我们将末端执行器跟踪误差降低了3.2倍。我们利用这种精确的末端执行器跟踪器构建了一个模块化系统，用于移动操作，其中使用开放词汇的大型视觉模型实现强视觉泛化。我们的系统能够在多样化的现实环境中运行，从办公室到咖啡店，机器人能够可靠地操作各种日常物品（如杯子、苹果、玩具），这些物品位于43cm到92cm高度的表面上。我们在模拟和现实世界中进行了系统化的模块化和端到端测试，验证了我们所提出设计的有效性。我们认为本文的进展可以为人形机器人与日常物品的交互训练开辟新的途径。</div>
</details>
</div>
<div class="card">
<div class="title">Reinforced Fast Weights with Next-Sequence Prediction</div>
<div class="meta-line">Authors: Hee Seung Hwang, Xindi Wu, Sanghyuk Chun, Olga Russakovsky</div>
<div class="meta-line">First: 2026-02-18T18:53:18+00:00 · Latest: 2026-02-18T18:53:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16704v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16704v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fast weight architectures offer a promising alternative to attention-based transformers for long-context modeling by maintaining constant memory overhead regardless of context length. However, their potential is limited by the next-token prediction (NTP) training paradigm. NTP optimizes single-token predictions and ignores semantic coherence across multiple tokens following a prefix. Consequently, fast weight models, which dynamically update their parameters to store contextual information, learn suboptimal representations that fail to capture long-range dependencies. We introduce REFINE (Reinforced Fast weIghts with Next sEquence prediction), a reinforcement learning framework that trains fast weight models under the next-sequence prediction (NSP) objective. REFINE selects informative token positions based on prediction entropy, generates multi-token rollouts, assigns self-supervised sequence-level rewards, and optimizes the model with group relative policy optimization (GRPO). REFINE is applicable throughout the training lifecycle of pre-trained language models: mid-training, post-training, and test-time training. Our experiments on LaCT-760M and DeltaNet-1.3B demonstrate that REFINE consistently outperforms supervised fine-tuning with NTP across needle-in-a-haystack retrieval, long-context question answering, and diverse tasks in LongBench. REFINE provides an effective and versatile framework for improving long-context modeling in fast weight architectures.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于下文序列预测的强化快速权重</div>
<div class="mono" style="margin-top:8px">快速权重架构通过保持恒定的内存开销，为长上下文建模提供了一种有前景的替代方案，无论上下文长度如何。然而，其潜力受到基于下个词预测（NTP）训练范式的限制。NTP优化单个词的预测，忽略了前缀后多个词的语义连贯性。因此，动态更新参数以存储上下文信息的快速权重模型，学习到的表示无法捕捉长距离依赖关系。我们引入了REFINE（基于下文序列预测的强化快速权重），这是一种在下文序列预测（NSP）目标下训练快速权重模型的强化学习框架。REFINE基于预测熵选择信息性词位置，生成多词 rollout，分配自监督序列级奖励，并使用组相对策略优化（GRPO）优化模型。REFINE适用于预训练语言模型的整个训练生命周期：训练中期、训练后以及测试时训练。我们在LaCT-760M和DeltaNet-1.3B上的实验表明，REFINE在针尖上的检索、长上下文问答以及LongBench中的多样化任务中，始终优于基于NTP的监督微调。REFINE为提升快速权重架构中的长上下文建模提供了一个有效且通用的框架。</div>
</details>
</div>
<div class="card">
<div class="title">Measuring Mid-2025 LLM-Assistance on Novice Performance in Biology</div>
<div class="meta-line">Authors: Shen Zhou Hong, Alex Kleinman, Alyssa Mathiowetz, Adam Howes, Julian Cohen, Suveer Ganta, Alex Letizia, Dora Liao, Deepika Pahari, Xavier Roberts-Gaal, Luca Righetti, Joe Torres</div>
<div class="meta-line">First: 2026-02-18T18:51:28+00:00 · Latest: 2026-02-18T18:51:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16703v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16703v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills. Yet, whether this translates to improved human performance in the physical laboratory remains unclear. To address this, we conducted a pre-registered, investigator-blinded, randomized controlled trial (June-August 2025; n = 153) evaluating whether LLMs improve novice performance in tasks that collectively model a viral reverse genetics workflow. We observed no significant difference in the primary endpoint of workflow completion (5.2% LLM vs. 6.6% Internet; P = 0.759), nor in the success rate of individual tasks. However, the LLM arm had numerically higher success rates in four of the five tasks, most notably for the cell culture task (68.8% LLM vs. 55.3% Internet; P = 0.059). Post-hoc Bayesian modeling of pooled data estimates an approximate 1.4-fold increase (95% CrI 0.74-2.62) in success for a &quot;typical&quot; reverse genetics task under LLM assistance. Ordinal regression modelling suggests that participants in the LLM arm were more likely to progress through intermediate steps across all tasks (posterior probability of a positive effect: 81%-96%). Overall, mid-2025 LLMs did not substantially increase novice completion of complex laboratory procedures but were associated with a modest performance benefit. These results reveal a gap between in silico benchmarks and real-world utility, underscoring the need for physical-world validation of AI biosecurity assessments as model capabilities and user proficiency evolve.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>评估2025年中期大型语言模型对生物学新手表现的影响</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在生物基准测试中表现出色，引发了对它们是否能帮助新手获取双用途实验室技能的担忧。然而，这种表现是否能转化为实际实验室中人类表现的提升仍不清楚。为了解决这一问题，我们进行了一项预先注册、研究者盲法、随机对照试验（2025年6月至8月；n = 153），评估LLMs是否能提升新手在模拟病毒反向遗传学工作流程任务中的表现。我们观察到，在工作流程完成这一主要终点上，LLMs组与互联网组无显著差异（5.2% LLM vs. 6.6% Internet；P = 0.759），个体任务的成功率也无显著差异。然而，LLMs组在五个任务中的四个任务中表现出更高的成功率，尤其是在细胞培养任务（68.8% LLM vs. 55.3% Internet；P = 0.059）。对合并数据的后验贝叶斯建模估计，在LLMs辅助下，一个&quot;典型&quot;的反向遗传学任务的成功率大约提高了1.4倍（95% CrI 0.74-2.62）。序数回归建模表明，LLMs组的参与者在所有任务中更有可能完成中间步骤（后验正效应概率：81%-96%）。总体而言，2025年中期的LLMs并未显著提高新手完成复杂实验室程序的能力，但与适度的表现优势相关。这些结果揭示了硅基基准测试与现实世界应用之间的差距，强调了随着模型能力和用户熟练度的发展，对AI生物安全评估进行物理世界验证的必要性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) perform strongly on biological benchmarks, raising concerns that they may help novice actors acquire dual-use laboratory skills.</div>
</details>
</div>
<div class="card">
<div class="title">Saliency-Aware Multi-Route Thinking: Revisiting Vision-Language Reasoning</div>
<div class="meta-line">Authors: Mingjia Shi, Yinhan He, Yaochen Zhu, Jundong Li</div>
<div class="meta-line">First: 2026-02-18T18:49:56+00:00 · Latest: 2026-02-18T18:49:56+00:00</div>
<div class="meta-line">Comments: preprint 10 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16702v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16702v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-language models (VLMs) aim to reason by jointly leveraging visual and textual modalities. While allocating additional inference-time computation has proven effective for large language models (LLMs), achieving similar scaling in VLMs remains challenging. A key obstacle is that visual inputs are typically provided only once at the start of generation, while textual reasoning (e.g., early visual summaries) is generated autoregressively, causing reasoning to become increasingly text-dominated and allowing early visual grounding errors to accumulate. Moreover, vanilla guidance for visual grounding during inference is often coarse and noisy, making it difficult to steer reasoning over long texts. To address these challenges, we propose \emph{Saliency-Aware Principle} (SAP) selection. SAP operates on high-level reasoning principles rather than token-level trajectories, which enable stable control over discrete generation under noisy feedback while allowing later reasoning steps to re-consult visual evidence when renewed grounding is required. In addition, SAP supports multi-route inference, enabling parallel exploration of diverse reasoning behaviors. SAP is model-agnostic and data-free, requiring no additional training. Empirical results show that SAP achieves competitive performance, especially in reducing object hallucination, under comparable token-generation budgets while yielding more stable reasoning and lower response latency than CoT-style long sequential reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>显著性感知多路径推理：重新审视视觉-语言推理</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）旨在通过联合利用视觉和文本模态进行推理。虽然在推理过程中增加额外的计算已被证明对大语言模型（LLMs）有效，但在VLMs中实现类似的扩展仍具挑战性。一个关键障碍是视觉输入通常仅在生成开始时提供一次，而文本推理（如早期视觉摘要）是自回归生成的，导致推理逐渐以文本为主，早期的视觉基础错误会不断累积。此外，推理过程中对视觉基础的常规指导往往粗糙且嘈杂，难以在长文本上引导推理。为了解决这些问题，我们提出了\emph{显著性感知原则}（SAP）选择。SAP基于高层次的推理原则而非token级轨迹进行操作，这使得在嘈杂反馈下能够稳定地控制离散生成，同时在需要重新建立视觉基础时允许后续推理步骤重新参考视觉证据。此外，SAP支持多路径推理，能够并行探索多样化的推理行为。SAP是模型无关且无需数据的，不需要额外训练。实验证明，在相同的token生成预算下，SAP实现了具有竞争力的性能，尤其是在减少物体幻觉方面，同时表现出更稳定的推理和更低的响应延迟，优于基于CoT的长序列推理方法。</div>
</details>
</div>
<div class="card">
<div class="title">Calibrate-Then-Act: Cost-Aware Exploration in LLM Agents</div>
<div class="meta-line">Authors: Wenxuan Ding, Nicholas Tomlin, Greg Durrett</div>
<div class="meta-line">First: 2026-02-18T18:46:14+00:00 · Latest: 2026-02-18T18:46:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16699v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16699v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLMs are increasingly being used for complex problems which are not necessarily resolved in a single response, but require interacting with an environment to acquire information. In these scenarios, LLMs must reason about inherent cost-uncertainty tradeoffs in when to stop exploring and commit to an answer. For instance, on a programming task, an LLM should test a generated code snippet if it is uncertain about the correctness of that code; the cost of writing a test is nonzero, but typically lower than the cost of making a mistake. In this work, we show that we can induce LLMs to explicitly reason about balancing these cost-uncertainty tradeoffs, then perform more optimal environment exploration. We formalize multiple tasks, including information retrieval and coding, as sequential decision-making problems under uncertainty. Each problem has latent environment state that can be reasoned about via a prior which is passed to the LLM agent. We introduce a framework called Calibrate-Then-Act (CTA), where we feed the LLM this additional context to enable it to act more optimally. This improvement is preserved even under RL training of both the baseline and CTA. Our results on information-seeking QA and on a simplified coding task show that making cost-benefit tradeoffs explicit with CTA can help agents discover more optimal decision-making strategies.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>先校准再行动：LLM代理中的成本感知探索</div>
<div class="mono" style="margin-top:8px">LLM越来越多地被用于解决需要与环境交互以获取信息的复杂问题，这些问题不一定能在单个响应中解决。在这些场景中，LLM必须权衡何时停止探索并确定答案的内在成本不确定性。例如，在编程任务中，如果LLM对生成的代码片段的正确性不确定，它应该测试该代码片段；编写测试的成本非零，但通常低于犯错的成本。在本工作中，我们展示了如何引导LLM显式地权衡这些成本不确定性，从而进行更优的环境探索。我们将多个任务，包括信息检索和编程，形式化为不确定性下的序列决策问题。每个问题都有潜在的环境状态，可以通过传递给LLM代理的先验进行推理。我们引入了一个名为“先校准再行动”（CTA）的框架，通过向LLM提供额外的上下文信息，使其能够更优地行动。这种改进在基线和CTA的强化学习训练中都得以保留。我们在信息获取型问答和简化编程任务上的实验结果表明，通过CTA显式地进行成本收益权衡，可以帮助代理发现更优的决策策略。</div>
</details>
</div>
<div class="card">
<div class="title">Causality is Key for Interpretability Claims to Generalise</div>
<div class="meta-line">Authors: Shruti Joshi, Aaron Mueller, David Klindt, Wieland Brendel, Patrik Reizinger, Dhanya Sridhar</div>
<div class="meta-line">First: 2026-02-18T18:45:04+00:00 · Latest: 2026-02-18T18:45:04+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16698v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16698v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Interpretability research on large language models (LLMs) has yielded important insights into model behaviour, yet recurring pitfalls persist: findings that do not generalise, and causal interpretations that outrun the evidence. Our position is that causal inference specifies what constitutes a valid mapping from model activations to invariant high-level structures, the data or assumptions needed to achieve it, and the inferences it can support. Specifically, Pearl&#x27;s causal hierarchy clarifies what an interpretability study can justify. Observations establish associations between model behaviour and internal components. Interventions (e.g., ablations or activation patching) support claims how these edits affect a behavioural metric (\eg, average change in token probabilities) over a set of prompts. However, counterfactual claims -- i.e., asking what the model output would have been for the same prompt under an unobserved intervention -- remain largely unverifiable without controlled supervision. We show how causal representation learning (CRL) operationalises this hierarchy, specifying which variables are recoverable from activations and under what assumptions. Together, these motivate a diagnostic framework that helps practitioners select methods and evaluations matching claims to evidence such that findings generalise.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>因果性是可解释性主张泛化的关键</div>
<div class="mono" style="margin-top:8px">关于大型语言模型（LLMs）的可解释性研究已提供了对模型行为的重要见解，但一些反复出现的误区依然存在：无法泛化的发现，以及超出证据的因果解释。我们认为，因果推断指明了从模型激活到不变的高层结构的有效映射，明确了实现这一映射所需的数据或假设，以及它能够支持的推断。具体而言，Pearl的因果层次结构阐明了可解释性研究能够证明什么。观察可以建立模型行为与内部组件之间的关联。干预（如消融或激活修补）则支持关于这些修改如何影响行为指标（如平均token概率变化）的主张。然而，反事实主张——即询问在未观察到的干预下，相同提示下模型输出会是什么——在缺乏受控监督的情况下仍难以验证。我们展示了因果表示学习（CRL）如何操作化这一层次结构，明确哪些变量可以从激活中恢复，以及在何种假设下可以实现。这些内容共同促成了一个诊断框架，帮助从业者选择与证据相匹配的方法和评估，从而使发现能够泛化。</div>
</details>
</div>
<div class="card">
<div class="title">Protecting the Undeleted in Machine Unlearning</div>
<div class="meta-line">Authors: Aloni Cohen, Refael Kohen, Kobbi Nissim, Uri Stemmer</div>
<div class="meta-line">First: 2026-02-18T18:44:21+00:00 · Latest: 2026-02-18T18:44:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16697v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16697v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Machine unlearning aims to remove specific data points from a trained model, often striving to emulate &quot;perfect retraining&quot;, i.e., producing the model that would have been obtained had the deleted data never been included. We demonstrate that this approach, and security definitions that enable it, carry significant privacy risks for the remaining (undeleted) data points. We present a reconstruction attack showing that for certain tasks, which can be computed securely without deletions, a mechanism adhering to perfect retraining allows an adversary controlling merely $ω(1)$ data points to reconstruct almost the entire dataset merely by issuing deletion requests. We survey existing definitions for machine unlearning, showing they are either susceptible to such attacks or too restrictive to support basic functionalities like exact summation. To address this problem, we propose a new security definition that specifically safeguards undeleted data against leakage caused by the deletion of other points. We show that our definition permits several essential functionalities, such as bulletin boards, summations, and statistical learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>机器去学习中的未删除数据保护</div>
<div class="mono" style="margin-top:8px">机器去学习旨在从训练模型中移除特定数据点，通常追求模拟『完美再训练』，即生成一个从未包含被删除数据的模型。我们证明，这种方法及其支持的安全定义对剩余（未删除）数据点带来了显著的隐私风险。我们提出了一种重建攻击，表明对于某些可以安全计算的任务，一个遵循完美再训练机制的系统允许仅控制 $ω(1)$ 数据点的对手通过发出删除请求来几乎完全重建整个数据集。我们回顾了现有的机器去学习定义，发现它们要么容易受到此类攻击，要么过于严格，无法支持基本功能如精确求和。为了解决这一问题，我们提出了一种新的安全定义，专门保护未删除数据免受其他数据点删除导致的信息泄露。我们展示了该定义允许实现诸如公告板、求和和统计学习等基本功能。</div>
</details>
</div>
<div class="card">
<div class="title">Parameter-free representations outperform single-cell foundation models on downstream benchmarks</div>
<div class="meta-line">Authors: Huan Souza, Pankaj Mehta</div>
<div class="meta-line">First: 2026-02-18T18:42:29+00:00 · Latest: 2026-02-18T18:42:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16696v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16696v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Single-cell RNA sequencing (scRNA-seq) data exhibit strong and reproducible statistical structure. This has motivated the development of large-scale foundation models, such as TranscriptFormer, that use transformer-based architectures to learn a generative model for gene expression by embedding genes into a latent vector space. These embeddings have been used to obtain state-of-the-art (SOTA) performance on downstream tasks such as cell-type classification, disease-state prediction, and cross-species learning. Here, we ask whether similar performance can be achieved without utilizing computationally intensive deep learning-based representations. Using simple, interpretable pipelines that rely on careful normalization and linear methods, we obtain SOTA or near SOTA performance across multiple benchmarks commonly used to evaluate single-cell foundation models, including outperforming foundation models on out-of-distribution tasks involving novel cell types and organisms absent from the training data. Our findings highlight the need for rigorous benchmarking and suggest that the biology of cell identity can be captured by simple linear representations of single cell gene expression data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无需参数的表示方法在下游基准测试中优于单细胞基础模型</div>
<div class="mono" style="margin-top:8px">单细胞RNA测序（scRNA-seq）数据表现出强烈且可重复的统计结构。这推动了大规模基础模型（如TranscriptFormer）的发展，这些模型采用基于Transformer的架构，通过将基因嵌入到潜在向量空间中来学习基因表达的生成模型。这些嵌入已被用于在下游任务（如细胞类型分类、疾病状态预测和跨物种学习）中获得最先进的（SOTA）性能。在此，我们探讨是否可以在不使用计算密集型的深度学习表示方法的情况下实现类似性能。通过依赖仔细归一化和线性方法的简单、可解释的流程，我们在多个常用于评估单细胞基础模型的基准测试中获得了SOTA或接近SOTA的性能，包括在涉及训练数据中未出现的新细胞类型和生物体的分布外任务中超越基础模型。我们的发现强调了严格基准测试的必要性，并表明单细胞基因表达数据的生物学身份可以通过简单的线性表示来捕捉。</div>
</details>
</div>
<div class="card">
<div class="title">EconEvals: Benchmarks and Litmus Tests for Economic Decision-Making by LLM Agents</div>
<div class="meta-line">Authors: Sara Fish, Julia Shephard, Minkai Li, Ran I. Shorrer, Yannai A. Gonczarowski</div>
<div class="meta-line">First: 2025-03-24T16:06:04+00:00 · Latest: 2026-02-18T18:37:52+00:00</div>
<div class="meta-line">Comments: v3 was a major revision with updated experiments and analysis; v4 consists of minor edits</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.18825v4">Abs</a> · <a href="https://arxiv.org/pdf/2503.18825v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We develop evaluation methods for measuring the economic decision-making capabilities and tendencies of LLMs. First, we develop benchmarks derived from key problems in economics -- procurement, scheduling, and pricing -- that test an LLM&#x27;s ability to learn from the environment in context. Second, we develop the framework of litmus tests, evaluations that quantify an LLM&#x27;s choice behavior on a stylized decision-making task with multiple conflicting objectives. Each litmus test outputs a litmus score, which quantifies an LLM&#x27;s tradeoff response, a reliability score, which measures the coherence of an LLM&#x27;s choice behavior, and a competency score, which measures an LLM&#x27;s capability at the same task when the conflicting objectives are replaced by a single, well-specified objective. Evaluating a broad array of frontier LLMs, we (1) investigate changes in LLM capabilities and tendencies over time, (2) derive economically meaningful insights from the LLMs&#x27; choice behavior and chain-of-thought, (3) validate our litmus test framework by testing self-consistency, robustness, and generalizability. Overall, this work provides a foundation for evaluating LLM agents as they are further integrated into economic decision-making.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EconEvals：LLM代理经济决策的基准与测试</div>
<div class="mono" style="margin-top:8px">我们开发了评估方法，用于衡量大型语言模型（LLMs）的经济决策能力和倾向。首先，我们基于经济学中的关键问题——采购、调度和定价——开发了基准测试，以检验LLM在上下文中从环境中学习的能力。其次，我们构建了‘litmus测试’的框架，这是一种评估方法，用于量化LLM在具有多个冲突目标的标准化决策任务中的选择行为。每个litmus测试输出一个litmus分数，该分数量化了LLM的权衡响应，一个可靠性分数，用于衡量LLM选择行为的一致性，以及一个能力分数，用于衡量LLM在冲突目标被单一明确目标替代时完成该任务的能力。通过评估一系列前沿的LLM，我们（1）研究了LLM能力与倾向随时间的变化，（2）从LLM的选择行为和思维链中提取具有经济意义的见解，（3）通过测试自洽性、鲁棒性和可推广性来验证我们的litmus测试框架。总体而言，这项工作为LLM代理在进一步融入经济决策时提供了评估基础。</div>
</details>
</div>
<div class="card">
<div class="title">Random Scaling of Emergent Capabilities</div>
<div class="meta-line">Authors: Rosie Zhao, Tian Qin, David Alvarez-Melis, Sham Kakade, Naomi Saphra</div>
<div class="meta-line">First: 2025-02-24T17:34:45+00:00 · Latest: 2026-02-18T18:37:12+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.17356v5">Abs</a> · <a href="https://arxiv.org/pdf/2502.17356v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Language models famously improve under a smooth scaling law, but some specific capabilities exhibit sudden breakthroughs in performance. Advocates of &quot;emergence&quot; view these capabilities as unlocked at a specific scale, but others attribute breakthroughs to superficial metric thresholding effects. We propose that breakthroughs are instead driven by continuous changes in the probability distribution of training outcomes when performance is bimodally distributed across random seeds. we show that different random seeds can produce either smooth or emergent scaling trends in synthetic length generalization tasks, multiple choice question answering, and grammatical generalization. We reveal that sharp breakthroughs in metrics are produced by underlying continuous changes in their distribution across seeds. These distributions may become abruptly bimodal at a capacity threshold but this threshold appears at scales well before most seeds achieve breakthrough. Our observations hold true even under continuous loss metrics, confirming that random variation must be considered when predicting a model&#x27;s performance from its scale.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>随机种子下涌现能力的随机缩放</div>
<div class="mono" style="margin-top:8px">语言模型在平滑的缩放定律下显著提升，但某些特定能力在性能上会出现突然的突破。&quot;涌现&quot;的支持者认为这些能力在特定规模下被解锁，但其他人则将其归因于表面指标阈值效应。我们提出，这些突破实际上是由于在性能在随机种子间呈现双峰分布时，训练结果的概率分布随规模连续变化所驱动的。我们在合成长度泛化任务、多项选择题回答和语法泛化任务中展示了不同随机种子可以产生平滑或涌现的缩放趋势。我们揭示了指标上的尖锐突破是由其在种子间的分布连续变化所导致的。这些分布可能在能力阈值下突然变为双峰，但该阈值出现在大多数种子实现突破之前。即使在连续损失指标下，我们的观察结果依然成立，这证实了在从模型规模预测其性能时，必须考虑随机变化。</div>
</details>
</div>
<div class="card">
<div class="title">Synthetic-Powered Multiple Testing with FDR Control</div>
<div class="meta-line">Authors: Yonghoon Lee, Meshi Bashari, Edgar Dobriban, Yaniv Romano</div>
<div class="meta-line">First: 2026-02-18T18:36:24+00:00 · Latest: 2026-02-18T18:36:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16690v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16690v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multiple hypothesis testing with false discovery rate (FDR) control is a fundamental problem in statistical inference, with broad applications in genomics, drug screening, and outlier detection. In many such settings, researchers may have access not only to real experimental observations but also to auxiliary or synthetic data -- from past, related experiments or generated by generative models -- that can provide additional evidence about the hypotheses of interest. We introduce SynthBH, a synthetic-powered multiple testing procedure that safely leverages such synthetic data. We prove that SynthBH guarantees finite-sample, distribution-free FDR control under a mild PRDS-type positive dependence condition, without requiring the pooled-data p-values to be valid under the null. The proposed method adapts to the (unknown) quality of the synthetic data: it enhances the sample efficiency and may boost the power when synthetic data are of high quality, while controlling the FDR at a user-specified level regardless of their quality. We demonstrate the empirical performance of SynthBH on tabular outlier detection benchmarks and on genomic analyses of drug-cancer sensitivity associations, and further study its properties through controlled experiments on simulated data.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于合成数据的多重检验方法与FDR控制</div>
<div class="mono" style="margin-top:8px">在统计推断中，带有错误发现率（FDR）控制的多重假设检验是一个基础问题，在基因组学、药物筛选和异常检测等领域有广泛应用。在许多这类场景中，研究人员不仅可以获得真实的实验观测数据，还可以获取辅助或合成数据——这些数据可能来自以往的相关实验或由生成模型产生，能够为所关注的假设提供额外的证据。我们提出了一种名为SynthBH的合成数据增强的多重检验方法，该方法能够安全地利用这些合成数据。我们证明了在轻微的PRDS型正相关条件下，SynthBH能够在有限样本和分布无关的情况下保证FDR控制，而无需假设合并数据的p值在原假设下有效。所提出的方法能够适应合成数据的（未知）质量：当合成数据质量较高时，可以提高样本效率和检验功效，同时无论合成数据质量如何，都能在用户指定的水平上控制FDR。我们在表格异常检测基准和药物-癌症敏感性关联的基因组分析中展示了SynthBH的实证性能，并通过在模拟数据上的受控实验进一步研究了其特性。</div>
</details>
</div>
<div class="card">
<div class="title">Are Object-Centric Representations Better At Compositional Generalization?</div>
<div class="meta-line">Authors: Ferdinand Kapl, Amir Mohammad Karimi Mamaghan, Maximilian Seitzer, Karl Henrik Johansson, Carsten Marr, Stefan Bauer, Andrea Dittadi</div>
<div class="meta-line">First: 2026-02-18T18:34:07+00:00 · Latest: 2026-02-18T18:34:07+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16689v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16689v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Compositional generalization, the ability to reason about novel combinations of familiar concepts, is fundamental to human cognition and a critical challenge for machine learning. Object-centric (OC) representations, which encode a scene as a set of objects, are often argued to support such generalization, but systematic evidence in visually rich settings is limited. We introduce a Visual Question Answering benchmark across three controlled visual worlds (CLEVRTex, Super-CLEVR, and MOVi-C) to measure how well vision encoders, with and without object-centric biases, generalize to unseen combinations of object properties. To ensure a fair and comprehensive comparison, we carefully account for training data diversity, sample size, representation size, downstream model capacity, and compute. We use DINOv2 and SigLIP2, two widely used vision encoders, as the foundation models and their OC counterparts. Our key findings reveal that (1) OC approaches are superior in harder compositional generalization settings; (2) original dense representations surpass OC only on easier settings and typically require substantially more downstream compute; and (3) OC models are more sample efficient, achieving stronger generalization with fewer images, whereas dense encoders catch up or surpass them only with sufficient data and diversity. Overall, object-centric representations offer stronger compositional generalization when any one of dataset size, training data diversity, or downstream compute is constrained.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>以对象为中心的表示是否在组合泛化方面表现更好？</div>
<div class="mono" style="margin-top:8px">组合泛化，即对熟悉概念的新组合进行推理的能力，是人类认知的基本特征，也是机器学习中的关键挑战。以对象为中心（Object-Centric, OC）的表示方法将场景编码为一组对象，通常被认为有助于这种泛化，但在视觉丰富的环境中系统性证据有限。我们引入了一个跨三个受控视觉世界的视觉问答基准（CLEVRTex、Super-CLEVR 和 MOVi-C），用于评估视觉编码器（有无以对象为中心的偏见）在面对未见过的对象属性组合时的泛化能力。为确保公平且全面的比较，我们仔细考虑了训练数据的多样性、样本数量、表示大小、下游模型容量以及计算资源。我们使用 DINOv2 和 SigLIP2 这两种广泛使用的视觉编码器作为基础模型，并使用它们的以对象为中心的对应模型。我们的主要发现表明：(1) 在更难的组合泛化设置中，以对象为中心的方法表现更优；(2) 原始密集表示仅在较简单的设置中超越以对象为中心的表示，通常需要更多的下游计算资源；(3) 以对象为中心的模型在样本效率方面更优，使用更少的图像即可实现更强的泛化能力，而密集编码器只有在拥有足够数据和多样性时才能追平或超越它们。总体而言，当数据集大小、训练数据多样性或下游计算资源受到限制时，以对象为中心的表示方法在组合泛化方面表现更强。</div>
</details>
</div>
<div class="card">
<div class="title">On the Hardness of Approximation of the Fair k-Center Problem</div>
<div class="meta-line">Authors: Suhas Thejaswi</div>
<div class="meta-line">First: 2026-02-18T18:33:27+00:00 · Latest: 2026-02-18T18:33:27+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16688v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16688v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">In this work, we study the hardness of approximation of the fair $k$-center problem. Here the data points are partitioned into groups and the task is to choose a prescribed number of data points from each group, called centers, while minimizing the maximum distance from any point to its closest center. Although a polynomial-time $3$-approximation is known for this problem in general metrics, it has remained open whether this approximation guarantee is tight or could be further improved, especially since the unconstrained $k$-center problem admits a polynomial-time factor-$2$ approximation. We resolve this open question by proving that, for every $ε&gt;0$, achieving a $(3-ε)$-approximation is NP-hard, assuming $\text{P} \neq \text{NP}$.
  Our inapproximability results hold even when only two disjoint groups are present and at least one center must be chosen from each group. Further, it extends to the canonical one-per-group setting with $k$-groups (for arbitrary $k$), where exactly one center must be selected from each group. Consequently, the factor-$3$ barrier for fair $k$-center in general metric spaces is inherent, and existing $3$-approximation algorithms are optimal up to lower-order terms even in these restricted regimes. This result stands in sharp contrast to the $k$-supplier formulation, where both the unconstrained and fair variants admit factor-$3$ approximation in polynomial time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>关于公平k中心问题近似难度的研究</div>
<div class="mono" style="margin-top:8px">在这项工作中，我们研究了公平$k$-中心问题的近似难度。在此问题中，数据点被划分为若干组，任务是从每组中选择指定数量的数据点作为中心，同时最小化任意一点到其最近中心的最大距离。尽管在一般的度量空间中已知存在多项式时间的$3$-近似算法，但该近似保证是否最优，或者是否可以进一步改进，仍是一个开放问题，尤其是在无约束的$k$-中心问题中存在多项式时间的因子$2$近似算法的情况下。我们通过证明，在假设$\text{P} \neq \text{NP}$的前提下，对于任意$ε&gt;0$，达到$(3-ε)$-近似是NP难的，从而解决了这一开放问题。
我们的近似难度结果即使在仅有两个不相交组且每组至少需选择一个中心的情况下也成立。此外，该结果还适用于具有$k$个组的典型每组选一个中心的设置（对于任意的$k$）。因此，公平$k$-中心问题在一般度量空间中的因子$3$近似难度是固有的，现有的$3$-近似算法在这些受限情况下已经是最佳的，甚至在更低阶项上也最优。这一结果与$k$-供应商问题形成鲜明对比，在$k$-供应商问题中，无约束和公平变体都存在多项式时间的因子$3$近似算法。</div>
</details>
</div>
<div class="card">
<div class="title">MC-LLaVA: Multi-Concept Personalized Vision-Language Model</div>
<div class="meta-line">Authors: Ruichuan An, Sihan Yang, Renrui Zhang, Ming Lu, Tianyi Jiang, Kai Zeng, Yulin Luo, Jiajun Cao, Hao Liang, Ying Chen, Qi She, Shanghang Zhang, Wentao Zhang</div>
<div class="meta-line">First: 2024-11-18T16:33:52+00:00 · Latest: 2026-02-18T18:33:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2411.11706v4">Abs</a> · <a href="https://arxiv.org/pdf/2411.11706v4">PDF</a> · <a href="https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA">Code1</a> · <a href="https://github.com/arctanxarc/MC-LLaVA">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Current vision-language models (VLMs) show exceptional abilities across diverse tasks, such as visual question answering. To enhance user experience, recent studies have investigated VLM personalization to understand user-provided concepts. However, they mainly focus on single concepts, neglecting the existence and interplay of multiple concepts, which limits real-world applicability. This paper proposes MC-LLaVA, a multi-concept personalization paradigm. Specifically, MC-LLaVA employs a multi-concept instruction tuning strategy, effectively integrating multiple concepts in a single training step. To reduce the training costs, we propose a personalized textual prompt that uses visual token information to initialize concept tokens. Additionally, we introduce a personalized visual prompt during inference, aggregating location maps for enhanced recognition and grounding capabilities. To further push the performance upper bound, we incorporate an optional auxiliary loss, better enhancing the proposed personalized prompts. To decorate the VLM personalization research, we contribute a high-quality dataset. We carefully collect images with multiple characters and objects from movies and manually create question-answer samples for multi-concept scenarios, featuring superior diversity. Comprehensive experiments demonstrate that MC-LLaVA achieves impressive multi-concept personalized responses, paving the way for VLMs to become better user assistants. The code and dataset will be released at \href{https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA}.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MC-LLaVA：多概念个性化视觉-语言模型</div>
<div class="mono" style="margin-top:8px">当前的视觉-语言模型（VLMs）在多种任务中表现出色，例如视觉问答。为了提升用户体验，近期研究已探讨VLM的个性化方法以理解用户提供的概念。然而，这些研究主要关注单个概念，忽略了多个概念的存在及其相互作用，这限制了其在现实场景中的应用。本文提出MC-LLaVA，一种多概念个性化范式。具体而言，MC-LLaVA采用多概念指令微调策略，有效在单次训练步骤中整合多个概念。为降低训练成本，我们提出一种个性化文本提示，利用视觉标记信息初始化概念标记。此外，我们在推理过程中引入个性化视觉提示，通过聚合位置图来增强识别和定位能力。为进一步提升性能上限，我们引入了一个可选的辅助损失函数，更好地增强所提出的个性化提示。为了丰富VLM个性化研究，我们贡献了一个高质量的数据集。我们精心收集了包含多个角色和物体的电影图像，并手动创建了多概念场景下的问答样本，具有卓越的多样性。全面的实验表明，MC-LLaVA能够实现令人印象深刻的多概念个性化响应，为VLM成为更好的用户助手铺平道路。代码和数据集将在\href{https://github.com/arctanxarc/MC-LLaVA}{https://github.com/arctanxarc/MC-LLaVA}发布。</div>
</details>
</div>
<div class="card">
<div class="title">Retrieval-Augmented Foundation Models for Matched Molecular Pair Transformations to Recapitulate Medicinal Chemistry Intuition</div>
<div class="meta-line">Authors: Bo Pan, Peter Zhiping Zhang, Hao-Wei Pang, Alex Zhu, Xiang Yu, Liying Zhang, Liang Zhao</div>
<div class="meta-line">First: 2026-02-18T18:27:21+00:00 · Latest: 2026-02-18T18:27:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16684v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16684v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Matched molecular pairs (MMPs) capture the local chemical edits that medicinal chemists routinely use to design analogs, but existing ML approaches either operate at the whole-molecule level with limited edit controllability or learn MMP-style edits from restricted settings and small models. We propose a variable-to-variable formulation of analog generation and train a foundation model on large-scale MMP transformations (MMPTs) to generate diverse variables conditioned on an input variable. To enable practical control, we develop prompting mechanisms that let the users specify preferred transformation patterns during generation. We further introduce MMPT-RAG, a retrieval-augmented framework that uses external reference analogs as contextual guidance to steer generation and generalize from project-specific series. Experiments on general chemical corpora and patent-specific datasets demonstrate improved diversity, novelty, and controllability, and show that our method recovers realistic analog structures in practical discovery scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于检索增强的基模型用于匹配分子对转换以重现药物化学直觉</div>
<div class="mono" style="margin-top:8px">匹配分子对（MMPs）捕捉了药物化学家在设计类似物时常用的局部化学修饰，但现有的机器学习方法要么在分子整体层面操作且修饰控制有限，要么在受限条件下从小模型中学习MMP风格的修饰。我们提出了一种变量到变量的类似物生成框架，并在大规模匹配分子对转换（MMPTs）数据集上训练了一个基模型，以根据输入变量生成多样化的输出变量。为了实现实用的控制，我们开发了提示机制，使用户能够在生成过程中指定偏好的转换模式。我们进一步引入了MMPT-RAG框架，该框架利用外部参考类似物作为上下文指导，以引导生成并从项目特定的系列中进行泛化。在通用化学语料库和专利特定数据集上的实验表明，我们的方法在多样性、新颖性和可控性方面有所提升，并展示了在实际发现场景中能够恢复现实的类似物结构。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Situated Awareness in the Real World</div>
<div class="meta-line">Authors: Chuhan Li, Ruilin Han, Joy Hsu, Yongyuan Liang, Rajiv Dhawan, Jiajun Wu, Ming-Hsuan Yang, Xin Eric Wang</div>
<div class="meta-line">First: 2026-02-18T18:22:52+00:00 · Latest: 2026-02-18T18:22:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16682v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16682v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A core aspect of human perception is situated awareness, the ability to relate ourselves to the surrounding physical environment and reason over possible actions in context. However, most existing benchmarks for multimodal foundation models (MFMs) emphasize environment-centric spatial relations (relations among objects in a scene), while largely overlooking observer-centric relationships that require reasoning relative to agent&#x27;s viewpoint, pose, and motion. To bridge this gap, we introduce SAW-Bench (Situated Awareness in the Real World), a novel benchmark for evaluating egocentric situated awareness using real-world videos. SAW-Bench comprises 786 self-recorded videos captured with Ray-Ban Meta (Gen 2) smart glasses spanning diverse indoor and outdoor environments, and over 2,071 human-annotated question-answer pairs. It probes a model&#x27;s observer-centric understanding with six different awareness tasks. Our comprehensive evaluation reveals a human-model performance gap of 37.66%, even with the best-performing MFM, Gemini 3 Flash. Beyond this gap, our in-depth analysis uncovers several notable findings; for example, while models can exploit partial geometric cues in egocentric videos, they often fail to infer a coherent camera geometry, leading to systematic spatial reasoning errors. We position SAW-Bench as a benchmark for situated spatial intelligence, moving beyond passive observation to understanding physically grounded, observer-centric dynamics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在真实世界中学习情境意识</div>
<div class="mono" style="margin-top:8px">人类感知的一个核心方面是情境意识，即能够将自己与周围的物理环境建立联系，并在特定情境下推理可能的动作。然而，大多数现有的多模态基础模型（MFMs）基准测试强调以环境为中心的空间关系（场景中物体之间的关系），而忽视了以观察者为中心的关系，这种关系需要基于代理的视角、姿态和运动进行推理。为弥合这一差距，我们引入了SAW-Bench（真实世界中的情境意识），这是一个全新的基准测试，用于评估以第一人称视角的情境意识。SAW-Bench包含786段使用Ray-Ban Meta（Gen 2）智能眼镜自拍录制的真实世界视频，涵盖多样化的室内外环境，并包含超过2,071对人工标注的问题-答案对。它通过六个不同的意识任务来探测模型对以观察者为中心的理解能力。我们的全面评估显示，即使使用表现最好的多模态基础模型Gemini 3 Flash，人类与模型在性能上仍存在37.66%的差距。此外，我们的深入分析揭示了几个显著发现；例如，虽然模型可以利用第一人称视频中的部分几何线索，但它们常常无法推断出连贯的摄像机几何结构，导致系统性的空间推理错误。我们将SAW-Bench定位为情境空间智能的基准测试，超越被动观察，转向对物理基础、以观察者为中心的动态的理解。</div>
</details>
</div>
<div class="card">
<div class="title">VETime: Vision Enhanced Zero-Shot Time Series Anomaly Detection</div>
<div class="meta-line">Authors: Yingyuan Yang, Tian Lan, Yifei Gao, Yimeng Lu, Wenjun He, Meng Wang, Chenghao Liu, Chen Zhang</div>
<div class="meta-line">First: 2026-02-18T18:22:22+00:00 · Latest: 2026-02-18T18:22:22+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16681v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16681v1">PDF</a> · <a href="https://github.com/yyyangcoder/VETime">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Time-series anomaly detection (TSAD) requires identifying both immediate Point Anomalies and long-range Context Anomalies. However, existing foundation models face a fundamental trade-off: 1D temporal models provide fine-grained pointwise localization but lack a global contextual perspective, while 2D vision-based models capture global patterns but suffer from information bottlenecks due to a lack of temporal alignment and coarse-grained pointwise detection. To resolve this dilemma, we propose VETime, the first TSAD framework that unifies temporal and visual modalities through fine-grained visual-temporal alignment and dynamic fusion. VETime introduces a Reversible Image Conversion and a Patch-Level Temporal Alignment module to establish a shared visual-temporal timeline, preserving discriminative details while maintaining temporal sensitivity. Furthermore, we design an Anomaly Window Contrastive Learning mechanism and a Task-Adaptive Multi-Modal Fusion to adaptively integrate the complementary perceptual strengths of both modalities. Extensive experiments demonstrate that VETime significantly outperforms state-of-the-art models in zero-shot scenarios, achieving superior localization precision with lower computational overhead than current vision-based approaches. Code available at: https://github.com/yyyangcoder/VETime.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VETime: 基于视觉增强的零样本时间序列异常检测</div>
<div class="mono" style="margin-top:8px">时间序列异常检测（TSAD）需要识别即时点异常和长程上下文异常。然而，现有基础模型面临根本性的权衡：1D时序模型提供细粒度的点级定位，但缺乏全局上下文视角；而2D视觉模型捕捉全局模式，但由于缺乏时序对齐和粗粒度点级检测，存在信息瓶颈。为了解决这一困境，我们提出了VETime，这是首个通过细粒度的视觉-时序对齐和动态融合统一时序与视觉模态的TSAD框架。VETime引入了可逆图像转换和基于补丁的时序对齐模块，以建立共享的视觉-时序时间线，在保持判别细节的同时维持时序敏感性。此外，我们设计了异常窗口对比学习机制和任务自适应多模态融合方法，以自适应地整合两种模态互补的感知优势。大量实验表明，VETime在零样本场景下显著优于现有最先进的模型，在计算开销方面也低于当前基于视觉的方法，同时实现了更优的定位精度。代码可在：https://github.com/yyyangcoder/VETime 获取。</div>
</details>
</div>
<div class="card">
<div class="title">Ab Initio Auxiliary-Field Quantum Monte Carlo in the Thermodynamic Limit</div>
<div class="meta-line">Authors: Jinghong Zhang, Meng-Fu Chen, Adam Rettig, Tong Jiang, Paul J. Robinson, Hieu Q. Dinh, Anton Z. Ni, Joonho Lee</div>
<div class="meta-line">First: 2026-02-18T18:19:33+00:00 · Latest: 2026-02-18T18:19:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16679v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16679v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ab initio auxiliary-field quantum Monte Carlo (AFQMC) is a systematically improvable many-body method, but its application to extended solids has been severely limited by unfavorable computational scaling and memory requirements that obstruct direct access to the thermodynamic and complete-basis-set limits. By combining tensor hypercontraction via interpolative separable density fitting with $\mathbf{k}$-point symmetry, we reduce the computational and memory scaling of ab initio AFQMC for solids to $O(N^3)$ and $O(N^2)$ with arbitrary basis, respectively, comparable to diffusion Monte Carlo. This enables direct and simultaneous thermodynamic-limit and complete-basis-set AFQMC calculations across insulating, metallic, and strongly correlated solids, without embedding, local approximations, empirical finite-size corrections, or composite schemes. Our results establish AFQMC as a general-purpose, systematically improvable alternative to diffusion Monte Carlo and coupled-cluster methods for predictive ab initio simulations of solids, enabling accurate energies and magnetic observables within a unified framework.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从头算辅助场量子蒙特卡洛在热力学极限中的应用</div>
<div class="mono" style="margin-top:8px">从头算辅助场量子蒙特卡洛（AFQMC）是一种可以系统改进的多体方法，但其在扩展固体中的应用受到不利的计算标度和内存需求的严重限制，阻碍了直接访问热力学极限和完整基组极限。通过将张量超收缩与插值可分离密度拟合结合，并利用$\mathbf{k}$-点对称性，我们将从头算AFQMC在固体中的计算和内存标度分别降低到任意基组下的$O(N^3)$和$O(N^2)$，与扩散蒙特卡洛相当。这使得可以在绝缘体、金属和强关联固体中直接且同时进行热力学极限和完整基组极限的AFQMC计算，无需嵌入、局部近似、经验有限尺寸修正或复合方案。我们的结果确立了AFQMC作为扩散蒙特卡洛和耦合簇方法在预测从头算固体模拟中的通用、可系统改进的替代方案，能够在统一框架内实现精确的能量和磁可观测量计算。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Ab initio auxiliary-field quantum Monte Carlo (AFQMC) is a systematically improvable many-body method, but its application to extended solids has been severely limited by unfavorable computational scaling and memory requirements that obstruct direct access to the thermodynamic and complete-basis-set limits.</div>
</details>
</div>
<div class="card">
<div class="title">Mixture-of-Experts as Soft Clustering: A Dual Jacobian-PCA Spectral Geometry Perspective</div>
<div class="meta-line">Authors: Feilong Liu</div>
<div class="meta-line">First: 2026-01-09T23:07:14+00:00 · Latest: 2026-02-18T18:17:41+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.11616v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.11616v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mixture-of-Experts (MoE) architectures are widely used for efficiency and conditional computation, but their effect on the geometry of learned functions and representations remains poorly understood. We study MoEs through a geometric lens, interpreting routing as soft partitioning into overlapping expert-local charts. We introduce a Dual Jacobian-PCA spectral probe that analyzes local function geometry via Jacobian singular value spectra and representation geometry via weighted PCA of routed hidden states. Using a controlled MLP-MoE setting with exact Jacobian computation, we compare dense, Top-k, and fully soft routing under matched capacity. Across random seeds, MoE routing consistently reduces local sensitivity: expert-local Jacobians show smaller leading singular values and faster spectral decay than dense baselines. Weighted PCA reveals that expert-local representations distribute variance across more principal directions, indicating higher effective rank. We further observe low alignment among expert Jacobians, suggesting decomposition into low-overlap expert-specific transformations. Routing sharpness modulates these effects: Top-k routing yields more concentrated, lower-rank expert structure, while fully soft routing produces broader, higher-rank representations. Experiments on a 3-layer transformer with WikiText confirm curvature reduction on natural language and show lower cross-expert alignment for Top-k routing. These findings support interpreting MoEs as soft partitionings of function space that flatten local curvature while redistributing representation variance, yielding testable predictions for expert scaling, hallucination reduction, and ensemble diversity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>专家混合作为软聚类：双雅可比-主成分分析谱几何视角</div>
<div class="mono" style="margin-top:8px">专家混合（MoE）架构因其效率和条件计算而被广泛使用，但其对学习函数和表示几何的影响仍不明确。我们通过几何视角研究MoE，将路由解释为重叠专家局部图的软划分。我们引入一种双雅可比-主成分分析（Jacobian-PCA）谱探针，通过雅可比奇异值谱分析局部函数几何，通过路由隐藏状态的加权主成分分析（PCA）研究表示几何。在具有精确雅可比计算的受控MLP-MoE设置中，我们比较了在匹配容量下密集、Top-k和完全软路由的效果。在不同随机种子下，MoE路由始终降低局部敏感性：专家局部雅可比矩阵显示出更小的主奇异值和更快的谱衰减。加权PCA表明，专家局部表示在更多主方向上分布方差，表明其有效秩更高。我们进一步观察到专家雅可比矩阵之间存在较低的对齐度，这表明可以分解为低重叠的专家特定变换。路由锐度调节这些效果：Top-k路由产生更集中、更低秩的专家结构，而完全软路由则产生更广泛、更高秩的表示。在带有WikiText数据集的3层Transformer上的实验验证了自然语言中曲率的降低，并显示Top-k路由具有较低的跨专家对齐度。这些发现支持将MoE解释为函数空间的软划分，该划分在局部曲率上变平，同时重新分配表示方差，从而为专家扩展、减少幻觉和增强集成多样性提供了可验证的预测。</div>
</details>
</div>
<div class="card">
<div class="title">Statistical Inference Leveraging Synthetic Data with Distribution-Free Guarantees</div>
<div class="meta-line">Authors: Meshi Bashari, Yonghoon Lee, Roy Maor Lotan, Edgar Dobriban, Yaniv Romano</div>
<div class="meta-line">First: 2025-09-24T17:37:14+00:00 · Latest: 2026-02-18T18:13:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.20345v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.20345v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid proliferation of high-quality synthetic data -- generated by advanced AI models or collected as auxiliary data from related tasks -- presents both opportunities and challenges for statistical inference. This paper introduces a GEneral Synthetic-Powered Inference (GESPI) framework that wraps around any statistical inference procedure to safely enhance sample efficiency by combining synthetic and real data. Our framework leverages high-quality synthetic data to boost statistical power, yet adaptively defaults to the standard inference method using only real data when synthetic data is of low quality. The error of our method remains below a user-specified bound without any distributional assumptions on the synthetic data, and decreases as the quality of the synthetic data improves. This flexibility enables seamless integration with conformal prediction, risk control, hypothesis testing, and multiple testing procedures, all without modifying the base inference method. We demonstrate the benefits of our method on challenging tasks with limited labeled data, including AlphaFold protein structure prediction, and comparing large reasoning models on complex math problems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用无分布假设保证的合成数据进行统计推断</div>
<div class="mono" style="margin-top:8px">高质量合成数据的快速普及——由先进AI模型生成或从相关任务中收集的辅助数据——为统计推断带来了机遇与挑战。本文提出了一种通用的合成数据增强推断框架（GEneral Synthetic-Powered Inference, GESPI），该框架可包裹任何统计推断过程，通过结合合成数据和真实数据安全地提升样本效率。我们的框架利用高质量合成数据来增强统计功效，但在合成数据质量较低时，会自适应地默认仅使用真实数据进行标准推断。在不依赖合成数据分布假设的前提下，我们的方法误差始终低于用户指定的界限，并且随着合成数据质量的提高而降低。这种灵活性使得该方法能够无缝集成到符合性预测、风险控制、假设检验和多重检验等过程中，而无需修改基础推断方法。我们在数据有限的挑战性任务上展示了该方法的优势，包括AlphaFold蛋白质结构预测，以及在复杂数学问题上比较大型推理模型。</div>
</details>
</div>
<div class="card">
<div class="title">Neighborhood Stability as a Measure of Nearest Neighbor Searchability</div>
<div class="meta-line">Authors: Thomas Vecchiato, Sebastian Bruch</div>
<div class="meta-line">First: 2026-02-18T18:09:47+00:00 · Latest: 2026-02-18T18:09:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16673v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16673v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Clustering-based Approximate Nearest Neighbor Search (ANNS) organizes a set of points into partitions, and searches only a few of them to find the nearest neighbors of a query. Despite its popularity, there are virtually no analytical tools to determine the suitability of clustering-based ANNS for a given dataset -- what we call &quot;searchability.&quot; To address that gap, we present two measures for flat clusterings of high-dimensional points in Euclidean space. First is Clustering-Neighborhood Stability Measure (clustering-NSM), an internal measure of clustering quality -- a function of a clustering of a dataset -- that we show to be predictive of ANNS accuracy. The second, Point-Neighborhood Stability Measure (point-NSM), is a measure of clusterability -- a function of the dataset itself -- that is predictive of clustering-NSM. The two together allow us to determine whether a dataset is searchable by clustering-based ANNS given only the data points. Importantly, both are functions of nearest neighbor relationships between points, not distances, making them applicable to various distance functions including inner product.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>邻域稳定性作为最近邻搜索能力的度量</div>
<div class="mono" style="margin-top:8px">基于聚类的近似最近邻搜索（ANNS）将点集划分为若干分区，并仅搜索其中少数分区以找到查询点的最近邻。尽管这种方法很流行，但目前几乎没有分析工具可以判断给定数据集是否适合使用基于聚类的ANNS——我们称之为&quot;搜索能力&quot;。为了解决这一问题，我们提出了两种用于欧几里得空间中高维点的平坦聚类的度量方法。第一种是聚类邻域稳定性度量（clustering-NSM），这是一种聚类质量的内部度量——一个数据集聚类的函数，我们证明其可以预测ANNS的准确性。第二种是点邻域稳定性度量（point-NSM），这是一种聚类性度量——一个数据集本身的函数，可以预测聚类-NSM。这两种度量方法共同使得我们仅凭数据点即可判断一个数据集是否可以通过基于聚类的ANNS进行搜索。重要的是，这两种度量方法都基于点之间的最近邻关系，而不是距离，因此适用于包括内积在内的各种距离函数。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Clustering-based Approximate Nearest Neighbor Search (ANNS) organizes a set of points into partitions, and searches only a few of them to find the nearest neighbors of a query.</div>
</details>
</div>
<div class="card">
<div class="title">Modeling Human Behavior in a Strategic Network Game with Complex Group Dynamics</div>
<div class="meta-line">Authors: Jonathan Skaggs, Jacob W. Crandall</div>
<div class="meta-line">First: 2025-05-01T18:13:20+00:00 · Latest: 2026-02-18T18:09:33+00:00</div>
<div class="meta-line">Comments: In Proc. of the 25th International Conference on Autonomous Agents and Multiagent Systems, Paphos, Cyprus, 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.03795v3">Abs</a> · <a href="https://arxiv.org/pdf/2505.03795v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Human networks greatly impact important societal outcomes, including wealth and health inequality, poverty, and bullying. As such, understanding human networks is critical to learning how to promote favorable societal outcomes. As a step toward better understanding human networks, we compare and contrast several methods for learning models of human behavior in a strategic network game called the Junior High Game (JHG) [39]. These modeling methods differ with respect to the assumptions they use to parameterize human behavior (behavior matching vs. community-aware behavior) and the moments they model (mean vs. distribution). Results show that the highest-performing method, called hCAB, models the distribution of human behavior rather than the mean and assumes humans use community-aware behavior rather than behavior matching. When applied to small societies, the hCAB model closely mirrors the population dynamics of human groups (with notable differences). Additionally, in a user study, human participants had difficulty distinguishing hCAB agents from other humans, thus illustrating that the hCAB model also produces plausible (individual) behavior in this strategic network game.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在具有复杂群体动态的战略网络游戏中建模人类行为</div>
<div class="mono" style="margin-top:8px">人类网络对重要的社会结果有重大影响，包括财富和健康不平等、贫困和欺凌。因此，理解人类网络对于学习如何促进有利的社会结果至关重要。为了更好地理解人类网络，我们比较了几种在名为Junior High Game（JHG）的战略网络游戏中学习人类行为模型的方法[39]。这些建模方法在用于参数化人类行为的假设（行为匹配 vs. 社区感知行为）以及所建模的时刻（均值 vs. 分布）上有所不同。结果表明，表现最好的方法称为hCAB，它建模的是人类行为的分布而非均值，并假设人类使用的是社区感知行为而非行为匹配。当应用于小型社会时，hCAB模型与人类群体的人口动态非常相似（有显著差异）。此外，在用户研究中，人类参与者难以区分hCAB代理与其他人类，这表明hCAB模型在该战略网络游戏中也能产生合理的个体行为。</div>
</details>
</div>
<div class="card">
<div class="title">SPARC: Scenario Planning and Reasoning for Automated C Unit Test Generation</div>
<div class="meta-line">Authors: Jaid Monwar Chowdhury, Chi-An Fu, Reyhaneh Jabbarvand</div>
<div class="meta-line">First: 2026-02-18T18:09:03+00:00 · Latest: 2026-02-18T18:09:03+00:00</div>
<div class="meta-line">Comments: 9 pages, 6 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16671v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16671v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automated unit test generation for C remains a formidable challenge due to the semantic gap between high-level program intent and the rigid syntactic constraints of pointer arithmetic and manual memory management. While Large Language Models (LLMs) exhibit strong generative capabilities, direct intent-to-code synthesis frequently suffers from the leap-to-code failure mode, where models prematurely emit code without grounding in program structure, constraints, and semantics. This will result in non-compilable tests, hallucinated function signatures, low branch coverage, and semantically irrelevant assertions that cannot properly capture bugs. We introduce SPARC, a neuro-symbolic, scenario-based framework that bridges this gap through four stages: (1) Control Flow Graph (CFG) analysis, (2) an Operation Map that grounds LLM reasoning in validated utility helpers, (3) Path-targeted test synthesis, and (4) an iterative, self-correction validation loop using compiler and runtime feedback. We evaluate SPARC on 59 real-world and algorithmic subjects, where it outperforms the vanilla prompt generation baseline by 31.36% in line coverage, 26.01% in branch coverage, and 20.78% in mutation score, matching or exceeding the symbolic execution tool KLEE on complex subjects. SPARC retains 94.3% of tests through iterative repair and produces code with significantly higher developer-rated readability and maintainability. By aligning LLM reasoning with program structure, SPARC provides a scalable path for industrial-grade testing of legacy C codebases.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SPARC：用于自动C单元测试生成的场景规划与推理</div>
<div class="mono" style="margin-top:8px">由于高级程序意图与指针运算和手动内存管理的严格语法约束之间的语义差距，C语言的自动单元测试生成仍然是一个严峻的挑战。尽管大型语言模型（LLMs）表现出强大的生成能力，但直接从意图生成代码经常面临‘跳步生成’的失败模式，即模型在缺乏程序结构、约束和语义基础的情况下提前生成代码。这会导致无法编译的测试用例、虚构的函数签名、低分支覆盖率以及无法正确捕获错误的语义无关断言。我们引入了SPARC，这是一个基于神经符号推理和场景的框架，通过四个阶段弥合这一差距：（1）控制流图（CFG）分析，（2）操作映射，将LLM推理与经过验证的实用工具函数结合，（3）路径导向的测试合成，以及（4）使用编译器和运行时反馈的迭代自修正验证循环。我们在59个真实世界和算法项目上评估了SPARC，其在行覆盖、分支覆盖和变异得分方面分别比纯提示生成基线高出31.36%、26.01%和20.78%，在复杂项目上与符号执行工具KLEE表现相当甚至更优。SPARC通过迭代修复保留了94.3%的测试用例，并生成了开发者评价更高的可读性和可维护性代码。通过将LLM推理与程序结构对齐，SPARC为遗留C代码库的工业级测试提供了一条可扩展的路径。</div>
</details>
</div>
<div class="card">
<div class="title">PredMapNet: Future and Historical Reasoning for Consistent Online HD Vectorized Map Construction</div>
<div class="meta-line">Authors: Bo Lang, Nirav Savaliya, Zhihao Zheng, Jinglun Feng, Zheng-Hang Yeh, Mooi Choo Chuah</div>
<div class="meta-line">Venue: WACV 2026</div>
<div class="meta-line">First: 2026-02-18T18:08:26+00:00 · Latest: 2026-02-18T18:08:26+00:00</div>
<div class="meta-line">Comments: WACV 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16669v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16669v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">High-definition (HD) maps are crucial to autonomous driving, providing structured representations of road elements to support navigation and planning. However, existing query-based methods often employ random query initialization and depend on implicit temporal modeling, which lead to temporal inconsistencies and instabilities during the construction of a global map. To overcome these challenges, we introduce a novel end-to-end framework for consistent online HD vectorized map construction, which jointly performs map instance tracking and short-term prediction. First, we propose a Semantic-Aware Query Generator that initializes queries with spatially aligned semantic masks to capture scene-level context globally. Next, we design a History Rasterized Map Memory to store fine-grained instance-level maps for each tracked instance, enabling explicit historical priors. A History-Map Guidance Module then integrates rasterized map information into track queries, improving temporal continuity. Finally, we propose a Short-Term Future Guidance module to forecast the immediate motion of map instances based on the stored history trajectories. These predicted future locations serve as hints for tracked instances to further avoid implausible predictions and keep temporal consistency. Extensive experiments on the nuScenes and Argoverse2 datasets demonstrate that our proposed method outperforms state-of-the-art (SOTA) methods with good efficiency.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PredMapNet：用于一致在线高精度向量地图构建的未来与历史推理</div>
<div class="mono" style="margin-top:8px">高精度（HD）地图对自动驾驶至关重要，它们提供道路元素的结构化表示以支持导航和路径规划。然而，现有的基于查询的方法通常采用随机查询初始化，并依赖隐式的时序建模，这导致在构建全局地图时出现时序不一致和不稳定的问题。为克服这些挑战，我们引入了一种新颖的端到端框架，用于一致的在线HD向量地图构建，该框架联合执行地图实例跟踪和短期预测。首先，我们提出了一种语义感知的查询生成器，通过空间对齐的语义掩码初始化查询，以全局捕捉场景级上下文。接着，我们设计了一个历史栅格化地图记忆模块，用于存储每个跟踪实例的细粒度实例级地图，从而实现显式的时序先验。随后，历史地图引导模块将栅格化地图信息整合到跟踪查询中，提升时序连续性。最后，我们提出了一种短期未来引导模块，基于存储的历史轨迹预测地图实例的即时运动。这些预测的未来位置可作为跟踪实例的提示，进一步避免不合理的预测并保持时序一致性。在nuScenes和Argoverse2数据集上的大量实验表明，我们的方法在效率方面表现良好，并优于现有的最先进（SOTA）方法。</div>
</details>
</div>
<div class="card">
<div class="title">Towards a Science of AI Agent Reliability</div>
<div class="meta-line">Authors: Stephan Rabanser, Sayash Kapoor, Peter Kirgis, Kangheng Liu, Saiteja Utpala, Arvind Narayanan</div>
<div class="meta-line">First: 2026-02-18T18:05:44+00:00 · Latest: 2026-02-18T18:05:44+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16666v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16666v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI agents are increasingly deployed to execute important tasks. While rising accuracy scores on standard benchmarks suggest rapid progress, many agents still continue to fail in practice. This discrepancy highlights a fundamental limitation of current evaluations: compressing agent behavior into a single success metric obscures critical operational flaws. Notably, it ignores whether agents behave consistently across runs, withstand perturbations, fail predictably, or have bounded error severity. Grounded in safety-critical engineering, we provide a holistic performance profile by proposing twelve concrete metrics that decompose agent reliability along four key dimensions: consistency, robustness, predictability, and safety. Evaluating 14 agentic models across two complementary benchmarks, we find that recent capability gains have only yielded small improvements in reliability. By exposing these persistent limitations, our metrics complement traditional evaluations while offering tools for reasoning about how agents perform, degrade, and fail.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>迈向AI代理可靠性科学</div>
<div class="mono" style="margin-top:8px">AI代理正越来越多地被部署以执行重要任务。尽管标准基准上的准确率得分持续上升表明进展迅速，但许多代理在实际应用中仍会失败。这种差异凸显了当前评估方法的一个根本性局限：将代理行为压缩为单一的成功指标掩盖了关键的操作缺陷。值得注意的是，它忽略了代理在多次运行中是否行为一致、是否能抵御扰动、是否能可预测地失败，或其错误严重程度是否有限。基于安全关键型工程，我们通过提出十二项具体指标，从四个关键维度——一致性、鲁棒性、可预测性和安全性——对代理的可靠性进行整体性能分析。在两个互补的基准上评估了14个代理模型后，我们发现近期的能力提升仅带来了可靠性的小幅改进。通过揭示这些持续存在的局限性，我们的指标不仅补充了传统评估方法，还提供了分析代理如何执行、退化和失败的工具。</div>
</details>
</div>
<div class="card">
<div class="title">Unpaired Image-to-Image Translation via a Self-Supervised Semantic Bridge</div>
<div class="meta-line">Authors: Jiaming Liu, Felix Petersen, Yunhe Gao, Yabin Zhang, Hyojin Kim, Akshay S. Chaudhari, Yu Sun, Stefano Ermon, Sergios Gatidis</div>
<div class="meta-line">First: 2026-02-18T18:05:00+00:00 · Latest: 2026-02-18T18:05:00+00:00</div>
<div class="meta-line">Comments: 36 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.16664v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.16664v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adversarial diffusion and diffusion-inversion methods have advanced unpaired image-to-image translation, but each faces key limitations. Adversarial approaches require target-domain adversarial loss during training, which can limit generalization to unseen data, while diffusion-inversion methods often produce low-fidelity translations due to imperfect inversion into noise-latent representations. In this work, we propose the Self-Supervised Semantic Bridge (SSB), a versatile framework that integrates external semantic priors into diffusion bridge models to enable spatially faithful translation without cross-domain supervision. Our key idea is to leverage self-supervised visual encoders to learn representations that are invariant to appearance changes but capture geometric structure, forming a shared latent space that conditions the diffusion bridges. Extensive experiments show that SSB outperforms strong prior methods for challenging medical image synthesis in both in-domain and out-of-domain settings, and extends easily to high-quality text-guided editing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过自监督语义桥梁实现无配对图像到图像的翻译</div>
<div class="mono" style="margin-top:8px">对抗生成扩散和扩散-逆方法在无配对图像到图像翻译中取得了进展，但各自都面临关键限制。对抗方法在训练过程中需要目标域对抗损失，这可能会限制其对未见过数据的泛化能力，而扩散-逆方法通常由于对噪声潜在表示的逆过程不完美，导致生成的翻译质量较低。在本文中，我们提出了自监督语义桥梁（SSB），这是一个灵活的框架，通过将外部语义先验整合到扩散桥梁模型中，实现空间上忠实的翻译，而无需跨域监督。我们的核心思想是利用自监督视觉编码器学习一种对表观变化不变但能捕捉几何结构的表示，从而构建一个用于扩散桥梁的共享潜在空间。大量实验表明，SSB在领域内和领域外的挑战性医学图像合成任务中均优于现有强大方法，并且可以轻松扩展到高质量的文本引导编辑。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Adversarial diffusion and diffusion-inversion methods have advanced unpaired image-to-image translation, but each faces key limitations.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260219_0419.html">20260219_0419</a>
<a href="archive/20260218_0416.html">20260218_0416</a>
<a href="archive/20260217_0410.html">20260217_0410</a>
<a href="archive/20260216_0403.html">20260216_0403</a>
<a href="archive/20260215_0401.html">20260215_0401</a>
<a href="archive/20260213_0421.html">20260213_0421</a>
<a href="archive/20260212_0425.html">20260212_0425</a>
<a href="archive/20260210_0435.html">20260210_0435</a>
<a href="archive/20260209_0404.html">20260209_0404</a>
<a href="archive/20260208_0353.html">20260208_0353</a>
<a href="archive/20260207_0410.html">20260207_0410</a>
<a href="archive/20260206_0412.html">20260206_0412</a>
<a href="archive/20260205_0417.html">20260205_0417</a>
<a href="archive/20260204_0421.html">20260204_0421</a>
<a href="archive/20260202_0402.html">20260202_0402</a>
<a href="archive/20260201_0354.html">20260201_0354</a>
<a href="archive/20260131_0408.html">20260131_0408</a>
<a href="archive/20260130_0406.html">20260130_0406</a>
<a href="archive/20260129_0407.html">20260129_0407</a>
<a href="archive/20260128_0407.html">20260128_0407</a>
<a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
