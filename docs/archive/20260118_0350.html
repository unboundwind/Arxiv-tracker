<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-01-18 03:50</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260118_0350</div>
    <div class="row"><div class="card">
<div class="title">WildRayZer: Self-supervised Large View Synthesis in Dynamic Environments</div>
<div class="meta-line">Authors: Xuweiyi Chen, Wentao Zhou, Zezhou Cheng</div>
<div class="meta-line">First: 2026-01-15T18:59:58+00:00 · Latest: 2026-01-15T18:59:58+00:00</div>
<div class="meta-line">Comments: Project Page: https://wild-rayzer.cs.virginia.edu/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10716v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10716v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://wild-rayzer.cs.virginia.edu/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present WildRayZer, a self-supervised framework for novel view synthesis (NVS) in dynamic environments where both the camera and objects move. Dynamic content breaks the multi-view consistency that static NVS models rely on, leading to ghosting, hallucinated geometry, and unstable pose estimation. WildRayZer addresses this by performing an analysis-by-synthesis test: a camera-only static renderer explains rigid structure, and its residuals reveal transient regions. From these residuals, we construct pseudo motion masks, distill a motion estimator, and use it to mask input tokens and gate loss gradients so supervision focuses on cross-view background completion. To enable large-scale training and evaluation, we curate Dynamic RealEstate10K (D-RE10K), a real-world dataset of 15K casually captured dynamic sequences, and D-RE10K-iPhone, a paired transient and clean benchmark for sparse-view transient-aware NVS. Experiments show that WildRayZer consistently outperforms optimization-based and feed-forward baselines in both transient-region removal and full-frame NVS quality with a single feed-forward pass.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>WildRayZer：动态环境中的自监督大视角合成</div>
<div class="mono" style="margin-top:8px">我们提出了WildRayZer，这是一个用于动态环境中新型视角合成（NVS）的自监督框架，其中相机和物体都在运动。动态内容破坏了静态NVS模型依赖的多视角一致性，导致鬼影、几何幻觉和不稳定的姿态估计。WildRayZer通过合成分析测试来解决这一问题：仅使用相机的静态渲染器可以解释刚性结构，其残差揭示了瞬态区域。我们从这些残差中构建伪运动掩码，提炼运动估计器，并用其来屏蔽输入标记和控制损失梯度，使监督集中在跨视角背景补全上。为了实现大规模训练和评估，我们整理了Dynamic RealEstate10K（D-RE10K），这是一个包含15000个随意捕捉的动态序列的真实世界数据集，以及D-RE10K-iPhone，一个用于稀疏视角瞬态感知NVS的配对瞬态与干净数据的基准。实验表明，WildRayZer在单次前向传递下，在瞬态区域去除和全帧NVS质量方面均优于基于优化和前馈的基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We present WildRayZer, a self-supervised framework for novel view synthesis (NVS) in dynamic environments where both the camera and objects move.</div>
</details>
</div>
<div class="card">
<div class="title">DInf-Grid: A Neural Differential Equation Solver with Differentiable Feature Grids</div>
<div class="meta-line">Authors: Navami Kairanda, Shanthika Naik, Marc Habermann, Avinash Sharma, Christian Theobalt, Vladislav Golyanik</div>
<div class="meta-line">First: 2026-01-15T18:59:57+00:00 · Latest: 2026-01-15T18:59:57+00:00</div>
<div class="meta-line">Comments: 25 pages; 16 figures; project page: https://4dqv.mpi-inf.mpg.de/DInf-Grid/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10715v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10715v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a novel differentiable grid-based representation for efficiently solving differential equations (DEs). Widely used architectures for neural solvers, such as sinusoidal neural networks, are coordinate-based MLPs that are both computationally intensive and slow to train. Although grid-based alternatives for implicit representations (e.g., Instant-NGP and K-Planes) train faster by exploiting signal structure, their reliance on linear interpolation restricts their ability to compute higher-order derivatives, rendering them unsuitable for solving DEs. Our approach overcomes these limitations by combining the efficiency of feature grids with radial basis function interpolation, which is infinitely differentiable. To effectively capture high-frequency solutions and enable stable and faster computation of global gradients, we introduce a multi-resolution decomposition with co-located grids. Our proposed representation, DInf-Grid, is trained implicitly using the differential equations as loss functions, enabling accurate modelling of physical fields. We validate DInf-Grid on a variety of tasks, including the Poisson equation for image reconstruction, the Helmholtz equation for wave fields, and the Kirchhoff-Love boundary value problem for cloth simulation. Our results demonstrate a 5-20x speed-up over coordinate-based MLP-based methods, solving differential equations in seconds or minutes while maintaining comparable accuracy and compactness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DInf-Grid：一种使用可微特征网格的神经微分方程求解器</div>
<div class="mono" style="margin-top:8px">我们提出了一种新颖的可微网格表示方法，以高效求解微分方程（DEs）。广泛使用的神经求解器架构，如正弦神经网络，是基于坐标的多层感知机（MLP），计算成本高且训练缓慢。尽管基于网格的隐式表示替代方案（如Instant-NGP和K-Planes）通过利用信号结构实现了更快的训练，但它们依赖于线性插值，限制了高阶导数的计算能力，因此不适合求解微分方程。我们的方法通过结合特征网格的高效性与无限可微的径向基函数插值，克服了这些限制。为了有效捕捉高频解并实现全局梯度的稳定快速计算，我们引入了共定位网格的多分辨率分解。我们提出的DInf-Grid表示方法通过将微分方程作为损失函数进行隐式训练，从而能够准确建模物理场。我们在多种任务上验证了DInf-Grid，包括用于图像重建的泊松方程、用于波场的亥姆霍兹方程，以及用于布料模拟的Kirchhoff-Love边值问题。实验结果表明，与基于坐标的MLP方法相比，我们的方法在求解微分方程时速度提升了5-20倍，能够在几秒或几分钟内完成计算，同时保持相当的精度和紧凑性。</div>
</details>
</div>
<div class="card">
<div class="title">Alterbute: Editing Intrinsic Attributes of Objects in Images</div>
<div class="meta-line">Authors: Tal Reiss, Daniel Winter, Matan Cohen, Alex Rav-Acha, Yael Pritch, Ariel Shamir, Yedid Hoshen</div>
<div class="meta-line">First: 2026-01-15T18:59:53+00:00 · Latest: 2026-01-15T18:59:53+00:00</div>
<div class="meta-line">Comments: Project page is available at https://talreiss.github.io/alterbute/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10714v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10714v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://talreiss.github.io/alterbute/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce Alterbute, a diffusion-based method for editing an object&#x27;s intrinsic attributes in an image. We allow changing color, texture, material, and even the shape of an object, while preserving its perceived identity and scene context. Existing approaches either rely on unsupervised priors that often fail to preserve identity or use overly restrictive supervision that prevents meaningful intrinsic variations. Our method relies on: (i) a relaxed training objective that allows the model to change both intrinsic and extrinsic attributes conditioned on an identity reference image, a textual prompt describing the target intrinsic attributes, and a background image and object mask defining the extrinsic context. At inference, we restrict extrinsic changes by reusing the original background and object mask, thereby ensuring that only the desired intrinsic attributes are altered; (ii) Visual Named Entities (VNEs) - fine-grained visual identity categories (e.g., &#x27;&#x27;Porsche 911 Carrera&#x27;&#x27;) that group objects sharing identity-defining features while allowing variation in intrinsic attributes. We use a vision-language model to automatically extract VNE labels and intrinsic attribute descriptions from a large public image dataset, enabling scalable, identity-preserving supervision. Alterbute outperforms existing methods on identity-preserving object intrinsic attribute editing.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Alterbute：图像中物体内在属性的编辑</div>
<div class="mono" style="margin-top:8px">我们引入了Alterbute，这是一种基于扩散模型的方法，用于在图像中编辑物体的内在属性。我们允许在保持其感知身份和场景上下文的前提下，改变物体的颜色、纹理、材质，甚至形状。现有方法要么依赖于通常无法保持身份的无监督先验，要么使用过于严格的监督，从而限制了有意义的内在变化。我们的方法依赖于：(i) 一个放松的训练目标，允许模型在给定身份参考图像、描述目标内在属性的文本提示以及定义外在上下文的背景图像和物体掩码的条件下，同时修改内在和外在属性；(ii) 视觉命名实体（VNEs）——细粒度的视觉身份类别（例如&#x27;Porsche 911 Carrera&#x27;），这些类别将具有身份定义特征的物体分组，同时允许内在属性的变化。我们使用视觉-语言模型从大规模公开图像数据集中自动提取VNE标签和内在属性描述，从而实现可扩展且保持身份的监督。Alterbute在保持身份的物体内在属性编辑任务中优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">MatchTIR: Fine-Grained Supervision for Tool-Integrated Reasoning via Bipartite Matching</div>
<div class="meta-line">Authors: Changle Qu, Sunhao Dai, Hengyi Cai, Jun Xu, Shuaiqiang Wang, Dawei Yin</div>
<div class="meta-line">First: 2026-01-15T18:59:23+00:00 · Latest: 2026-01-15T18:59:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10712v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10712v1">PDF</a> · <a href="https://github.com/quchangle1/MatchTIR">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Tool-Integrated Reasoning (TIR) empowers large language models (LLMs) to tackle complex tasks by interleaving reasoning steps with external tool interactions. However, existing reinforcement learning methods typically rely on outcome- or trajectory-level rewards, assigning uniform advantages to all steps within a trajectory. This coarse-grained credit assignment fails to distinguish effective tool calls from redundant or erroneous ones, particularly in long-horizon multi-turn scenarios. To address this, we propose MatchTIR, a framework that introduces fine-grained supervision via bipartite matching-based turn-level reward assignment and dual-level advantage estimation. Specifically, we formulate credit assignment as a bipartite matching problem between predicted and ground-truth traces, utilizing two assignment strategies to derive dense turn-level rewards. Furthermore, to balance local step precision with global task success, we introduce a dual-level advantage estimation scheme that integrates turn-level and trajectory-level signals, assigning distinct advantage values to individual interaction turns. Extensive experiments on three benchmarks demonstrate the superiority of MatchTIR. Notably, our 4B model surpasses the majority of 8B competitors, particularly in long-horizon and multi-turn tasks. Our codes are available at https://github.com/quchangle1/MatchTIR.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MatchTIR：通过二部图匹配实现工具集成推理的细粒度监督</div>
<div class="mono" style="margin-top:8px">工具集成推理（TIR）通过将推理步骤与外部工具交互交织在一起，使大语言模型（LLMs）能够处理复杂任务。然而，现有的强化学习方法通常依赖于结果或轨迹级别的奖励，对轨迹内的所有步骤赋予统一的优势。这种粗粒度的信用分配无法区分有效的工具调用与冗余或错误的调用，尤其是在长视距多轮场景中。为了解决这一问题，我们提出了MatchTIR框架，通过基于二部图匹配的回合级别奖励分配和双层级优势估计引入细粒度监督。具体而言，我们将信用分配建模为预测轨迹与真实轨迹之间的二部图匹配问题，利用两种分配策略生成密集的回合级别奖励。此外，为了在局部步骤精度与全局任务成功之间取得平衡，我们引入了一种双层级优势估计方案，整合回合级别和轨迹级别的信号，为每个交互回合分配不同的优势值。在三个基准数据集上的大量实验表明MatchTIR的优越性。值得注意的是，我们的4B模型在长视距和多轮任务中超越了大多数8B竞争对手。我们的代码可在https://github.com/quchangle1/MatchTIR获取。</div>
</details>
</div>
<div class="card">
<div class="title">Unbounded symbols, heat flow, and Toeplitz operators</div>
<div class="meta-line">Authors: Sam Looi</div>
<div class="meta-line">First: 2026-01-15T18:59:18+00:00 · Latest: 2026-01-15T18:59:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10711v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10711v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We disprove the natural domain extension of the Berger--Coburn heat-flow conjecture for Toeplitz operators on the Bargmann space and identify the failure mechanism as a gap between pointwise and uniform control of a Gaussian averaging of the squared modulus of the symbol, a gap that is invisible to the linear form $T_g$. We establish that the form-defined operator $T_g$ and the natural-domain operator $U_g$ decouple in the unbounded symbols regime: while $T_g$ is governed by linear averaging, $U_g$ is controlled by the quadratic intensity of $|g|^2$. We construct a smooth, nonnegative radial symbol $g$ satisfying the coherent-state admissibility hypothesis with bounded heat transforms for all time $t&gt;0$; for this symbol, $T_g$ is bounded, yet $U_g$ is unbounded. This is a strictly global phenomenon: under the coherent-state hypothesis, local singularities are insufficient to cause unboundedness, leaving the ``geometry at infinity&#x27;&#x27; as the sole obstruction. Boundedness of $U_g$ is equivalent to the condition that $|g|^2 dμ$ is a Fock--Carleson measure, a condition strictly stronger than the linear average $g dμ$ governing $T_g$. Finally, regarding the gap between the known sub-critical sufficiency condition and the critical heat time, we prove that heat-flow regularity is irreversible in this context and show that bootstrapping strategies cannot resolve the gap between sufficiency and critical time.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>无界符号、热流与Toeplitz算子</div>
<div class="mono" style="margin-top:8px">我们反驳了Bargmann空间上Toeplitz算子的Berger--Coburn热流猜想的自然域扩展，并将失败机制归因于高斯平均的符号平方模的点wise控制与均匀控制之间的差距，这种差距对线性形式$T_g$是不可见的。我们证明了在无界符号情形下，由形式定义的算子$T_g$与自然域算子$U_g$是解耦的：$T_g$由线性平均支配，而$U_g$则由$|g|^2$的二次强度控制。我们构造了一个光滑、非负的径向符号$g$，满足相干态可容性假设，并且对于所有时间$t&gt;0$，其热变换是有界的；对于这个符号，$T_g$是有界的，而$U_g$却是无界的。这是一个严格全局的现象：在相干态假设下，局部奇异性不足以导致无界性，唯一障碍是“无穷远处的几何结构”。$U_g$的有界性等价于条件$|g|^2 dμ$是一个Fock--Carleson测度，这一条件严格强于支配$T_g$的线性平均$g dμ$。最后，关于已知次临界充分条件与临界热时间之间的差距，我们证明了在此背景下热流正则性是不可逆的，并表明提升策略无法弥合充分条件与临界时间之间的差距。</div>
</details>
</div>
<div class="card">
<div class="title">From One-to-One to Many-to-Many: Dynamic Cross-Layer Injection for Deep Vision-Language Fusion</div>
<div class="meta-line">Authors: Cheng Chen, Yuyu Guo, Pengpeng Zeng, Jingkuan Song, Peng Di, Hang Yu, Lianli Gao</div>
<div class="meta-line">First: 2026-01-15T18:59:10+00:00 · Latest: 2026-01-15T18:59:10+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10710v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10710v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language Models (VLMs) create a severe visual feature bottleneck by using a crude, asymmetric connection that links only the output of the vision encoder to the input of the large language model (LLM). This static architecture fundamentally limits the ability of LLMs to achieve comprehensive alignment with hierarchical visual knowledge, compromising their capacity to accurately integrate local details with global semantics into coherent reasoning. To resolve this, we introduce Cross-Layer Injection (CLI), a novel and lightweight framework that forges a dynamic many-to-many bridge between the two modalities. CLI consists of two synergistic, parameter-efficient components: an Adaptive Multi-Projection (AMP) module that harmonizes features from diverse vision layers, and an Adaptive Gating Fusion (AGF) mechanism that empowers the LLM to selectively inject the most relevant visual information based on its real-time decoding context. We validate the effectiveness and versatility of CLI by integrating it into LLaVA-OneVision and LLaVA-1.5. Extensive experiments on 18 diverse benchmarks demonstrate significant performance improvements, establishing CLI as a scalable paradigm that unlocks deeper multimodal understanding by granting LLMs on-demand access to the full visual hierarchy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从一对一到多对多：用于深度视觉-语言融合的动态跨层注入</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）通过一种粗糙且不对称的连接方式，仅将视觉编码器的输出与大语言模型（LLM）的输入相链接，从而造成严重的视觉特征瓶颈。这种静态架构从根本上限制了LLMs在与分层视觉知识实现全面对齐方面的能力，影响其准确地将局部细节与全局语义整合为连贯推理的能力。为了解决这一问题，我们引入了跨层注入（CLI），这是一种新颖且轻量级的框架，能够在两种模态之间建立动态的多对多桥梁。CLI包含两个协同、参数高效的组件：一个自适应多投影（AMP）模块，用于协调来自不同视觉层的特征；以及一个自适应门控融合（AGF）机制，使LLM能够根据实时解码上下文选择性地注入最相关的视觉信息。我们通过将CLI集成到LLaVA-OneVision和LLaVA-1.5中，验证了CLI的有效性和通用性。在18个多样化的基准测试中进行的广泛实验表明，CLI显著提升了性能，确立了其作为可扩展范式的地位，通过为LLMs提供按需访问完整的视觉层次结构，解锁了更深层次的多模态理解。</div>
</details>
</div>
<div class="card">
<div class="title">High-accuracy and dimension-free sampling with diffusions</div>
<div class="meta-line">Authors: Khashayar Gatmiry, Sitan Chen, Adil Salim</div>
<div class="meta-line">First: 2026-01-15T18:58:50+00:00 · Latest: 2026-01-15T18:58:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10708v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10708v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Diffusion models have shown remarkable empirical success in sampling from rich multi-modal distributions. Their inference relies on numerically solving a certain differential equation. This differential equation cannot be solved in closed form, and its resolution via discretization typically requires many small iterations to produce \emph{high-quality} samples.
  More precisely, prior works have shown that the iteration complexity of discretization methods for diffusion models scales polynomially in the ambient dimension and the inverse accuracy $1/\varepsilon$. In this work, we propose a new solver for diffusion models relying on a subtle interplay between low-degree approximation and the collocation method (Lee, Song, Vempala 2018), and we prove that its iteration complexity scales \emph{polylogarithmically} in $1/\varepsilon$, yielding the first ``high-accuracy&#x27;&#x27; guarantee for a diffusion-based sampler that only uses (approximate) access to the scores of the data distribution. In addition, our bound does not depend explicitly on the ambient dimension; more precisely, the dimension affects the complexity of our solver through the \emph{effective radius} of the support of the target distribution only.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于扩散的高精度、无维采样方法</div>
<div class="mono" style="margin-top:8px">扩散模型在从丰富多模态分布中进行采样方面表现出显著的实证成功。其推理依赖于数值求解某个微分方程。该微分方程无法闭合求解，通常通过离散化求解需要大量的小迭代步骤才能生成\emph{高质量}的样本。
更具体地说，先前的工作表明，扩散模型的离散化方法的迭代复杂度在环境维度和精度倒数 $1/\varepsilon$ 上呈多项式增长。在本文中，我们提出了一种新的扩散模型求解器，其依赖于低次近似与配点法（Lee, Song, Vempala 2018）之间的微妙结合，并证明其迭代复杂度在 $1/\varepsilon$ 上呈\emph{对数多项式}增长，从而为仅使用（近似）数据分布得分访问的扩散采样器提供了首个\emph{高精度}保证。此外，我们的界不显式依赖于环境维度；更准确地说，维度仅通过目标分布支撑集的\emph{有效半径}影响求解器的复杂度。</div>
</details>
</div>
<div class="card">
<div class="title">See Less, Drive Better: Generalizable End-to-End Autonomous Driving via Foundation Models Stochastic Patch Selection</div>
<div class="meta-line">Authors: Amir Mallak, Erfan Aasi, Shiva Sreeram, Tsun-Hsuan Wang, Daniela Rus, Alaa Maalouf</div>
<div class="meta-line">First: 2026-01-15T18:58:33+00:00 · Latest: 2026-01-15T18:58:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10707v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10707v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in end-to-end autonomous driving show that policies trained on patch-aligned features extracted from foundation models generalize better to Out-of-Distribution (OOD). We hypothesize that due to the self-attention mechanism, each patch feature implicitly embeds/contains information from all other patches, represented in a different way and intensity, making these descriptors highly redundant. We quantify redundancy in such (BLIP2) features via PCA and cross-patch similarity: $90$% of variance is captured by $17/64$ principal components, and strong inter-token correlations are pervasive. Training on such overlapping information leads the policy to overfit spurious correlations, hurting OOD robustness. We present Stochastic-Patch-Selection (SPS), a simple yet effective approach for learning policies that are more robust, generalizable, and efficient. For every frame, SPS randomly masks a fraction of patch descriptors, not feeding them to the policy model, while preserving the spatial layout of the remaining patches. Thus, the policy is provided with different stochastic but complete views of the (same) scene: every random subset of patches acts like a different, yet still sensible, coherent projection of the world. The policy thus bases its decisions on features that are invariant to which specific tokens survive. Extensive experiments confirm that across all OOD scenarios, our method outperforms the state of the art (SOTA), achieving a $6.2$% average improvement and up to $20.4$% in closed-loop simulations, while being $2.4\times$ faster. We conduct ablations over masking rates and patch-feature reorganization, training and evaluating 9 systems, with 8 of them surpassing prior SOTA. Finally, we show that the same learned policy transfers to a physical, real-world car without any tuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>少看多行：通过基础模型随机补丁选择实现可泛化的端到端自动驾驶</div>
<div class="mono" style="margin-top:8px">最近的端到端自动驾驶进展表明，基于基础模型提取的对齐补丁特征训练的策略在分布外（OOD）场景中具有更好的泛化能力。我们假设由于自注意力机制，每个补丁特征隐式地嵌入/包含来自其他所有补丁的信息，以不同的方式和强度表示，使得这些描述符高度冗余。我们通过主成分分析（PCA）和跨补丁相似性量化这种（BLIP2）特征的冗余性：90%的方差由17/64个主成分捕获，且存在普遍的token间强相关性。在这样的重叠信息上进行训练会导致策略过拟合虚假相关性，从而损害OOD鲁棒性。我们提出了随机补丁选择（Stochastic-Patch-Selection, SPS），这是一种简单而有效的学习策略的方法，使策略更加鲁棒、可泛化和高效。对于每一帧，SPS随机屏蔽一部分补丁描述符，不将它们输入策略模型，同时保留剩余补丁的空间布局。因此，策略会接收到不同但完整的场景的随机视图：每个随机补丁子集都像世界的不同但仍然合理的、一致的投影。策略因此基于那些不依赖于特定token是否存活的不变特征做出决策。大量实验验证了我们的方法在所有OOD场景中均优于当前最先进的方法（SOTA），平均提升6.2%，在闭环模拟中最高提升20.4%，同时速度提高了2.4倍。我们对屏蔽率和补丁特征重组进行了消融实验，训练并评估了9个系统，其中8个超越了之前的SOTA。最后，我们展示了相同的训练策略可以直接迁移到物理的现实世界汽车上，无需任何调优。</div>
</details>
</div>
<div class="card">
<div class="title">Distributed Perceptron under Bounded Staleness, Partial Participation, and Noisy Communication</div>
<div class="meta-line">Authors: Keval Jain, Anant Raj, Saurav Prakash, Girish Varma</div>
<div class="meta-line">First: 2026-01-15T18:56:54+00:00 · Latest: 2026-01-15T18:56:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10705v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10705v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study a semi-asynchronous client-server perceptron trained via iterative parameter mixing (IPM-style averaging): clients run local perceptron updates and a server forms a global model by aggregating the updates that arrive in each communication round. The setting captures three system effects in federated and distributed deployments: (i) stale updates due to delayed model delivery and delayed application of client computations (two-sided version lag), (ii) partial participation (intermittent client availability), and (iii) imperfect communication on both downlink and uplink, modeled as effective zero-mean additive noise with bounded second moment. We introduce a server-side aggregation rule called staleness-bucket aggregation with padding that deterministically enforces a prescribed staleness profile over update ages without assuming any stochastic model for delays or participation. Under margin separability and bounded data radius, we prove a finite-horizon expected bound on the cumulative weighted number of perceptron mistakes over a given number of server rounds: the impact of delay appears only through the mean enforced staleness, whereas communication noise contributes an additional term that grows on the order of the square root of the horizon with the total noise energy. In the noiseless case, we show how a finite expected mistake budget yields an explicit finite-round stabilization bound under a mild fresh-participation condition.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>受限新鲜度、部分参与和噪声通信下的分布式感知机</div>
<div class="mono" style="margin-top:8px">我们研究一种半异步的客户端-服务器感知机，其通过迭代参数混合（IPM风格平均）进行训练：客户端运行本地感知机更新，服务器通过每个通信轮次中接收到的更新进行聚合以形成全局模型。该设置涵盖了联邦和分布式部署中的三种系统效应：(i) 由于模型传递延迟和客户端计算应用延迟导致的过时更新（双向版本滞后），(ii) 部分参与（客户端间歇性可用性），以及 (iii) 上下行链路中不完美的通信，建模为具有有界二阶矩的有效零均值加性噪声。我们引入了一种称为填充式新鲜度桶聚合的服务器端聚合规则，该规则在不假设任何随机延迟或参与模型的情况下，确定性地强制执行指定的新鲜度配置文件。在边距可分性和有界数据半径的条件下，我们证明了在给定服务器轮次数量下，累积加权感知机错误数的有限时间范围内的期望界：延迟的影响仅通过强制执行的新鲜度均值体现，而通信噪声则贡献了一个额外项，其增长速率与时间范围的平方根成正比，且与总噪声能量相关。在无噪声情况下，我们展示了在轻微的新鲜参与条件下，有限期望错误预算如何产生一个显式的有限轮次稳定化界。</div>
</details>
</div>
<div class="card">
<div class="title">Grounding Agent Memory in Contextual Intent</div>
<div class="meta-line">Authors: Ruozhen Yang, Yucheng Jiang, Yueqi Jiang, Priyanka Kargupta, Yunyi Zhang, Jiawei Han</div>
<div class="meta-line">First: 2026-01-15T18:55:13+00:00 · Latest: 2026-01-15T18:55:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10702v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10702v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Deploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence. We propose STITCH (Structured Intent Tracking in Contextual History), an agentic memory system that indexes each trajectory step with a structured retrieval cue, contextual intent, and retrieves history by matching the current step&#x27;s intent. Contextual intent provides compact signals that disambiguate repeated mentions and reduce interference: (1) the current latent goal defining a thematic segment, (2) the action type, and (3) the salient entity types anchoring which attributes matter. During inference, STITCH filters and prioritizes memory snippets by intent compatibility, suppressing semantically similar but context-incompatible history.
  For evaluation, we introduce CAME-Bench, a benchmark for context-aware retrieval in realistic, dynamic, goal-oriented trajectories. Across CAME-Bench and LongMemEval, STITCH achieves state-of-the-art performance, outperforming the strongest baseline by 35.6%, with the largest gains as trajectory length increases. Our analysis shows that intent indexing substantially reduces retrieval noise, supporting intent-aware memory for robust long-horizon reasoning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>在上下文意图中锚定代理记忆</div>
<div class="mono" style="margin-top:8px">在长时域、目标导向的交互中部署大型语言模型仍然具有挑战性，因为相似实体和事实会在不同的潜在目标和约束下重复出现，导致记忆系统检索到上下文不匹配的证据。我们提出了STITCH（上下文历史中的结构化意图追踪），这是一种代理记忆系统，它通过结构化检索提示符对每个轨迹步骤进行索引，并通过匹配当前步骤的意图来检索历史信息。上下文意图提供了紧凑的信号，以消除重复提及的歧义并减少干扰：(1) 当前潜在目标，定义一个主题段；(2) 动作类型；以及(3) 锚定哪些属性重要的显著实体类型。在推理过程中，STITCH通过意图兼容性过滤和优先级排序记忆片段，抑制语义相似但上下文不兼容的历史信息。
  为了评估，我们引入了CAME-Bench，这是一个用于现实、动态、目标导向轨迹中上下文感知检索的基准。在CAME-Bench和LongMemEval上，STITCH实现了最先进的性能，优于最强基线35.6%，随着轨迹长度的增加，性能提升最为显著。我们的分析表明，意图索引显著降低了检索噪声，支持意图感知的记忆，从而实现稳健的长时域推理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Deploying large language models in long-horizon, goal-oriented interactions remains challenging because similar entities and facts recur under different latent goals and constraints, causing memory systems to retrieve context-mismatched evidence.</div>
</details>
</div>
<div class="card">
<div class="title">Communication-Efficient and Privacy-Adaptable Mechanism -- a Federated Learning Scheme with Convergence Analysis</div>
<div class="meta-line">Authors: Chun Hei Michael Shiu, Chih Wei Ling</div>
<div class="meta-line">First: 2026-01-15T18:55:00+00:00 · Latest: 2026-01-15T18:55:00+00:00</div>
<div class="meta-line">Comments: 19 pages, 5 figures. This work is submitted in part to the 2026 IEEE International Symposium on Information Theory (ISIT). arXiv admin note: substantial text overlap with arXiv:2501.12046</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10701v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10701v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated learning enables multiple parties to jointly train learning models without sharing their own underlying data, offering a practical pathway to privacy-preserving collaboration under data-governance constraints. Continued study of federated learning is essential to address key challenges in it, including communication efficiency and privacy protection between parties. A recent line of work introduced a novel approach called the Communication-Efficient and Privacy-Adaptable Mechanism (CEPAM), which achieves both objectives simultaneously. CEPAM leverages the rejection-sampled universal quantizer (RSUQ), a randomized vector quantizer whose quantization error is equivalent to a prescribed noise, which can be tuned to customize privacy protection between parties. In this work, we theoretically analyze the privacy guarantees and convergence properties of CEPAM. Moreover, we assess CEPAM&#x27;s utility performance through experimental evaluations, including convergence profiles compared with other baselines, and accuracy-privacy trade-offs between different parties.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通信高效且隐私自适应机制——一种具有收敛性分析的联邦学习方案</div>
<div class="mono" style="margin-top:8px">联邦学习使得多方能够在不共享其原始数据的情况下联合训练学习模型，为在数据治理约束下实现隐私保护的协作提供了实际途径。持续研究联邦学习对于解决其中的关键挑战至关重要，包括通信效率和各方之间的隐私保护。最近的一项工作引入了一种名为通信高效且隐私自适应机制（CEPAM）的新方法，该方法同时实现了这两个目标。CEPAM利用了拒绝采样通用量化器（RSUQ），这是一种随机向量量化器，其量化误差等同于指定的噪声，可以通过调整来定制各方之间的隐私保护。在本文中，我们从理论上分析了CEPAM的隐私保证和收敛性质。此外，我们通过实验评估来考察CEPAM的实用性表现，包括与其他基线方法的收敛曲线对比，以及不同方之间的准确率与隐私权衡。</div>
</details>
</div>
<div class="card">
<div class="title">LIBERTy: A Causal Framework for Benchmarking Concept-Based Explanations of LLMs with Structural Counterfactuals</div>
<div class="meta-line">Authors: Gilat Toker, Nitay Calderon, Ohad Amosy, Roi Reichart</div>
<div class="meta-line">First: 2026-01-15T18:54:50+00:00 · Latest: 2026-01-15T18:54:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10700v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10700v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains. Recent work evaluates the faithfulness of such explanations by comparing them to reference causal effects estimated from counterfactuals. In practice, existing benchmarks rely on costly human-written counterfactuals that serve as an imperfect proxy. To address this, we introduce a framework for constructing datasets containing structural counterfactual pairs: LIBERTy (LLM-based Interventional Benchmark for Explainability with Reference Targets). LIBERTy is grounded in explicitly defined Structured Causal Models (SCMs) of the text generation, interventions on a concept propagate through the SCM until an LLM generates the counterfactual. We introduce three datasets (disease detection, CV screening, and workplace violence prediction) together with a new evaluation metric, order-faithfulness. Using them, we evaluate a wide range of methods across five models and identify substantial headroom for improving concept-based explanations. LIBERTy also enables systematic analysis of model sensitivity to interventions: we find that proprietary LLMs show markedly reduced sensitivity to demographic concepts, likely due to post-training mitigation. Overall, LIBERTy provides a much-needed benchmark for developing faithful explainability methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LIBERTy：基于结构反事实的LLM概念解释基准框架</div>
<div class="mono" style="margin-top:8px">基于概念的解释方法衡量高级概念（如性别或经验）如何影响模型行为，这对于高风险领域的决策者至关重要。近期研究通过将这些解释与从反事实中估计的参考因果效应进行比较，评估其忠实度。然而，现有基准依赖于昂贵的人工编写反事实，这些反事实作为不完美的代理。为了解决这一问题，我们引入了一个框架，用于构建包含结构反事实对的数据集：LIBERTy（基于LLM的可解释性干预基准，具有参考目标）。LIBERTy基于文本生成的显式定义的结构因果模型（SCMs），对概念进行干预后，通过SCM传播直到LLM生成反事实。我们引入了三个数据集（疾病检测、计算机视觉筛选和职场暴力预测）以及一个新的评估指标——顺序忠实度。利用这些数据集，我们评估了五种模型中多种方法的性能，并发现概念解释方法仍有显著提升空间。此外，LIBERTy还支持对模型对干预的敏感性进行系统分析：我们发现专有LLM对人口统计学概念的敏感性明显降低，这可能归因于训练后的缓解措施。总体而言，LIBERTy为开发忠实的可解释性方法提供了一个急需的基准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Concept-based explanations quantify how high-level concepts (e.g., gender or experience) influence model behavior, which is crucial for decision-makers in high-stakes domains.</div>
</details>
</div>
<div class="card">
<div class="title">The Impact of Generative AI on Architectural Conceptual Design: Performance, Creative Self-Efficacy and Cognitive Load</div>
<div class="meta-line">Authors: Han Jiang, Yao Xiao, Rachel Hurley, Shichao Liu</div>
<div class="meta-line">First: 2026-01-15T18:52:59+00:00 · Latest: 2026-01-15T18:52:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10696v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10696v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks. Thirty-six student participants from Architectural Engineering and other disciplines completed a two-phase architectural design task, first independently and then with external tools (GenAI-assisted condition and control condition using an online repository of existing architectural projects). Design outcomes were evaluated by expert raters, while self-efficacy and cognitive load were self-reported after each phase. Difference-in-differences analyses revealed no overall performance advantage of GenAI across participants; however, subgroup analyses showed that GenAI significantly improved design performance for novice designers. In contrast, general creative self-efficacy declined for students using GenAI. Cognitive load did not differ significantly between conditions, though prompt usage patterns showed that iterative idea generation and visual feedback prompts were linked to greater reductions in cognitive load. These findings suggest that GenAI effectiveness depends on users&#x27; prior expertise and interaction strategies through prompting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成式人工智能对建筑概念设计的影响：表现、创造性自我效能感与认知负荷</div>
<div class="mono" style="margin-top:8px">我们的研究探讨了生成式人工智能（GenAI）如何影响建筑概念设计任务中的表现、创造性自我效能感和认知负荷。36名来自建筑工程及其他学科的学生完成了两阶段的建筑设计任务，第一阶段独立完成，第二阶段使用外部工具（GenAI辅助组和对照组，使用在线现有建筑项目的数据库）。设计成果由专家评分者评估，而自我效能感和认知负荷则在每个阶段后由参与者自我报告。差异-in-差异分析显示，GenAI对参与者整体表现没有显著优势；然而，子群分析表明，GenAI显著提升了新手设计师的设计表现。相反，使用GenAI的学生的总体创造性自我效能感有所下降。认知负荷在不同条件下没有显著差异，但提示使用模式显示，迭代想法生成和视觉反馈提示与更大的认知负荷降低相关。这些发现表明，GenAI的效果取决于用户先前的专业知识和通过提示的交互策略。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Our study examines how generative AI (GenAI) influences performance, creative self-efficacy, and cognitive load in architectural conceptual design tasks.</div>
</details>
</div>
<div class="card">
<div class="title">Data-driven stochastic reduced-order modeling of parametrized dynamical systems</div>
<div class="meta-line">Authors: Andrew F. Ilersich, Kevin Course, Prasanth B. Nair</div>
<div class="meta-line">First: 2026-01-15T18:50:18+00:00 · Latest: 2026-01-15T18:50:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10690v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10690v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Modeling complex dynamical systems under varying conditions is computationally intensive, often rendering high-fidelity simulations intractable. Although reduced-order models (ROMs) offer a promising solution, current methods often struggle with stochastic dynamics and fail to quantify prediction uncertainty, limiting their utility in robust decision-making contexts. To address these challenges, we introduce a data-driven framework for learning continuous-time stochastic ROMs that generalize across parameter spaces and forcing conditions. Our approach, based on amortized stochastic variational inference, leverages a reparametrization trick for Markov Gaussian processes to eliminate the need for computationally expensive forward solvers during training. This enables us to jointly learn a probabilistic autoencoder and stochastic differential equations governing the latent dynamics, at a computational cost that is independent of the dataset size and system stiffness. Additionally, our approach offers the flexibility of incorporating physics-informed priors if available. Numerical studies are presented for three challenging test problems, where we demonstrate excellent generalization to unseen parameter combinations and forcings, and significant efficiency gains compared to existing approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于数据驱动的参数化动态系统随机降阶建模</div>
<div class="mono" style="margin-top:8px">在变化条件下对复杂动态系统进行建模计算成本很高，通常使得高保真度模拟难以处理。尽管降阶模型（ROMs）提供了一种有前景的解决方案，但当前方法在处理随机动力学时往往表现不佳，且无法量化预测的不确定性，这限制了它们在鲁棒决策制定中的应用。为了解决这些挑战，我们提出了一种数据驱动的框架，用于学习能够跨参数空间和外力条件泛化的连续时间随机ROMs。我们的方法基于变分推理的期望最大化，利用马尔可夫高斯过程的重参数化技巧，在训练过程中消除对计算成本高昂的正向求解器的需求。这使我们能够联合学习一个概率自编码器和描述潜在动力学的随机微分方程，其计算成本与数据集大小和系统刚度无关。此外，我们的方法还提供了将物理信息先验纳入模型的灵活性。我们针对三个具有挑战性的测试问题进行了数值研究，展示了对未见过的参数组合和外力的出色泛化能力，以及与现有方法相比显著的效率提升。</div>
</details>
</div>
<div class="card">
<div class="title">Matrix convex sets over the Euclidean ball and polar duals of real free spectrahedra</div>
<div class="meta-line">Authors: Eric Evert, Benjamin Passer</div>
<div class="meta-line">First: 2025-07-27T15:34:41+00:00 · Latest: 2026-01-15T18:46:57+00:00</div>
<div class="meta-line">Comments: 25 pages. Version 2 has minor updates and typo corrections. To appear in Linear Algebra and Its Applications</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.20325v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.20325v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We show that the free spectrahedron determined by universal anticommuting self-adjoint unitaries is not equal to the minimal matrix convex set over the ball in dimension three or higher. This example, as well as other matrix convex sets over the ball, then provides context for structure results on the extreme points of coordinate projections. In particular, we show that the free polar dual of a real free spectrahedron is rarely the projection of a real free spectrahedron, contrasting a prior result of Helton, Klep, and McCullough over the complexes. We use this to show that spanning results for free spectrahedra that are closed under complex conjugation do not extend to free spectrahedrops that meet the same assumption. These results further clarify the role of the coefficient field.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>欧几里得球上的矩阵凸集与实自由谱面的极对偶</div>
<div class="mono" style="margin-top:8px">我们证明由普遍反交换自伴酉元确定的自由谱面不等于三维及以上维度球上的最小矩阵凸集。这个例子以及其他球上的矩阵凸集为坐标投影的极值点结构结果提供了背景。特别是，我们证明了实自由谱面的自由极对偶很少是实自由谱面的投影，这与Helton、Klep和McCullough在复数域上的先前结果形成对比。我们利用这一点来说明，对于满足复共轭封闭条件的自由谱面的张成结果，并不适用于满足相同假设的自由谱面。这些结果进一步澄清了系数域的作用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We show that the free spectrahedron determined by universal anticommuting self-adjoint unitaries is not equal to the minimal matrix convex set over the ball in dimension three or higher.</div>
</details>
</div>
<div class="card">
<div class="title">On the origin of neural scaling laws: from random graphs to natural language</div>
<div class="meta-line">Authors: Maissam Barkeshli, Alberto Alfarano, Andrey Gromov</div>
<div class="meta-line">First: 2026-01-15T18:46:09+00:00 · Latest: 2026-01-15T18:46:09+00:00</div>
<div class="meta-line">Comments: 33 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10684v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10684v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Scaling laws have played a major role in the modern AI revolution, providing practitioners predictive power over how the model performance will improve with increasing data, compute, and number of model parameters. This has spurred an intense interest in the origin of neural scaling laws, with a common suggestion being that they arise from power law structure already present in the data. In this paper we study scaling laws for transformers trained to predict random walks (bigrams) on graphs with tunable complexity. We demonstrate that this simplified setting already gives rise to neural scaling laws even in the absence of power law structure in the data correlations. We further consider dialing down the complexity of natural language systematically, by training on sequences sampled from increasingly simplified generative language models, from 4,2,1-layer transformer language models down to language bigrams, revealing a monotonic evolution of the scaling exponents. Our results also include scaling laws obtained from training on random walks on random graphs drawn from Erdös-Renyi and scale-free Barabási-Albert ensembles. Finally, we revisit conventional scaling laws for language modeling, demonstrating that several essential results can be reproduced using 2 layer transformers with context length of 50, provide a critical analysis of various fits used in prior literature, demonstrate an alternative method for obtaining compute optimal curves as compared with current practice in published literature, and provide preliminary evidence that maximal update parameterization may be more parameter efficient than standard parameterization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>神经扩展定律的起源：从随机图到自然语言</div>
<div class="mono" style="margin-top:8px">扩展定律在现代人工智能革命中发挥了重要作用，为从业者提供了关于模型性能如何随数据量、计算资源和模型参数数量增加而提升的预测能力。这激发了对神经扩展定律起源的浓厚兴趣，一个常见的观点是这些定律源于数据中已有的幂律结构。在本文中，我们研究了在具有可调复杂度的图上训练以预测随机游走（二元组）的变压器模型的扩展定律。我们证明，即使在数据相关性中不存在幂律结构的情况下，这种简化设置也能产生神经扩展定律。我们进一步通过训练序列，系统地降低自然语言的复杂度，从4层、2层、1层的Transformer语言模型，一直到语言二元组，揭示了扩展指数的单调演化。我们的结果还包括从Erdös-Renyi和无标度Barabási-Albert模型中随机图的随机游走训练得到的扩展定律。最后，我们重新审视传统的语言建模扩展定律，展示了使用上下文长度为50的2层Transformer可以复现多个关键结果，对之前文献中使用的各种拟合方法进行了批判性分析，提出了一种与当前文献中实践不同的获取计算最优曲线的替代方法，并提供了初步证据表明最大更新参数化可能比标准参数化更节省参数。</div>
</details>
</div>
<div class="card">
<div class="title">Structure and Diversity Aware Context Bubble Construction for Enterprise Retrieval Augmented Systems</div>
<div class="meta-line">Authors: Amir Khurshid, Abhishek Sehgal</div>
<div class="meta-line">First: 2026-01-15T18:43:19+00:00 · Latest: 2026-01-15T18:43:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10681v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10681v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages. The approach causes fragmentation in information graphs in document structures, over-retrieval, and duplication of content alongside insufficient query context, including 2nd and 3rd order facets. In this paper, a structure-informed and diversity-constrained context bubble construction framework is proposed that assembles coherent, citable bundles of spans under a strict token budget. The method preserves and exploits inherent document structure by organising multi-granular spans (e.g., sections and rows) and using task-conditioned structural priors to guide retrieval. Starting from high-relevance anchor spans, a context bubble is constructed through constrained selection that balances query relevance, marginal coverage, and redundancy penalties. It will explicitly constrain diversity and budget, producing compact and informative context sets, unlike top-k retrieval. Moreover, a full retrieval is emitted that traces the scoring and selection choices of the records, thus providing auditability and deterministic tuning. Experiments on enterprise documents demonstrate the efficiency of context bubble as it significantly reduces redundant context, is better able to cover secondary facets and has a better answer quality and citation faithfulness within a limited context window. Ablation studies demonstrate that both structural priors as well as diversity constraint selection are necessary; removing either component results in a decline in coverage and an increase in redundant or incomplete context.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language model (LLM) contexts are typically constructed using retrieval-augmented generation (RAG), which involves ranking and selecting the top-k passages.</div>
</details>
</div>
<div class="card">
<div class="title">Are Your Reasoning Models Reasoning or Guessing? A Mechanistic Analysis of Hierarchical Reasoning Models</div>
<div class="meta-line">Authors: Zirui Ren, Ziming Liu</div>
<div class="meta-line">First: 2026-01-15T18:42:50+00:00 · Latest: 2026-01-15T18:42:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10679v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10679v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Hierarchical reasoning model (HRM) achieves extraordinary performance on various reasoning tasks, significantly outperforming large language model-based reasoners. To understand the strengths and potential failure modes of HRM, we conduct a mechanistic study on its reasoning patterns and find three surprising facts: (a) Failure of extremely simple puzzles, e.g., HRM can fail on a puzzle with only one unknown cell. We attribute this failure to the violation of the fixed point property, a fundamental assumption of HRM. (b) &quot;Grokking&quot; dynamics in reasoning steps, i.e., the answer is not improved uniformly, but instead there is a critical reasoning step that suddenly makes the answer correct; (c) Existence of multiple fixed points. HRM &quot;guesses&quot; the first fixed point, which could be incorrect, and gets trapped there for a while or forever. All facts imply that HRM appears to be &quot;guessing&quot; instead of &quot;reasoning&quot;. Leveraging this &quot;guessing&quot; picture, we propose three strategies to scale HRM&#x27;s guesses: data augmentation (scaling the quality of guesses), input perturbation (scaling the number of guesses by leveraging inference randomness), and model bootstrapping (scaling the number of guesses by leveraging training randomness). On the practical side, by combining all methods, we develop Augmented HRM, boosting accuracy on Sudoku-Extreme from 54.5% to 96.9%. On the scientific side, our analysis provides new insights into how reasoning models &quot;reason&quot;.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>你的推理模型是在推理还是在猜测？对分层推理模型的机制分析</div>
<div class="mono" style="margin-top:8px">分层推理模型（HRM）在各种推理任务中表现出色，显著优于基于大语言模型的推理器。为了理解HRM的优势和潜在失败模式，我们对其推理模式进行了机制研究，并发现了三个令人惊讶的事实：(a) 极其简单的谜题失败，例如HRM可能在只有一个未知单元格的谜题上失败。我们将这种失败归因于违反了HRM的基本假设——固定点性质；(b) 推理步骤中存在&quot;理解&quot;动态，即答案并非均匀改进，而是存在一个关键推理步骤，突然使答案正确；(c) 存在多个固定点。HRM会&quot;猜测&quot;第一个固定点，该猜测可能不正确，并可能因此陷入其中一段时间或永远。所有这些事实表明，HRM似乎是在&quot;猜测&quot;而非&quot;推理&quot;。基于这种&quot;猜测&quot;的视角，我们提出了三种策略来扩展HRM的猜测能力：数据增强（提升猜测质量）、输入扰动（利用推理随机性扩展猜测数量）和模型自举（利用训练随机性扩展猜测数量）。在实际应用方面，通过结合所有方法，我们开发了增强型HRM（Augmented HRM），将Sudoku-Extreme的准确率从54.5%提升至96.9%。在科学层面，我们的分析为推理模型如何进行&quot;推理&quot;提供了新的见解。</div>
</details>
</div>
<div class="card">
<div class="title">Single-Stage Huffman Encoder for ML Compression</div>
<div class="meta-line">Authors: Aditya Agrawal, Albert Magyar, Hiteshwar Eswaraiah, Patrick Sheridan, Pradeep Janedula, Ravi Krishnan Venkatesan, Krishna Nair, Ravi Iyer</div>
<div class="meta-line">First: 2026-01-15T18:37:56+00:00 · Latest: 2026-01-15T18:37:56+00:00</div>
<div class="meta-line">Comments: 5 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10673v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10673v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Training and serving Large Language Models (LLMs) require partitioning data across multiple accelerators, where collective operations are frequently bottlenecked by network bandwidth. Lossless compression using Huffman codes is an effective way to alleviate the issue, however, its three-stage design requiring on-the-fly frequency analysis, codebook generation and transmission of codebook along with data introduces computational, latency and data overheads which are prohibitive for latency-sensitive scenarios such as die-to-die communication. This paper proposes a single-stage Huffman encoder that eliminates these overheads by using fixed codebooks derived from the average probability distribution of previous data batches. Through our analysis of the Gemma 2B model, we demonstrate that tensors exhibit high statistical similarity across layers and shards. Using this approach we achieve compression within 0.5% of per-shard Huffman coding and within 1% of the ideal Shannon compressibility, enabling efficient on-the-fly compression.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于机器学习压缩的单阶段霍夫曼编码器</div>
<div class="mono" style="margin-top:8px">训练和部署大型语言模型（LLMs）需要将数据分布在多个加速器上，其中集体操作经常受到网络带宽的限制。使用霍夫曼码进行无损压缩是一种有效缓解该问题的方法，但其三阶段设计需要实时频率分析、码本生成以及码本与数据的传输，这会引入计算、延迟和数据开销，对于延迟敏感的场景（如芯片间通信）是不可行的。本文提出了一种单阶段霍夫曼编码器，通过使用从先前数据批次的平均概率分布中导出的固定码本，消除了这些开销。通过对Gemma 2B模型的分析，我们证明了张量在层和分片之间具有高度的统计相似性。利用这种方法，我们实现了接近每分片霍夫曼编码0.5%的压缩率，以及接近理想香农压缩率1%的压缩效果，从而实现了高效的实时压缩。</div>
</details>
</div>
<div class="card">
<div class="title">BASIL: Bayesian Assessment of Sycophancy in LLMs</div>
<div class="meta-line">Authors: Katherine Atwell, Pedram Heydari, Anthony Sicilia, Malihe Alikhani</div>
<div class="meta-line">First: 2025-08-23T00:11:00+00:00 · Latest: 2026-01-15T18:31:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.16846v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.16846v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sycophancy (overly agreeable or flattering behavior) poses a fundamental challenge for human-AI collaboration, particularly in high-stakes decision-making domains such as health, law, and education. A central difficulty in studying sycophancy in large language models (LLMs) is disentangling sycophantic belief shifts from rational changes in behavior driven by new evidence or user-provided information. Existing approaches either measure descriptive behavior changes or apply normative evaluations that rely on objective ground truth, limiting their applicability to subjective or uncertain tasks. We introduce a Bayesian probabilistic framework, grounded in behavioral economics and rational decision theory, that explicitly separates sycophancy from rational belief updating. Within this framework, we achieve three objectives: (i) a descriptive metric that measures sycophancy while controlling for rational responses to evidence; (ii) a normative metric that quantifies how sycophancy leads models astray from Bayesian-consistent belief updating; and (iii) the ability to apply both metrics in settings without ground-truth labels. Applying our framework across multiple LLMs and three uncertainty-driven tasks, we find robust evidence of sycophantic belief shifts and show that their impact on rationality depends on whether models systematically over- or under-update their beliefs. Finally, we demonstrate that a post-hoc calibration method and two fine-tuning strategies (SFT and DPO) substantially reduce Bayesian inconsistency, with particularly strong improvements under explicit sycophancy prompting.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>BASIL：基于贝叶斯的LLMs谄媚行为评估</div>
<div class="mono" style="margin-top:8px">谄媚行为（过度顺从或奉承的行为）对人机协作构成了根本性的挑战，尤其是在高风险决策领域，如医疗、法律和教育。研究大型语言模型（LLMs）中的谄媚行为的主要困难在于区分由谄媚驱动的信念变化与由新证据或用户信息驱动的理性行为变化。现有方法要么衡量描述性的行为变化，要么依赖客观真实值的规范性评估，这限制了它们在主观或不确定任务中的适用性。我们引入了一种基于行为经济学和理性决策理论的贝叶斯概率框架，明确区分谄媚行为与理性信念更新。在该框架下，我们实现了三个目标：(i) 一种描述性指标，用于衡量谄媚行为，同时控制理性对证据的反应；(ii) 一种规范性指标，量化谄媚行为如何使模型偏离贝叶斯一致的信念更新；(iii) 在没有真实标签的环境中应用这两种指标的能力。我们在多个LLMs和三个由不确定性驱动的任务中应用该框架，发现了谄媚信念变化的稳健证据，并表明其对理性的影响取决于模型是否系统性地过度或不足地更新信念。最后，我们展示了后验校准方法和两种微调策略（SFT和DPO）能够显著减少贝叶斯不一致性，特别是在显式谄媚提示下表现出特别显著的改进。</div>
</details>
</div>
<div class="card">
<div class="title">Pareto-Grid-Guided Large Language Models for Fast and High-Quality Heuristics Design in Multi-Objective Combinatorial Optimization</div>
<div class="meta-line">Authors: Minh Hieu Ha, Hung Phan, Tung Duy Doan, Tung Dao, Dao Tran, Huynh Thi Thanh Binh</div>
<div class="meta-line">Venue: AAAI</div>
<div class="meta-line">First: 2025-07-28T15:26:43+00:00 · Latest: 2026-01-15T18:28:50+00:00</div>
<div class="meta-line">Comments: Accepted at AAAI-26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.20923v3">Abs</a> · <a href="https://arxiv.org/pdf/2507.20923v3">PDF</a> · <a href="https://github.com/langkhachhoha/MPaGE">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-objective combinatorial optimization problems (MOCOP) frequently arise in practical applications that require the simultaneous optimization of conflicting objectives. Although traditional evolutionary algorithms can be effective, they typically depend on domain knowledge and repeated parameter tuning, limiting flexibility when applied to unseen MOCOP instances. Recently, integration of Large Language Models (LLMs) into evolutionary computation has opened new avenues for automatic heuristic generation, using their advanced language understanding and code synthesis capabilities. Nevertheless, most existing approaches predominantly focus on single-objective tasks, often neglecting key considerations such as runtime efficiency and heuristic diversity in multi-objective settings. To bridge this gap, we introduce Multi-heuristics for MOCOP via Pareto-Grid-guided Evolution of LLMs (MPaGE), a novel enhancement of the Simple Evolutionary Multiobjective Optimization (SEMO) framework that leverages LLMs and Pareto Front Grid (PFG) technique. By partitioning the objective space into grids and retaining top-performing candidates to guide heuristic generation, MPaGE utilizes LLMs to prioritize heuristics with semantically distinct logical structures during variation, thus promoting diversity and mitigating redundancy within the population. Through extensive evaluations, MPaGE demonstrates superior performance over existing LLM-based frameworks, and achieves competitive results to traditional Multi-objective evolutionary algorithms (MOEAs), with significantly faster runtime. Our code is available at: https://github.com/langkhachhoha/MPaGE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于帕累托网格引导的大语言模型用于多目标组合优化中的快速高质量启发式设计</div>
<div class="mono" style="margin-top:8px">多目标组合优化问题（MOCOP）在实际应用中经常出现，这些问题需要同时优化相互冲突的目标。尽管传统进化算法可以有效解决这些问题，但它们通常依赖于领域知识和重复的参数调整，这在应用于未见过的MOCOP实例时限制了灵活性。最近，将大语言模型（LLMs）整合到进化计算中，为自动启发式生成开辟了新的途径，利用其先进的语言理解和代码合成能力。然而，大多数现有方法主要关注单目标任务，常常忽视多目标设置中的关键因素，如运行时效率和启发式多样性。为弥合这一差距，我们引入了基于帕累托网格引导的LLM进化多启发式方法（MPaGE），这是对简单进化多目标优化（SEMO）框架的一种创新增强，结合了LLMs和帕累托前沿网格（PFG）技术。通过将目标空间划分为网格并保留表现最佳的候选方案以指导启发式生成，MPaGE利用LLMs在变异过程中优先选择语义上不同的逻辑结构启发式，从而促进种群的多样性并减少冗余。通过广泛的评估，MPaGE在现有基于LLM的框架上表现出优越的性能，并在运行时间上显著加快，同时与传统多目标进化算法（MOEAs）相比也取得了具有竞争力的结果。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-objective combinatorial optimization problems (MOCOP) frequently arise in practical applications that require the simultaneous optimization of conflicting objectives.</div>
</details>
</div>
<div class="card">
<div class="title">Moonworks Lunara Aesthetic Dataset</div>
<div class="meta-line">Authors: Yan Wang, M M Sayeef Abdullah, Partho Hassan, Sabit Hassan</div>
<div class="meta-line">First: 2026-01-12T19:11:41+00:00 · Latest: 2026-01-15T18:27:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07941v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07941v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The dataset spans diverse artistic styles, including regionally grounded aesthetics from the Middle East, Northern Europe, East Asia, and South Asia, alongside general categories such as sketch and oil painting. All images are generated using the Moonworks Lunara model and intentionally crafted to embody distinct, high-quality aesthetic styles, yielding a first-of-its-kind dataset with substantially higher aesthetic scores, exceeding even aesthetics-focused datasets, and general-purpose datasets by a larger margin. Each image is accompanied by a human-refined prompt and structured annotations that jointly describe salient objects, attributes, relationships, and stylistic cues. Unlike large-scale web-derived datasets that emphasize breadth over precision, the Lunara Aesthetic Dataset prioritizes aesthetic quality, stylistic diversity, and licensing transparency, and is released under the Apache 2.0 license to support research and unrestricted academic and commercial use.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Moonworks Lunara美学数据集</div>
<div class="mono" style="margin-top:8px">该数据集涵盖多种艺术风格，包括中东、北欧、东亚和南亚地区的本土美学，以及素描和油画等通用类别。所有图像均使用Moonworks Lunara模型生成，并经过有意设计以体现独特的高质量美学风格，从而创建了一个前所未有的数据集，其美学评分显著高于专门以美学为导向的数据集和通用数据集。每张图像均配有经过人类优化的提示语和结构化注释，共同描述显著物体、属性、关系和风格线索。与强调广度而非精确性的大规模网络衍生数据集不同，Lunara美学数据集优先考虑美学质量、风格多样性和授权透明度，并在Apache 2.0许可下发布，以支持研究和不受限制的学术及商业用途。</div>
</details>
</div>
<div class="card">
<div class="title">Knowledge Homophily in Large Language Models</div>
<div class="meta-line">Authors: Utkarsh Sahu, Zhisheng Qi, Mahantesh Halappanavar, Nedim Lipka, Ryan A. Rossi, Franck Dernoncourt, Yu Zhang, Yao Ma, Yu Wang</div>
<div class="meta-line">First: 2025-09-28T09:40:27+00:00 · Latest: 2026-01-15T18:26:36+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.23773v2">Abs</a> · <a href="https://arxiv.org/pdf/2509.23773v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have been increasingly studied as neural knowledge bases for supporting knowledge-intensive applications such as question answering and fact checking. However, the structural organization of their knowledge remains unexplored. Inspired by cognitive neuroscience findings, such as semantic clustering and priming, where knowing one fact increases the likelihood of recalling related facts, we investigate an analogous knowledge homophily pattern in LLMs. To this end, we map LLM knowledge into a graph representation through knowledge checking at both the triplet and entity levels. After that, we analyze the knowledgeability relationship between an entity and its neighbors, discovering that LLMs tend to possess a similar level of knowledge about entities positioned closer in the graph. Motivated by this homophily principle, we propose a Graph Neural Network (GNN) regression model to estimate entity-level knowledgeability scores for triplets by leveraging their neighborhood scores. The predicted knowledgeability enables us to prioritize checking less well-known triplets, thereby maximizing knowledge coverage under the same labeling budget. This not only improves the efficiency of active labeling for fine-tuning to inject knowledge into LLMs but also enhances multi-hop path retrieval in reasoning-intensive question answering.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>大语言模型中的知识同质性</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）作为神经知识库，越来越多地被研究以支持知识密集型应用，如问答和事实核查。然而，其知识的结构组织尚未被探索。受认知神经科学发现的启发，例如语义聚类和启动效应，其中了解一个事实会增加回忆相关事实的可能性，我们研究了LLMs中类似的知识同质性模式。为此，我们通过三元组和实体级别的知识验证，将LLM的知识映射到图表示中。随后，我们分析了实体与其邻居之间的知识关系，发现LLMs倾向于对图中位置相近的实体具有相似的知识水平。基于这一同质性原则，我们提出了一种图神经网络（GNN）回归模型，通过利用三元组的邻域得分来估计实体级别的知识得分。预测的知识得分使我们能够优先检查知识较少的三元组，从而在相同的标注预算下最大化知识覆盖。这不仅提高了用于微调以向LLMs注入知识的主动标注的效率，还增强了推理密集型问答中的多跳路径检索。</div>
</details>
</div>
<div class="card">
<div class="title">PACEvolve: Enabling Long-Horizon Progress-Aware Consistent Evolution</div>
<div class="meta-line">Authors: Minghao Yan, Bo Peng, Benjamin Coleman, Ziqi Chen, Zhouhang Xie, Zhankui He, Noveen Sachdeva, Isabella Ye, Weili Wang, Chi Wang, Ed H. Chi, Wang-Cheng Kang, Derek Zhiyuan Cheng, Beidou Wang</div>
<div class="meta-line">First: 2026-01-15T18:25:23+00:00 · Latest: 2026-01-15T18:25:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10657v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10657v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Models (LLMs) have emerged as powerful operators for evolutionary search, yet the design of efficient search scaffolds remains ad hoc. While promising, current LLM-in-the-loop systems lack a systematic approach to managing the evolutionary process. We identify three distinct failure modes: Context Pollution, where experiment history biases future candidate generation; Mode Collapse, where agents stagnate in local minima due to poor exploration-exploitation balance; and Weak Collaboration, where rigid crossover strategies fail to leverage parallel search trajectories effectively. We introduce Progress-Aware Consistent Evolution (PACEvolve), a framework designed to robustly govern the agent&#x27;s context and search dynamics, to address these challenges. PACEvolve combines hierarchical context management (HCM) with pruning to address context pollution; momentum-based backtracking (MBB) to escape local minima; and a self-adaptive sampling policy that unifies backtracking and crossover for dynamic search coordination (CE), allowing agents to balance internal refinement with cross-trajectory collaboration. We demonstrate that PACEvolve provides a systematic path to consistent, long-horizon self-improvement, achieving state-of-the-art results on LLM-SR and KernelBench, while discovering solutions surpassing the record on Modded NanoGPT.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PACEvolve: 实现长周期进展感知的一致性演化</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）已成为进化搜索的强大操作符，但高效搜索框架的设计仍属临时性。尽管有前景，当前的LLM-in-the-loop系统缺乏对进化过程的系统管理方法。我们识别出三种不同的失败模式：上下文污染，即实验历史会偏倚未来候选生成；模式崩溃，即由于探索与利用之间的平衡不佳，智能体陷入局部极小值；以及协作薄弱，即僵化的交叉策略未能有效利用并行搜索轨迹。我们引入了进展感知一致性演化（PACEvolve），这是一种旨在稳健地管理智能体上下文和搜索动态的框架，以解决这些挑战。PACEvolve结合了分层上下文管理（HCM）和剪枝来应对上下文污染；基于动量的回溯（MBB）以逃离局部极小值；以及一种自适应采样策略，将回溯与交叉统一起来，用于动态搜索协调（CE），使智能体能够在内部优化与跨轨迹协作之间取得平衡。我们证明，PACEvolve为实现一致的长周期自我提升提供了一条系统路径，在LLM-SR和KernelBench上取得了最先进的结果，同时发现了超越Modded NanoGPT记录的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">A response-adaptive multi-arm design for continuous endpoints based on a weighted information measure</div>
<div class="meta-line">Authors: Gianmarco Caruso, Pavel Mozgunov</div>
<div class="meta-line">First: 2024-09-08T04:30:46+00:00 · Latest: 2026-01-15T18:19:57+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2409.04970v2">Abs</a> · <a href="https://arxiv.org/pdf/2409.04970v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-arm trials are gaining interest in practice given the statistical and logistical advantages they can offer. The standard approach uses a fixed allocation ratio, but there is a call for making it adaptive and skewing the allocation of patients towards better-performing arms. However, it is well-known that these approaches might suffer from lower statistical power. We present a response-adaptive design for continuous endpoints which explicitly allows to control the trade-off between the number of patients allocated to the &quot;optimal&quot; arm and the statistical power. Such a balance is achieved through the calibration of a tuning parameter, and we explore robust procedures to select it. The proposed criterion is based on a context-dependent information measure which gives greater weight to treatment arms with characteristics close to a pre-specified clinical target. We establish conditions under which the procedure consistently selects the target arm and derive the corresponding limiting allocation ratios. We also introduce a simulation-based hypothesis testing procedure which focuses on selecting the target arm and discuss strategies to effectively control the type-I error rate. The practical implementation of the proposed criterion and its potential advantage over currently used alternatives are illustrated in the context of early Phase IIa proof-of-concept oncology trials.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一种基于加权信息度量的响应自适应多臂设计用于连续终点</div>
<div class="mono" style="margin-top:8px">鉴于多臂试验在统计和操作上的优势，其在实践中受到越来越多的关注。标准方法采用固定分配比例，但有呼声要求使其适应性更强，并将患者分配向表现更好的臂倾斜。然而，众所周知，这些方法可能会导致统计功效降低。我们提出了一种用于连续终点的响应自适应设计，该设计明确允许控制分配到“最佳”臂的患者数量与统计功效之间的权衡。这种平衡是通过校准一个调节参数实现的，我们探讨了稳健的参数选择方法。所提出的准则基于一种情境相关的信息度量，该度量对具有接近预设临床目标特征的治疗臂赋予更大的权重。我们建立了在何种条件下该程序能够一致地选择目标臂，并推导出相应的极限分配比例。此外，我们还引入了一种基于模拟的假设检验程序，专注于选择目标臂，并讨论了有效控制I类错误率的策略。在早期IIa期概念验证肿瘤试验的背景下，我们展示了该准则的实用实现及其相对于当前使用方法的潜在优势。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Property Synthesis</div>
<div class="meta-line">Authors: Christoph Weinhuber, Yannik Schnitzer, Alessandro Abate, David Parker, Giuseppe De Giacomo, Moshe Y. Vardi</div>
<div class="meta-line">First: 2026-01-15T18:18:33+00:00 · Latest: 2026-01-15T18:18:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10651v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10651v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study LTLf synthesis with multiple properties, where satisfying all properties may be impossible. Instead of enumerating subsets of properties, we compute in one fixed-point computation the relation between product-game states and the goal sets that are realizable from them, and we synthesize strategies achieving maximal realizable sets. We develop a fully symbolic algorithm that introduces Boolean goal variables and exploits monotonicity to represent exponentially many goal combinations compactly. Our approach substantially outperforms enumeration-based baselines, with speedups of up to two orders of magnitude.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>多属性综合</div>
<div class="mono" style="margin-top:8px">我们研究具有多个属性的LTLf综合问题，其中满足所有属性可能是不可能的。我们不通过枚举属性子集来处理，而是在一个固定点计算中确定产品游戏状态与从该状态可实现的目标集之间的关系，并综合出实现最大可实现目标集的策略。我们开发了一个完全符号化的算法，引入布尔目标变量并利用单调性来紧凑地表示指数级多的目标组合。我们的方法在性能上显著优于基于枚举的基线方法，速度提升可达两个数量级。</div>
</details>
</div>
<div class="card">
<div class="title">PMOA-TTS: Introducing the PubMed Open Access Textual Times Series Corpus</div>
<div class="meta-line">Authors: Shahriar Noroozizadeh, Sayantan Kumar, George H. Chen, Jeremy C. Weiss</div>
<div class="meta-line">First: 2025-05-23T18:01:09+00:00 · Latest: 2026-01-15T18:18:24+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2505.20323v2">Abs</a> · <a href="https://arxiv.org/pdf/2505.20323v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Clinical narratives encode temporal dynamics essential for modeling patient trajectories, yet large-scale temporally annotated resources are scarce. We introduce PMOA-TTS, a corpus of 124,699 single-patient PubMed Open Access case reports converted into structured textual timelines of (event, time) pairs using a scalable large-language-model pipeline (Llama 3.3 70B and DeepSeek-R1). The corpus comprises over 5.6 million timestamped events, alongside extracted demographics and diagnoses. Technical validation uses a clinician-curated gold set and three measures: semantic event matching, temporal concordance (c-index), and alignment error summarized with Area Under the Log-Time CDF (AULTC). We benchmark alternative prompting and model choices and provide documentation to support reproduction. PMOA-TTS enables research on timeline extraction, temporal reasoning, survival modeling and event forecasting from narrative text, and offers broad diagnostic and demographic coverage. Data and code are openly available in public repositories.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PMOA-TTS：介绍PubMed开放获取文本时间序列语料库</div>
<div class="mono" style="margin-top:8px">临床叙事包含对建模患者轨迹至关重要的时间动态信息，但大规模的时间标注资源却十分有限。我们引入了PMOA-TTS，这是一个包含124,699份单患者PubMed开放获取案例报告的语料库，通过可扩展的大语言模型流水线（Llama 3.3 70B和DeepSeek-R1）将其转换为结构化的文本时间线，以（事件，时间）对的形式呈现。该语料库包含超过560万个带时间戳的事件，以及提取的患者人口统计信息和诊断信息。技术验证使用了临床专家构建的黄金标准集，并采用三种度量方式：语义事件匹配、时间一致性（c指数）以及通过对数时间CDF面积（AULTC）总结的对齐误差。我们对替代提示和模型选择进行了基准测试，并提供了文档以支持结果的复现。PMOA-TTS支持从叙事文本中进行时间线提取、时间推理、生存建模和事件预测的研究，并提供了广泛的人口统计和诊断覆盖。数据和代码已公开在公共存储库中。</div>
</details>
</div>
<div class="card">
<div class="title">CURVE: A Benchmark for Cultural and Multilingual Long Video Reasoning</div>
<div class="meta-line">Authors: Darshan Singh, Arsha Nagrani, Kawshik Manikantan, Harman Singh, Dinesh Tewari, Tobias Weyand, Cordelia Schmid, Anelia Angelova, Shachi Dave</div>
<div class="meta-line">First: 2026-01-15T18:15:06+00:00 · Latest: 2026-01-15T18:15:06+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.10649v1">Abs</a> · <a href="https://arxiv.org/pdf/2601.10649v1">PDF</a> · <a href="https://github.com/google-deepmind/neptune?tab=readme-ov-file\#minerva-cultural">Code1</a> · <a href="https://github.com/google-deepmind/neptune?tab=readme-ov-file">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advancements in video models have shown tremendous progress, particularly in long video understanding. However, current benchmarks predominantly feature western-centric data and English as the dominant language, introducing significant biases in evaluation. To address this, we introduce CURVE (Cultural Understanding and Reasoning in Video Evaluation), a challenging benchmark for multicultural and multilingual video reasoning. CURVE comprises high-quality, entirely human-generated annotations from diverse, region-specific cultural videos across 18 global locales. Unlike prior work that relies on automatic translations, CURVE provides complex questions, answers, and multi-step reasoning steps, all crafted in native languages. Making progress on CURVE requires a deeply situated understanding of visual cultural context. Furthermore, we leverage CURVE&#x27;s reasoning traces to construct evidence-based graphs and propose a novel iterative strategy using these graphs to identify fine-grained errors in reasoning. Our evaluations reveal that SoTA Video-LLMs struggle significantly, performing substantially below human-level accuracy, with errors primarily stemming from the visual perception of cultural elements. CURVE will be publicly available under https://github.com/google-deepmind/neptune?tab=readme-ov-file\#minerva-cultural</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CURVE：文化与多语言长视频推理基准</div>
<div class="mono" style="margin-top:8px">近期视频模型的进展在长视频理解方面取得了显著成果。然而，当前的基准数据主要以西方为中心，英语为主要语言，导致评估中存在显著偏差。为了解决这一问题，我们引入了CURVE（视频评估中的文化理解与推理），这是一个针对多文化、多语言视频推理的具有挑战性的基准。CURVE包含来自18个全球地区的高质量、完全由人类生成的注释，涵盖多样化的文化视频。与以往依赖自动翻译的工作不同，CURVE提供复杂的问题、答案和多步骤推理过程，全部以原生语言编写。在CURVE上取得进展需要对视觉文化背景有深入的理解。此外，我们利用CURVE的推理轨迹构建基于证据的图，并提出了一种使用这些图识别推理中细粒度错误的新颖迭代策略。我们的评估结果显示，当前最先进的视频语言模型在CURVE上表现不佳，其准确率远低于人类水平，错误主要源于对文化元素的视觉感知不足。CURVE将在https://github.com/google-deepmind/neptune?tab=readme-ov-file\#minerva-cultural公开。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advancements in video models have shown tremendous progress, particularly in long video understanding.</div>
</details>
</div>
<div class="card">
<div class="title">Sparse Nonparametric Contextual Bandits</div>
<div class="meta-line">Authors: Hamish Flynn, Julia Olkhovskaya, Paul Rognon-Vael</div>
<div class="meta-line">First: 2025-03-20T17:44:56+00:00 · Latest: 2026-01-15T18:10:26+00:00</div>
<div class="meta-line">Comments: 44 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.16382v2">Abs</a> · <a href="https://arxiv.org/pdf/2503.16382v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We study the benefits of sparsity in nonparametric contextual bandit problems, in which the set of candidate features is countably or uncountably infinite. Our contribution is two-fold. First, using a novel reduction to sequences of multi-armed bandit problems, we provide lower bounds on the minimax regret, which show that polynomial dependence on the number of actions is generally unavoidable in this setting. Second, we show that a variant of the Feel-Good Thompson Sampling algorithm enjoys regret bounds that match our lower bounds up to logarithmic factors of the horizon, and have logarithmic dependence on the effective number of candidate features. When we apply our results to kernelised and neural contextual bandits, we find that sparsity enables better regret bounds whenever the horizon is large enough relative to the sparsity and the number of actions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>稀疏非参数上下文老虎机</div>
<div class="mono" style="margin-top:8px">我们研究了非参数上下文老虎机问题中稀疏性的优势，其中候选特征集是可数或不可数无限的。我们的贡献有两个方面。首先，通过一种新颖的多臂老虎机问题序列的归约方法，我们提供了最小最大遗憾的下界，表明在这种情况下，行动数量的多项式依赖通常是不可避免的。其次，我们展示了Feel-Good Thompson Sampling算法的一个变体，在时间跨度上具有对数因子的遗憾上界，并且其遗憾上界在有效候选特征数量上具有对数依赖性。当我们将这些结果应用于核化和神经网络上下文老虎机时，我们发现当时间跨度足够大时，稀疏性能够带来更好的遗憾上界。</div>
</details>
</div>
<div class="card">
<div class="title">RMBRec: Robust Multi-Behavior Recommendation towards Target Behaviors</div>
<div class="meta-line">Authors: Miaomiao Cai, Zhijie Zhang, Junfeng Fang, Zhiyong Cheng, Xiang Wang, Meng Wang</div>
<div class="meta-line">First: 2026-01-13T16:34:17+00:00 · Latest: 2026-01-15T18:06:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.08705v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.08705v2">PDF</a> · <a href="https://github.com/miaomiao-cai2/RMBRec/">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-behavior recommendation faces a critical challenge in practice: auxiliary behaviors (e.g., clicks, carts) are often noisy, weakly correlated, or semantically misaligned with the target behavior (e.g., purchase), which leads to biased preference learning and suboptimal performance. While existing methods attempt to fuse these heterogeneous signals, they inherently lack a principled mechanism to ensure robustness against such behavioral inconsistency.
  In this work, we propose Robust Multi-Behavior Recommendation towards Target Behaviors (RMBRec), a robust multi-behavior recommendation framework grounded in an information-theoretic robustness principle. We interpret robustness as a joint process of maximizing predictive information while minimizing its variance across heterogeneous behavioral environments. Under this perspective, the Representation Robustness Module (RRM) enhances local semantic consistency by maximizing the mutual information between users&#x27; auxiliary and target representations, whereas the Optimization Robustness Module (ORM) enforces global stability by minimizing the variance of predictive risks across behaviors, which is an efficient approximation to invariant risk minimization. This local-global collaboration bridges representation purification and optimization invariance in a theoretically coherent way. Extensive experiments on three real-world datasets demonstrate that RMBRec not only outperforms state-of-the-art methods in accuracy but also maintains remarkable stability under various noise perturbations. For reproducibility, our code is available at https://github.com/miaomiao-cai2/RMBRec/.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>RMBRec：面向目标行为的鲁棒多行为推荐</div>
<div class="mono" style="margin-top:8px">多行为推荐在实践中面临关键挑战：辅助行为（如点击、加入购物车）往往具有噪声、弱相关性或语义不一致，这会导致偏好学习偏差和性能不佳。尽管现有方法尝试融合这些异构信号，但它们本质上缺乏一种原理性的机制来确保对这种行为不一致的鲁棒性。
在本工作中，我们提出了一种基于信息论鲁棒性原则的面向目标行为的鲁棒多行为推荐框架（RMBRec）。我们将鲁棒性解释为在异构行为环境中最大化预测信息同时最小化其方差的联合过程。在此视角下，表示鲁棒性模块（RRM）通过最大化用户辅助行为与目标行为表示之间的互信息来增强局部语义一致性，而优化鲁棒性模块（ORM）则通过最小化跨行为的预测风险方差来强制全局稳定性，这可以视为不变风险最小化的有效近似。这种局部-全局协作方式在理论上连贯地连接了表示净化与优化不变性。在三个真实世界数据集上的大量实验表明，RMBRec不仅在准确性上优于现有最先进的方法，而且在各种噪声扰动下保持了显著的稳定性。为了可复现性，我们的代码可在 https://github.com/miaomiao-cai2/RMBRec/ 获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-behavior recommendation faces a critical challenge in practice: auxiliary behaviors (e.g., clicks, carts) are often noisy, weakly correlated, or semantically misaligned with the target behavior (e.g., purchase), which leads to biased preference learning and suboptimal performance.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
