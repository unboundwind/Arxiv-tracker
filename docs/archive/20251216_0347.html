<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2025-12-16 03:47</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20251216_0347</div>
    <div class="row"><div class="card">
<div class="title">Moment-Based 3D Gaussian Splatting: Resolving Volumetric Occlusion with Order-Independent Transmittance</div>
<div class="meta-line">Authors: Jan U. Müller, Robin Tim Landsgesell, Leif Van Holland, Patrick Stotko, Reinhard Klein</div>
<div class="meta-line">First: 2025-12-12T18:59:55+00:00 · Latest: 2025-12-12T18:59:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11800v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11800v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The recent success of 3D Gaussian Splatting (3DGS) has reshaped novel view synthesis by enabling fast optimization and real-time rendering of high-quality radiance fields. However, it relies on simplified, order-dependent alpha blending and coarse approximations of the density integral within the rasterizer, thereby limiting its ability to render complex, overlapping semi-transparent objects. In this paper, we extend rasterization-based rendering of 3D Gaussian representations with a novel method for high-fidelity transmittance computation, entirely avoiding the need for ray tracing or per-pixel sample sorting. Building on prior work in moment-based order-independent transparency, our key idea is to characterize the density distribution along each camera ray with a compact and continuous representation based on statistical moments. To this end, we analytically derive and compute a set of per-pixel moments from all contributing 3D Gaussians. From these moments, a continuous transmittance function is reconstructed for each ray, which is then independently sampled within each Gaussian. As a result, our method bridges the gap between rasterization and physical accuracy by modeling light attenuation in complex translucent media, significantly improving overall reconstruction and rendering quality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于矩的三维高斯点渲染：通过顺序无关的透光率解决体积遮挡</div>
<div class="mono" style="margin-top:8px">近年来，三维高斯点渲染（3DGS）的成功改变了新视角合成的方法，使得高质量辐射场的快速优化和实时渲染成为可能。然而，它依赖于简化的、顺序相关的alpha混合以及光栅化器中密度积分的粗略近似，从而限制了其对复杂、重叠半透明物体的渲染能力。本文提出了一种新的方法，用于高保真度透光率计算，完全避免了需要光线追踪或逐像素采样排序的需求。基于之前关于基于矩的顺序无关透明度的工作，我们的核心思想是通过统计矩建立紧凑且连续的表示，来描述每个相机光线路径上的密度分布。为此，我们从所有贡献的三维高斯点中分析并计算出一组逐像素的矩。基于这些矩，我们为每条光线重建了一个连续的透光率函数，然后在每个高斯点内独立采样。因此，我们的方法通过建模复杂半透明介质中的光衰减，弥合了光栅化与物理准确性的差距，显著提升了整体的重建和渲染质量。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper addresses the limitations of 3D Gaussian Splatting in rendering complex, overlapping semi-transparent objects due to its reliance on simplified, order-dependent alpha blending and coarse density approximations. The authors introduce a moment-based approach to compute high-fidelity transmittance by characterizing the density distribution along each camera ray using statistical moments. They analytically derive per-pixel moments from all contributing Gaussians and reconstruct a continuous transmittance function for each ray, which is then sampled independently within each Gaussian. This method improves rendering quality by modeling light attenuation in translucent media more accurately without requiring ray tracing or per-pixel sorting.</div>
<div class="mono" style="margin-top:8px">本研究的动机是通过解决3D高斯点云（3DGS）在处理复杂、重叠半透明物体时的局限性，来提升渲染质量。作者提出了一种利用统计矩表示沿摄像机光线的密度分布的新方法，以计算高保真度的透射率，从而避免了光线追踪和逐像素排序。实验结果表明，该方法能够有效模拟透明介质中的光衰减，显著提高了整体的重建和渲染质量。</div>
</details>
</div>
<div class="card">
<div class="title">V-RGBX: Video Editing with Accurate Controls over Intrinsic Properties</div>
<div class="meta-line">Authors: Ye Fang, Tong Wu, Valentin Deschaintre, Duygu Ceylan, Iliyan Georgiev, Chun-Hao Paul Huang, Yiwei Hu, Xuelin Chen, Tuanfeng Yang Wang</div>
<div class="meta-line">First: 2025-12-12T18:59:54+00:00 · Latest: 2025-12-12T18:59:54+00:00</div>
<div class="meta-line">Comments: Project Page: https://aleafy.github.io/vrgbx</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11799v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11799v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://aleafy.github.io/vrgbx">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large-scale video generation models have shown remarkable potential in modeling photorealistic appearance and lighting interactions in real-world scenes. However, a closed-loop framework that jointly understands intrinsic scene properties (e.g., albedo, normal, material, and irradiance), leverages them for video synthesis, and supports editable intrinsic representations remains unexplored. We present V-RGBX, the first end-to-end framework for intrinsic-aware video editing. V-RGBX unifies three key capabilities: (1) video inverse rendering into intrinsic channels, (2) photorealistic video synthesis from these intrinsic representations, and (3) keyframe-based video editing conditioned on intrinsic channels. At the core of V-RGBX is an interleaved conditioning mechanism that enables intuitive, physically grounded video editing through user-selected keyframes, supporting flexible manipulation of any intrinsic modality. Extensive qualitative and quantitative results show that V-RGBX produces temporally consistent, photorealistic videos while propagating keyframe edits across sequences in a physically plausible manner. We demonstrate its effectiveness in diverse applications, including object appearance editing and scene-level relighting, surpassing the performance of prior methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>V-RGBX：基于内在属性精确控制的视频编辑</div>
<div class="mono" style="margin-top:8px">大规模视频生成模型在建模现实场景中的逼真外观和光照交互方面展现出显著潜力。然而，一个能够联合理解内在场景属性（如反照率、法线、材质和辐照度），利用这些属性进行视频合成，并支持可编辑内在表示的闭环框架尚未被探索。我们提出了V-RGBX，这是首个面向内在感知视频编辑的端到端框架。V-RGBX统一了三项关键能力：（1）视频逆渲染到内在通道，（2）从这些内在表示生成逼真视频，（3）基于关键帧的视频编辑，其条件基于内在通道。V-RGBX的核心是一个交错的条件机制，通过用户选择的关键帧实现直观且物理合理的视频编辑，支持对任何内在模态的灵活操控。大量定性和定量实验结果表明，V-RGBX能够生成时间一致且逼真的视频，同时以物理合理的方式在序列中传播关键帧编辑。我们在多种应用中展示了其有效性，包括物体外观编辑和场景级重光照，其性能超越了以往的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind V-RGBX is to address the lack of a closed-loop framework for video editing that can accurately control intrinsic scene properties such as albedo, normal, material, and irradiance. The method introduces an end-to-end framework that integrates video inverse rendering, photorealistic video synthesis, and keyframe-based editing using intrinsic representations. Experimental results demonstrate that V-RGBX generates temporally consistent and photorealistic videos, effectively propagating edits across sequences in a physically plausible manner, and outperforms previous methods in applications like object appearance editing and scene relighting.</div>
<div class="mono" style="margin-top:8px">V-RGBX的动机是解决缺乏对场景内在属性（如albedo、normal、material和irradiance）进行闭环控制的视频编辑框架的问题。该方法提出了一种端到端框架，结合了视频逆渲染、基于内在通道的逼真视频合成以及关键帧驱动的编辑功能。实验结果表明，V-RGBX能够生成时间一致且逼真的视频，在序列中有效传播编辑效果，并以物理合理的方式超越了现有方法在物体外观编辑和场景级重新照明任务中的表现。</div>
</details>
</div>
<div class="card">
<div class="title">Particulate: Feed-Forward 3D Object Articulation</div>
<div class="meta-line">Authors: Ruining Li, Yuxin Yao, Chuanxia Zheng, Christian Rupprecht, Joan Lasenby, Shangzhe Wu, Andrea Vedaldi</div>
<div class="meta-line">First: 2025-12-12T18:59:51+00:00 · Latest: 2025-12-12T18:59:51+00:00</div>
<div class="meta-line">Comments: Project page: https://ruiningli.com/particulate</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11798v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11798v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present Particulate, a feed-forward approach that, given a single static 3D mesh of an everyday object, directly infers all attributes of the underlying articulated structure, including its 3D parts, kinematic structure, and motion constraints. At its core is a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh using a flexible and scalable architecture to predict all the aforementioned attributes with native multi-joint support. We train the network end-to-end on a diverse collection of articulated 3D assets from public datasets. During inference, Particulate lifts the network&#x27;s feed-forward prediction to the input mesh, yielding a fully articulated 3D model in seconds, much faster than prior approaches that require per-object optimization. Particulate can also accurately infer the articulated structure of AI-generated 3D assets, enabling full-fledged extraction of articulated 3D objects from a single (real or synthetic) image when combined with an off-the-shelf image-to-3D generator. We further introduce a new challenging benchmark for 3D articulation estimation curated from high-quality public 3D assets, and redesign the evaluation protocol to be more consistent with human preferences. Quantitative and qualitative results show that Particulate significantly outperforms state-of-the-art approaches.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Particulate: 前馈式3D物体运动结构</div>
<div class="mono" style="margin-top:8px">我们提出了Particulate，这是一种前馈方法，给定一个日常物体的单个静态3D网格，可以直接推断出其底层运动结构的所有属性，包括3D部件、运动结构和运动约束。其核心是一个变压器网络，即Part Articulation Transformer，它通过灵活且可扩展的架构处理输入网格的点云，以原生多关节支持的方式预测上述所有属性。我们在来自公开数据集的多样化运动3D资产上对网络进行端到端训练。在推理过程中，Particulate将网络的前馈预测提升到输入网格，从而在几秒钟内生成一个完整的运动3D模型，比需要逐个物体优化的先前方法快得多。此外，Particulate还能准确推断AI生成3D资产的运动结构，当与现成的图像到3D生成器结合时，可以实现从单张（真实或合成）图像中完整提取运动3D物体。我们进一步引入了一个新的具有挑战性的3D运动结构估计基准，该基准从高质量的公开3D资产中整理而来，并重新设计了评估协议，使其更符合人类偏好。定量和定性结果表明，Particulate显著优于当前最先进的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation behind Particulate is to enable efficient and accurate inference of articulated structures from single static 3D meshes. The method employs a transformer network, Part Articulation Transformer, which processes a point cloud of the input mesh to predict 3D parts, kinematic structure, and motion constraints. Experimental results demonstrate that Particulate achieves faster inference compared to prior methods that require per-object optimization, and it can accurately infer articulated structures from both real and AI-generated 3D assets. When combined with an image-to-3D generator, it enables the extraction of articulated 3D models from single images. The approach also introduces a new benchmark for 3D articulation estimation and improves evaluation consistency with human preferences, showing significant performance improvements over state-of-the-art methods.</div>
<div class="mono" style="margin-top:8px">Particulate 的研究动机是实现从单个静态 3D 网格中高效准确地推断出可动结构。该方法采用名为 Part Articulation Transformer 的变压器网络，通过处理输入网格的点云来预测 3D 部件、运动结构和约束条件。实验结果表明，与需要逐对象优化的先前方法相比，Particulate 的推理速度更快，并且在真实和 AI 生成的 3D 资产上表现良好，在定量和定性评估中均显著优于现有最先进的方法。</div>
</details>
</div>
<div class="card">
<div class="title">AnchorDream: Repurposing Video Diffusion for Embodiment-Aware Robot Data Synthesis</div>
<div class="meta-line">Authors: Junjie Ye, Rong Xue, Basile Van Hoorick, Pavel Tokmakov, Muhammad Zubair Irshad, Yue Wang, Vitor Guizilini</div>
<div class="meta-line">First: 2025-12-12T18:59:45+00:00 · Latest: 2025-12-12T18:59:45+00:00</div>
<div class="meta-line">Comments: Project page: https://jay-ye.github.io/AnchorDream/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11797v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11797v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://jay-ye.github.io/AnchorDream/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The collection of large-scale and diverse robot demonstrations remains a major bottleneck for imitation learning, as real-world data acquisition is costly and simulators offer limited diversity and fidelity with pronounced sim-to-real gaps. While generative models present an attractive solution, existing methods often alter only visual appearances without creating new behaviors, or suffer from embodiment inconsistencies that yield implausible motions. To address these limitations, we introduce AnchorDream, an embodiment-aware world model that repurposes pretrained video diffusion models for robot data synthesis. AnchorDream conditions the diffusion process on robot motion renderings, anchoring the embodiment to prevent hallucination while synthesizing objects and environments consistent with the robot&#x27;s kinematics. Starting from only a handful of human teleoperation demonstrations, our method scales them into large, diverse, high-quality datasets without requiring explicit environment modeling. Experiments show that the generated data leads to consistent improvements in downstream policy learning, with relative gains of 36.4% in simulator benchmarks and nearly double performance in real-world studies. These results suggest that grounding generative world models in robot motion provides a practical path toward scaling imitation learning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AnchorDream: 重新利用视频扩散模型进行具身感知的机器人数据合成</div>
<div class="mono" style="margin-top:8px">大规模且多样化的机器人演示数据收集仍然是模仿学习的主要瓶颈，因为真实世界数据采集成本高昂，而模拟器在多样性和保真度方面存在局限，并且存在显著的模拟到现实的差距。尽管生成模型提供了有吸引力的解决方案，但现有方法往往仅改变视觉外观而无法生成新的行为，或因具身不一致导致不合理的运动。为了解决这些限制，我们引入了AnchorDream，这是一种具身感知的世界模型，重新利用预训练的视频扩散模型进行机器人数据合成。AnchorDream通过机器人运动渲染条件化扩散过程，将具身锚定以防止幻觉，同时合成与机器人运动学一致的对象和环境。我们的方法仅需少量的人类远程操作演示即可将其扩展为大规模、多样且高质量的数据集，而无需显式的环境建模。实验表明，生成的数据在下游策略学习中带来了持续的性能提升，在模拟器基准测试中相对提升达36.4%，在现实世界研究中性能几乎翻倍。这些结果表明，将生成世界模型锚定在机器人运动上，为扩展模仿学习提供了一条实用路径。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research is to overcome the challenges of collecting large-scale and diverse robot demonstrations, which is hindered by the high cost of real-world data and the limitations of simulators. AnchorDream, an embodiment-aware world model, leverages pretrained video diffusion models to synthesize robot data by conditioning the diffusion process on robot motion renderings, ensuring consistency with the robot&#x27;s kinematics. The method scales a small number of human teleoperation demonstrations into large, diverse, and high-quality datasets without explicit environment modeling. Experimental results demonstrate that the synthesized data significantly improves downstream policy learning, achieving a 36.4% relative gain in simulator benchmarks and nearly double performance in real-world studies.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大规模和多样化机器人演示数据收集的难题，因为实际数据采集成本高，而模拟器在多样性和真实性方面存在局限。AnchorDream是一种基于机器人运动的具身意识世界模型，通过重新利用预训练的视频扩散模型来合成机器人数据。该方法通过将扩散过程条件化于机器人运动渲染，确保合成的物体和环境与机器人的运动学一致，避免具身不一致的问题。实验结果表明，生成的数据显著提升了下游策略学习的效果，在模拟器基准测试中相对提升36.4%，在实际应用中性能几乎翻倍。</div>
</details>
</div>
<div class="card">
<div class="title">A General Algorithm for Detecting Higher-Order Interactions via Random Sequential Additions</div>
<div class="meta-line">Authors: Ahmad Shamail, Claire McWhite</div>
<div class="meta-line">First: 2025-12-12T18:57:29+00:00 · Latest: 2025-12-12T18:57:29+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11793v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11793v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Many systems exhibit complex interactions between their components: some features or actions amplify each other&#x27;s effects, others provide redundant information, and some contribute independently. We present a simple geometric method for discovering interactions and redundancies: when elements are added in random sequential orders and their contributions plotted over many trials, characteristic L-shaped patterns emerge that directly reflect interaction structure. The approach quantifies how the contribution of each element depends on those added before it, revealing patterns that distinguish interaction, independence, and redundancy on a unified scale. When pairwise contributions are visualized as two--dimensional point clouds, redundant pairs form L--shaped patterns where only the first-added element contributes, while synergistic pairs form L--shaped patterns where only elements contribute together. Independent elements show order--invariant distributions. We formalize this with the L--score, a continuous measure ranging from $-1$ (perfect synergy, e.g. $Y=X_1X_2$) to $0$ (independence) to $+1$ (perfect redundancy, $X_1 \approx X_2$). The relative scaling of the L--shaped arms reveals feature dominance in which element consistently provides more information. Although computed only from pairwise measurements, higher--order interactions among three or more elements emerge naturally through consistent cross--pair relationships (e.g. AB, AC, BC). The method is metric--agnostic and broadly applicable to any domain where performance can be evaluated incrementally over non-repeating element sequences, providing a unified geometric approach to uncovering interaction structure.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过随机顺序添加检测高阶相互作用的一般算法</div>
<div class="mono" style="margin-top:8px">许多系统表现出其组件之间的复杂相互作用：某些特征或行为会放大彼此的效果，某些提供冗余信息，而另一些则独立贡献。我们提出了一种简单的几何方法来发现相互作用和冗余：当元素以随机顺序逐个添加，并在多次试验中绘制其贡献时，会呈现出特征性的L形模式，直接反映相互作用的结构。该方法量化了每个元素的贡献如何依赖于之前添加的元素，揭示了在统一尺度上区分相互作用、独立性和冗余性的模式。当将成对贡献可视化为二维点云时，冗余对会形成只有第一个添加元素有贡献的L形模式，而协同对则形成只有元素共同贡献的L形模式。独立元素表现出顺序不变的分布。我们通过L分数这一连续度量来形式化这一方法，其范围从-1（完美协同，例如Y=X1X2）到0（独立性）再到+1（完美冗余，X1≈X2）。L形臂的相对比例揭示了特征主导性，即某个元素在序列中持续提供更多信息。尽管该方法仅基于成对测量计算，但三个或更多元素之间的高阶相互作用会通过一致的跨对关系自然显现（例如AB、AC、BC）。该方法与度量无关，广泛适用于任何可以通过非重复元素序列逐步评估性能的领域，提供了一种统一的几何方法来揭示相互作用结构。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces a geometric method for detecting higher-order interactions in systems by analyzing the incremental contributions of elements added in random sequential orders. The approach visualizes contributions as L-shaped patterns in two-dimensional point clouds, which indicate interaction, independence, or redundancy among components. The L-score, a continuous metric ranging from -1 to +1, quantifies these patterns, with values reflecting synergy, independence, or redundancy. Experimental results show that the method can identify higher-order interactions from pairwise measurements by examining consistent cross-pair relationships, and it is applicable across domains where performance can be assessed incrementally.</div>
<div class="mono" style="margin-top:8px">该论文旨在解决复杂系统中检测高阶相互作用的挑战，其中组件可能以协同、冗余或独立的方式相互影响。它提出了一种基于随机顺序添加元素并绘制其贡献的几何方法，通过多次试验揭示L形模式，直接反映相互作用结构。L分数是一个连续指标，范围从-1（完全协同，如Y=X1X2）到+1（完全冗余，X1≈X2），用于量化这些模式，区分协同、独立和冗余。实验结果表明，冗余对形成仅第一个元素贡献的L形，协同对形成共同贡献的L形，独立元素则显示顺序不变的分布。即使仅通过成对测量计算，高阶相互作用（涉及三个或更多元素）也能通过一致的跨对关系自然显现。</div>
</details>
</div>
<div class="card">
<div class="title">Probing forced responses and causality in data-driven climate emulators: conceptual limitations and the role of reduced-order models</div>
<div class="meta-line">Authors: Fabrizio Falasca</div>
<div class="meta-line">First: 2025-06-27T18:04:36+00:00 · Latest: 2025-12-12T18:57:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.22552v6">Abs</a> · <a href="https://arxiv.org/pdf/2506.22552v6">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A central challenge in climate science and applied mathematics is developing data-driven models of multiscale systems that capture both stationary statistics and responses to external perturbations. Current neural climate emulators aim to resolve the atmosphere-ocean system in all its complexity but often struggle to reproduce forced responses, limiting their use in causal studies such as Green&#x27;s function experiments. To explore the origin of these limitations, we first examine a simplified dynamical system that retains key features of climate variability. We interpret the results through linear response theory, providing a rigorous framework to evaluate neural models beyond stationary statistics and to probe causal mechanisms. We argue that the ability of emulators of multiscale systems to reproduce perturbed statistics depends critically on (i) the choice of an appropriate coarse-grained representation and (ii) careful parameterizations of unresolved processes. These insights highlight reduced-order models, tailored to specific goals, processes, and scales, as valuable alternatives to general-purpose emulators. We next consider a real-world application by developing a neural model to investigate the joint variability of the surface temperature field and radiative fluxes. The model infers a multiplicative noise process directly from data, largely reproduces the system&#x27;s probability distribution, and enables causal studies through forced responses. We discuss its limitations and outline directions for future work. Overall, these results expose key challenges in data-driven modeling of multiscale physical systems and underscore the value of coarse-grained, stochastic approaches, with response theory providing a principled framework to guide model design and enhance causal understanding.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>探究数据驱动气候模拟器中的强迫响应与因果性：概念局限性及降阶模型的作用</div>
<div class="mono" style="margin-top:8px">气候科学和应用数学中的一个核心挑战是开发能够捕捉稳定统计特征和对外部扰动响应的数据驱动模型，用于多尺度系统。当前的神经气候模拟器旨在解决大气-海洋系统的全部复杂性，但往往难以再现强迫响应，从而限制了其在因果研究（如格林函数实验）中的应用。为探讨这些局限性的根源，我们首先研究了一个简化动力系统，该系统保留了气候变率的关键特征。我们通过线性响应理论来解释结果，提供了一个严谨的框架，用于评估神经模型超越稳定统计特性，并探究因果机制。我们认为，多尺度系统模拟器再现扰动统计的能力在很大程度上取决于（i）选择合适的粗粒化表示方法，以及（ii）对未解析过程的细致参数化。这些见解突显了针对特定目标、过程和尺度设计的降阶模型作为通用模拟器的有价值替代方案。随后，我们考虑一个实际应用，通过构建神经模型来研究地表温度场和辐射通量的联合变率。该模型直接从数据中推断乘法噪声过程，基本再现了系统的概率分布，并通过强迫响应实现了因果研究。我们讨论了其局限性，并概述了未来工作的方向。总体而言，这些结果揭示了数据驱动多尺度物理系统建模中的关键挑战，并强调了粗粒化、随机方法的价值，响应理论为模型设计提供了原则性的框架，有助于增强因果理解。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of developing data-driven climate models that can accurately represent both stationary statistics and responses to external perturbations. It evaluates the limitations of current neural climate emulators in capturing forced responses, which hinders their use in causal analysis. By analyzing a simplified dynamical system and applying linear response theory, the study identifies that the success of multiscale emulators in reproducing perturbed statistics hinges on the selection of an appropriate coarse-grained representation and the accurate parameterization of unresolved processes. A real-world neural model is then developed to study the joint variability of surface temperature and radiative fluxes, which infers a multiplicative noise process from data and largely reproduces the system&#x27;s probability distribution, enabling causal investigations through forced responses.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决当前数据驱动气候模拟器在捕捉强迫响应和因果关系方面的局限性，这对于理解气候动力学至关重要。作者采用简化动力系统来探讨这些问题，并应用线性响应理论来评估神经模型，超越静态统计特性以探究因果机制。主要发现表明，模拟器在再现受扰统计特性方面的准确性取决于合适的粗粒化表示和对未解析过程的精确参数化。他们提出针对特定气候研究目标、过程和尺度的降阶模型作为更有效的替代方案，并展示了一个从数据中推断乘法噪声的神经模型，能够再现地表温度和辐射通量的概率分布，同时通过强迫响应实现因果分析。</div>
</details>
</div>
<div class="card">
<div class="title">Structure From Tracking: Distilling Structure-Preserving Motion for Video Generation</div>
<div class="meta-line">Authors: Yang Fei, George Stoica, Jingyuan Liu, Qifeng Chen, Ranjay Krishna, Xiaojuan Wang, Benlin Liu</div>
<div class="meta-line">First: 2025-12-12T18:56:35+00:00 · Latest: 2025-12-12T18:56:35+00:00</div>
<div class="meta-line">Comments: Project Website: https://sam2videox.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11792v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11792v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sam2videox.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reality is a dance between rigid constraints and deformable structures. For video models, that means generating motion that preserves fidelity as well as structure. Despite progress in diffusion models, producing realistic structure-preserving motion remains challenging, especially for articulated and deformable objects such as humans and animals. Scaling training data alone, so far, has failed to resolve physically implausible transitions. Existing approaches rely on conditioning with noisy motion representations, such as optical flow or skeletons extracted using an external imperfect model. To address these challenges, we introduce an algorithm to distill structure-preserving motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX). With our method, we train SAM2VideoX, which contains two innovations: (1) a bidirectional feature fusion module that extracts global structure-preserving motion priors from a recurrent model like SAM2; (2) a Local Gram Flow loss that aligns how local features move together. Experiments on VBench and in human studies show that SAM2VideoX delivers consistent gains (+2.60\% on VBench, 21-22\% lower FVD, and 71.4\% human preference) over prior baselines. Specifically, on VBench, we achieve 95.51\%, surpassing REPA (92.91\%) by 2.60\%, and reduce FVD to 360.57, a 21.20\% and 22.46\% improvement over REPA- and LoRA-finetuning, respectively. The project website can be found at https://sam2videox.github.io/ .</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从跟踪中构建结构：从自回归视频跟踪模型中提炼结构保持运动用于视频生成</div>
<div class="mono" style="margin-top:8px">现实是刚性约束与可变形结构之间的舞蹈。对于视频模型来说，这意味着生成既保持保真度又保持结构的运动。尽管扩散模型取得了进展，但生成逼真的结构保持运动仍然具有挑战性，尤其是在处理具有关节和可变形结构的对象（如人类和动物）时。迄今为止，仅靠扩展训练数据未能解决物理上不合理的运动过渡问题。现有方法依赖于使用嘈杂的运动表示进行条件建模，例如通过外部不完美的模型提取的光流或骨骼。为了解决这些挑战，我们提出了一种算法，将结构保持运动先验从自回归视频跟踪模型（SAM2）提炼并注入到双向视频扩散模型（CogVideoX）中。通过我们的方法，我们训练了SAM2VideoX，其中包含两项创新：(1) 双向特征融合模块，从类似SAM2的循环模型中提取全局结构保持运动先验；(2) 局部Gram流损失，对齐局部特征的运动方式。在VBench和人类研究实验中，SAM2VideoX在先前基线基础上实现了显著提升（在VBench上提升+2.60%，FVD降低21-22%，人类偏好达到71.4%）。具体而言，在VBench上，我们达到了95.51%，超过了REPA（92.91%）2.60%，并将FVD降低至360.57，分别比REPA和LoRA微调提升了21.20%和22.46%。项目网站可访问 https://sam2videox.github.io/ 。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to generate realistic and structure-preserving motion in videos, particularly for articulated and deformable objects. The authors propose a method that distills motion priors from an autoregressive video tracking model (SAM2) into a bidirectional video diffusion model (CogVideoX), introducing two key innovations: a bidirectional feature fusion module and a Local Gram Flow loss. Experimental results on VBench and human studies demonstrate significant improvements, with SAM2VideoX achieving 95.51% on VBench, reducing FVD to 360.57, and showing 71.4% human preference compared to existing methods.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决视频生成中保持结构真实性的挑战，尤其是对复杂物体如人类和动物的运动生成。作者提出了一种方法，将自回归视频跟踪模型（SAM2）中的运动先验知识蒸馏到双向视频扩散模型（CogVideoX）中，引入了双向特征融合模块和局部Gram流损失两个创新点。实验结果表明，SAM2VideoX在VBench上达到95.51\%，FVD降低至360.57，并在人类偏好测试中获得71.4\%的偏好率，显著优于现有方法。</div>
</details>
</div>
<div class="card">
<div class="title">Uncertainty-Aware Domain Adaptation for Vitiligo Segmentation in Clinical Photographs</div>
<div class="meta-line">Authors: Wentao Jiang, Vamsi Varra, Caitlin Perez-Stable, Harrison Zhu, Meredith Apicella, Nicole Nyamongo</div>
<div class="meta-line">First: 2025-12-12T18:56:21+00:00 · Latest: 2025-12-12T18:56:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11791v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11791v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurately quantifying vitiligo extent in routine clinical photographs is crucial for longitudinal monitoring of treatment response. We propose a trustworthy, frequency-aware segmentation framework built on three synergistic pillars: (1) a data-efficient training strategy combining domain-adaptive pre-training on the ISIC 2019 dataset with an ROI-constrained dual-task loss to suppress background noise; (2) an architectural refinement via a ConvNeXt V2-based encoder enhanced with a novel High-Frequency Spectral Gating (HFSG) module and stem-skip connections to capture subtle textures; and (3) a clinical trust mechanism employing K-fold ensemble and Test-Time Augmentation (TTA) to generate pixel-wise uncertainty maps. Extensive validation on an expert-annotated clinical cohort demonstrates superior performance, achieving a Dice score of 85.05% and significantly reducing boundary error (95% Hausdorff Distance improved from 44.79 px to 29.95 px), consistently outperforming strong CNN (ResNet-50 and UNet++) and Transformer (MiT-B5) baselines. Notably, our framework demonstrates high reliability with zero catastrophic failures and provides interpretable entropy maps to identify ambiguous regions for clinician review. Our approach suggests that the proposed framework establishes a robust and reliable standard for automated vitiligo assessment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向临床照片的白癜风分割的不确定性感知领域自适应方法</div>
<div class="mono" style="margin-top:8px">在常规临床照片中准确量化白癜风范围对于治疗反应的纵向监测至关重要。我们提出了一种可信、频率感知的分割框架，基于三个协同支柱：(1) 一种数据高效的训练策略，结合在ISIC 2019数据集上的领域自适应预训练与ROI约束的双任务损失以抑制背景噪声；(2) 通过基于ConvNeXt V2的编码器和一种新颖的高频频谱门控（HFSG）模块以及stem-skip连接进行架构优化，以捕捉细微纹理；(3) 一种临床信任机制，采用K折集成和测试时增强（TTA）生成像素级不确定性图。在专家标注的临床队列上的广泛验证表明，该框架性能优越，达到了85.05%的Dice分数，并显著降低了边界误差（95% Hausdorff距离从44.79像素改善至29.95像素），持续优于强大的CNN（ResNet-50和UNet++）和Transformer（MiT-B5）基线。值得注意的是，我们的框架表现出高可靠性，无灾难性失败，并提供可解释的熵图以识别临床医生需要复核的模糊区域。我们的方法表明，所提出的框架为自动化白癜风评估建立了一个稳健且可靠的标准。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research is to improve the accuracy of vitiligo segmentation in clinical photographs for better treatment monitoring. The proposed framework integrates three key components: a domain-adaptive training strategy using ISIC 2019 data and a dual-task loss function, an encoder with ConvNeXt V2 architecture and a High-Frequency Spectral Gating module, and a clinical trust mechanism based on K-fold ensemble and Test-Time Augmentation. Experimental results on an expert-annotated dataset show that the framework achieves a Dice score of 85.05% and reduces boundary error by 95% Hausdorff Distance, from 44.79 to 29.95 pixels, outperforming existing CNN and Transformer models. It also provides uncertainty maps with no catastrophic failures, enhancing reliability and interpretability for clinical use.</div>
<div class="mono" style="margin-top:8px">本研究的动机是提高临床照片中白癜风分割的准确性和可靠性，以更好地监测治疗效果。所提出的框架结合了高效的数据训练策略、增强的神经网络架构（包含高频率频谱门控模块）以及基于K折集成和测试时增强的临床信任机制。在专家标注的数据集上进行的实验表明，该框架实现了85.05%的Dice分数，并显著降低了边界误差，优于CNN和Transformer基线模型。此外，它提供了可解释的不确定性图，且无灾难性失败，增强了临床信任和可解释性。</div>
</details>
</div>
<div class="card">
<div class="title">Softmax as Linear Attention in the Large-Prompt Regime: a Measure-based Perspective</div>
<div class="meta-line">Authors: Etienne Boursier, Claire Boyer</div>
<div class="meta-line">First: 2025-12-12T18:54:52+00:00 · Latest: 2025-12-12T18:54:52+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11784v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11784v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Softmax attention is a central component of transformer architectures, yet its nonlinear structure poses significant challenges for theoretical analysis. We develop a unified, measure-based framework for studying single-layer softmax attention under both finite and infinite prompts. For i.i.d. Gaussian inputs, we lean on the fact that the softmax operator converges in the infinite-prompt limit to a linear operator acting on the underlying input-token measure. Building on this insight, we establish non-asymptotic concentration bounds for the output and gradient of softmax attention, quantifying how rapidly the finite-prompt model approaches its infinite-prompt counterpart, and prove that this concentration remains stable along the entire training trajectory in general in-context learning settings with sub-Gaussian tokens. In the case of in-context linear regression, we use the tractable infinite-prompt dynamics to analyze training at finite prompt length. Our results allow optimization analyses developed for linear attention to transfer directly to softmax attention when prompts are sufficiently long, showing that large-prompt softmax attention inherits the analytical structure of its linear counterpart. This, in turn, provides a principled and broadly applicable toolkit for studying the training dynamics and statistical behavior of softmax attention layers in large prompt regimes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Softmax作为大提示规模下的线性注意力：一种测度视角</div>
<div class="mono" style="margin-top:8px">Softmax注意力是Transformer架构的核心组件，但其非线性结构给理论分析带来了显著挑战。我们开发了一个统一的、基于测度的框架，用于研究有限和无限提示规模下的单层Softmax注意力。对于独立同分布的高斯输入，我们利用Softmax算子在无限提示极限下收敛为作用于输入标记测度的线性算子这一特性。基于这一见解，我们建立了Softmax注意力输出和梯度的非渐近集中界，量化了有限提示模型接近其无限提示对应模型的速度，并证明在一般的情景学习设置中，这种集中性在整个训练过程中保持稳定。在上下文线性回归的情形下，我们利用可处理的无限提示动态来分析有限提示长度下的训练过程。我们的结果表明，当提示足够长时，针对线性注意力开发的优化分析可以直接转移到Softmax注意力，展示了大提示规模下的Softmax注意力继承了其线性对应物的分析结构。这反过来为研究大提示规模下Softmax注意力层的训练动态和统计行为提供了一个原理性且广泛适用的工具箱。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to better understand the behavior of softmax attention in transformer models, particularly in the large-prompt regime, where its nonlinear nature complicates theoretical analysis. The authors introduce a measure-based framework to study single-layer softmax attention, showing that it converges to a linear operator when dealing with i.i.d. Gaussian inputs in the infinite-prompt limit. They derive non-asymptotic concentration bounds for the output and gradient of softmax attention, demonstrating how quickly finite-prompt models approach their infinite-prompt counterparts and proving that this concentration remains stable throughout training. In the context of in-context linear regression, they leverage these insights to analyze finite-prompt training dynamics, revealing that large-prompt softmax attention inherits the analytical properties of linear attention, thus enabling the application of linear attention optimization techniques to softmax attention.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决变压器模型中softmax注意力非线性结构带来的理论分析困难。作者提出了一种基于测度的框架，用于研究单层softmax注意力在有限和无限提示场景下的行为，利用softmax在无限提示极限下收敛为线性算子的特性，针对独立同分布高斯输入进行分析。他们推导了softmax注意力输出和梯度的非渐近集中界，展示了有限提示模型如何快速接近其无限提示版本，并证明了这种集中性在训练过程中保持稳定。在上下文线性回归的场景下，他们利用这些洞察分析有限提示长度的训练，表明当提示足够长时，softmax注意力可以像线性注意力一样进行优化分析。</div>
</details>
</div>
<div class="card">
<div class="title">Super Suffixes: Bypassing Text Generation Alignment and Guard Models Simultaneously</div>
<div class="meta-line">Authors: Andrew Adiletta, Kathryn Adiletta, Kemal Derya, Berk Sunar</div>
<div class="meta-line">First: 2025-12-12T18:52:09+00:00 · Latest: 2025-12-12T18:52:09+00:00</div>
<div class="meta-line">Comments: 13 pages, 5 Figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11783v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11783v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid deployment of Large Language Models (LLMs) has created an urgent need for enhanced security and privacy measures in Machine Learning (ML). LLMs are increasingly being used to process untrusted text inputs and even generate executable code, often while having access to sensitive system controls. To address these security concerns, several companies have introduced guard models, which are smaller, specialized models designed to protect text generation models from adversarial or malicious inputs. In this work, we advance the study of adversarial inputs by introducing Super Suffixes, suffixes capable of overriding multiple alignment objectives across various models with different tokenization schemes. We demonstrate their effectiveness, along with our joint optimization technique, by successfully bypassing the protection mechanisms of Llama Prompt Guard 2 on five different text generation models for malicious text and code generation. To the best of our knowledge, this is the first work to reveal that Llama Prompt Guard 2 can be compromised through joint optimization.
  Additionally, by analyzing the changing similarity of a model&#x27;s internal state to specific concept directions during token sequence processing, we propose an effective and lightweight method to detect Super Suffix attacks. We show that the cosine similarity between the residual stream and certain concept directions serves as a distinctive fingerprint of model intent. Our proposed countermeasure, DeltaGuard, significantly improves the detection of malicious prompts generated through Super Suffixes. It increases the non-benign classification rate to nearly 100%, making DeltaGuard a valuable addition to the guard model stack and enhancing robustness against adversarial prompt attacks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超级后缀：同时绕过文本生成对齐和防护模型</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）的快速部署使得在机器学习（ML）中增强安全性和隐私保护措施变得尤为迫切。LLMs越来越多地被用于处理不可信的文本输入，甚至生成可执行代码，通常同时拥有对敏感系统控制的访问权限。为了解决这些安全问题，多家公司引入了防护模型，这些模型较小且专门化，旨在保护文本生成模型免受对抗性或恶意输入的影响。在本工作中，我们通过引入超级后缀（Super Suffixes）推进对抗性输入的研究，这些后缀能够覆盖不同模型和不同分词方案下的多种对齐目标。我们通过成功绕过Llama Prompt Guard 2在五种不同的文本生成模型上的防护机制，展示了其有效性以及我们联合优化技术的效果。据我们所知，这是首次揭示Llama Prompt Guard 2可以通过联合优化被攻破。此外，通过分析模型在处理标记序列过程中内部状态与特定概念方向之间的相似性变化，我们提出了一种有效且轻量的检测超级后缀攻击的方法。我们表明，残差流与某些概念方向之间的余弦相似性可以作为模型意图的独特指纹。我们提出的对策DeltaGuard显著提高了对通过超级后缀生成的恶意提示的检测能力，将非良性分类率提升至近100%，使DeltaGuard成为防护模型堆栈中的宝贵补充，并增强了对对抗性提示攻击的鲁棒性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research is motivated by the growing security risks associated with the deployment of Large Language Models (LLMs), particularly in handling untrusted inputs and generating malicious content. The authors introduce Super Suffixes, adversarial suffixes that can override alignment objectives across different models with varying tokenization schemes. They demonstrate that these suffixes can bypass the protection mechanisms of Llama Prompt Guard 2 on five text generation models. Additionally, they propose DeltaGuard, a lightweight detection method based on analyzing cosine similarity between the residual stream and concept directions, which significantly improves the classification of malicious prompts.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于大型语言模型（LLMs）在处理不可信输入和生成代码时日益增长的安全风险。作者提出了Super Suffixes，这是一种能够覆盖不同模型和分词方案下多种对齐目标的对抗性后缀。他们展示了这些后缀可以绕过Llama Prompt Guard 2在五个文本生成模型上的保护机制。此外，他们提出了一种轻量级的检测方法DeltaGuard，通过分析残差流与概念方向之间的余弦相似度来识别恶意提示，显著提高了对通过Super Suffixes生成的恶意提示的分类准确率。</div>
</details>
</div>
<div class="card">
<div class="title">MatAnyone 2: Scaling Video Matting via a Learned Quality Evaluator</div>
<div class="meta-line">Authors: Peiqing Yang, Shangchen Zhou, Kai Hao, Qingyi Tao</div>
<div class="meta-line">First: 2025-12-12T18:51:49+00:00 · Latest: 2025-12-12T18:51:49+00:00</div>
<div class="meta-line">Comments: Project page: https://pq-yang.github.io/projects/MatAnyone2/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11782v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11782v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://pq-yang.github.io/projects/MatAnyone2/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video matting remains limited by the scale and realism of existing datasets. While leveraging segmentation data can enhance semantic stability, the lack of effective boundary supervision often leads to segmentation-like mattes lacking fine details. To this end, we introduce a learned Matting Quality Evaluator (MQE) that assesses semantic and boundary quality of alpha mattes without ground truth. It produces a pixel-wise evaluation map that identifies reliable and erroneous regions, enabling fine-grained quality assessment. The MQE scales up video matting in two ways: (1) as an online matting-quality feedback during training to suppress erroneous regions, providing comprehensive supervision, and (2) as an offline selection module for data curation, improving annotation quality by combining the strengths of leading video and image matting models. This process allows us to build a large-scale real-world video matting dataset, VMReal, containing 28K clips and 2.4M frames. To handle large appearance variations in long videos, we introduce a reference-frame training strategy that incorporates long-range frames beyond the local window for effective training. Our MatAnyone 2 achieves state-of-the-art performance on both synthetic and real-world benchmarks, surpassing prior methods across all metrics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MatAnyone 2：通过学习的质量评估器扩展视频抠图</div>
<div class="mono" style="margin-top:8px">视频抠图仍然受到现有数据集规模和真实感的限制。虽然利用分割数据可以增强语义稳定性，但缺乏有效的边界监督通常会导致分割式抠图缺少细节。为此，我们引入了一个学习得到的抠图质量评估器（MQE），可以在没有真实标签的情况下评估alpha抠图的语义和边界质量。它生成一个像素级的评估图，以识别可靠和错误的区域，从而实现细粒度的质量评估。MQE通过两种方式扩展视频抠图：(1) 在训练过程中作为在线抠图质量反馈，以抑制错误区域，提供全面的监督；(2) 作为离线数据筛选模块，通过结合领先的视频和图像抠图模型的优势，提高标注质量。这一过程使我们能够构建一个大规模的真实世界视频抠图数据集VMReal，包含28000个片段和240万个帧。为了处理长视频中的大外观变化，我们引入了一种参考帧训练策略，该策略结合了超出局部窗口的长程帧以实现有效的训练。我们的MatAnyone 2在合成和真实世界基准测试中均实现了最先进的性能，在所有指标上均超越了先前的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of current video matting datasets in terms of scale and realism, particularly the lack of effective boundary supervision leading to poor fine detail in mattes. The authors propose a learned Matting Quality Evaluator (MQE) that assesses the semantic and boundary quality of alpha mattes without ground truth, generating a pixel-wise evaluation map to identify reliable and erroneous regions. This MQE is used both during training to provide feedback and for data curation to improve annotation quality, resulting in the creation of the VMReal dataset with 28K clips and 2.4M frames. Additionally, a reference-frame training strategy is introduced to handle appearance variations in long videos. MatAnyone 2 achieves state-of-the-art performance on both synthetic and real-world benchmarks, outperforming previous methods across all metrics.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决现有视频抠图方法在处理大规模和真实数据集时的局限性。作者提出了一种学习的Matting Quality Evaluator（MQE），能够在不依赖真实标签的情况下评估alpha抠图的语义和边界质量。该MQE生成像素级的评估图，以识别可靠和错误的区域，用于训练期间的反馈以及数据筛选以提升标注质量。通过MQE，构建了一个包含28000个片段和240万个帧的大规模真实世界视频抠图数据集VMReal，并引入了参考帧训练策略来应对长视频中的外观变化。实验结果表明，MatAnyone 2在合成和真实世界基准测试中均达到了最先进的性能，所有指标均优于之前的方法。</div>
</details>
</div>
<div class="card">
<div class="title">Agile Flight Emerges from Multi-Agent Competitive Racing</div>
<div class="meta-line">Authors: Vineet Pasumarti, Lorenzo Bianchi, Antonio Loquercio</div>
<div class="meta-line">First: 2025-12-12T18:48:50+00:00 · Latest: 2025-12-12T18:48:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11781v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11781v1">PDF</a> · <a href="https://github.com/Jirl-upenn/AgileFlight_MultiAgent">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Through multi-agent competition and the sparse high-level objective of winning a race, we find that both agile flight (e.g., high-speed motion pushing the platform to its physical limits) and strategy (e.g., overtaking or blocking) emerge from agents trained with reinforcement learning. We provide evidence in both simulation and the real world that this approach outperforms the common paradigm of training agents in isolation with rewards that prescribe behavior, e.g., progress on the raceline, in particular when the complexity of the environment increases, e.g., in the presence of obstacles. Moreover, we find that multi-agent competition yields policies that transfer more reliably to the real world than policies trained with a single-agent progress-based reward, despite the two methods using the same simulation environment, randomization strategy, and hardware. In addition to improved sim-to-real transfer, the multi-agent policies also exhibit some degree of generalization to opponents unseen at training time. Overall, our work, following in the tradition of multi-agent competitive game-play in digital domains, shows that sparse task-level rewards are sufficient for training agents capable of advanced low-level control in the physical world.
  Code: https://github.com/Jirl-upenn/AgileFlight_MultiAgent</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>敏捷飞行从多智能体竞争赛车中涌现</div>
<div class="mono" style="margin-top:8px">通过多智能体竞争和稀疏的高层目标（如赢得比赛），我们发现无论是敏捷飞行（例如高速运动将平台推至物理极限）还是策略（例如超车或阻挡）都能从使用强化学习训练的智能体中涌现。我们在仿真和现实世界中提供了证据，表明这种方法优于传统训练方式，即在隔离环境中使用奖励来规定行为（如沿赛道前进），尤其是在环境复杂度增加时（如存在障碍物）。此外，我们发现多智能体竞争产生的策略比使用单智能体基于进度奖励的策略更可靠地迁移到现实世界。除了改进的仿真到现实迁移能力，多智能体策略还表现出一定程度的泛化能力，能够应对训练时未见过的对手。总体而言，我们的工作延续了数字领域中多智能体竞争游戏玩法的传统，表明稀疏的任务级奖励足以训练出能够在物理世界中实现高级底层控制的智能体。
  代码: https://github.com/Jirl-upenn/AgileFlight_MultiAgent</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This research investigates how multi-agent competitive racing can lead to the emergence of agile flight and strategic behaviors in reinforcement learning agents. By training agents in a competitive environment with a sparse reward structure focused on winning a race, the study demonstrates that these agents develop both high-speed motion capabilities and tactical strategies. Experimental results, both in simulation and on real-world platforms, show that this approach outperforms traditional single-agent training methods that rely on explicit progress-based rewards, especially in complex environments with obstacles. Additionally, the multi-agent policies exhibit better sim-to-real transfer and generalization against unseen opponents.</div>
<div class="mono" style="margin-top:8px">本研究通过多智能体竞争和稀疏的高阶目标（如赢得比赛）来探索多智能体系统中敏捷飞行和策略行为的产生，旨在解决在现实世界中实现复杂物理控制的挑战。研究采用强化学习方法，使用稀疏奖励结构而非孤立的进度奖励进行训练，并展示了这种方法在模拟和现实环境中的优越性。实验结果表明，多智能体竞争在复杂环境中（如存在障碍物时）比单智能体训练更有效，且生成的策略在面对训练中未见过的对手时表现出更强的泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Conditional Coverage Diagnostics for Conformal Prediction</div>
<div class="meta-line">Authors: Sacha Braun, David Holzmüller, Michael I. Jordan, Francis Bach</div>
<div class="meta-line">First: 2025-12-12T18:47:39+00:00 · Latest: 2025-12-12T18:47:39+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11779v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11779v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Evaluating conditional coverage remains one of the most persistent challenges in assessing the reliability of predictive systems. Although conformal methods can give guarantees on marginal coverage, no method can guarantee to produce sets with correct conditional coverage, leaving practitioners without a clear way to interpret local deviations. To overcome sample-inefficiency and overfitting issues of existing metrics, we cast conditional coverage estimation as a classification problem. Conditional coverage is violated if and only if any classifier can achieve lower risk than the target coverage. Through the choice of a (proper) loss function, the resulting risk difference gives a conservative estimate of natural miscoverage measures such as L1 and L2 distance, and can even separate the effects of over- and under-coverage, and non-constant target coverages. We call the resulting family of metrics excess risk of the target coverage (ERT). We show experimentally that the use of modern classifiers provides much higher statistical power than simple classifiers underlying established metrics like CovGap. Additionally, we use our metric to benchmark different conformal prediction methods. Finally, we release an open-source package for ERT as well as previous conditional coverage metrics. Together, these contributions provide a new lens for understanding, diagnosing, and improving the conditional reliability of predictive systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于置信预测的条件覆盖诊断</div>
<div class="mono" style="margin-top:8px">评估条件覆盖仍然是评估预测系统可靠性中最持久的挑战之一。尽管符合方法可以提供边缘覆盖的保证，但没有任何方法能保证生成具有正确条件覆盖的集合，这使得从业者无法清晰地解释局部偏差。为了解决现有度量指标在样本效率和过拟合方面的问题，我们将条件覆盖估计视为一个分类问题。当且仅当任何分类器都能达到比目标覆盖更低的风险时，条件覆盖才会被违反。通过选择适当的损失函数，所得到的风险差异可以保守地估计自然的覆盖偏差度量，如L1和L2距离，甚至可以区分过覆盖和欠覆盖以及非恒定目标覆盖的影响。我们将这一系列度量指标称为目标覆盖的超额风险（ERT）。我们通过实验表明，使用现代分类器可以提供比基于传统度量指标（如CovGap）的简单分类器更高的统计功效。此外，我们还利用该度量指标对不同的符合预测方法进行基准测试。最后，我们发布了用于ERT以及之前条件覆盖度量指标的开源包。这些贡献共同为理解、诊断和改进预测系统的条件可靠性提供了新的视角。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The paper addresses the challenge of evaluating conditional coverage in predictive systems, which is crucial for ensuring reliability but difficult to assess with existing methods that only guarantee marginal coverage. It proposes a novel approach by framing conditional coverage estimation as a classification problem, introducing the Excess Risk of the Target Coverage (ERT) metric. The method leverages modern classifiers and a proper loss function to provide a more accurate and powerful assessment of conditional coverage, effectively distinguishing between over- and under-coverage and handling non-constant target coverages. Experimental results demonstrate that ERT outperforms traditional metrics like CovGap in statistical power and enables benchmarking of different conformal prediction methods.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决预测系统中条件覆盖评估的挑战，这比边际覆盖更为复杂，且缺乏可靠的度量方法。作者提出了一种将条件覆盖估计转化为分类问题的方法，利用现代分类器计算目标覆盖的超额风险（ERT）。这种方法能够更准确地评估条件覆盖，区分过覆盖和欠覆盖，并处理非恒定的目标覆盖。实验结果表明，与传统指标如CovGap相比，ERT具有更高的统计功效，并用于评估不同的置信预测方法。</div>
</details>
</div>
<div class="card">
<div class="title">How Muon&#x27;s Spectral Design Benefits Generalization: A Study on Imbalanced Data</div>
<div class="meta-line">Authors: Bhavya Vasudeva, Puneesh Deora, Yize Zhao, Vatsal Sharan, Christos Thrampoulidis</div>
<div class="meta-line">First: 2025-10-27T04:00:42+00:00 · Latest: 2025-12-12T18:45:29+00:00</div>
<div class="meta-line">Comments: 36 pages, 32 figures, 1 table</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.22980v3">Abs</a> · <a href="https://arxiv.org/pdf/2510.22980v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The growing adoption of spectrum-aware matrix-valued optimizers such as Muon and Shampoo in deep learning motivates a systematic study of their generalization properties and, in particular, when they might outperform competitive algorithms. We approach this question by introducing appropriate simplifying abstractions as follows: First, we use imbalanced data as a testbed. Second, we study the canonical form of such optimizers, which is Spectral Gradient Descent (SpecGD) -- each update step is $UV^T$ where $UΣV^T$ is the truncated SVD of the gradient. Third, within this framework we identify a canonical setting for which we precisely quantify when SpecGD outperforms vanilla Euclidean GD. For a Gaussian mixture data model and both linear and bilinear models, we show that unlike GD, which prioritizes learning dominant principal components of the data first, SpecGD learns all principal components of the data at equal rates. We demonstrate how this translates to a growing gap in class balanced loss favoring SpecGD early in training and further show that the gap remains consistent even when the GD counterpart uses adaptive step-sizes via normalization. By extending the analysis to deep linear models, we show that depth amplifies these effects. We empirically verify our theoretical findings on a variety of imbalanced datasets. Our experiments compare practical variants of spectral methods, like Muon and Shampoo, against their Euclidean counterparts and Adam. The results validate our findings that these spectral optimizers achieve superior generalization by promoting a more balanced learning of the data&#x27;s underlying components.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>穆子的谱设计如何促进泛化：一项关于不平衡数据的研究</div>
<div class="mono" style="margin-top:8px">随着谱感知矩阵值优化器如穆子和Shampoo在深度学习中的广泛应用，我们对它们的泛化性质进行系统研究，特别是它们在何种情况下可能优于竞争算法。我们通过引入适当的简化抽象来探讨这一问题：首先，我们使用不平衡数据作为实验平台；其次，我们研究这类优化器的规范形式，即谱梯度下降（SpecGD）——每个更新步骤为$UV^T$，其中$UΣV^T$是梯度的截断奇异值分解；第三，在此框架下，我们识别出一个规范设置，并精确量化SpecGD在何种情况下优于普通的欧几里得梯度下降。对于高斯混合数据模型以及线性和双线性模型，我们表明与优先学习数据主导主成分的GD不同，SpecGD以相同速率学习所有主成分。我们展示了这种特性如何导致在训练初期类平衡损失的差距扩大，进一步证明即使GD的对应方法通过归一化使用自适应步长，这种差距依然保持一致。通过将分析扩展到深度线性模型，我们表明深度会放大这些效应。我们在多种不平衡数据集上实证验证了我们的理论发现。我们的实验将穆子和Shampoo等谱方法的实用变体与它们的欧几里得对应方法和Adam进行比较。结果验证了我们的发现，即这些谱优化器通过促进数据潜在成分的更均衡学习，实现了更优的泛化性能。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This study investigates how the spectral design of optimizers like Muon and Shampoo affects generalization, particularly in the context of imbalanced data. The authors analyze the canonical form of spectral gradient descent (SpecGD), where each update is represented as $UV^T$ derived from the truncated SVD of the gradient, and compare it to standard Euclidean gradient descent. They show that SpecGD learns all principal components of the data at equal rates, unlike Euclidean GD which prioritizes dominant components, leading to a larger early advantage in class balanced loss. The findings are validated through experiments on Gaussian mixture models and deep linear models, demonstrating that depth enhances these benefits and that spectral optimizers outperform Euclidean methods and Adam in generalization on imbalanced datasets.</div>
<div class="mono" style="margin-top:8px">本研究探讨了Muon和Shampoo等谱优化器在处理不平衡数据时如何提升泛化能力。作者分析了这些优化器的规范形式——谱梯度下降（SpecGD），其每个更新步骤使用梯度的截断奇异值分解（SVD）得到的$UV^T$。他们发现，与标准欧几里得梯度下降不同，SpecGD在学习数据主成分时保持均衡，从而在训练初期显著降低类别不平衡损失。这一优势在使用自适应步长的梯度下降方法中依然存在。实验结果验证了理论分析，表明谱优化器在促进均衡学习和提升泛化能力方面优于其欧几里得版本和Adam。</div>
</details>
</div>
<div class="card">
<div class="title">The Adaptive Vekua Cascade: A Differentiable Spectral-Analytic Solver for Physics-Informed Representation</div>
<div class="meta-line">Authors: Vladimer Khasia</div>
<div class="meta-line">First: 2025-12-12T18:41:35+00:00 · Latest: 2025-12-12T18:41:35+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11776v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11776v1">PDF</a> · <a href="https://github.com/VladimerKhasia/vecua">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coordinate-based neural networks have emerged as a powerful tool for representing continuous physical fields, yet they face two fundamental pathologies: spectral bias, which hinders the learning of high-frequency dynamics, and the curse of dimensionality, which causes parameter explosion in discrete feature grids. We propose the Adaptive Vekua Cascade (AVC), a hybrid architecture that bridges deep learning and classical approximation theory. AVC decouples manifold learning from function approximation by using a deep network to learn a diffeomorphic warping of the physical domain, projecting complex spatiotemporal dynamics onto a latent manifold where the solution is represented by a basis of generalized analytic functions. Crucially, we replace the standard gradient-descent output layer with a differentiable linear solver, allowing the network to optimally resolve spectral coefficients in a closed form during the forward pass. We evaluate AVC on a suite of five rigorous physics benchmarks, including high-frequency Helmholtz wave propagation, sparse medical reconstruction, and unsteady 3D Navier-Stokes turbulence. Our results demonstrate that AVC achieves state-of-the-art accuracy while reducing parameter counts by orders of magnitude (e.g., 840 parameters vs. 4.2 million for 3D grids) and converging 2-3x faster than implicit neural representations. This work establishes a new paradigm for memory-efficient, spectrally accurate scientific machine learning. The code is available at https://github.com/VladimerKhasia/vecua.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>自适应Vekua级数：一种用于物理信息表示的可微谱分析求解器</div>
<div class="mono" style="margin-top:8px">基于坐标的神经网络已成为表示连续物理场的强大工具，但它们面临两个根本性问题：谱偏倚，阻碍高频动态的学习；以及维度灾难，导致离散特征网格中的参数爆炸。我们提出了自适应Vekua级数（AVC），这是一种结合深度学习与经典逼近理论的混合架构。AVC通过使用深度网络学习物理域的微分同胚变形，将复杂的时空动态投影到一个潜在流形上，该流形上的解由广义解析函数基表示。关键的是，我们用一个可微线性求解器替代了标准的梯度下降输出层，使得网络能够在前向传播过程中以闭合形式最优地求解谱系数。我们在五个严格的物理基准测试中评估了AVC，包括高频Helmholtz波传播、稀疏医学重建和非稳态三维Navier-Stokes湍流。我们的结果表明，AVC在保持最先进精度的同时，将参数数量减少了几个数量级（例如，三维网格中从420万个减少到840个），并且收敛速度比隐式神经表示快2-3倍。这项工作建立了一种新的范式，为内存高效且谱精确的科学机器学习提供了新途径。代码可在https://github.com/VladimerKhasia/vecua获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the limitations of coordinate-based neural networks in capturing high-frequency dynamics and managing parameter explosion in high-dimensional spaces. The Adaptive Vekua Cascade (AVC) addresses these issues by combining manifold learning with function approximation through a diffeomorphic warping of the physical domain and a differentiable linear solver for spectral coefficient resolution. Experimental results on five physics benchmarks show that AVC achieves state-of-the-art accuracy with significantly fewer parameters and faster convergence compared to implicit neural representations.</div>
<div class="mono" style="margin-top:8px">该研究旨在解决坐标型神经网络在表示连续物理场时面临的两个根本问题：频谱偏差和维度灾难。提出的自适应Vekua级数（AVC）是一种混合架构，结合深度学习与经典逼近理论，通过学习物理域的微分同胚映射，并在潜在流形上使用广义解析函数表示解。研究将标准梯度下降输出层替换为可微分线性求解器，使网络在前向传播过程中能够最优地求解频谱系数。在五个物理基准测试中，实验结果表明AVC在参数数量和收敛速度方面均优于隐式神经表示，同时保持了最先进的精度水平。</div>
</details>
</div>
<div class="card">
<div class="title">Smudged Fingerprints: A Systematic Evaluation of the Robustness of AI Image Fingerprints</div>
<div class="meta-line">Authors: Kai Yao, Marc Juarez</div>
<div class="meta-line">First: 2025-12-12T18:33:14+00:00 · Latest: 2025-12-12T18:33:14+00:00</div>
<div class="meta-line">Comments: This work has been accepted for publication in the 4th IEEE Conference on Secure and Trustworthy Machine Learning (IEEE SaTML 2026). The final version will be available on IEEE Xplore</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11771v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11771v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Model fingerprint detection techniques have emerged as a promising approach for attributing AI-generated images to their source models, but their robustness under adversarial conditions remains largely unexplored. We present the first systematic security evaluation of these techniques, formalizing threat models that encompass both white- and black-box access and two attack goals: fingerprint removal, which erases identifying traces to evade attribution, and fingerprint forgery, which seeks to cause misattribution to a target model. We implement five attack strategies and evaluate 14 representative fingerprinting methods across RGB, frequency, and learned-feature domains on 12 state-of-the-art image generators. Our experiments reveal a pronounced gap between clean and adversarial performance. Removal attacks are highly effective, often achieving success rates above 80% in white-box settings and over 50% under constrained black-box access. While forgery is more challenging than removal, its success significantly varies across targeted models. We also identify a utility-robustness trade-off: methods with the highest attribution accuracy are often vulnerable to attacks. Although some techniques exhibit robustness in specific settings, none achieves high robustness and accuracy across all evaluated threat models. These findings highlight the need for techniques balancing robustness and accuracy, and identify the most promising approaches for advancing this goal.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>模糊的指纹：AI图像指纹技术鲁棒性系统评估</div>
<div class="mono" style="margin-top:8px">模型指纹检测技术已成为将AI生成图像追溯到其源模型的一种有前景的方法，但其在对抗性条件下的鲁棒性仍 largely 未被探索。我们提出了这些技术的第一项系统性安全评估，形式化了涵盖白盒和黑盒访问的威胁模型，并定义了两种攻击目标：指纹移除，旨在消除可识别痕迹以逃避归属；以及指纹伪造，旨在将图像错误地归因于目标模型。我们实现了五种攻击策略，并在12种最先进的图像生成器上，针对RGB、频率和学习特征域评估了14种代表性指纹方法。实验结果揭示了干净数据与对抗性数据之间显著的性能差距。移除攻击在白盒设置中通常成功率达到80%以上，在受限的黑盒访问下也超过50%。尽管伪造比移除更具挑战性，但其成功程度在不同目标模型之间差异显著。我们还发现了一个效用与鲁棒性之间的权衡：具有最高归属准确性的方法往往容易受到攻击。虽然某些技术在特定场景下表现出鲁棒性，但没有任何方法在所有评估的威胁模型中同时实现高鲁棒性和高准确性。这些发现强调了需要在鲁棒性和准确性之间取得平衡的技术，并指出了最有可能推动这一目标发展的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need to evaluate the robustness of AI image fingerprinting techniques against adversarial attacks. The study systematically examines five attack strategies on 14 fingerprinting methods across different domains using 12 state-of-the-art image generators. Experimental results show that removal attacks are highly effective, achieving success rates above 80% in white-box scenarios and over 50% in constrained black-box settings. Forgery attacks, while more challenging, still succeed to varying degrees depending on the target model. The study also highlights a trade-off between utility and robustness, where high accuracy methods are often less secure, and no method achieves both high accuracy and robustness across all threat models.</div>
<div class="mono" style="margin-top:8px">本研究探讨了AI图像指纹技术在对抗攻击下的鲁棒性。研究动机源于对这些技术在不同攻击场景下能否有效将AI生成图像溯源到其来源模型的理解需求。作者评估了五种攻击策略对14种指纹方法在不同领域上的影响，使用了12种最先进的图像生成器。实验结果表明，指纹移除攻击效果显著，在白盒环境下成功率超过80%，在受限的黑盒环境下也达到50%以上。相比之下，指纹伪造攻击的成功率因目标模型而异。研究还揭示了效用与鲁棒性之间的权衡，指出高准确率的方法往往在安全性上较弱。尽管某些技术在特定环境下表现出鲁棒性，但没有一种方法能在所有评估的威胁模型中同时实现高准确性和高鲁棒性。</div>
</details>
</div>
<div class="card">
<div class="title">Learning Minimal Representations of Fermionic Ground States</div>
<div class="meta-line">Authors: Felix Frohnert, Emiel Koridon, Stefano Polla</div>
<div class="meta-line">First: 2025-12-12T18:26:05+00:00 · Latest: 2025-12-12T18:26:05+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11767v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11767v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce an unsupervised machine-learning framework that discovers optimally compressed representations of quantum many-body ground states. Using an autoencoder neural network architecture on data from $L$-site Fermi-Hubbard models, we identify minimal latent spaces with a sharp reconstruction quality threshold at $L-1$ latent dimensions, matching the system&#x27;s intrinsic degrees of freedom. We demonstrate the use of the trained decoder as a differentiable variational ansatz to minimize energy directly within the latent space. Crucially, this approach circumvents the $N$-representability problem, as the learned manifold implicitly restricts the optimization to physically valid quantum states.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习费米子基态的最小表示</div>
<div class="mono" style="margin-top:8px">我们引入了一种无监督机器学习框架，用于发现量子多体基态的最优压缩表示。通过在$L$-位点费米-哈伯德模型数据上使用自编码器神经网络架构，我们识别出在$L-1$潜在维度下具有明确重建质量阈值的最小潜在空间，与系统的固有自由度相匹配。我们展示了训练后的解码器作为可微变分假设的用途，直接在潜在空间中最小化能量。关键的是，这种方法避开了$N$-可表示性问题，因为学习到的流形隐式地将优化限制在物理有效的量子态上。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research is motivated by the need for efficient representation of quantum many-body ground states in the context of fermionic systems. The authors employ an unsupervised machine learning framework based on an autoencoder neural network to analyze data from L-site Fermi-Hubbard models. Their method identifies minimal latent spaces where the reconstruction quality sharply improves at L-1 dimensions, aligning with the system&#x27;s intrinsic degrees of freedom. The trained decoder is then used as a differentiable variational ansatz to minimize energy within the latent space, effectively bypassing the N-representability problem by implicitly restricting the optimization to physically valid quantum states.</div>
<div class="mono" style="margin-top:8px">该研究旨在为费米子系统中的量子多体基态寻找高效的表示方法。作者采用基于自编码器神经网络的无监督机器学习框架，利用L位点费米-哈伯德模型的数据来发现这些状态的最优压缩表示。研究结果表明，具有L-1维的最小潜在空间能够达到锐利的重建质量阈值，与系统的固有自由度一致。此外，训练后的解码器被用作可微分变分假设，直接在潜在空间中最小化能量，从而绕过了N表示性问题，通过隐式限制优化范围在物理有效的量子态内。</div>
</details>
</div>
<div class="card">
<div class="title">Reducing Domain Gap with Diffusion-Based Domain Adaptation for Cell Counting</div>
<div class="meta-line">Authors: Mohammad Dehghanmanshadi, Wallapak Tavanapong</div>
<div class="meta-line">First: 2025-12-12T18:19:41+00:00 · Latest: 2025-12-12T18:19:41+00:00</div>
<div class="meta-line">Comments: Accepted at ICMLA 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11763v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11763v1">PDF</a> · <a href="https://github.com/MohammadDehghan/InST-Microscopy">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generating realistic synthetic microscopy images is critical for training deep learning models in label-scarce environments, such as cell counting with many cells per image. However, traditional domain adaptation methods often struggle to bridge the domain gap when synthetic images lack the complex textures and visual patterns of real samples. In this work, we adapt the Inversion-Based Style Transfer (InST) framework originally designed for artistic style transfer to biomedical microscopy images. Our method combines latent-space Adaptive Instance Normalization with stochastic inversion in a diffusion model to transfer the style from real fluorescence microscopy images to synthetic ones, while weakly preserving content structure.
  We evaluate the effectiveness of our InST-based synthetic dataset for downstream cell counting by pre-training and fine-tuning EfficientNet-B0 models on various data sources, including real data, hard-coded synthetic data, and the public Cell200-s dataset. Models trained with our InST-synthesized images achieve up to 37\% lower Mean Absolute Error (MAE) compared to models trained on hard-coded synthetic data, and a 52\% reduction in MAE compared to models trained on Cell200-s (from 53.70 to 25.95 MAE). Notably, our approach also outperforms models trained on real data alone (25.95 vs. 27.74 MAE). Further improvements are achieved when combining InST-synthesized data with lightweight domain adaptation techniques such as DACS with CutMix. These findings demonstrate that InST-based style transfer most effectively reduces the domain gap between synthetic and real microscopy data. Our approach offers a scalable path for enhancing cell counting performance while minimizing manual labeling effort. The source code and resources are publicly available at: https://github.com/MohammadDehghan/InST-Microscopy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用基于扩散的领域自适应减少领域差距用于细胞计数</div>
<div class="mono" style="margin-top:8px">生成逼真的合成显微图像对于在标签稀缺环境中训练深度学习模型至关重要，例如每张图像包含大量细胞的细胞计数任务。然而，传统领域自适应方法在合成图像缺乏真实样本复杂纹理和视觉模式的情况下，往往难以弥合领域差距。在本工作中，我们将最初为艺术风格迁移设计的基于反向的风格迁移（InST）框架适应于生物医学显微图像。我们的方法结合了潜在空间自适应实例归一化与扩散模型中的随机反向，将真实荧光显微图像的风格迁移到合成图像上，同时弱化地保留内容结构。
我们通过在多种数据源上预训练和微调EfficientNet-B0模型，评估了基于InST的合成数据集在下游细胞计数任务中的有效性，包括真实数据、硬编码合成数据以及公开的Cell200-s数据集。使用我们InST合成图像训练的模型，其平均绝对误差（MAE）比使用硬编码合成数据训练的模型降低了高达37%，比使用Cell200-s数据集训练的模型降低了52%（从53.70降至25.95 MAE）。值得注意的是，我们的方法在MAE上也优于仅使用真实数据训练的模型（25.95 vs. 27.74 MAE）。当将InST合成数据与轻量级领域自适应技术（如DACS与CutMix）结合使用时，进一步提升了性能。这些发现表明，基于InST的风格迁移最有效地减少了合成与真实显微图像之间的领域差距。我们的方法提供了一条可扩展的路径，以提升细胞计数性能，同时减少手动标注的工作量。源代码和资源可在以下链接中获取：https://github.com/MohammadDehghan/InST-Microscopy。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the domain gap between synthetic and real fluorescence microscopy images, which hinders the performance of deep learning models in cell counting tasks. The authors adapt the Inversion-Based Style Transfer (InST) framework to transfer the style from real images to synthetic ones, preserving content structure through latent-space Adaptive Instance Normalization and stochastic inversion in a diffusion model. Experimental results show that models trained on InST-synthesized images achieve a 37% lower Mean Absolute Error (MAE) than those trained on hard-coded synthetic data and a 52% reduction in MAE compared to models trained on the Cell200-s dataset. Additionally, the approach outperforms models trained solely on real data, and combining it with domain adaptation techniques like DACS with CutMix further improves performance.</div>
<div class="mono" style="margin-top:8px">本研究旨在解决合成与真实荧光显微镜图像之间的领域差距问题，这对细胞计数等深度学习任务的性能产生负面影响。提出的方法将基于反转的风格迁移（InST）框架应用于生物医学显微镜图像，通过扩散模型中的潜在空间自适应实例归一化和随机反转技术，将真实图像的风格迁移到合成图像上，同时保留内容结构。实验结果表明，使用InST生成的合成图像训练的模型在均绝对误差（MAE）上比使用硬编码合成数据训练的模型降低了37\%，比在Cell200-s数据集上训练的模型降低了52\%。此外，该方法在仅使用真实数据训练的模型上表现更优，并且与轻量级领域自适应技术如DACS结合CutMix后，进一步提升了性能。</div>
</details>
</div>
<div class="card">
<div class="title">REDELEX: A Framework for Relational Deep Learning Exploration</div>
<div class="meta-line">Authors: Jakub Peleška, Gustav Šír</div>
<div class="meta-line">First: 2025-06-27T13:05:15+00:00 · Latest: 2025-12-12T18:15:25+00:00</div>
<div class="meta-line">Comments: Accepted to ECMLPKDD 2025 at Porto, Portugal</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.22199v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.22199v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Relational databases (RDBs) are widely regarded as the gold standard for storing structured information. Consequently, predictive tasks leveraging this data format hold significant application promise. Recently, Relational Deep Learning (RDL) has emerged as a novel paradigm wherein RDBs are conceptualized as graph structures, enabling the application of various graph neural architectures to effectively address these tasks. However, given its novelty, there is a lack of analysis into the relationships between the performance of various RDL models and the characteristics of the underlying RDBs.
  In this study, we present REDELEX$-$a comprehensive exploration framework for evaluating RDL models of varying complexity on the most diverse collection of over 70 RDBs, which we make available to the community. Benchmarked alongside key representatives of classic methods, we confirm the generally superior performance of RDL while providing insights into the main factors shaping performance, including model complexity, database sizes and their structural properties.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>REDELEX：关系深度学习探索框架</div>
<div class="mono" style="margin-top:8px">关系型数据库（RDBs）被广泛认为是存储结构化信息的黄金标准。因此，利用这种数据格式的预测任务具有重要的应用前景。最近，关系深度学习（RDL）作为一种新范式出现，其中将RDBs视为图结构，从而能够应用各种图神经网络架构来有效解决这些任务。然而，由于其新颖性，目前缺乏对不同RDL模型性能与其底层RDBs特征之间关系的分析。
在本研究中，我们提出了REDELEX——一个全面的探索框架，用于在超过70个最多样化的RDBs集合上评估不同复杂度的RDL模型，我们将其提供给研究社区。通过与经典方法的关键代表进行对比实验，我们验证了RDL通常具有更优的性能，并提供了影响性能的主要因素的见解，包括模型复杂度、数据库规模及其结构特性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this study is to explore the relationship between the performance of Relational Deep Learning (RDL) models and the characteristics of relational databases (RDBs). The authors introduce REDELEX, a framework designed to evaluate a variety of RDL models on a diverse set of over 70 RDBs. Through benchmarking against traditional methods, they demonstrate that RDL models generally outperform classical approaches and identify key factors influencing performance, such as model complexity, database size, and structural properties.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于对关系深度学习（RDL）模型性能与关系数据库（RDB）特征之间关系的理解需求。作者提出了REDELEX框架，用于在包含70多个RDB的多样化数据集上评估各种RDL模型。通过与传统方法的基准测试，他们证实了RDL模型通常优于经典方法，并识别了影响性能的关键因素，如模型复杂度、数据库规模和结构特性。</div>
</details>
</div>
<div class="card">
<div class="title">SpectralKrum: A Spectral-Geometric Defense Against Byzantine Attacks in Federated Learning</div>
<div class="meta-line">Authors: Aditya Tripathi, Karan Sharma, Rahul Mishra, Tapas Kumar Maiti</div>
<div class="meta-line">First: 2025-12-12T18:12:37+00:00 · Latest: 2025-12-12T18:12:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11760v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11760v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Federated Learning (FL) distributes model training across clients who retain their data locally, but this architecture exposes a fundamental vulnerability: Byzantine clients can inject arbitrarily corrupted updates that degrade or subvert the global model. While robust aggregation methods (including Krum, Bulyan, and coordinate-wise defenses) offer theoretical guarantees under idealized assumptions, their effectiveness erodes substantially when client data distributions are heterogeneous (non-IID) and adversaries can observe or approximate the defense mechanism.
  This paper introduces SpectralKrum, a defense that fuses spectral subspace estimation with geometric neighbor-based selection. The core insight is that benign optimization trajectories, despite per-client heterogeneity, concentrate near a low-dimensional manifold that can be estimated from historical aggregates. SpectralKrum projects incoming updates into this learned subspace, applies Krum selection in compressed coordinates, and filters candidates whose orthogonal residual energy exceeds a data-driven threshold. The method requires no auxiliary data, operates entirely on model updates, and preserves FL privacy properties.
  We evaluate SpectralKrum against eight robust baselines across seven attack scenarios on CIFAR-10 with Dirichlet-distributed non-IID partitions (alpha = 0.1). Experiments spanning over 56,000 training rounds show that SpectralKrum is competitive against directional and subspace-aware attacks (adaptive-steer, buffer-drift), but offers limited advantage under label-flip and min-max attacks where malicious updates remain spectrally indistinguishable from benign ones.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SpectralKrum：一种针对联邦学习中拜占庭攻击的谱几何防御方法</div>
<div class="mono" style="margin-top:8px">联邦学习（FL）将模型训练分布在客户端，客户端保留数据本地，但这种架构暴露了一个根本性漏洞：拜占庭客户端可以注入任意被污染的更新，从而损害或颠覆全局模型。尽管鲁棒聚合方法（包括Krum、Bulyan和逐坐标防御）在理想假设下提供了理论保证，但当客户端数据分布异质（非独立同分布）且对手可以观察或近似防御机制时，其有效性会显著下降。
本文提出SpectralKrum，一种融合谱子空间估计与几何邻居选择的防御方法。其核心思想是，尽管每个客户端存在异质性，良性优化轨迹仍会集中在可通过历史聚合估计的低维流形附近。SpectralKrum将传入的更新投影到该学习子空间中，在压缩坐标下应用Krum选择，并过滤那些正交残差能量超过数据驱动阈值的候选更新。该方法无需辅助数据，完全基于模型更新进行操作，并保留联邦学习的隐私属性。
我们在CIFAR-10数据集上，针对具有Dirichlet分布非独立同分布分割（alpha = 0.1）的七个攻击场景，评估了SpectralKrum与八个鲁棒基线方法的性能。实验覆盖超过56,000轮训练，结果显示SpectralKrum在方向性攻击和子空间感知攻击（如自适应引导、缓冲漂移）中具有竞争力，但在标签翻转和最小最大攻击中，由于恶意更新在谱上难以与良性更新区分，其优势有限。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the vulnerability of Federated Learning (FL) to Byzantine attacks, where malicious clients can corrupt the global model. SpectralKrum is introduced as a defense mechanism that combines spectral subspace estimation with geometric neighbor-based selection to filter out malicious updates. The method projects incoming model updates into a low-dimensional manifold estimated from historical aggregates and applies Krum selection in compressed coordinates, using a data-driven threshold to filter out updates with high orthogonal residual energy. Experimental results on CIFAR-10 with non-IID data show that SpectralKrum is competitive against directional and subspace-aware attacks but provides limited protection against label-flip and min-max attacks where malicious updates are spectrally indistinguishable from benign ones.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决联邦学习（FL）中拜占庭攻击的脆弱性，恶意客户端可能注入任意损坏的更新以破坏全局模型。SpectralKrum是一种结合谱子空间估计和几何邻域选择的防御机制，用于过滤恶意更新。该方法将传入的模型更新投影到由历史聚合估计的低维流形上，并在压缩坐标中应用Krum选择，使用数据驱动的阈值过滤异常值。实验结果表明，在非独立同分布（non-IID）数据的CIFAR-10数据集上，SpectralKrum对方向性和子空间感知攻击有效，但在标签翻转和最小最大攻击中，恶意更新与良性更新在谱上难以区分，因此其优势有限。</div>
</details>
</div>
<div class="card">
<div class="title">Advancing physiological time series reconstruction and imputation via mixture of receptive fields and experts fusion</div>
<div class="meta-line">Authors: Ci Zhang, Huayu Li, Changdi Yang, Jiangnan Xia, Yanzhi Wang, Xiaolong Ma, Jin Lu, Ao Li, Geng Yuan</div>
<div class="meta-line">First: 2025-11-27T04:06:55+00:00 · Latest: 2025-12-12T17:57:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.07873v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.07873v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent studies show that using diffusion models for time series signal reconstruction holds great promise. However, such approaches remain largely unexplored in the domain of medical time series. The unique characteristics of the physiological time series signals, such as multivariate, high temporal variability, highly noisy, and artifact-prone, make deep learning-based approaches still challenging for tasks such as imputation. Hence, we propose a novel Mixture of Experts (MoE)-based noise estimator within a score-based diffusion framework. Specifically, the Receptive Field Adaptive MoE (RFAMoE) module is designed to enable each channel to adaptively select desired receptive fields throughout the diffusion process. Moreover, recent literature has found that when generating a physiological signal, performing multiple inferences and averaging the reconstructed signals can effectively reduce reconstruction errors, but at the cost of significant computational and latency overhead. We design a Fusion MoE module and innovatively leverage the nature of MoE module to generate K noise signals in parallel, fuse them using a routing mechanism, and complete signal reconstruction in a single inference step. This design not only improves performance over previous methods but also eliminates the substantial computational cost and latency associated with multiple inference processes. Extensive results demonstrate that our proposed framework consistently outperforms diffusion-based SOTA works on different tasks and datasets.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过感受野与专家混合的融合方法推进生理时间序列重建与插补</div>
<div class="mono" style="margin-top:8px">近期研究表明，使用扩散模型进行时间序列信号重建具有巨大潜力。然而，此类方法在医学时间序列领域仍处于初步探索阶段。生理时间序列信号具有多变量、高时间变异性、高度噪声和易受干扰等独特特性，使得基于深度学习的方法在插补等任务中仍面临挑战。因此，我们提出了一种基于专家混合（Mixture of Experts, MoE）的新型噪声估计器，嵌入在得分扩散框架中。具体而言，设计了接收域自适应MoE（RFAMoE）模块，使每个通道能够在扩散过程中自适应地选择所需的接收域。此外，近期文献发现，在生成生理信号时，进行多次推理并平均重建信号可以有效降低重建误差，但代价是显著的计算和延迟开销。我们设计了融合MoE模块，并创新性地利用MoE模块的特性，实现K个噪声信号的并行生成，通过路由机制进行融合，并在单次推理步骤中完成信号重建。该设计不仅提升了性能，还消除了多次推理过程所带来的显著计算成本和延迟。大量实验结果表明，我们的框架在不同任务和数据集上均优于基于扩散的当前最优方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for this research stems from the challenges of reconstructing and imputing physiological time series signals, which are often multivariate, noisy, and prone to artifacts. To address these issues, the authors propose a novel approach that integrates a Mixture of Experts (MoE) noise estimator within a score-based diffusion framework. Their method introduces the Receptive Field Adaptive MoE (RFAMoE) module, which allows each channel to adaptively select receptive fields during the diffusion process. Additionally, they design a Fusion MoE module that generates multiple noise signals in parallel, fuses them using a routing mechanism, and completes signal reconstruction in one inference step, thereby reducing computational overhead and latency. Experimental results show that their framework outperforms existing diffusion-based state-of-the-art methods across various tasks and datasets.</div>
<div class="mono" style="margin-top:8px">本研究的动机源于生理时间序列信号在重建和插补过程中面临的挑战，这些信号通常具有多变量、高时间变异性、高度噪声和易受干扰的特点。为了解决这些问题，作者提出了一种结合专家混合框架与基于分数的扩散模型的新方法，引入了接收场自适应MoE（RFAMoE）模块，以在扩散过程中自适应选择所需的接收场。此外，他们设计了一个融合MoE模块，通过并行生成多个噪声信号、使用路由机制进行融合，并在单次推理步骤中完成信号重建，从而降低了计算开销和延迟。实验结果表明，所提出的框架在不同任务和数据集上均优于现有的基于扩散的方法。</div>
</details>
</div>
<div class="card">
<div class="title">MTTR-A: Measuring Cognitive Recovery Latency in Multi-Agent Systems</div>
<div class="meta-line">Authors: Barak Or</div>
<div class="meta-line">First: 2025-11-08T21:29:18+00:00 · Latest: 2025-12-12T17:56:26+00:00</div>
<div class="meta-line">Comments: preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.20663v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.20663v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring cognitive stability in autonomous multi-agent systems (MAS) is a central challenge for large-scale, distributed AI. While existing observability tools monitor system outputs, they cannot quantify how rapidly agentic workflows recover once reasoning coherence has been lost. We adapt classical reliability metrics-Mean Time-to-Recovery (MTTR), Mean Time Between Failures (MTBF), and related ratios-into the cognitive domain, defining MTTR-A (Mean Time-to-Recovery for Agentic Systems) as a runtime measure of cognitive recovery latency. MTTR-A quantifies the time required for a MAS to detect reasoning drift and restore consistent operation, capturing the recovery of reasoning coherence rather than infrastructural repair.
  A benchmark simulation using the AG~News corpus and the LangGraph orchestration framework was conducted, modeling recovery latencies across multiple reflex modes. Automated reflexes restored stability within approximately 6s on average, while human-approval interventions required about 12s. Across 200 runs, the median simulated MTTR-A was 6.21+-2.14s, MTBF=6.7+-2.14s, and NRR=0.08, demonstrating measurable runtime resilience across reflex strategies.
  By formalizing recovery latency as a quantifiable property of distributed reasoning-and deriving reliability bounds linking recovery time and cognitive uptime-this work establishes a foundation for runtime dependability in agentic cognition, transforming cognitive recovery from an ad-hoc process into a standardized, interpretable performance</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MTTR-A：多智能体系统中认知恢复延迟的度量</div>
<div class="mono" style="margin-top:8px">确保自主多智能体系统（MAS）中的认知稳定性是大规模分布式AI的核心挑战。尽管现有的可观测性工具可以监控系统输出，但无法量化当推理一致性丧失后，智能体工作流恢复的速度。我们将经典可靠性指标——恢复时间均值（MTTR）、故障间隔时间均值（MTBF）及相关比率——引入认知领域，定义MTTR-A（智能体系统恢复时间均值）为一种运行时认知恢复延迟的度量。MTTR-A量化了MAS检测推理漂移并恢复一致操作所需的时间，关注的是推理一致性的恢复，而非基础设施的修复。我们使用AG~News语料库和LangGraph编排框架进行基准模拟，建模了多种反射模式下的恢复延迟。自动反射机制平均在6秒内恢复稳定性，而需要人工审批的干预则平均需要约12秒。在200次运行中，模拟的MTTR-A中位数为6.21±2.14秒，MTBF=6.7±2.14秒，NRR=0.08，展示了不同反射策略在运行时的可衡量的稳定性。通过将恢复延迟形式化为分布式推理的可量化属性，并推导出将恢复时间和认知运行时间联系起来的可靠性界限，本工作为智能体认知的运行时可靠性奠定了基础，将认知恢复从一种随意过程转变为标准化、可解释的性能指标。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to address the challenge of ensuring cognitive stability in multi-agent systems by introducing MTTR-A, a metric that quantifies the time required for cognitive recovery after reasoning coherence is lost. The study adapts classical reliability metrics to the cognitive domain, using a benchmark simulation with the AG~News corpus and LangGraph framework to model recovery latencies across different reflex modes. Experimental results show that automated reflexes restore stability in about 6 seconds on average, while human-approval interventions take approximately 12 seconds, with a median MTTR-A of 6.21±2.14 seconds and MTBF of 6.7±2.14 seconds across 200 runs, indicating measurable runtime resilience in agentic systems.</div>
<div class="mono" style="margin-top:8px">该研究旨在确保自主多智能体系统（MAS）的认知稳定性，并衡量系统在推理一致性丧失后恢复所需的时间。研究引入了MTTR-A（用于智能体系统的平均恢复时间），该指标基于经典可靠性度量方法，用于量化MAS的认知恢复延迟。实验使用AG~News语料库和LangGraph框架进行基准模拟，结果显示自动化反射机制平均在6秒内恢复稳定性，而需要人工批准的干预则平均耗时12秒。在200次运行中，MTTR-A的中位数为6.21±2.14秒，MTBF为6.7±2.14秒，NRR为0.08，表明不同的反射策略对MAS的运行时韧性有显著影响。</div>
</details>
</div>
<div class="card">
<div class="title">UpBench: A Dynamically Evolving Real-World Labor-Market Agentic Benchmark Framework Built for Human-Centric AI</div>
<div class="meta-line">Authors: Darvin Yi, Teng Liu, Mattie Terzolo, Lance Hasson, Ayan Sinha, Pablo Mendes, Andrew Rabinovich</div>
<div class="meta-line">First: 2025-11-15T17:39:37+00:00 · Latest: 2025-12-12T17:51:50+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.12306v2">Abs</a> · <a href="https://arxiv.org/pdf/2511.12306v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language model (LLM) agents increasingly undertake digital work, reliable frameworks are needed to evaluate their real-world competence, adaptability, and capacity for human collaboration. Existing benchmarks remain largely static, synthetic, or domain-limited, providing limited insight into how agents perform in dynamic, economically meaningful environments. We introduce UpBench, a dynamically evolving benchmark grounded in real jobs drawn from the global Upwork labor marketplace. Each task corresponds to a verified client transaction, anchoring evaluation in genuine work activity and financial outcomes. UpBench employs a rubric-based evaluation framework, in which expert freelancers decompose each job into detailed, verifiable acceptance criteria and assess AI submissions with per-criterion feedback. This structure enables fine-grained analysis of model strengths, weaknesses, and instruction-following fidelity beyond binary pass/fail metrics. Human expertise is integrated throughout the data pipeline (from job curation and rubric construction to evaluation) ensuring fidelity to real professional standards and supporting research on human-AI collaboration. By regularly refreshing tasks to reflect the evolving nature of online work, UpBench provides a scalable, human-centered foundation for evaluating agentic systems in authentic labor-market contexts, offering a path toward a collaborative framework, where AI amplifies human capability through partnership rather than replacement.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>UpBench：面向以人为本的AI的动态演进真实劳动市场代理基准框架</div>
<div class="mono" style="margin-top:8px">随着大型语言模型（LLM）代理越来越多地承担数字工作，需要可靠的框架来评估其在现实世界中的能力、适应性和与人类协作的潜力。现有基准大多保持静态、合成或领域受限，难以提供对代理在动态、具有经济意义环境中的表现的深入洞察。我们引入UpBench，这是一个基于全球Upwork劳动市场的实际工作构建的动态演进基准。每个任务对应一个已验证的客户交易，将评估锚定在真实的工作活动和财务结果上。UpBench采用基于评分标准的评估框架，其中专家自由职业者将每个工作分解为详细的、可验证的验收标准，并对AI提交内容提供每项标准的反馈。这种结构使得能够超越二元通过/失败指标，对模型的优势、劣势和指令遵循的忠实度进行细致分析。人类专业知识贯穿于数据管道的各个环节（从工作筛选、评分标准构建到评估），确保与真实专业标准的一致性，并支持人类与AI协作的研究。通过定期更新任务以反映在线工作的演变特性，UpBench为在真实劳动市场环境中评估代理系统提供了可扩展、以人为本的基础，为构建一种协作框架提供了路径，其中AI通过合作而非替代来增强人类能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for UpBench stems from the need to evaluate large language model agents in real-world, dynamic labor-market scenarios, as current benchmarks are static or limited in scope. UpBench is built using real jobs from the Upwork marketplace, with each task based on verified client transactions, ensuring authenticity and alignment with actual work outcomes. The framework employs a rubric-based assessment system where expert freelancers define detailed acceptance criteria and provide per-criterion feedback on AI submissions, enabling a nuanced analysis of model performance. Experimental results show that UpBench effectively captures the complexity of real-world tasks and highlights the strengths and limitations of AI agents in terms of adaptability, competence, and collaboration with humans.</div>
<div class="mono" style="margin-top:8px">UpBench的开发动机源于对大型语言模型代理在真实动态劳动市场中表现评估的需求，因为现有基准多为静态或领域受限。UpBench基于Upwork全球劳动市场的真实工作构建，每个任务都与已验证的客户交易相关联，从而允许根据实际工作和财务结果进行评估。该框架采用基于评分标准的评估系统，由专家自由职业者将任务分解为详细的验收标准，并对AI提交物提供每项标准的反馈，实现对模型性能的细致分析。关键实验结果表明，UpBench能够有效反映真实工作场景的复杂性，并突出AI代理在适应性、能力及与人类协作方面的优缺点。</div>
</details>
</div>
<div class="card">
<div class="title">Sensitivity Analysis for Climate Science with Generative Flow Models</div>
<div class="meta-line">Authors: Alex Dobra, Jakiw Pidstrigach, Tim Reichelt, Paolo Fraccaro, Anne Jones, Johannes Jakubik, Christian Schroeder de Witt, Philip Torr, Philip Stier</div>
<div class="meta-line">First: 2025-11-01T18:57:01+00:00 · Latest: 2025-12-12T17:47:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2511.00663v3">Abs</a> · <a href="https://arxiv.org/pdf/2511.00663v3">PDF</a> · <a href="https://github.com/Kwartzl8/cbottle_adjoint_sensitivity">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Sensitivity analysis is a cornerstone of climate science, essential for understanding phenomena ranging from storm intensity to long-term climate feedbacks. However, computing these sensitivities using traditional physical models is often prohibitively expensive in terms of both computation and development time. While modern AI-based generative models are orders of magnitude faster to evaluate, computing sensitivities with them remains a significant bottleneck. This work addresses this challenge by applying the adjoint state method for calculating gradients in generative flow models. We apply this method to the cBottle generative model, trained on ERA5 and ICON data, to perform sensitivity analysis of any atmospheric variable with respect to sea surface temperatures. We quantitatively validate the computed sensitivities against the model&#x27;s own outputs. Our results provide initial evidence that this approach can produce reliable gradients, reducing the computational cost of sensitivity analysis from weeks on a supercomputer with a physical model to hours on a GPU, thereby simplifying a critical workflow in climate science. The code can be found at https://github.com/Kwartzl8/cbottle_adjoint_sensitivity.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>使用生成流模型进行气候科学的敏感性分析</div>
<div class="mono" style="margin-top:8px">敏感性分析是气候科学的基石，对于理解从风暴强度到长期气候反馈等现象至关重要。然而，使用传统物理模型计算这些敏感性在计算和开发时间上往往成本过高。尽管现代基于AI的生成模型在评估速度上快了几个数量级，但使用它们进行敏感性分析仍是一个显著的瓶颈。本工作通过应用伴随状态法来计算生成流模型中的梯度，以解决这一挑战。我们将该方法应用于基于ERA5和ICON数据训练的cBottle生成模型，以对任何大气变量相对于海面温度进行敏感性分析。我们通过模型自身的输出对计算出的敏感性进行了定量验证。我们的结果提供了初步证据，表明这种方法可以生成可靠的梯度，将使用物理模型在超级计算机上进行敏感性分析所需的时间从数周缩短到使用GPU的数小时，从而简化了气候科学中的关键工作流程。代码可在https://github.com/Kwartzl8/cbottle_adjoint_sensitivity获取。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Sensitivity analysis is crucial in climate science for understanding how changes in sea surface temperatures affect atmospheric variables. This study introduces the adjoint state method to compute gradients efficiently in generative flow models, specifically applying it to the cBottle model trained on ERA5 and ICON data. The method enables rapid sensitivity analysis, reducing computational time from weeks on a supercomputer to hours on a GPU, and is validated against the model&#x27;s own outputs, showing promising reliability.</div>
<div class="mono" style="margin-top:8px">本研究旨在通过使用生成流模型来提高气候科学中的敏感性分析效率，因为传统物理模型计算成本过高。研究应用了伴随状态法，对使用ERA5和ICON数据训练的cBottle生成模型进行处理，以计算大气变量相对于海面温度的梯度。结果表明，该方法能够可靠地计算敏感性，并将原本需要超级计算机数周时间的计算缩短至GPU上的数小时。</div>
</details>
</div>
<div class="card">
<div class="title">LUCID: Learning-Enabled Uncertainty-Aware Certification of Stochastic Dynamical Systems</div>
<div class="meta-line">Authors: Ernesto Casablanca, Oliver Schön, Paolo Zuliani, Sadegh Soudjani</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-12-12T17:46:50+00:00 · Latest: 2025-12-12T17:46:50+00:00</div>
<div class="meta-line">Comments: The manuscript has been accepted for publication in the main track of AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11750v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11750v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Ensuring the safety of AI-enabled systems, particularly in high-stakes domains such as autonomous driving and healthcare, has become increasingly critical. Traditional formal verification tools fall short when faced with systems that embed both opaque, black-box AI components and complex stochastic dynamics. To address these challenges, we introduce LUCID (Learning-enabled Uncertainty-aware Certification of stochastIc Dynamical systems), a verification engine for certifying safety of black-box stochastic dynamical systems from a finite dataset of random state transitions. As such, LUCID is the first known tool capable of establishing quantified safety guarantees for such systems. Thanks to its modular architecture and extensive documentation, LUCID is designed for easy extensibility. LUCID employs a data-driven methodology rooted in control barrier certificates, which are learned directly from system transition data, to ensure formal safety guarantees. We use conditional mean embeddings to embed data into a reproducing kernel Hilbert space (RKHS), where an RKHS ambiguity set is constructed that can be inflated to robustify the result to out-of-distribution behavior. A key innovation within LUCID is its use of a finite Fourier kernel expansion to reformulate a semi-infinite non-convex optimization problem into a tractable linear program. The resulting spectral barrier allows us to leverage the fast Fourier transform to generate the relaxed problem efficiently, offering a scalable yet distributionally robust framework for verifying safety. LUCID thus offers a robust and efficient verification framework, able to handle the complexities of modern black-box systems while providing formal guarantees of safety. These unique capabilities are demonstrated on challenging benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LUCID：基于学习的不确定性感知的随机动态系统安全性认证</div>
<div class="mono" style="margin-top:8px">确保AI赋能系统的安全性，特别是在自动驾驶和医疗等高风险领域，变得越来越关键。传统的形式化验证工具在处理同时包含不透明黑盒AI组件和复杂随机动态的系统时存在局限。为应对这些挑战，我们引入了LUCID（Learning-enabled Uncertainty-aware Certification of Stochastic Dynamical Systems），这是一种用于从有限的随机状态转移数据集验证黑盒随机动态系统安全性的验证引擎。因此，LUCID是目前已知的第一个能够为这类系统建立量化安全保证的工具。得益于其模块化架构和详尽的文档，LUCID被设计为易于扩展。LUCID采用基于控制障碍证书的数据驱动方法，直接从系统转移数据中学习，以确保形式化安全保证。我们使用条件均值嵌入将数据嵌入到再生核希尔伯特空间（RKHS）中，在该空间中构建了一个RKHS模糊集，可以被扩展以增强对分布外行为的鲁棒性。LUCID的一个关键创新是使用有限傅里叶核展开，将一个半无限非凸优化问题转化为可处理的线性规划问题。由此产生的谱障碍证书使我们能够利用快速傅里叶变换高效生成松弛问题，从而提供一个可扩展且分布鲁棒的安全性验证框架。因此，LUCID提供了一种鲁棒且高效的验证框架，能够处理现代黑盒系统的复杂性，同时提供形式化安全保证。这些独特的能力在具有挑战性的基准测试中得到了验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for LUCID arises from the need to ensure safety in AI-enabled systems with both black-box components and stochastic dynamics, which traditional verification methods cannot handle effectively. LUCID employs a data-driven approach based on control barrier certificates, learning them from a finite dataset of random state transitions. It uses conditional mean embeddings to construct an ambiguity set in a reproducing kernel Hilbert space, which is then expanded using a finite Fourier kernel to transform a non-convex optimization problem into a linear program. This allows for efficient and scalable verification of safety guarantees against out-of-distribution behaviors. The key experimental results demonstrate that LUCID successfully provides formal safety certification for complex stochastic systems, showcasing its robustness and effectiveness in real-world applications.</div>
<div class="mono" style="margin-top:8px">LUCID的动机是解决传统形式化验证方法在确保包含黑箱组件和随机动态的AI系统安全性方面的不足。LUCID采用基于控制障碍证书的数据驱动方法，从有限的随机状态转移数据中学习这些证书。它利用条件均值嵌入将数据映射到再生核希尔伯特空间，并通过有限傅里叶核扩展将非凸优化问题转化为可处理的线性规划问题。这种方法使得对分布外行为的安全性验证变得高效且可扩展。实验结果表明，LUCID能够为复杂的随机系统提供量化的安全性保证，在现实世界基准测试中展现出其鲁棒性和有效性。</div>
</details>
</div>
<div class="card">
<div class="title">SVG-T2I: Scaling Up Text-to-Image Latent Diffusion Model Without Variational Autoencoder</div>
<div class="meta-line">Authors: Minglei Shi, Haolin Wang, Borui Zhang, Wenzhao Zheng, Bohan Zeng, Ziyang Yuan, Xiaoshi Wu, Yuanxing Zhang, Huan Yang, Xintao Wang, Pengfei Wan, Kun Gai, Jie Zhou, Jiwen Lu</div>
<div class="meta-line">First: 2025-12-12T17:45:03+00:00 · Latest: 2025-12-12T17:45:03+00:00</div>
<div class="meta-line">Comments: Code Repository: https://github.com/KlingTeam/SVG-T2I; Model Weights: https://huggingface.co/KlingTeam/SVG-T2I</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11749v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11749v1">PDF</a> · <a href="https://github.com/KlingTeam/SVG-T2I">Code1</a> · <a href="https://huggingface.co/KlingTeam/SVG-T2I">Code2</a> · <a href="https://huggingface.co/huggingface">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Visual generation grounded in Visual Foundation Model (VFM) representations offers a highly promising unified pathway for integrating visual understanding, perception, and generation. Despite this potential, training large-scale text-to-image diffusion models entirely within the VFM representation space remains largely unexplored. To bridge this gap, we scale the SVG (Self-supervised representations for Visual Generation) framework, proposing SVG-T2I to support high-quality text-to-image synthesis directly in the VFM feature domain. By leveraging a standard text-to-image diffusion pipeline, SVG-T2I achieves competitive performance, reaching 0.75 on GenEval and 85.78 on DPG-Bench. This performance validates the intrinsic representational power of VFMs for generative tasks. We fully open-source the project, including the autoencoder and generation model, together with their training, inference, evaluation pipelines, and pre-trained weights, to facilitate further research in representation-driven visual generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SVG-T2I：在视觉基础模型表示空间中无需变分自编码器扩展文本到图像扩散模型</div>
<div class="mono" style="margin-top:8px">基于视觉基础模型（VFM）表示的视觉生成提供了一条极具前景的统一路径，用于整合视觉理解、感知和生成。尽管具有这种潜力，但完全在VFM表示空间内训练大规模文本到图像扩散模型仍鲜有探索。为弥合这一差距，我们扩展了SVG（用于视觉生成的自监督表示）框架，提出SVG-T2I以支持在VFM特征域内直接进行高质量的文本到图像合成。通过利用标准的文本到图像扩散流程，SVG-T2I实现了具有竞争力的性能，在GenEval上达到0.75，在DPG-Bench上达到85.78。这一性能验证了VFM在生成任务中的内在表示能力。我们完全开源了该项目，包括自编码器和生成模型，以及它们的训练、推理和评估流程和预训练权重，以促进基于表示的视觉生成进一步研究。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The research aims to explore the potential of Visual Foundation Model (VFM) representations for training large-scale text-to-image diffusion models without relying on variational autoencoders. The proposed method, SVG-T2I, extends the SVG framework to operate directly within the VFM feature domain using a standard text-to-image diffusion pipeline. Experimental results show that SVG-T2I achieves competitive performance with scores of 0.75 on GenEval and 85.78 on DPG-Bench, demonstrating the effectiveness of VFM representations for text-to-image synthesis tasks.</div>
<div class="mono" style="margin-top:8px">本研究的动机是探索视觉基础模型（VFM）表示在训练大规模文本到图像扩散模型中的潜力，无需依赖变分自编码器。所提出的方法SVG-T2I扩展了SVG框架，使其能够在VFM特征域内直接进行高质量的文本到图像合成。实验结果表明，SVG-T2I在GenEval上达到0.75，在DPG-Bench上达到85.78，验证了VFM表示在生成任务中的内在表现力。</div>
</details>
</div>
<div class="card">
<div class="title">Generative Parametric Design (GPD): A framework for real-time geometry generation and on-the-fly multiparametric approximation</div>
<div class="meta-line">Authors: Mohammed El Fallaki Idrissi, Jad Mounayer, Sebastian Rodriguez, Fodil Meraghni, Francisco Chinesta</div>
<div class="meta-line">First: 2025-12-12T17:44:38+00:00 · Latest: 2025-12-12T17:44:38+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11748v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11748v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper presents a novel paradigm in simulation-based engineering sciences by introducing a new framework called Generative Parametric Design (GPD). The GPD framework enables the generation of new designs along with their corresponding parametric solutions given as a reduced basis. To achieve this, two Rank Reduction Autoencoders (RRAEs) are employed, one for encoding and generating the design or geometry, and the other for encoding the sparse Proper Generalized Decomposition (sPGD) mode solutions. These models are linked in the latent space using regression techniques, allowing efficient transitions between design and their associated sPGD modes. By empowering design exploration and optimization, this framework also advances digital and hybrid twin development, enhancing predictive modeling and real-time decision-making in engineering applications. The developed framework is demonstrated on two-phase microstructures, in which the multiparametric solutions account for variations in two key material parameters.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>生成参数化设计（GPD）：一种用于实时几何生成和即时多参数逼近的框架</div>
<div class="mono" style="margin-top:8px">本文通过引入一种名为生成参数化设计（GPD）的新框架，提出了模拟工程科学中的新范式。GPD框架能够在给定一个降维基底的情况下，生成新的设计及其对应的参数化解。为实现这一目标，采用了两个秩降维自编码器（RRAEs），一个用于编码和生成设计或几何，另一个用于编码稀疏广义分解（sPGD）模式解。这些模型通过回归技术在潜在空间中相互连接，从而实现设计与其相关sPGD模式之间的高效转换。该框架不仅增强了设计探索和优化能力，还推动了数字孪生和混合孪生的发展，提升了工程应用中的预测建模和实时决策能力。该框架在两相微结构中进行了演示，其中多参数解考虑了两个关键材料参数的变化。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">This paper introduces the Generative Parametric Design (GPD) framework to enable real-time geometry generation and multiparametric approximation in simulation-based engineering sciences. The framework utilizes two Rank Reduction Autoencoders (RRAEs): one for generating geometries and another for encoding sparse Proper Generalized Decomposition (sPGD) mode solutions. These models are connected in the latent space through regression techniques, facilitating efficient transitions between design and their corresponding sPGD modes. The framework is validated on two-phase microstructures, demonstrating its ability to account for variations in two key material parameters through multiparametric solutions.</div>
<div class="mono" style="margin-top:8px">本文提出了一种称为生成参数化设计（GPD）的框架，以提升基于仿真的工程科学，实现几何实时生成和多参数近似。该框架采用两个秩减少自动编码器（RRAEs），一个用于生成几何形状，另一个用于编码稀疏的广义分解（sPGD）模式解。这两个模型通过回归技术在潜在空间中连接，从而高效地在设计与其对应的sPGD模式之间进行转换。该框架在两相微观结构上进行了验证，展示了其在捕捉两个关键材料参数变化方面的能力。</div>
</details>
</div>
<div class="card">
<div class="title">mViSE: A Visual Search Engine for Analyzing Multiplex IHC Brain Tissue Images</div>
<div class="meta-line">Authors: Liqiang Huang, Rachel W. Mills, Saikiran Mandula, Lin Bai, Mahtab Jeyhani, John Redell, Hien Van Nguyen, Saurabh Prasad, Dragan Maric, Badrinath Roysam</div>
<div class="meta-line">First: 2025-12-12T17:39:54+00:00 · Latest: 2025-12-12T17:39:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11745v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11745v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Whole-slide multiplex imaging of brain tissue generates massive information-dense images that are challenging to analyze and require custom software. We present an alternative query-driven programming-free strategy using a multiplex visual search engine (mViSE) that learns the multifaceted brain tissue chemoarchitecture, cytoarchitecture, and myeloarchitecture. Our divide-and-conquer strategy organizes the data into panels of related molecular markers and uses self-supervised learning to train a multiplex encoder for each panel with explicit visual confirmation of successful learning. Multiple panels can be combined to process visual queries for retrieving similar communities of individual cells or multicellular niches using information-theoretic methods. The retrievals can be used for diverse purposes including tissue exploration, delineating brain regions and cortical cell layers, profiling and comparing brain regions without computer programming. We validated mViSE&#x27;s ability to retrieve single cells, proximal cell pairs, tissue patches, delineate cortical layers, brain regions and sub-regions. mViSE is provided as an open-source QuPath plug-in.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>mViSE：用于分析多色IHC脑组织图像的视觉搜索引擎</div>
<div class="mono" style="margin-top:8px">全片多色成像技术生成大量信息密集型图像，分析这些图像具有挑战性，需要定制软件。我们提出了一种基于查询驱动且无需编程的替代策略，使用多色视觉搜索引擎（mViSE）来学习脑组织的多维化学结构、细胞结构和髓鞘结构。我们的分而治之策略将数据组织成相关分子标记的面板，并通过自监督学习为每个面板训练多色编码器，同时通过显式的视觉确认确保学习成功。多个面板可以结合使用，通过信息论方法处理视觉查询，以检索相似的单细胞或多细胞微环境。检索结果可用于多种目的，包括组织探索、界定脑区和皮层细胞层、在无需计算机编程的情况下对脑区进行分析和比较。我们验证了mViSE检索单细胞、邻近细胞对、组织斑块、界定皮层层、脑区及其子区域的能力。mViSE作为开源的QuPath插件提供。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation for developing mViSE stems from the challenges of analyzing complex, information-dense whole-slide multiplex IHC brain tissue images, which require specialized software. mViSE employs a query-driven, programming-free approach by learning the chemoarchitecture, cytoarchitecture, and myeloarchitecture of brain tissue through a divide-and-conquer strategy that organizes molecular markers into panels. Each panel is used to train a self-supervised multiplex encoder with visual confirmation of learning. The system enables the retrieval of similar cellular communities or multicellular niches by combining multiple panels and applying information-theoretic methods. Experimental results demonstrate mViSE&#x27;s effectiveness in retrieving single cells, cell pairs, tissue patches, and delineating cortical layers and brain regions without requiring computer programming.</div>
<div class="mono" style="margin-top:8px">开发mViSE的动机源于分析复杂且信息密集的全切片多色IHC脑组织图像所面临的挑战。该方法采用一种基于查询、无需编程的视觉搜索引擎，通过分子标记面板的自监督学习来掌握脑组织的化学结构、细胞结构和髓鞘结构。实验结果表明，mViSE能够有效检索单个细胞、邻近细胞对和组织斑块，并能界定皮层层和脑区，无需编程即可实现这些功能。</div>
</details>
</div>
<div class="card">
<div class="title">REASONING COMPILER: LLM-Guided Optimizations for Efficient Model Serving</div>
<div class="meta-line">Authors: Annabelle Sujun Tang, Christopher Priebe, Rohan Mahapatra, Lianhui Qin, Hadi Esmaeilzadeh</div>
<div class="meta-line">Venue: NeurIPS 2025</div>
<div class="meta-line">First: 2025-06-02T07:02:46+00:00 · Latest: 2025-12-12T17:38:28+00:00</div>
<div class="meta-line">Comments: NeurIPS 2025</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.01374v4">Abs</a> · <a href="https://arxiv.org/pdf/2506.01374v4">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While model serving has unlocked unprecedented capabilities, the high cost of serving large-scale models continues to be a significant barrier to widespread accessibility and rapid innovation. Compiler optimizations have long driven substantial performance improvements, but existing compilers struggle with neural workloads due to the exponentially large and highly interdependent space of possible transformations. Although existing stochastic search techniques can be effective, they are often sample-inefficient and fail to leverage the structural context underlying compilation decisions. We set out to investigate the research question of whether reasoning with large language models (LLMs), without any retraining, can leverage the context-aware decision space of compiler optimizations to significantly improve sample efficiency. To that end, we introduce a novel compilation framework (dubbed Reasoning Compiler) that formulates optimization as a sequential, context-aware decision process guided by a large language model and structured Monte Carlo tree search (MCTS). The LLM acts as a proposal mechanism, suggesting hardware-informed transformations that reflect the current program state and accumulated performance feedback. MCTS incorporates the LLM-generated proposals to balance exploration and exploitation, facilitating structured, context-sensitive traversal of the expansive compiler optimization space. By achieving substantial speedups with markedly fewer samples than leading neural compilers, our approach demonstrates the potential of LLM-guided reasoning to transform the landscape of compiler optimization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>推理编译器：基于大语言模型的高效模型服务优化</div>
<div class="mono" style="margin-top:8px">尽管模型服务已经释放了前所未有的能力，但大规模模型的服务成本仍然是广泛可访问性和快速创新的重要障碍。编译器优化长期以来推动了显著的性能提升，但现有的编译器在处理神经网络工作负载时面临挑战，因为可能的转换空间呈指数级增长且高度相互依赖。虽然现有的随机搜索技术可以有效，但它们通常样本效率低下，未能利用编译决策背后的结构化上下文。我们旨在探讨一个研究问题：是否可以在不进行任何重新训练的情况下，利用大语言模型（LLMs）进行推理，以利用编译器优化的上下文感知决策空间，显著提高样本效率。为此，我们提出了一种新颖的编译框架（称为推理编译器），将优化过程建模为一个由大语言模型和结构化蒙特卡洛树搜索（MCTS）引导的顺序、上下文感知的决策过程。LLM充当建议机制，提出反映当前程序状态和累积性能反馈的硬件感知转换。MCTS通过整合LLM生成的建议来平衡探索与利用，从而促进对广阔编译器优化空间的结构化、上下文敏感的遍历。与领先的神经编译器相比，我们的方法通过显著减少样本数量实现了显著的速度提升，展示了基于LLM的推理在编译器优化领域中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to address the high cost of serving large-scale models, which limits their accessibility and innovation. The study introduces a novel compilation framework called the Reasoning Compiler, which uses a large language model (LLM) and structured Monte Carlo tree search (MCTS) to guide compiler optimizations. The LLM suggests hardware-informed transformations based on the current program state and performance feedback, while MCTS balances exploration and exploitation of these proposals. Experimental results show that this approach achieves significant speedups with fewer samples compared to existing neural compilers, demonstrating the effectiveness of LLM-guided reasoning in improving sample efficiency for model serving.</div>
<div class="mono" style="margin-top:8px">本研究的动机是解决大规模模型服务的高昂成本问题，这限制了其普及性和创新性。论文提出了一种名为Reasoning Compiler的新编译框架，利用大型语言模型（LLM）和结构化蒙特卡洛树搜索（MCTS）来指导编译器优化。LLM根据当前程序状态和性能反馈提出硬件相关的转换建议，而MCTS则平衡探索与利用，以高效地遍历优化空间。实验结果表明，该方法在比现有神经编译器更少的样本数下实现了显著的速度提升，展示了LLM引导推理在提高样本效率方面在编译器优化中的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">CogniSNN: Enabling Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability with Random Graph Architectures in Spiking Neural Networks</div>
<div class="meta-line">Authors: Yongsheng Huang, Peibo Duan, Yujie Wu, Kai Sun, Zhipeng Liu, Changsheng Zhang, Bin Zhang, Mingkun Xu</div>
<div class="meta-line">First: 2025-12-12T17:36:31+00:00 · Latest: 2025-12-12T17:36:31+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.11743v1">Abs</a> · <a href="https://arxiv.org/pdf/2512.11743v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Spiking neural networks (SNNs), regarded as the third generation of artificial neural networks, are expected to bridge the gap between artificial intelligence and computational neuroscience. However, most mainstream SNN research directly adopts the rigid, chain-like hierarchical architecture of traditional artificial neural networks (ANNs), ignoring key structural characteristics of the brain. Biological neurons are stochastically interconnected, forming complex neural pathways that exhibit Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability. In this paper, we introduce a new SNN paradigm, named Cognition-aware SNN (CogniSNN), by incorporating Random Graph Architecture (RGA). Furthermore, we address the issues of network degradation and dimensional mismatch in deep pathways by introducing an improved pure spiking residual mechanism alongside an adaptive pooling strategy. Then, we design a Key Pathway-based Learning without Forgetting (KP-LwF) approach, which selectively reuses critical neural pathways while retaining historical knowledge, enabling efficient multi-task transfer. Finally, we propose a Dynamic Growth Learning (DGL) algorithm that allows neurons and synapses to grow dynamically along the internal temporal dimension. Extensive experiments demonstrate that CogniSNN achieves performance comparable to, or even surpassing, current state-of-the-art SNNs on neuromorphic datasets and Tiny-ImageNet. The Pathway-Reusability enhances the network&#x27;s continuous learning capability across different scenarios, while the dynamic growth algorithm improves robustness against interference and mitigates the fixed-timestep constraints during neuromorphic chip deployment. This work demonstrates the potential of SNNs with random graph structures in advancing brain-inspired intelligence and lays the foundation for their practical application on neuromorphic hardware.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>CogniSNN：基于随机图架构的脉冲神经网络实现神经可扩展性、路径可复用性和动态可配置性</div>
<div class="mono" style="margin-top:8px">脉冲神经网络（SNNs）被视为第三代人工神经网络，有望弥合人工智能与计算神经科学之间的差距。然而，大多数主流SNN研究直接采用传统人工神经网络（ANNs）中刚性的链式分层架构，忽略了大脑的关键结构特征。生物神经元是随机互连的，形成复杂的神经路径，表现出神经可扩展性、路径可复用性和动态可配置性。本文引入了一种新的SNN范式，称为认知感知SNN（CogniSNN），通过结合随机图架构（RGA）。此外，我们通过引入改进的纯脉冲残差机制和自适应池化策略，解决了深层路径中的网络退化和维度不匹配问题。随后，我们设计了一种基于关键路径的学习无遗忘（KP-LwF）方法，该方法在保留历史知识的同时，选择性地复用关键神经路径，从而实现高效的多任务迁移。最后，我们提出了一种动态增长学习（DGL）算法，允许神经元和突触在内部时间维度上动态增长。大量实验表明，CogniSNN在神经形态数据集和Tiny-ImageNet上实现了与当前最先进的SNN相当甚至更优的性能。路径可复用性增强了网络在不同场景下的持续学习能力，而动态增长算法则提高了对干扰的鲁棒性，并缓解了在神经形态芯片部署过程中固定时间步的限制。本工作展示了具有随机图结构的SNN在推动脑启发智能方面的潜力，并为其在神经形态硬件上的实际应用奠定了基础。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The motivation of this research is to develop Spiking Neural Networks (SNNs) that better mimic the structural and functional characteristics of biological neural systems, such as Neuron-Expandability, Pathway-Reusability, and Dynamic-Configurability. The authors propose CogniSNN, a novel SNN paradigm that incorporates Random Graph Architecture (RGA) to enable more flexible and biologically plausible connectivity. To address challenges in deep SNNs, they introduce an improved pure spiking residual mechanism and adaptive pooling strategy, and design a Key Pathway-based Learning without Forgetting (KP-LwF) approach for efficient multi-task learning. The Dynamic Growth Learning (DGL) algorithm allows neurons and synapses to grow dynamically over time. Experimental results show that CogniSNN achieves performance comparable to or better than existing state-of-the-art SNNs on neuromorphic datasets and Tiny-ImageNet, with enhanced continuous learning and robustness against interference.</div>
<div class="mono" style="margin-top:8px">本文提出了一种名为CogniSNN的新颖脉冲神经网络范式，通过引入随机图架构（RGA）来更贴近生物神经元的随机互联特性。为了解决深层路径学习中的问题，作者提出了改进的纯脉冲残差机制和自适应池化策略。此外，他们设计了基于关键路径的学习无遗忘（KP-LwF）方法和动态生长学习（DGL）算法，以实现高效的多任务学习和动态神经元-突触生长。实验结果表明，CogniSNN在神经形态数据集和Tiny-ImageNet上的表现与现有最先进的SNN相当甚至更优，展示了其在连续学习和抗干扰方面的增强能力。</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
