<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-28 04:07</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260228_0407</div>
    <div class="row"><div class="card">
<div class="title">MediX-R1: Open Ended Medical Reinforcement Learning</div>
<div class="meta-line">Authors: Sahal Shaji Mullappilly, Mohammed Irfan Kurpath, Omair Mohamed, Mohamed Zidan, Fahad Khan, Salman Khan, Rao Anwer, Hisham Cholakkal</div>
<div class="meta-line">First: 2026-02-26T18:59:46+00:00 · Latest: 2026-02-26T18:59:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23363v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23363v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce MediX-R1, an open-ended Reinforcement Learning (RL) framework for medical multimodal large language models (MLLMs) that enables clinically grounded, free-form answers beyond multiple-choice formats. MediX-R1 fine-tunes a baseline vision-language backbone with Group Based RL and a composite reward tailored for medical reasoning: an LLM-based accuracy reward that judges semantic correctness with a strict YES/NO decision, a medical embedding-based semantic reward to capture paraphrases and terminology variants, and lightweight format and modality rewards that enforce interpretable reasoning and modality recognition. This multi-signal design provides stable, informative feedback for open-ended outputs where traditional verifiable or MCQ-only rewards fall short. To measure progress, we propose a unified evaluation framework for both text-only and image+text tasks that uses a Reference-based LLM-as-judge in place of brittle string-overlap metrics, capturing semantic correctness, reasoning, and contextual alignment. Despite using only $\sim51$K instruction examples, MediX-R1 achieves excellent results across standard medical LLM (text-only) and VLM (image + text) benchmarks, outperforming strong open-source baselines and delivering particularly large gains on open-ended clinical tasks. Our results demonstrate that open-ended RL with comprehensive reward signals and LLM-based evaluation is a practical path toward reliable medical reasoning in multimodal models. Our trained models, curated datasets and source code are available at https://medix.cvmbzuai.com</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MediX-R1：开放式的医疗强化学习</div>
<div class="mono" style="margin-top:8px">我们引入了MediX-R1，这是一个用于医疗多模态大语言模型（MLLMs）的开放式强化学习（RL）框架，能够生成超出多项选择格式的临床依据、自由形式的答案。MediX-R1通过基于组的强化学习和针对医疗推理设计的综合奖励对基线视觉-语言模型进行微调：包括一个基于大语言模型的准确性奖励，用于严格判断语义正确性（YES/NO决策）；一个基于医疗嵌入的语义奖励，用于捕捉同义表达和术语变体；以及轻量级的格式和模态奖励，以确保可解释推理和模态识别。这种多信号设计为开放式输出提供了稳定且信息丰富的反馈，弥补了传统可验证或仅MCQ奖励机制的不足。为了衡量进展，我们提出了一种统一的评估框架，适用于纯文本和图像+文本任务，该框架使用基于参考的LLM作为裁判，替代了脆弱的字符串重叠度量，从而捕捉语义正确性、推理能力和上下文对齐。尽管仅使用约51,000个指令示例，MediX-R1在标准医疗LLM（纯文本）和VLM（图像+文本）基准测试中均取得了优异的成绩，优于强大的开源基线模型，并在开放式临床任务中表现出特别显著的提升。我们的结果表明，结合全面奖励信号和基于LLM的评估的开放式强化学习，是实现多模态模型中可靠医疗推理的可行路径。我们的训练模型、精选数据集和源代码可在https://medix.cvmbzuai.com获取。</div>
</details>
</div>
<div class="card">
<div class="title">Joint Optimization for 4D Human-Scene Reconstruction in the Wild</div>
<div class="meta-line">Authors: Zhizheng Liu, Joe Lin, Wayne Wu, Bolei Zhou</div>
<div class="meta-line">First: 2025-01-04T01:53:51+00:00 · Latest: 2026-02-26T18:59:39+00:00</div>
<div class="meta-line">Comments: Project Page: https://vail-ucla.github.io/JOSH/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2501.02158v2">Abs</a> · <a href="https://arxiv.org/pdf/2501.02158v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://vail-ucla.github.io/JOSH/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Reconstructing human motion and its surrounding environment is crucial for understanding human-scene interaction and predicting human movements in the scene. While much progress has been made in capturing human-scene interaction in constrained environments, those prior methods can hardly reconstruct the natural and diverse human motion and scene context from web videos. In this work, we propose JOSH, a novel optimization-based method for 4D human-scene reconstruction in the wild from monocular videos. JOSH uses techniques in both dense scene reconstruction and human mesh recovery as initialization, and then it leverages the human-scene contact constraints to jointly optimize the scene, the camera poses, and the human motion. Experiment results show JOSH achieves better results on both global human motion estimation and dense scene reconstruction by joint optimization of scene geometry and human motion. We further design a more efficient model, JOSH3R, and directly train it with pseudo-labels from web videos. JOSH3R outperforms other optimization-free methods by only training with labels predicted from JOSH, further demonstrating its accuracy and generalization ability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>野外单目视频中4D人-场景联合重建方法</div>
<div class="mono" style="margin-top:8px">重建人体运动及其周围环境对于理解人-场景交互和预测场景中的人体运动至关重要。尽管在受控环境中捕捉人-场景交互已取得显著进展，但这些方法难以从网络视频中重建自然且多样的人体运动和场景上下文。本文提出JOSH，一种基于优化的新型方法，用于从单目视频中进行野外4D人-场景重建。JOSH结合了密集场景重建和人体网格恢复技术作为初始化手段，随后利用人-场景接触约束联合优化场景、相机姿态和人体运动。实验结果表明，通过联合优化场景几何和人体运动，JOSH在全局人体运动估计和密集场景重建方面均取得了更好的效果。我们进一步设计了一个更高效的模型JOSH3R，并直接使用网络视频中的伪标签对其进行训练。JOSH3R仅通过JOSH预测的标签进行训练，就优于其他无需优化的方法，进一步验证了其准确性和泛化能力。</div>
</details>
</div>
<div class="card">
<div class="title">Long-range nonstabilizerness and quantum codes, phases, and complexity</div>
<div class="meta-line">Authors: Fuchuan Wei, Zi-Wen Liu</div>
<div class="meta-line">First: 2025-03-06T15:53:59+00:00 · Latest: 2026-02-26T18:59:37+00:00</div>
<div class="meta-line">Comments: 41 pages, 5 figures. Substantially revised and expanded. Comments are welcome</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2503.04566v3">Abs</a> · <a href="https://arxiv.org/pdf/2503.04566v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As a necessary resource for quantum computational advantage, quantum magic (nonstabilizerness) is of fundamental importance in the study of quantum computation and physics. We develop a systematic theory of \emph{long-range magic (LRM)} -- nonstabilizerness that cannot be erased by shallow unitary circuits -- and demonstrate its broad relevance. By bridging LRM with fault-tolerant logical gate theory, we show the emergence of LRM families from quantum error-correcting codes and devise a simple yet powerful method for testing transversal logical gates. Further, we introduce and characterize \emph{LRM phases} in which all ground states exhibit LRM, and identify certain non-Abelian topological orders as representative examples. Then, adopting a complexity theory perspective, we demonstrate the classicality of non-LRM systems in e.g.~preparation and learning settings, and present a ``no low-energy trivial magic&#x27;&#x27; (NLTM) conjecture with key motivation in the quantum PCP context, for which our LRM results suggest a promising route. Additionally, we demonstrate how to diagnose LRM with correlation functions. The concepts and results admit substantive extensions to approximate (robust) and nongeometric scenarios. Our LRM theory illuminates profound new connections among quantum resources, computational advantage, error correction and fault tolerance, many-body physics, and complexity theory.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>长程非稳定化性和量子码、相位与复杂性</div>
<div class="mono" style="margin-top:8px">作为量子计算优势的必要资源，量子魔法（非稳定化性）在量子计算和物理学研究中具有根本性的重要性。我们发展了一套关于\emph{长程魔法（LRM）}的系统理论——一种无法被浅层酉电路消除的非稳定化性，并展示了其广泛的相关性。通过将LRM与容错逻辑门理论联系起来，我们展示了LRM家族如何从量子纠错码中出现，并提出了一种简单而强大的方法来测试跨域逻辑门。此外，我们引入并刻画了\emph{LRM相位}，其中所有基态都表现出LRM，并识别了某些非阿贝尔拓扑序作为代表性例子。接着，从复杂性理论的角度出发，我们展示了非LRM系统的经典性，例如在制备和学习场景中，并提出了一个具有关键动机的“无低能平凡魔法”（NLTM）猜想，我们的LRM结果为该猜想提供了有希望的途径。此外，我们还展示了如何利用关联函数来诊断LRM。这些概念和结果可以扩展到近似（鲁棒）和非几何场景。我们的LRM理论揭示了量子资源、计算优势、纠错与容错、多体物理和复杂性理论之间新的深刻联系。</div>
</details>
</div>
<div class="card">
<div class="title">VGG-T$^3$: Offline Feed-Forward 3D Reconstruction at Scale</div>
<div class="meta-line">Authors: Sven Elflein, Ruilong Li, Sérgio Agostinho, Zan Gojcic, Laura Leal-Taixé, Qunjie Zhou, Aljosa Osep</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-02-26T18:59:33+00:00 · Latest: 2026-02-26T18:59:33+00:00</div>
<div class="meta-line">Comments: CVPR 2026, Project page: https://research.nvidia.com/labs/dvl/projects/vgg-ttt</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23361v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23361v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://research.nvidia.com/labs/dvl/projects/vgg-ttt">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We present a scalable 3D reconstruction model that addresses a critical limitation in offline feed-forward methods: their computational and memory requirements grow quadratically w.r.t. the number of input images. Our approach is built on the key insight that this bottleneck stems from the varying-length Key-Value (KV) space representation of scene geometry, which we distill into a fixed-size Multi-Layer Perceptron (MLP) via test-time training. VGG-T$^3$ (Visual Geometry Grounded Test Time Training) scales linearly w.r.t. the number of input views, similar to online models, and reconstructs a $1k$ image collection in just $54$ seconds, achieving a $11.6\times$ speed-up over baselines that rely on softmax attention. Since our method retains global scene aggregation capability, our point map reconstruction error outperforming other linear-time methods by large margins. Finally, we demonstrate visual localization capabilities of our model by querying the scene representation with unseen images.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>VGG-T$^3$: 大规模离线前馈三维重建</div>
<div class="mono" style="margin-top:8px">我们提出了一种可扩展的三维重建模型，解决了离线前馈方法中的一个关键限制：其计算和内存需求随着输入图像数量呈二次增长。我们的方法基于一个关键洞察，即这一瓶颈源于场景几何的变长键值（KV）空间表示，我们通过测试时训练将其蒸馏为固定大小的多层感知机（MLP）。VGG-T$^3$（基于视觉几何的测试时训练）在输入视角数量上呈线性扩展，与在线模型类似，并能在54秒内重建一个$1k$图像集合，比依赖softmax注意力的基线方法快$11.6\times$。由于我们的方法保留了全局场景聚合能力，因此在点云重建误差上显著优于其他线性时间方法。最后，我们通过使用未见过的图像查询场景表示，展示了模型的视觉定位能力。</div>
</details>
</div>
<div class="card">
<div class="title">Model Agreement via Anchoring</div>
<div class="meta-line">Authors: Eric Eaton, Surbhi Goel, Marcel Hussing, Michael Kearns, Aaron Roth, Sikata Bela Sengupta, Jessica Sorrell</div>
<div class="meta-line">First: 2026-02-26T18:59:32+00:00 · Latest: 2026-02-26T18:59:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23360v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23360v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Numerous lines of aim to control $\textit{model disagreement}$ -- the extent to which two machine learning models disagree in their predictions. We adopt a simple and standard notion of model disagreement in real-valued prediction problems, namely the expected squared difference in predictions between two models trained on independent samples, without any coordination of the training processes. We would like to be able to drive disagreement to zero with some natural parameter(s) of the training procedure using analyses that can be applied to existing training methodologies.
  We develop a simple general technique for proving bounds on independent model disagreement based on $\textit{anchoring}$ to the average of two models within the analysis. We then apply this technique to prove disagreement bounds for four commonly used machine learning algorithms: (1) stacked aggregation over an arbitrary model class (where disagreement is driven to 0 with the number of models $k$ being stacked) (2) gradient boosting (where disagreement is driven to 0 with the number of iterations $k$) (3) neural network training with architecture search (where disagreement is driven to 0 with the size $n$ of the architecture being optimized over) and (4) regression tree training over all regression trees of fixed depth (where disagreement is driven to 0 with the depth $d$ of the tree architecture). For clarity, we work out our initial bounds in the setting of one-dimensional regression with squared error loss -- but then show that all of our results generalize to multi-dimensional regression with any strongly convex loss.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过锚定实现模型协议</div>
<div class="mono" style="margin-top:8px">我们旨在控制模型分歧——两个机器学习模型在预测上不一致的程度。我们采用一个简单且标准的模型分歧定义，即在实值预测问题中，两个基于独立样本训练的模型预测之间的期望平方差异。我们希望利用可以应用于现有训练方法的分析，通过一些自然的训练参数将分歧驱动至零。我们开发了一种简单的通用技术，基于在分析中对两个模型平均值的锚定，来证明独立模型分歧的界限。然后，我们将该技术应用于四种常用的机器学习算法，证明其分歧界限：(1) 在任意模型类上进行堆叠聚合（分歧随堆叠模型数量 $k$ 趋于零）；(2) 梯度提升（分歧随迭代次数 $k$ 趋于零）；(3) 带有架构搜索的神经网络训练（分歧随优化的架构大小 $n$ 趋于零）；(4) 在固定深度的所有回归树上进行回归树训练（分歧随树的深度 $d$ 趋于零）。为清晰起见，我们首先在一维回归且使用平方误差损失的场景下推导初始界限，但随后展示了所有结果都可以推广到任意强凸损失函数下的多维回归。</div>
</details>
</div>
<div class="card">
<div class="title">SeeThrough3D: Occlusion Aware 3D Control in Text-to-Image Generation</div>
<div class="meta-line">Authors: Vaibhav Agrawal, Rishubh Parihar, Pradhaan Bhat, Ravi Kiran Sarvadevabhatla, R. Venkatesh Babu</div>
<div class="meta-line">Venue: CVPR 2026</div>
<div class="meta-line">First: 2026-02-26T18:59:05+00:00 · Latest: 2026-02-26T18:59:05+00:00</div>
<div class="meta-line">Comments: Project page: https://seethrough3d.github.io. Accepted at CVPR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23359v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23359v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://seethrough3d.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We identify occlusion reasoning as a fundamental yet overlooked aspect for 3D layout-conditioned generation. It is essential for synthesizing partially occluded objects with depth-consistent geometry and scale. While existing methods can generate realistic scenes that follow input layouts, they often fail to model precise inter-object occlusions. We propose SeeThrough3D, a model for 3D layout conditioned generation that explicitly models occlusions. We introduce an occlusion-aware 3D scene representation (OSCR), where objects are depicted as translucent 3D boxes placed within a virtual environment and rendered from desired camera viewpoint. The transparency encodes hidden object regions, enabling the model to reason about occlusions, while the rendered viewpoint provides explicit camera control during generation. We condition a pretrained flow based text-to-image image generation model by introducing a set of visual tokens derived from our rendered 3D representation. Furthermore, we apply masked self-attention to accurately bind each object bounding box to its corresponding textual description, enabling accurate generation of multiple objects without object attribute mixing. To train the model, we construct a synthetic dataset with diverse multi-object scenes with strong inter-object occlusions. SeeThrough3D generalizes effectively to unseen object categories and enables precise 3D layout control with realistic occlusions and consistent camera control.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SeeThrough3D: 3D布局生成中的遮挡感知控制</div>
<div class="mono" style="margin-top:8px">我们识别出遮挡推理是3D布局条件生成中一个基础但被忽视的方面。它对于合成具有深度一致几何形状和比例的部分遮挡物体至关重要。尽管现有方法可以生成符合输入布局的逼真场景，但它们通常无法精确建模物体间的遮挡关系。我们提出SeeThrough3D，这是一个用于3D布局条件生成的模型，显式地建模遮挡关系。我们引入了一种遮挡感知的3D场景表示（OSCR），其中物体被表示为半透明的3D框，放置在虚拟环境中，并从期望的摄像机视角进行渲染。透明度编码了隐藏物体的区域，使模型能够推理遮挡关系，而渲染视角则在生成过程中提供了显式的摄像机控制。我们通过引入从我们渲染的3D表示中提取的一组视觉标记，对预训练的基于流的文本到图像生成模型进行条件化。此外，我们应用了掩码自注意力机制，以准确地将每个物体的边界框与其对应的文本描述绑定，从而实现多个物体的准确生成，避免物体属性混淆。为了训练模型，我们构建了一个包含多样多物体场景且具有强遮挡关系的合成数据集。SeeThrough3D能够有效泛化到未见过的物体类别，并实现精确的3D布局控制，同时具备逼真的遮挡关系和一致的摄像机控制。</div>
</details>
</div>
<div class="card">
<div class="title">A Dataset is Worth 1 MB</div>
<div class="meta-line">Authors: Elad Kimchi Shoshani, Leeyam Gabay, Yedid Hoshen</div>
<div class="meta-line">First: 2026-02-26T18:59:03+00:00 · Latest: 2026-02-26T18:59:03+00:00</div>
<div class="meta-line">Comments: 23 pages, 9 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23358v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23358v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A dataset server must often distribute the same large payload to many clients, incurring massive communication costs. Since clients frequently operate on diverse hardware and software frameworks, transmitting a pre-trained model is often infeasible; instead, agents require raw data to train their own task-specific models locally. While dataset distillation attempts to compress training signals, current methods struggle to scale to high-resolution data and rarely achieve sufficiently small files. In this paper, we propose Pseudo-Labels as Data (PLADA), a method that completely eliminates pixel transmission. We assume agents are preloaded with a large, generic, unlabeled reference dataset (e.g., ImageNet-1K, ImageNet-21K) and communicate a new task by transmitting only the class labels for specific images. To address the distribution mismatch between the reference and target datasets, we introduce a pruning mechanism that filters the reference dataset to retain only the labels of the most semantically relevant images for the target task. This selection process simultaneously maximizes training efficiency and minimizes transmission payload. Experiments on 10 diverse datasets demonstrate that our approach can transfer task knowledge with a payload of less than 1 MB while retaining high classification accuracy, offering a promising solution for efficient dataset serving.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>一个数据集值1MB</div>
<div class="mono" style="margin-top:8px">数据集服务器常常需要将相同的大体积数据分发给多个客户端，导致巨大的通信开销。由于客户端经常运行在不同的硬件和软件框架上，传输预训练模型通常是不可行的；相反，代理需要原始数据以在本地训练其特定任务的模型。虽然数据集蒸馏试图压缩训练信号，但现有方法难以扩展到高分辨率数据，并且很少能生成足够小的文件。在本文中，我们提出了一种名为PLADA的方法，该方法完全消除了像素传输。我们假设代理预先加载了一个大型通用的未标记参考数据集（例如ImageNet-1K、ImageNet-21K），并通过仅传输特定图像的类别标签来传达新任务。为了解决参考数据集与目标数据集之间的分布不匹配问题，我们引入了一种剪枝机制，过滤参考数据集以仅保留与目标任务语义最相关的图像的标签。这一选择过程同时最大化了训练效率并最小化了传输负载。在10个多样化数据集上的实验表明，我们的方法可以在小于1MB的负载下实现任务知识的传输，同时保持高分类准确率，为高效的数据集服务提供了一种有前景的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Sensor Generalization for Adaptive Sensing in Event-based Object Detection via Joint Distribution Training</div>
<div class="meta-line">Authors: Aheli Saha, René Schuster, Didier Stricker</div>
<div class="meta-line">First: 2026-02-26T18:57:52+00:00 · Latest: 2026-02-26T18:57:52+00:00</div>
<div class="meta-line">Comments: 12 pages, International Conference on Pattern Recognition Applications and Methods</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23357v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23357v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Bio-inspired event cameras have recently attracted significant research due to their asynchronous and low-latency capabilities. These features provide a high dynamic range and significantly reduce motion blur. However, because of the novelty in the nature of their output signals, there is a gap in the variability of available data and a lack of extensive analysis of the parameters characterizing their signals. This paper addresses these issues by providing readers with an in-depth understanding of how intrinsic parameters affect the performance of a model trained on event data, specifically for object detection. We also use our findings to expand the capabilities of the downstream model towards sensor-agnostic robustness.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过联合分布训练实现基于事件的物体检测中传感器泛化的自适应感知</div>
<div class="mono" style="margin-top:8px">受生物启发的事件相机由于其异步和低延迟特性，最近引起了大量研究兴趣。这些特性提供了高动态范围，并显著减少了运动模糊。然而，由于其输出信号的新型特性，可用数据的变异性存在差距，且对其信号特征参数的广泛分析也较为缺乏。本文通过深入探讨内在参数如何影响基于事件数据训练的模型性能，特别是针对物体检测，来解决这些问题。我们还利用这些发现，扩展下游模型的传感器无关鲁棒性能力。</div>
</details>
</div>
<div class="card">
<div class="title">Revisiting the Perseus Cluster III: Role of Aspherical Explosions on its Chemical Composition and Extension to Metal-Poor Stars and Galaxies</div>
<div class="meta-line">Authors: Shing-Chi Leung, Henry Yerdon, Seth Walther, Ken&#x27;ichi Nomoto, Aurora Simionescu</div>
<div class="meta-line">First: 2026-02-26T18:57:36+00:00 · Latest: 2026-02-26T18:57:36+00:00</div>
<div class="meta-line">Comments: 23 pages, 39 figures. Submitted to the Astrophysical Journal on Nov 10 2025, accepted on Feb 7 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23356v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23356v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Perseus Cluster has been precisely measured by the legacy Hitomi telescope on the Si-group (Si, S, Ar, Ca) and Fe-group elements (Cr, Mn, Ni). These element abundance ratios provide insight into the typical behaviour of supernovae. In Paper II, we presented new massive star explosion models at various metallicity, assuming spherical explosions. We show that while the fitting is improved, some features (e.g., Ni/Fe) remain to be improved. In this article, we extend our calculation to an aspherical explosion using the jet-induced explosion mechanism. The detailed pre- and post-explosion chemical profiles are calculated with a large post-processing network to capture the production of odd-number elements (V, Mn, Cu) and iron-group elements. We further explore how the jet-driven explosions create the diversity of models which could be compatible with the observed diversity in terms of $^{56}$Ni-mass vs ejecta mass, Ti-V relation, and stellar abundances. Finally, we apply the new collapsar models in the Galactic Chemical Evolution context. We study how the galactic stars, including the Zn-enriched star HE 1327-2326, can put constraints on the relative rates of collapsar and some of its model parameters. We show that collapsar could lead to significant changes in some elements, e.g., Zn. Our study shows that the collapsar is a necessary component to explain multiple elemental trends observed in the Milky Way Galaxy.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>重新审视室女座星系团III：非球对称爆炸对其化学成分的影响及对贫金属恒星和星系的扩展研究</div>
<div class="mono" style="margin-top:8px">室女座星系团的Si组（Si、S、Ar、Ca）和Fe组元素（Cr、Mn、Ni）已被遗产卫星Hitomi精确测量。这些元素丰度比提供了超新星典型行为的洞察。在论文II中，我们假设球对称爆炸，提出了不同金属丰度下的新大质量恒星爆炸模型。我们发现虽然拟合效果有所改善，但某些特征（如Ni/Fe）仍需进一步优化。在本文中，我们采用喷流诱导爆炸机制，将计算扩展到非球对称爆炸。通过一个大型后处理网络，我们详细计算了爆炸前后的化学分布，以捕捉奇数元素（V、Mn、Cu）和铁组元素的产生。我们进一步探讨了喷流驱动爆炸如何产生多样化的模型，这些模型在$^{56}$Ni质量与抛射物质量、Ti-V关系以及恒星丰度方面可能与观测到的多样性相兼容。最后，我们将新的塌缩星模型应用于银河化学演化背景中。我们研究了包括锌富集恒星HE 1327-2326在内的银河恒星如何对塌缩星及其模型参数的相对速率施加约束。我们表明塌缩星可能导致某些元素（如Zn）发生显著变化。我们的研究表明，塌缩星是解释银河系中多个元素趋势所必需的组成部分。</div>
</details>
</div>
<div class="card">
<div class="title">SOTAlign: Semi-Supervised Alignment of Unimodal Vision and Language Models via Optimal Transport</div>
<div class="meta-line">Authors: Simon Roschmann, Paul Krzakala, Sonia Mazelet, Quentin Bouniot, Zeynep Akata</div>
<div class="meta-line">First: 2026-02-26T18:55:06+00:00 · Latest: 2026-02-26T18:55:06+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23353v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23353v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Platonic Representation Hypothesis posits that neural networks trained on different modalities converge toward a shared statistical model of the world. Recent work exploits this convergence by aligning frozen pretrained vision and language models with lightweight alignment layers, but typically relies on contrastive losses and millions of paired samples. In this work, we ask whether meaningful alignment can be achieved with substantially less supervision. We introduce a semi-supervised setting in which pretrained unimodal encoders are aligned using a small number of image-text pairs together with large amounts of unpaired data. To address this challenge, we propose SOTAlign, a two-stage framework that first recovers a coarse shared geometry from limited paired data using a linear teacher, then refines the alignment on unpaired samples via an optimal-transport-based divergence that transfers relational structure without overconstraining the target space. Unlike existing semi-supervised methods, SOTAlign effectively leverages unpaired images and text, learning robust joint embeddings across datasets and encoder pairs, and significantly outperforming supervised and semi-supervised baselines.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SOTAlign：通过最优传输实现单模态视觉和语言模型的半监督对齐</div>
<div class="mono" style="margin-top:8px">柏拉图表示假说认为，不同模态上训练的神经网络会收敛到一个共享的世界统计模型。近期工作利用这一收敛特性，通过轻量级对齐层将冻结的预训练视觉和语言模型对齐，但通常依赖对比损失和数百万对配对样本。本文探讨是否可以在大幅减少监督的情况下实现有意义的对齐。我们引入了一种半监督设置，在该设置中，使用少量图像-文本对和大量未配对数据对预训练的单模态编码器进行对齐。为解决这一挑战，我们提出了SOTAlign，一个两阶段框架：首先利用线性教师从有限的配对数据中恢复粗略的共享几何结构，然后通过基于最优传输的发散度在未配对样本上优化对齐，从而在不过度约束目标空间的情况下转移关系结构。与现有半监督方法不同，SOTAlign有效利用未配对的图像和文本，学习跨数据集和编码器对的鲁棒联合嵌入，并显著优于监督和半监督基线。</div>
</details>
</div>
<div class="card">
<div class="title">Scale Can&#x27;t Overcome Pragmatics: The Impact of Reporting Bias on Vision-Language Reasoning</div>
<div class="meta-line">Authors: Amita Kamath, Jack Hessel, Khyathi Chandu, Jena D. Hwang, Kai-Wei Chang, Ranjay Krishna</div>
<div class="meta-line">First: 2026-02-26T18:54:06+00:00 · Latest: 2026-02-26T18:54:06+00:00</div>
<div class="meta-line">Comments: TACL 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23351v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23351v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The lack of reasoning capabilities in Vision-Language Models (VLMs) has remained at the forefront of research discourse. We posit that this behavior stems from a reporting bias in their training data. That is, how people communicate about visual content by default omits tacit information needed to supervise some types of reasoning; e.g., &quot;at the game today!&quot; is a more likely caption than &quot;a photo of 37 people standing behind a field&quot;. We investigate the data underlying the popular VLMs OpenCLIP, LLaVA-1.5 and Molmo through the lens of theories from pragmatics, and find that reporting bias results in insufficient representation of four reasoning skills (spatial, temporal, negation, and counting), despite the corpora being of web-scale, and/or synthetically generated. With a set of curated benchmarks, we demonstrate that: (i) VLMs perform poorly on the aforementioned types of reasoning suppressed in the training data by reporting bias; (ii) contrary to popular belief, scaling data size, model size, and to multiple languages does not result in emergence of these skills by default; but, promisingly, (iii) incorporating annotations specifically collected to obtain tacit information is effective. Our findings highlight the need for more intentional training data curation methods, rather than counting on scale for emergence of reasoning capabilities.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>规模无法克服语用学：报告偏差对视觉-语言推理的影响</div>
<div class="mono" style="margin-top:8px">视觉-语言模型（VLMs）缺乏推理能力一直是研究讨论的焦点。我们认为这种行为源于其训练数据中的报告偏差。也就是说，人们在默认情况下交流视觉内容时，会省略一些用于监督某些类型推理的隐含信息；例如，&quot;今天在比赛现场！&quot;比&quot;一张37人站在球场后的照片&quot;更可能作为标题。我们通过语用学理论的视角，研究了流行的VLMs OpenCLIP、LLaVA-1.5和Molmo背后的数据，并发现即使语料库是网络规模或合成生成的，报告偏差仍导致四种推理能力（空间、时间、否定和计数）的表示不足。通过一组精心设计的基准测试，我们证明了以下几点：(i) 由于报告偏差，VLMs在上述被训练数据抑制的推理类型上表现不佳；(ii) 与普遍看法相反，扩大数据量、模型规模或多语言训练并不能默认带来这些能力的出现；但令人鼓舞的是，(iii) 引入专门收集以获取隐含信息的注释是有效的。我们的研究结果强调了需要更有意图的训练数据编纂方法，而不是依赖规模来实现推理能力的涌现。</div>
</details>
</div>
<div class="card">
<div class="title">FlashOptim: Optimizers for Memory Efficient Training</div>
<div class="meta-line">Authors: Jose Javier Gonzalez Ortiz, Abhay Gupta, Chris Renard, Davis Blalock</div>
<div class="meta-line">First: 2026-02-26T18:52:22+00:00 · Latest: 2026-02-26T18:52:22+00:00</div>
<div class="meta-line">Comments: Source code is available at https://github.com/databricks/flashoptim</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23349v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23349v1">PDF</a> · <a href="https://github.com/databricks/flashoptim">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Standard mixed-precision training of neural networks requires many bytes of accelerator memory for each model parameter. These bytes reflect not just the parameter itself, but also its gradient and one or more optimizer state variables. With each of these values typically requiring 4 bytes, training even a 7 billion parameter model can be impractical for researchers with less than 100GB of accelerator memory.
  We introduce FlashOptim, a suite of optimizations that reduces per-parameter memory by over 50% while preserving model quality and API compatibility. Our approach introduces two key techniques. First, we improve master weight splitting by finding and exploiting a tight bound on its quantization error. Second, we design companding functions that greatly reduce the error in 8-bit optimizer state quantization. Together with 16-bit gradients, these techniques reduce AdamW memory from 16 bytes to 7 bytes per parameter, or 5 bytes with gradient release. They also cut model checkpoint sizes by more than half.
  Experiments with FlashOptim applied to SGD, AdamW, and Lion show no measurable quality degradation on any task from a collection of standard vision and language benchmarks, including Llama-3.1-8B finetuning.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>FlashOptim：内存高效的训练优化器</div>
<div class="mono" style="margin-top:8px">标准的混合精度神经网络训练需要为每个模型参数分配许多字节的加速器内存。这些字节不仅代表参数本身，还包括其梯度和一个或多个优化器状态变量。由于每个值通常需要4字节，即使是70亿参数的模型，对于加速器内存少于100GB的研究人员来说，训练也可能不切实际。
我们引入了FlashOptim，这是一套优化技术，可在保持模型质量和API兼容性的同时，将每个参数的内存消耗减少超过50%。我们的方法引入了两项关键技术。首先，我们通过找到并利用主权重分割的量化误差的紧界来改进主权重分割。其次，我们设计了压缩扩展函数，大幅降低了8位优化器状态量化的误差。结合16位梯度，这些技术将AdamW的内存消耗从每个参数16字节减少到7字节，或在梯度释放的情况下减少到5字节。它们还将模型检查点的大小减少了一半以上。
在SGD、AdamW和Lion上应用FlashOptim的实验表明，在一组标准的视觉和语言基准任务中，包括Llama-3.1-8B微调，没有任何任务出现可测量的质量下降。</div>
</details>
</div>
<div class="card">
<div class="title">AlayaLaser: Efficient Index Layout and Search Strategy for Large-scale High-dimensional Vector Similarity Search</div>
<div class="meta-line">Authors: Weijian Chen, Haotian Liu, Yangshen Deng, Long Xiang, Liang Huang, Gezi Li, Bo Tang</div>
<div class="meta-line">First: 2026-02-26T18:48:29+00:00 · Latest: 2026-02-26T18:48:29+00:00</div>
<div class="meta-line">Comments: The paper has been accepted by SIGMOD 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23342v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23342v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">On-disk graph-based approximate nearest neighbor search (ANNS) is essential for large-scale, high-dimensional vector retrieval, yet its performance is widely recognized to be limited by the prohibitive I/O costs. Interestingly, we observed that the performance of on-disk graph-based index systems is compute-bound, not I/O-bound, with the rising of the vector data dimensionality (e.g., hundreds or thousands). This insight uncovers a significant optimization opportunity: existing on-disk graph-based index systems universally target I/O reduction and largely overlook computational overhead, which leaves a substantial performance improvement space.
  In this work, we propose AlayaLaser, an efficient on-disk graph-based index system for large-scale high-dimensional vector similarity search. In particular, we first conduct performance analysis on existing on-disk graph-based index systems via the adapted roofline model, then we devise a novel on-disk data layout in AlayaLaser to effectively alleviate the compute-bound, which is revealed by the above roofline model analysis, by exploiting SIMD instructions on modern CPUs. We next design a suite of optimization techniques (e.g., degree-based node cache, cluster-based entry point selection, and early dispatch strategy) to further improve the performance of AlayaLaser. We last conduct extensive experimental studies on a wide range of large-scale high-dimensional vector datasets to verify the superiority of AlayaLaser. Specifically, AlayaLaser not only surpasses existing on-disk graph-based index systems but also matches or even exceeds the performance of in-memory index systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AlayaLaser：面向大规模高维向量相似性搜索的高效索引布局与搜索策略</div>
<div class="mono" style="margin-top:8px">基于磁盘的图结构近似最近邻搜索（ANNS）对于大规模、高维向量检索至关重要，但其性能通常受到高昂的I/O成本的限制。有趣的是，我们观察到随着向量数据维度的增加（例如数百或数千维），基于磁盘的图结构索引系统的性能是计算密集型的，而非I/O密集型的。这一发现揭示了一个重要的优化机会：现有的基于磁盘的图结构索引系统普遍关注I/O减少，而忽视了计算开销，这为性能提升留下了巨大空间。在本文中，我们提出了AlayaLaser，一种面向大规模高维向量相似性搜索的高效基于磁盘的图结构索引系统。具体而言，我们首先通过改进的屋顶线模型对现有基于磁盘的图结构索引系统进行性能分析，然后设计了一种新颖的磁盘数据布局，利用现代CPU的SIMD指令有效缓解计算密集型问题。接着，我们设计了一系列优化技术（如基于度数的节点缓存、基于聚类的入口点选择和早期调度策略），以进一步提升AlayaLaser的性能。最后，我们在多种大规模高维向量数据集上进行了广泛的实验研究，验证了AlayaLaser的优越性。具体来说，AlayaLaser不仅超越了现有的基于磁盘的图结构索引系统，还能够匹配甚至超过内存索引系统的性能。</div>
</details>
</div>
<div class="card">
<div class="title">Mean Estimation from Coarse Data: Characterizations and Efficient Algorithms</div>
<div class="meta-line">Authors: Alkis Kalavasis, Anay Mehrotra, Manolis Zampetakis, Felix Zhou, Ziyu Zhu</div>
<div class="meta-line">Venue: ICLR</div>
<div class="meta-line">First: 2026-02-26T18:47:06+00:00 · Latest: 2026-02-26T18:47:06+00:00</div>
<div class="meta-line">Comments: Abstract truncated to arXiv limits. To appear in ICLR&#x27;26</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23341v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23341v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Coarse data arise when learners observe only partial information about samples; namely, a set containing the sample rather than its exact value. This occurs naturally through measurement rounding, sensor limitations, and lag in economic systems. We study Gaussian mean estimation from coarse data, where each true sample $x$ is drawn from a $d$-dimensional Gaussian distribution with identity covariance, but is revealed only through the set of a partition containing $x$. When the coarse samples, roughly speaking, have ``low&#x27;&#x27; information, the mean cannot be uniquely recovered from observed samples (i.e., the problem is not identifiable). Recent work by Fotakis, Kalavasis, Kontonis, and Tzamos [FKKT21] established that sample-efficient mean estimation is possible when the unknown mean is identifiable and the partition consists of only convex sets. Moreover, they showed that without convexity, mean estimation becomes NP-hard. However, two fundamental questions remained open: (1) When is the mean identifiable under convex partitions? (2) Is computationally efficient estimation possible under identifiability and convex partitions? This work resolves both questions. [...]</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从粗略数据中估计均值：特征描述与高效算法</div>
<div class="mono" style="margin-top:8px">当学习者只能观察样本的部分信息时，就会产生粗略数据；即，一个包含样本而非其精确值的集合。这种情况自然出现在测量四舍五入、传感器限制以及经济系统滞后等场景中。我们研究从粗略数据中估计高斯分布的均值，其中每个真实的样本 $x$ 是从一个 $d$ 维高斯分布中抽取的，该分布具有单位协方差矩阵，但仅通过包含 $x$ 的划分集合被揭示。当粗略样本具有“低”信息量时，均值无法从观察到的样本中唯一恢复（即，问题不可识别）。Fotakis、Kalavasis、Kontonis 和 Tzamos [FKKT21] 的近期工作表明，当未知均值可识别且划分仅由凸集组成时，可以实现样本高效的均值估计。此外，他们还证明了在没有凸性的情况下，均值估计会变得NP难。然而，两个基本问题仍未解决：(1) 在凸划分下，何时均值可识别？(2) 在可识别性和凸划分下，是否可以实现计算高效的估计？本文解决了这两个问题。</div>
</details>
</div>
<div class="card">
<div class="title">Retrieve and Segment: Are a Few Examples Enough to Bridge the Supervision Gap in Open-Vocabulary Segmentation?</div>
<div class="meta-line">Authors: Tilemachos Aravanis, Vladan Stojnić, Bill Psomas, Nikos Komodakis, Giorgos Tolias</div>
<div class="meta-line">First: 2026-02-26T18:45:33+00:00 · Latest: 2026-02-26T18:45:33+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23339v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23339v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Open-vocabulary segmentation (OVS) extends the zero-shot recognition capabilities of vision-language models (VLMs) to pixel-level prediction, enabling segmentation of arbitrary categories specified by text prompts. Despite recent progress, OVS lags behind fully supervised approaches due to two challenges: the coarse image-level supervision used to train VLMs and the semantic ambiguity of natural language. We address these limitations by introducing a few-shot setting that augments textual prompts with a support set of pixel-annotated images. Building on this, we propose a retrieval-augmented test-time adapter that learns a lightweight, per-image classifier by fusing textual and visual support features. Unlike prior methods relying on late, hand-crafted fusion, our approach performs learned, per-query fusion, achieving stronger synergy between modalities. The method supports continually expanding support sets, and applies to fine-grained tasks such as personalized segmentation. Experiments show that we significantly narrow the gap between zero-shot and supervised segmentation while preserving open-vocabulary ability.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>检索与分割：几个示例是否足以弥合开放词汇分割中的监督鸿沟？</div>
<div class="mono" style="margin-top:8px">开放词汇分割（OVS）将视觉-语言模型（VLMs）的零样本识别能力扩展到像素级预测，从而实现通过文本提示对任意类别进行分割。尽管近期取得了一定进展，但OVS仍落后于完全监督的方法，主要由于两个挑战：用于训练VLMs的粗粒度图像级监督以及自然语言的语义歧义。我们通过引入一个少样本设置来解决这些限制，该设置通过添加带有像素标注的图像支持集来增强文本提示。在此基础上，我们提出了一种检索增强的测试时适配器，通过融合文本和视觉支持特征，学习一个轻量级的、针对单张图像的分类器。与依赖于后期手工融合的先前方法不同，我们的方法执行学习的、针对每个查询的融合，从而实现模态之间更强的协同作用。该方法支持持续扩展的支持集，并适用于细粒度任务，如个性化分割。实验表明，我们显著缩小了零样本分割与监督分割之间的差距，同时保留了开放词汇的能力。</div>
</details>
</div>
<div class="card">
<div class="title">Differentiable Zero-One Loss via Hypersimplex Projections</div>
<div class="meta-line">Authors: Camilo Gomez, Pengyang Wang, Liansheng Tang</div>
<div class="meta-line">First: 2026-02-26T18:41:31+00:00 · Latest: 2026-02-26T18:41:31+00:00</div>
<div class="meta-line">Comments: To appear in PAKDD 2026 (Pacific-Asia Conference on Knowledge Discovery and Data Mining), 12 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23336v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23336v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in machine learning have emphasized the integration of structured optimization components into end-to-end differentiable models, enabling richer inductive biases and tighter alignment with task-specific objectives. In this work, we introduce a novel differentiable approximation to the zero-one loss-long considered the gold standard for classification performance, yet incompatible with gradient-based optimization due to its non-differentiability. Our method constructs a smooth, order-preserving projection onto the n,k-dimensional hypersimplex through a constrained optimization framework, leading to a new operator we term Soft-Binary-Argmax. After deriving its mathematical properties, we show how its Jacobian can be efficiently computed and integrated into binary and multiclass learning systems. Empirically, our approach achieves significant improvements in generalization under large-batch training by imposing geometric consistency constraints on the output logits, thereby narrowing the performance gap traditionally observed in large-batch training.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过超单纯形投影实现可微分零一损失</div>
<div class="mono" style="margin-top:8px">近年来，机器学习的进展强调将结构化优化组件整合到端到端可微分模型中，从而实现更丰富的归纳偏置和更紧密的任务特定目标对齐。在本文中，我们引入了一种新颖的可微分零一损失近似方法——长期以来被视为分类性能的黄金标准，但由于其不可微性而无法与基于梯度的优化兼容。我们的方法通过一个约束优化框架，构建了一个平滑且保持顺序的投影到n,k维超单纯形上，从而得到一个新的运算符，我们称之为Soft-Binary-Argmax。在推导其数学性质之后，我们展示了如何高效计算其雅可比矩阵，并将其整合到二分类和多分类学习系统中。实验表明，通过在输出logits上施加几何一致性约束，我们的方法在大批次训练中显著提升了泛化能力，从而缩小了传统大批次训练中观察到的性能差距。</div>
</details>
</div>
<div class="card">
<div class="title">Understanding Usage and Engagement in AI-Powered Scientific Research Tools: The Asta Interaction Dataset</div>
<div class="meta-line">Authors: Dany Haddad, Dan Bareket, Joseph Chee Chang, Jay DeYoung, Jena D. Hwang, Uri Katz, Mark Polak, Sangho Suh, Harshit Surana, Aryeh Tiktinsky, Shriya Atmakuri, Jonathan Bragg, Mike D&#x27;Arcy, Sergey Feldman, Amal Hassan-Ali, Rubén Lozano, Bodhisattwa Prasad Majumder, Charles McGrady, Amanpreet Singh, Brooke Vlahos, Yoav Goldberg, Doug Downey</div>
<div class="meta-line">First: 2026-02-26T18:40:28+00:00 · Latest: 2026-02-26T18:40:28+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23335v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23335v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">AI-powered scientific research tools are rapidly being integrated into research workflows, yet the field lacks a clear lens into how researchers use these systems in real-world settings. We present and analyze the Asta Interaction Dataset, a large-scale resource comprising over 200,000 user queries and interaction logs from two deployed tools (a literature discovery interface and a scientific question-answering interface) within an LLM-powered retrieval-augmented generation platform. Using this dataset, we characterize query patterns, engagement behaviors, and how usage evolves with experience. We find that users submit longer and more complex queries than in traditional search, and treat the system as a collaborative research partner, delegating tasks such as drafting content and identifying research gaps. Users treat generated responses as persistent artifacts, revisiting and navigating among outputs and cited evidence in non-linear ways. With experience, users issue more targeted queries and engage more deeply with supporting citations, although keyword-style queries persist even among experienced users. We release the anonymized dataset and analysis with a new query intent taxonomy to inform future designs of real-world AI research assistants and to support realistic evaluation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>理解AI驱动科学研究工具的使用与参与：Asta交互数据集</div>
<div class="mono" style="margin-top:8px">AI驱动的科学研究工具正迅速融入研究流程，但该领域缺乏对研究人员在现实环境中如何使用这些系统的清晰视角。我们介绍了并分析了Asta交互数据集，这是一个大规模资源，包含两个已部署工具（文献发现界面和科学问答界面）在基于大语言模型（LLM）的检索增强生成平台中的超过200,000条用户查询和交互日志。利用该数据集，我们描述了查询模式、参与行为以及使用随经验变化的趋势。我们发现，用户提交的查询比传统搜索更长且更复杂，并将系统视为协作研究伙伴，委托诸如内容草拟和识别研究空白等任务。用户将生成的响应视为持久的成果，以非线性方式回顾和浏览输出及引用的证据。随着经验的积累，用户提出更具针对性的查询，并更深入地参与支持性引用，尽管关键词式查询在经验丰富的用户中仍然存在。我们发布了匿名化后的数据集和分析，并引入了一种新的查询意图分类法，以指导未来真实世界AI研究助手的设计，并支持现实的评估。</div>
</details>
</div>
<div class="card">
<div class="title">Bitwise Systolic Array Architecture for Runtime-Reconfigurable Multi-precision Quantized Multiplication on Hardware Accelerators</div>
<div class="meta-line">Authors: Yuhao Liu, Salim Ullah, Akash Kumar</div>
<div class="meta-line">First: 2026-02-26T18:40:02+00:00 · Latest: 2026-02-26T18:40:02+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23334v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23334v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Neural network accelerators have been widely applied to edge devices for complex tasks like object tracking, image recognition, etc. Previous works have explored the quantization technologies in related lightweight accelerator designs to reduce hardware resource consumption. However, low precision leads to high accuracy loss in inference. Therefore, mixed-precision quantization becomes an alternative solution by applying different precision in different layers to trade off resource consumption and accuracy. Because regular designs for multiplication on hardware cannot support the precision reconfiguration for a multi-precision Quantized Neural Network (QNN) model in runtime, we propose a runtime reconfigurable multi-precision multi-channel bitwise systolic array design for QNN accelerators. We have implemented and evaluated our work on the Ultra96 FPGA platform. Results show that our work can achieve 1.3185 to 3.5671 times speedup in inferring mixed-precision models and has less critical path delay, supporting a higher clock frequency (250MHz).</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于硬件加速器上运行时可重构多精度量化乘法的位级同步阵列架构</div>
<div class="mono" style="margin-top:8px">神经网络加速器已被广泛应用于边缘设备以处理对象跟踪、图像识别等复杂任务。先前的研究在相关轻量级加速器设计中探索了量化技术，以减少硬件资源消耗。然而，低精度会导致推理中出现较高的精度损失。因此，通过在不同层应用不同精度，混合精度量化成为一种替代方案，以在资源消耗和精度之间取得平衡。由于常规的硬件乘法设计无法在运行时支持多精度量化神经网络（QNN）模型的精度重构，我们提出了一种用于QNN加速器的运行时可重构多精度多通道位级同步阵列设计。我们在Ultra96 FPGA平台上实现了并评估了该工作。结果表明，我们的方法在推理混合精度模型时可实现1.3185到3.5671倍的速度提升，并具有更短的关键路径延迟，支持更高的时钟频率（250MHz）。</div>
</details>
</div>
<div class="card">
<div class="title">Utilizing LLMs for Industrial Process Automation</div>
<div class="meta-line">Authors: Salim Fares</div>
<div class="meta-line">First: 2026-02-26T18:38:00+00:00 · Latest: 2026-02-26T18:38:00+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23331v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23331v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">A growing number of publications address the best practices to use Large Language Models (LLMs) for software engineering in recent years. However, most of this work focuses on widely-used general purpose programming languages like Python due to their widespread usage training data. The utility of LLMs for software within the industrial process automation domain, with highly-specialized languages that are typically only used in proprietary contexts, remains underexplored. This research aims to utilize and integrate LLMs in the industrial development process, solving real-life programming tasks (e.g., generating a movement routine for a robotic arm) and accelerating the development cycles of manufacturing systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>利用大语言模型进行工业流程自动化</div>
<div class="mono" style="margin-top:8px">近年来，越来越多的出版物探讨了如何在软件工程中有效使用大型语言模型（LLMs）。然而，大多数相关工作集中于广泛使用的通用编程语言（如 Python），因为这些语言拥有丰富的训练数据。对于工业流程自动化领域中使用高度专业化的语言（通常仅在专有环境中使用）的软件，LLMs 的应用仍处于探索阶段。本研究旨在将 LLMs 应用于工业开发流程，解决实际编程任务（如生成机械臂的运动程序），并加快制造系统的开发周期。</div>
</details>
</div>
<div class="card">
<div class="title">Toward Expert Investment Teams:A Multi-Agent LLM System with Fine-Grained Trading Tasks</div>
<div class="meta-line">Authors: Kunihiro Miyazaki, Takanobu Kawahara, Stephen Roberts, Stefan Zohren</div>
<div class="meta-line">First: 2026-02-26T18:37:36+00:00 · Latest: 2026-02-26T18:37:36+00:00</div>
<div class="meta-line">Comments: 14 pages, 3 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23330v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23330v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The advancement of large language models (LLMs) has accelerated the development of autonomous financial trading systems. While mainstream approaches deploy multi-agent systems mimicking analyst and manager roles, they often rely on abstract instructions that overlook the intricacies of real-world workflows, which can lead to degraded inference performance and less transparent decision-making. Therefore, we propose a multi-agent LLM trading framework that explicitly decomposes investment analysis into fine-grained tasks, rather than providing coarse-grained instructions. We evaluate the proposed framework using Japanese stock data, including prices, financial statements, news, and macro information, under a leakage-controlled backtesting setting. Experimental results show that fine-grained task decomposition significantly improves risk-adjusted returns compared to conventional coarse-grained designs. Crucially, further analysis of intermediate agent outputs suggests that alignment between analytical outputs and downstream decision preferences is a critical driver of system performance. Moreover, we conduct standard portfolio optimization, exploiting low correlation with the stock index and the variance of each system&#x27;s output. This approach achieves superior performance. These findings contribute to the design of agent structure and task configuration when applying LLM agents to trading systems in practical settings.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向专家投资团队：一种具有细粒度交易任务的多智能体大语言模型系统</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）的发展加速了自主金融交易系统的发展。虽然主流方法部署多智能体系统来模拟分析师和经理的角色，但它们通常依赖于抽象指令，忽略了现实世界工作流程的复杂性，这可能导致推理性能下降和决策过程不够透明。因此，我们提出了一种多智能体LLM交易框架，明确地将投资分析分解为细粒度任务，而不是提供粗粒度指令。我们在泄漏控制的回测环境下，使用日本股市数据（包括价格、财务报表、新闻和宏观信息）评估了该框架。实验结果表明，细粒度任务分解显著提高了风险调整后的收益，相较于传统的粗粒度设计。关键的是，对中间智能体输出的进一步分析表明，分析输出与下游决策偏好的对齐是系统性能的关键驱动因素。此外，我们进行了标准投资组合优化，利用各系统输出与股票指数低相关性以及输出方差。这种方法实现了更优的性能。这些发现有助于在实际交易系统中应用LLM智能体时设计智能体结构和任务配置。</div>
</details>
</div>
<div class="card">
<div class="title">LLM Novice Uplift on Dual-Use, In Silico Biology Tasks</div>
<div class="meta-line">Authors: Chen Bo Calvin Zhang, Christina Q. Knight, Nicholas Kruus, Jason Hausenloy, Pedro Medeiros, Nathaniel Li, Aiden Kim, Yury Orlovskiy, Coleman Breen, Bryce Cai, Jasper Götting, Andrew Bo Liu, Samira Nedungadi, Paula Rodriguez, Yannis Yiming He, Mohamed Shaaban, Zifan Wang, Seth Donoughe, Julian Michael</div>
<div class="meta-line">First: 2026-02-26T18:37:23+00:00 · Latest: 2026-02-26T18:37:23+00:00</div>
<div class="meta-line">Comments: 59 pages, 33 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23329v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23329v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) perform increasingly well on biology benchmarks, but it remains unclear whether they uplift novice users -- i.e., enable humans to perform better than with internet-only resources. This uncertainty is central to understanding both scientific acceleration and dual-use risk. We conducted a multi-model, multi-benchmark human uplift study comparing novices with LLM access versus internet-only access across eight biosecurity-relevant task sets. Participants worked on complex problems with ample time (up to 13 hours for the most involved tasks). We found that LLM access provided substantial uplift: novices with LLMs were 4.16 times more accurate than controls (95% CI [2.63, 6.87]). On four benchmarks with available expert baselines (internet-only), novices with LLMs outperformed experts on three of them. Perhaps surprisingly, standalone LLMs often exceeded LLM-assisted novices, indicating that users were not eliciting the strongest available contributions from the LLMs. Most participants (89.6%) reported little difficulty obtaining dual-use-relevant information despite safeguards. Overall, LLMs substantially uplift novices on biological tasks previously reserved for trained practitioners, underscoring the need for sustained, interactive uplift evaluations alongside traditional benchmarks.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LLM对双用途和计算机生物学任务的初学者提升</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在生物学基准测试中的表现越来越好，但尚不清楚它们是否能提升初学者用户——即是否能让人类的表现优于仅使用互联网资源。这种不确定性是理解科学加速和双用途风险的关键。我们进行了一项多模型、多基准的人类提升研究，比较了拥有LLM访问权限的初学者与仅使用互联网资源的初学者在八个与生物安全相关的任务集上的表现。参与者有充足的时间（最复杂的任务可达13小时）解决复杂问题。我们发现，LLM的访问提供了显著的提升：拥有LLM的初学者比对照组准确率高出4.16倍（95% CI [2.63, 6.87]）。在四个有专家基准（仅互联网）可比的测试中，拥有LLM的初学者在其中三个测试中表现优于专家。令人惊讶的是，独立LLM经常超过LLM辅助的初学者，表明用户并未充分激发LLM的最强能力。大多数参与者（89.6%）表示，尽管有安全措施，获取与双用途相关的信息几乎没有困难。总体而言，LLMs在以前仅由受过训练的专业人员处理的生物任务上显著提升了初学者的表现，强调了在传统基准之外，需要持续进行互动式提升评估的重要性。</div>
</details>
</div>
<div class="card">
<div class="title">Beyond Single-Shot Fidelity: Chernoff-Based Throughput Optimization in Superconducting Qubit Readout</div>
<div class="meta-line">Authors: Sinan Bugu</div>
<div class="meta-line">First: 2026-02-25T18:21:15+00:00 · Latest: 2026-02-26T18:36:23+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.22174v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.22174v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Single-shot fidelity is the standard benchmark for superconducting qubit readout, but it does not directly minimize the total wall-clock time required to certify a quantum state. We develop an information-theoretic description of dispersive readout by treating the measurement record as a stochastic communication channel. Within a trajectory model that incorporates T1 relaxation with full cavity memory, we compute the classical Chernoff information governing the multi-shot error exponent. We find a consistent separation between the integration time that maximizes single-shot fidelity and the time that minimizes total certification time. For representative transmon parameters and hardware overheads, the throughput-optimal integration window is longer than the fidelity-optimal one, yielding certification speedups of approximately 9 to 11 percent, with the gain saturating near 1.13x in the high-readout-power and high-overhead regime. Comparing the extracted classical information to the unit-efficiency Gaussian Chernoff benchmark defines an information-extraction efficiency metric. Typical dispersive schemes are limited to about 45 percent capture at short integration times by detection efficiency, decreasing to approximately 12 percent at a throughput-optimal integration time of about 1.22 microseconds due to T1-induced trajectory smearing. This formulation connects readout calibration to the operational objective of minimizing certification time in high-throughput superconducting processors.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超越单次读取保真度：基于切尔诺夫的超导量子比特读取吞吐量优化</div>
<div class="mono" style="margin-top:8px">单次读取保真度是超导量子比特读取的标准评估指标，但它并不能直接最小化认证量子态所需的总实际时间。我们通过将测量记录视为随机通信信道，发展了一种信息论描述来分析色散读取。在包含T1弛豫和完整腔体记忆的轨迹模型中，我们计算了多次读取误差指数的古典切尔诺夫信息。我们发现，最大化单次读取保真度的积分时间与最小化总认证时间的积分时间之间存在一致的分离。对于具有代表性的transmon参数和硬件开销，吞吐量最优的积分窗口比保真度最优的窗口更长，从而带来约9%至11%的认证速度提升，且在高读取功率和高开销情况下，增益趋于饱和，达到约1.13倍。将提取的古典信息与单位效率高斯切尔诺夫基准进行比较，定义了一个信息提取效率指标。典型的色散方案由于检测效率的限制，在短积分时间下只能捕获约45%的信息，而在吞吐量最优的积分时间（约1.22微秒）下，由于T1引起的轨迹模糊，捕获率下降至约12%。这种表述将读取校准与高吞吐量超导处理器中最小化认证时间的操作目标联系起来。</div>
</details>
</div>
<div class="card">
<div class="title">Coherent Virtual Absorption in Dielectric Metasurfaces</div>
<div class="meta-line">Authors: Kaizad Rustomji, Nasim Mohammadi Estakhri, Nooshin M. Estakhri</div>
<div class="meta-line">First: 2026-02-26T18:36:14+00:00 · Latest: 2026-02-26T18:36:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23328v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23328v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Through temporal shaping of the excitation signal, the complex-frequency scattering zeros of a lossless structure can be accessed, enabling a storage-release mechanism referred to as coherent virtual absorption. Practical demonstrations of this mechanism, however, have been limited to simple configurations such as slabs and spheres, where analytical solutions allow accurate prediction of the complex-frequency scattering zeros. Here, we extend this concept into the realm of metasurfaces and demonstrate coherent virtual absorption in realistic and dispersive metasurface configurations. Through a combination of full-wave analysis and rational approximation, we present a practical scheme to identify suitable complex-frequency zeros and achieve coherent virtual absorption successfully. Our approach can be implemented in arbitrary metasurface configurations with any number of ports, providing a robust framework for optimized energy storage, memories, optical sensing, and modulation in practical photonic systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>介电超表面中的相干虚拟吸收</div>
<div class="mono" style="margin-top:8px">通过时间域激励信号的形状调控，可以访问无损耗结构的复频率散射零点，从而实现一种称为相干虚拟吸收的存储-释放机制。然而，该机制的实际演示仅限于简单的结构配置，如板状和球形结构，其中解析解允许准确预测复频率散射零点。在此，我们将这一概念扩展到超表面领域，并在实际且色散的超表面配置中演示相干虚拟吸收。通过全波分析与有理逼近的结合，我们提出了一种实用方案，以识别合适的复频率零点并成功实现相干虚拟吸收。我们的方法可以应用于任意超表面配置，并具有任意端口数量，为实际光子系统中的优化能量存储、记忆、光学传感和调制提供了稳健的框架。</div>
</details>
</div>
<div class="card">
<div class="title">Memory-induced active particle ratchets: Mean currents and large deviations</div>
<div class="meta-line">Authors: Venkata D. Pamulaparthy, Rosemary J. Harris</div>
<div class="meta-line">First: 2026-02-26T18:36:13+00:00 · Latest: 2026-02-26T18:36:13+00:00</div>
<div class="meta-line">Comments: 15 pages, 10 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23327v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23327v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We analyse a continuous-time random walk model with stochastic reversals of direction. There is no external potential but the reorientation mechanism generates a non-zero current from asymmetry in the forward and backward waiting-time distributions (even when they have the same mean); the system can therefore can be considered as a type of active particle ratchet. We derive an explicit expression for the mean ratchet current with exponentially distributed reorientation times and also develop a general renewal-theory framework to obtain the full large deviations, using this to comment on the possibility of dynamical phase transitions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>记忆诱导的活性粒子锯齿机制：平均电流与大偏差</div>
<div class="mono" style="margin-top:8px">我们分析了一个具有方向随机反转的连续时间随机游走模型。该系统没有外部势场，但重定向机制由于前向和后向等待时间分布的不对称性（即使它们的平均值相同）产生了非零的电流；因此，该系统可以被视为一种活性粒子锯齿机制。我们推导了在指数分布重定向时间下的平均锯齿电流的显式表达式，并发展了一个一般的更新理论框架以获得完整的大型偏差，利用这一结果对动态相变的可能性进行了评论。</div>
</details>
</div>
<div class="card">
<div class="title">DropVLA: An Action-Level Backdoor Attack on Vision--Language--Action Models</div>
<div class="meta-line">Authors: Zonghuan Xu, Xiang Zheng, Xingjun Ma, Yu-Gang Jiang</div>
<div class="meta-line">First: 2025-10-13T02:45:48+00:00 · Latest: 2026-02-26T18:32:27+00:00</div>
<div class="meta-line">Comments: 8 pages, 6 tables, 3 figures. Under review</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.10932v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.10932v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision-Language-Action (VLA) models map multimodal perception and language instructions to executable robot actions, making them particularly vulnerable to behavioral backdoor manipulation: a hidden trigger introduced during training can induce unintended physical actions while nominal task performance remains intact. Prior work on VLA backdoors primarily studies untargeted attacks or task-level hijacking, leaving fine-grained control over individual actions largely unexplored. In this work, we present DropVLA, an action-level backdoor attack that forces a reusable action primitive (e.g., open_gripper) to execute at attacker-chosen decision points under a realistic pipeline-black-box setting with limited data-poisoning access, using a window-consistent relabeling scheme for chunked fine-tuning. On OpenVLA-7B evaluated with LIBERO, vision-only poisoning achieves 98.67%-99.83% attack success rate (ASR) with only 0.31% poisoned episodes while preserving 98.50%-99.17% clean-task retention, and successfully triggers the targeted action within 25 control steps at 500 Hz (0.05 s). Text-only triggers are unstable at low poisoning budgets, and combining text with vision provides no consistent ASR improvement over vision-only attacks. The backdoor remains robust to moderate trigger variations and transfers across evaluation suites (96.27%, 99.09%), whereas text-only largely fails (0.72%). We further validate physical-world feasibility on a 7-DoF Franka arm with pi0-fast, demonstrating non-trivial attack efficacy under camera-relative motion that induces image-plane trigger drift. These results reveal that VLA models can be covertly steered at the granularity of safety-critical actions with minimal poisoning and without observable degradation of nominal performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DropVLA：一种针对视觉-语言-动作模型的行为后门攻击</div>
<div class="mono" style="margin-top:8px">视觉-语言-动作（VLA）模型将多模态感知和语言指令映射为可执行的机器人动作，因此特别容易受到行为后门操控的影响：在训练过程中引入的隐藏触发器可以在保持正常任务表现的同时诱导出未预期的物理动作。以往关于VLA后门的研究主要集中在无目标攻击或任务级劫持上，对个体动作的细粒度控制则鲜有探索。在本工作中，我们提出了DropVLA，这是一种在现实管道黑盒设置下，仅有限数据污染访问的情况下，强制可重用的动作原语（如open_gripper）在攻击者选择的决策点执行的行为后门攻击，使用窗口一致的重标签方案进行分块微调。在LIBERO评估的OpenVLA-7B上，仅使用视觉污染的攻击成功率（ASR）达到98.67%-99.83%，仅需0.31%的污染场景，同时保持98.50%-99.17%的正常任务保留率，并在25个控制步骤内成功触发目标动作（频率为500 Hz，即0.05秒）。仅使用文本触发器在低污染预算下不稳定，而将文本与视觉结合并未在视觉攻击基础上带来一致的ASR提升。后门在一定程度上的触发变化和跨评估套件迁移中仍保持鲁棒性（96.27%，99.09%），而仅使用文本触发器则大多失败（0.72%）。我们进一步在7自由度的Franka机械臂上使用pi0-fast验证了物理世界的可行性，展示了在相机相对运动下诱导图像平面触发漂移的非平凡攻击效果。这些结果表明，VLA模型可以通过最小的污染和不显著的正常性能退化，实现对安全关键动作的隐蔽操控。</div>
</details>
</div>
<div class="card">
<div class="title">Deep ensemble graph neural networks for probabilistic cosmic-ray direction and energy reconstruction in autonomous radio arrays</div>
<div class="meta-line">Authors: Arsène Ferrière, Aurélien Benoit-Lévy, Olivier Martineau-Huynh, Matías Tueros</div>
<div class="meta-line">First: 2026-02-26T18:29:48+00:00 · Latest: 2026-02-26T18:29:48+00:00</div>
<div class="meta-line">Comments: Submitted to Astroparticle Physics Journal</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23321v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23321v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Using advanced machine learning techniques, we developed a method for reconstructing precisely the arrival direction and energy of ultra-high-energy cosmic rays from the voltage traces they induced on ground-based radio detector arrays.
  In our approach, triggered antennas are represented as a graph structure, which serves as input for a graph neural network (GNN). By incorporating physical knowledge into both the GNN architecture and the input data, we improve the precision and reduce the required size of the training set with respect to a fully data-driven approach. This method achieves an angular resolution of 0.092° and an electromagnetic energy reconstruction resolution of 16.4% on simulated data with realistic noise conditions.
  We also employ uncertainty estimation methods to enhance the reliability of our predictions, quantifying the confidence of the GNN&#x27;s outputs and providing confidence intervals for both direction and energy reconstruction. Finally, we investigate strategies to verify the model&#x27;s consistency and robustness under real life variations, with the goal of identifying scenarios in which predictions remain reliable despite domain shifts between simulation and reality.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于自主射电阵列中宇宙射线方向和能量概率重建的深度集成图神经网络</div>
<div class="mono" style="margin-top:8px">我们利用先进的机器学习技术，开发了一种方法，用于从地面射电探测器阵列中由超高能宇宙射线诱导的电压轨迹中精确重建宇宙射线的到达方向和能量。在我们的方法中，触发的天线被表示为图结构，并作为图神经网络（GNN）的输入。通过将物理知识整合到GNN架构和输入数据中，我们提高了重建精度，并减少了与完全数据驱动方法相比所需的训练集规模。该方法在具有现实噪声条件的模拟数据上实现了0.092°的角度分辨率和16.4%的电磁能量重建分辨率。我们还采用了不确定性估计方法，以提高预测的可靠性，量化GNN输出的置信度，并为方向和能量重建提供置信区间。最后，我们研究了在现实变化条件下验证模型一致性和鲁棒性的策略，旨在识别在模拟与现实之间领域转移的情况下预测仍可靠的情景。</div>
</details>
</div>
<div class="card">
<div class="title">ParamMem: Augmenting Language Agents with Parametric Reflective Memory</div>
<div class="meta-line">Authors: Tianjun Yao, Yongqiang Chen, Yujia Zheng, Pan Li, Zhiqiang Shen, Kun Zhang</div>
<div class="meta-line">First: 2026-02-26T18:28:04+00:00 · Latest: 2026-02-26T18:28:04+00:00</div>
<div class="meta-line">Comments: 20 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23320v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23320v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Self-reflection enables language agents to iteratively refine solutions, yet often produces repetitive outputs that limit reasoning performance. Recent studies have attempted to address this limitation through various approaches, among which increasing reflective diversity has shown promise. Our empirical analysis reveals a strong positive correlation between reflective diversity and task success, further motivating the need for diverse reflection signals. We introduce ParamMem, a parametric memory module that encodes cross-sample reflection patterns into model parameters, enabling diverse reflection generation through temperature-controlled sampling. Building on this module, we propose ParamAgent, a reflection-based agent framework that integrates parametric memory with episodic and cross-sample memory. Extensive experiments on code generation, mathematical reasoning, and multi-hop question answering demonstrate consistent improvements over state-of-the-art baselines. Further analysis reveals that ParamMem is sample-efficient, enables weak-to-strong transfer across model scales, and supports self-improvement without reliance on stronger external model, highlighting the potential of ParamMem as an effective component for enhancing language agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ParamMem：通过参数化反射记忆增强语言代理</div>
<div class="mono" style="margin-top:8px">自我反思使语言代理能够迭代优化解决方案，但常常产生重复输出，限制了推理性能。近期研究尝试通过多种方法解决这一限制，其中增加反射多样性显示出潜力。我们的实证分析表明，反射多样性与任务成功之间存在显著的正相关关系，进一步推动了对多样化反射信号的需求。我们引入了ParamMem，这是一种参数化记忆模块，将跨样本的反射模式编码到模型参数中，通过温度控制采样实现多样化的反射生成。在此基础上，我们提出了ParamAgent，一种基于反射的代理框架，将参数化记忆与事件记忆和跨样本记忆相结合。在代码生成、数学推理和多跳问答等任务上的大量实验表明，ParamMem在多个任务上均优于当前最先进的基线模型。进一步分析表明，ParamMem具有样本高效性，能够在不同模型规模之间实现弱到强的迁移，并且无需依赖更强的外部模型即可支持自我提升，突显了其作为增强语言代理的有效组件的潜力。</div>
</details>
</div>
<div class="card">
<div class="title">LinGuinE: Longitudinal Guidance Estimation for Volumetric Tumour Segmentation</div>
<div class="meta-line">Authors: Nadine Garibli, Mayank Patwari, Bence Csiba, Yi Wei, Kostantinos Sidiropoulos</div>
<div class="meta-line">First: 2025-06-06T13:52:33+00:00 · Latest: 2026-02-26T18:27:23+00:00</div>
<div class="meta-line">Comments: 10 pages, 2 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2506.06092v2">Abs</a> · <a href="https://arxiv.org/pdf/2506.06092v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Longitudinal volumetric tumour segmentation is critical for radiotherapy planning and response assessment, yet this problem is underexplored and most methods produce single-timepoint semantic masks, lack lesion correspondence, and offer limited radiologist control. We introduce LinGuinE (Longitudinal Guidance Estimation), a PyTorch framework that combines image registration and guided segmentation to deliver lesion-level tracking and volumetric masks across all scans in a longitudinal study from a single radiologist prompt. LinGuinE is temporally direction agnostic, requires no training on longitudinal data, and allows any registration and semi-automatic segmentation algorithm to be repurposed for the task. We evaluate various combinations of registration and segmentation algorithms within the framework. LinGuinE achieves state-of-the-art segmentation and tracking performance across four datasets with a total of 456 longitudinal studies. Tumour segmentation performance shows minimal degradation with increasing temporal separation. We conduct ablation studies to determine the impact of autoregression, pathology specific finetuning, and the use of real radiologist prompts. We release our code and substantial public benchmarking for longitudinal segmentation, facilitating future research.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>LinGuinE：用于体积肿瘤分割的纵向引导估计</div>
<div class="mono" style="margin-top:8px">纵向体积肿瘤分割对于放疗计划和疗效评估至关重要，但这一问题尚未得到充分研究，大多数方法仅生成单时间点的语义掩膜，缺乏病灶对应关系，并且对放射科医生的控制有限。我们引入了LinGuinE（纵向引导估计），这是一个PyTorch框架，结合图像配准和引导分割，能够通过单个放射科医生的提示，在纵向研究中对所有扫描图像进行病灶级跟踪和体积掩膜生成。LinGuinE不依赖时间方向，无需在纵向数据上进行训练，且允许任何配准和半自动分割算法被重新用于该任务。我们在框架内评估了多种配准与分割算法的组合。LinGuinE在四个数据集（共456个纵向研究）上实现了最先进的分割和跟踪性能。随着时间间隔的增加，肿瘤分割性能仅略有下降。我们进行了消融研究，以确定自回归、病理特异性微调以及使用真实放射科医生提示的影响。我们发布了代码和大量公共基准测试数据，以促进纵向分割的未来研究。</div>
</details>
</div>
<div class="card">
<div class="title">Generalized Rapid Action Value Estimation in Memory-Constrained Environments</div>
<div class="meta-line">Authors: Aloïs Rautureau, Tristan Cazenave, Éric Piette</div>
<div class="meta-line">First: 2026-02-26T18:25:59+00:00 · Latest: 2026-02-26T18:25:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.23318v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.23318v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Generalized Rapid Action Value Estimation (GRAVE) has been shown to be a strong variant within the Monte-Carlo Tree Search (MCTS) family of algorithms for General Game Playing (GGP). However, its reliance on storing additional win/visit statistics at each node makes its use impractical in memory-constrained environments, thereby limiting its applicability in practice. In this paper, we introduce the GRAVE2, GRAVER and GRAVER2 algorithms, which extend GRAVE through two-level search, node recycling, and a combination of both techniques, respectively. We show that these enhancements enable a drastic reduction in the number of stored nodes while matching the playing strength of GRAVE.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>内存受限环境下广义快速行动价值估计</div>
<div class="mono" style="margin-top:8px">广义快速行动价值估计（GRAVE）已被证明是用于通用游戏博弈（GGP）的蒙特卡洛树搜索（MCTS）算法家族中的一个强大变种。然而，其依赖于在每个节点存储额外的胜负/访问统计信息，使得其在内存受限环境中难以实际应用。本文引入了GRAVE2、GRAVER和GRAVER2算法，分别通过两级搜索、节点回收以及两者的结合来扩展GRAVE。我们证明这些改进能够在保持GRAVE对弈强度的同时，显著减少存储的节点数量。</div>
</details>
</div>
<div class="card">
<div class="title">The Muon Magnetic Moment and Physics Beyond the Standard Model</div>
<div class="meta-line">Authors: Peter Athron, Kilian Möhling, Dominik Stöckinger, Hyejung Stöckinger-Kim</div>
<div class="meta-line">Venue: Prog.Part.Nucl.Phys. 148 (2026) 104225</div>
<div class="meta-line">First: 2025-07-12T13:59:29+00:00 · Latest: 2026-02-26T18:22:59+00:00</div>
<div class="meta-line">Comments: Invited review for Progress in Particle and Nuclear Physics; 278 pages, 50 figures. Published version, minor changes compared to V1</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2507.09289v2">Abs</a> · <a href="https://arxiv.org/pdf/2507.09289v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We review the role of the anomalous magnetic moment of the muon a_μas a powerful probe of physics beyond the Standard Model (BSM), taking advantage of the final result of the Fermilab g-2 experiment and the recently updated Standard Model value. This review provides both a comprehensive summary of the current status, as well as an accessible entry point for phenomenologists with interests in dark matter, Higgs and electroweak or neutrino and flavour physics in the context of a wide range of BSM scenarios. It begins with a qualitative overview of the field and a collection of key properties and typical results. It then focuses on model-independent, generic formulas and classifies types of BSM scenarios with or without chiral enhancements. A strong emphasis of the review are the connections to a large number of other observables -- ranging from the muon mass and the muon--Higgs coupling and related dipole observables to dark matter, neutrino masses and high-energy collider observables. Finally, we survey a number of well-motivated BSM scenarios such as dark photons, axion-like particles, the two-Higgs doublet model, supersymmetric models and models with leptoquarks, vector-like leptons or neutrino mass models. We discuss the impact of the updated Standard Model value for a_μand of complementary constraints, exploring the phenomenology and identifying excluded and viable parameter regions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>缪子磁矩与标准模型外的物理</div>
<div class="mono" style="margin-top:8px">我们回顾了缪子反常磁矩 a_μ 在探测标准模型外物理（BSM）中的重要作用，利用费米实验室 g-2 实验的最终结果和最近更新的标准模型值。本综述不仅提供了当前状态的全面总结，也为对暗物质、希格斯玻色子和弱电相互作用或中微子和味物理感兴趣的现象学家提供了一个易于理解的入门途径。综述首先从定性概述入手，收集了该领域的关键性质和典型结果。接着，重点讨论模型无关的通用公式，并对具有或不具有手性增强的 BSM 场景进行分类。综述还强调了 a_μ 与其他大量可观测量之间的联系，包括缪子质量、缪子-希格斯耦合及相关偶极可观测量，以及暗物质、中微子质量与高能对撞机可观测量。最后，我们调查了一些有良好动机的 BSM 场景，如暗光子、类轴子粒子、双希格斯双态模型、超对称模型以及包含轻子夸克、向量样轻子或中微子质量模型的理论。我们讨论了更新后的标准模型值对 a_μ 的影响，以及互补约束的效应，探索现象学并识别被排除和可行的参数区域。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We review the role of the anomalous magnetic moment of the muon a_μas a powerful probe of physics beyond the Standard Model (BSM), taking advantage of the final result of the Fermilab g-2 experiment and the recently updated Standard Model value.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260227_0412.html">20260227_0412</a>
<a href="archive/20260226_0418.html">20260226_0418</a>
<a href="archive/20260225_0421.html">20260225_0421</a>
<a href="archive/20260224_0435.html">20260224_0435</a>
<a href="archive/20260223_0401.html">20260223_0401</a>
<a href="archive/20260222_0402.html">20260222_0402</a>
<a href="archive/20260221_0415.html">20260221_0415</a>
<a href="archive/20260220_0410.html">20260220_0410</a>
<a href="archive/20260219_0419.html">20260219_0419</a>
<a href="archive/20260218_0416.html">20260218_0416</a>
<a href="archive/20260217_0410.html">20260217_0410</a>
<a href="archive/20260216_0403.html">20260216_0403</a>
<a href="archive/20260215_0401.html">20260215_0401</a>
<a href="archive/20260213_0421.html">20260213_0421</a>
<a href="archive/20260212_0425.html">20260212_0425</a>
<a href="archive/20260210_0435.html">20260210_0435</a>
<a href="archive/20260209_0404.html">20260209_0404</a>
<a href="archive/20260208_0353.html">20260208_0353</a>
<a href="archive/20260207_0410.html">20260207_0410</a>
<a href="archive/20260206_0412.html">20260206_0412</a>
<a href="archive/20260205_0417.html">20260205_0417</a>
<a href="archive/20260204_0421.html">20260204_0421</a>
<a href="archive/20260202_0402.html">20260202_0402</a>
<a href="archive/20260201_0354.html">20260201_0354</a>
<a href="archive/20260131_0408.html">20260131_0408</a>
<a href="archive/20260130_0406.html">20260130_0406</a>
<a href="archive/20260129_0407.html">20260129_0407</a>
<a href="archive/20260128_0407.html">20260128_0407</a>
<a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
