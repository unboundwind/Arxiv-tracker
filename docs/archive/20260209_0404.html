<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-09 04:04</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260209_0404</div>
    <div class="row"><div class="card">
<div class="title">EigenLoRAx: Recycling Adapters to Find Principal Subspaces for Resource-Efficient Adaptation and Inference</div>
<div class="meta-line">Authors: Prakhar Kaushik, Ankit Vaidya, Shravan Chaudhari, Alan Yuille</div>
<div class="meta-line">Venue: Proceedings of the Computer Vision and Pattern Recognition Conference, 2025, pages 649-659</div>
<div class="meta-line">First: 2025-02-07T07:07:04+00:00 · Latest: 2026-02-05T18:59:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2502.04700v5">Abs</a> · <a href="https://arxiv.org/pdf/2502.04700v5">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid growth of large models has raised concerns about their environmental impact and equity in accessibility due to significant computational costs. Low-Rank Adapters (LoRA) offer a lightweight solution for finetuning large models, resulting in an abundance of publicly available adapters tailored to diverse domains. We ask: Can these pretrained adapters be leveraged to further streamline adaptation to new tasks while addressing these challenges? We introduce EigenLoRAx, a parameter-efficient finetuning method that recycles existing adapters to create a principal subspace aligned with their shared domain knowledge which can be further augmented with orthogonal basis vectors in low-resource scenarios. This enables rapid adaptation to new tasks by learning only lightweight coefficients on the principal components of the subspace-eliminating the need to finetune entire adapters. EigenLoRAx requires significantly fewer parameters and memory, improving efficiency for both training and inference. Our method demonstrates strong performance across diverse domains and tasks, offering a scalable for edge-based applications, personalization, and equitable deployment of large models in resource-constrained environments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EigenLoRAx：利用适配器回收以找到主子空间，实现资源高效的适应与推理</div>
<div class="mono" style="margin-top:8px">大型模型的快速增长引发了对其环境影响和可访问性公平性的担忧，因为其计算成本显著。低秩适配器（LoRA）为微调大型模型提供了一种轻量级解决方案，导致大量针对不同领域的公开适配器出现。我们提出问题：能否利用这些预训练适配器进一步简化对新任务的适应，同时应对这些挑战？我们引入了EigenLoRAx，这是一种参数高效的微调方法，通过回收现有适配器来构建一个与它们共享领域知识对齐的主子空间，并在资源有限的情况下进一步用正交基向量进行扩充。这使得通过仅学习子空间主成分上的轻量级系数即可快速适应新任务，而无需微调整个适配器。EigenLoRAx所需的参数和内存显著减少，从而提高了训练和推理的效率。我们的方法在多种领域和任务中表现出色，为边缘设备应用、个性化以及资源受限环境中大型模型的公平部署提供了可扩展的解决方案。</div>
</details>
</div>
<div class="card">
<div class="title">Shared LoRA Subspaces for almost Strict Continual Learning</div>
<div class="meta-line">Authors: Prakhar Kaushik, Ankit Vaidya, Shravan Chaudhari, Rama Chellappa, Alan Yuille</div>
<div class="meta-line">First: 2026-02-05T18:59:58+00:00 · Latest: 2026-02-05T18:59:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06043v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06043v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining. While parameter-efficient tuning methods like low rank adaptation (LoRA) reduce computational demands, they lack mechanisms for strict continual learning and knowledge integration, without relying on data replay, or multiple adapters. We propose Share, a novel approach to parameter efficient continual finetuning that learns and dynamically updates a single, shared low-rank subspace, enabling seamless adaptation across multiple tasks and modalities. Share constructs a foundational subspace that extracts core knowledge from past tasks and incrementally integrates new information by identifying essential subspace directions. Knowledge from each new task is incorporated into this evolving subspace, facilitating forward knowledge transfer, while minimizing catastrophic interference. This approach achieves up to 100x parameter reduction and 281x memory savings over traditional LoRA methods, maintaining performance comparable to jointly trained models. A single Share model can replace hundreds of task-specific LoRA adapters, supporting scalable, asynchronous continual learning. Experiments across image classification, natural language understanding, 3D pose estimation, and text-to-image generation validate its effectiveness, making Share a practical and scalable solution for lifelong learning in large-scale AI systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于近似严格持续学习的共享LoRA子空间</div>
<div class="mono" style="margin-top:8px">高效且持续地将大型预训练模型适应到新任务对于实际部署至关重要，但由于灾难性遗忘和重新训练的高昂成本，这仍然具有挑战性。尽管参数高效微调方法如低秩适应（LoRA）可以降低计算需求，但它们缺乏严格持续学习和知识整合的机制，且不依赖数据回放或多适配器。我们提出Share，一种新颖的参数高效持续微调方法，它学习并动态更新一个单一的共享低秩子空间，从而实现跨多个任务和模态的无缝适应。Share构建了一个基础子空间，从过去任务中提取核心知识，并通过识别关键子空间方向逐步整合新信息。每个新任务的知识被整合到这个不断演化的子空间中，促进前向知识传递，同时减少灾难性干扰。该方法相比传统LoRA方法实现了高达100倍的参数减少和281倍的内存节省，同时保持与联合训练模型相当的性能。一个Share模型可以替代数百个任务特定的LoRA适配器，支持可扩展的异步持续学习。在图像分类、自然语言理解、3D姿态估计和文本到图像生成等实验中验证了其有效性，使Share成为大规模AI系统中终身学习的实用且可扩展的解决方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Adapting large pretrained models to new tasks efficiently and continually is crucial for real-world deployment but remains challenging due to catastrophic forgetting and the high cost of retraining.</div>
</details>
</div>
<div class="card">
<div class="title">Pseudo-Invertible Neural Networks</div>
<div class="meta-line">Authors: Yamit Ehrlich, Nimrod Berman, Assaf Shocher</div>
<div class="meta-line">First: 2026-02-05T18:59:58+00:00 · Latest: 2026-02-05T18:59:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06042v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06042v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The Moore-Penrose Pseudo-inverse (PInv) serves as the fundamental solution for linear systems. In this paper, we propose a natural generalization of PInv to the nonlinear regime in general and to neural networks in particular. We introduce Surjective Pseudo-invertible Neural Networks (SPNN), a class of architectures explicitly designed to admit a tractable non-linear PInv. The proposed non-linear PInv and its implementation in SPNN satisfy fundamental geometric properties. One such property is null-space projection or &quot;Back-Projection&quot;, $x&#x27; = x + A^\dagger(y-Ax)$, which moves a sample $x$ to its closest consistent state $x&#x27;$ satisfying $Ax=y$. We formalize Non-Linear Back-Projection (NLBP), a method that guarantees the same consistency constraint for non-linear mappings $f(x)=y$ via our defined PInv. We leverage SPNNs to expand the scope of zero-shot inverse problems. Diffusion-based null-space projection has revolutionized zero-shot solving for linear inverse problems by exploiting closed-form back-projection. We extend this method to non-linear degradations. Here, &quot;degradation&quot; is broadly generalized to include any non-linear loss of information, spanning from optical distortions to semantic abstractions like classification. This approach enables zero-shot inversion of complex degradations and allows precise semantic control over generative outputs without retraining the diffusion prior.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>伪可逆神经网络</div>
<div class="mono" style="margin-top:8px">Moore-Penrose伪逆（PInv）是线性系统的基本解法。本文提出了一种PInv在非线性领域的一般化方法，特别是在神经网络中。我们引入了Surjective Pseudo-invertible Neural Networks（SPNN），这是一种专门设计以允许可处理非线性伪逆的架构。所提出的非线性伪逆及其在SPNN中的实现满足基本的几何属性。其中一个属性是零空间投影或&quot;反投影&quot;，即 $x&#x27; = x + A^\dagger(y - Ax)$，该方法将样本 $x$ 移动到最接近的一致状态 $x&#x27;$，使得 $Ax = y$。我们形式化了Non-Linear Back-Projection（NLBP），这是一种方法，通过我们定义的伪逆，确保非线性映射 $f(x) = y$ 满足相同的自洽约束。我们利用SPNN扩展了零样本逆问题的范围。基于扩散的零空间投影方法通过利用闭式反投影，革新了线性逆问题的零样本求解。我们将该方法扩展到非线性退化情况。此处，&quot;退化&quot;被广义地定义为包括任何非线性信息损失，从光学畸变到语义抽象（如分类）等。这种方法使得对复杂退化的零样本逆成为可能，并且无需重新训练扩散先验，即可实现对生成输出的精确语义控制。</div>
</details>
</div>
<div class="card">
<div class="title">Predicting Camera Pose from Perspective Descriptions for Spatial Reasoning</div>
<div class="meta-line">Authors: Xuejun Zhang, Aditi Tiwari, Zhenhailong Wang, Heng Ji</div>
<div class="meta-line">First: 2026-02-05T18:59:55+00:00 · Latest: 2026-02-05T18:59:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06041v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06041v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-image spatial reasoning remains challenging for current multimodal large language models (MLLMs). While single-view perception is inherently 2D, reasoning over multiple views requires building a coherent scene understanding across viewpoints. In particular, we study perspective taking, where a model must build a coherent 3D understanding from multi-view observations and use it to reason from a new, language-specified viewpoint. We introduce CAMCUE, a pose-aware multi-image framework that uses camera pose as an explicit geometric anchor for cross-view fusion and novel-view reasoning. CAMCUE injects per-view pose into visual tokens, grounds natural-language viewpoint descriptions to a target camera pose, and synthesizes a pose-conditioned imagined target view to support answering. To support this setting, we curate CAMCUE-DATA with 27,668 training and 508 test instances pairing multi-view images and poses with diverse target-viewpoint descriptions and perspective-shift questions. We also include human-annotated viewpoint descriptions in the test split to evaluate generalization to human language. CAMCUE improves overall accuracy by 9.06% and predicts target poses from natural-language viewpoint descriptions with over 90% rotation accuracy within 20° and translation accuracy within a 0.5 error threshold. This direct grounding avoids expensive test-time search-and-match, reducing inference time from 256.6s to 1.45s per example and enabling fast, interactive use in real-world scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从视角描述预测相机姿态以实现空间推理</div>
<div class="mono" style="margin-top:8px">多图像空间推理对于当前的多模态大语言模型（MLLMs）仍然是一个具有挑战性的任务。虽然单视角感知本质上是2D的，但跨视角推理需要在不同视角之间建立一致的场景理解。我们特别研究了视角转换（perspective taking）问题，其中模型必须从多视角观察中构建一致的3D理解，并据此在新的语言指定视角下进行推理。我们提出了CAMCUE，这是一个具有姿态感知的多图像框架，它将相机姿态作为跨视角融合和新视角推理的显式几何锚点。CAMCUE将每个视角的姿态注入视觉标记中，将自然语言视角描述锚定到目标相机姿态，并合成基于姿态的想象目标视角以支持回答。为了支持这一设置，我们整理了CAMCUE-DATA数据集，包含27,668个训练实例和508个测试实例，这些实例将多视角图像与姿态配对，并包含多样化的目标视角描述和视角转换问题。我们还在测试集中加入了人工标注的视角描述，以评估模型对人类语言的泛化能力。CAMCUE将整体准确率提升了9.06%，并且能够从自然语言视角描述中预测目标姿态，其旋转准确率超过90%（在20°以内），平移准确率在0.5误差阈值内。这种直接的锚定方式避免了昂贵的测试时搜索和匹配过程，将推理时间从每个示例256.6秒减少到1.45秒，从而实现了快速、交互式的实际应用。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-image spatial reasoning remains challenging for current multimodal large language models (MLLMs).</div>
</details>
</div>
<div class="card">
<div class="title">DyTopo: Dynamic Topology Routing for Multi-Agent Reasoning via Semantic Matching</div>
<div class="meta-line">Authors: Yuxing Lu, Yucheng Hu, Xukai Zhao, Jiuxin Cao</div>
<div class="meta-line">First: 2026-02-05T18:59:51+00:00 · Latest: 2026-02-05T18:59:51+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06039v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06039v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent systems built from prompted large language models can improve multi-round reasoning, yet most existing pipelines rely on fixed, trajectory-wide communication patterns that are poorly matched to the stage-dependent needs of iterative problem solving. We introduce DyTopo, a manager-guided multi-agent framework that reconstructs a sparse directed communication graph at each round. Conditioned on the manager&#x27;s round goal, each agent outputs lightweight natural-language query (need) and \key (offer) descriptors; DyTopo embeds these descriptors and performs semantic matching, routing private messages only along the induced edges. Across code generation and mathematical reasoning benchmarks and four LLM backbones, DyTopo consistently outperforms over the strongest baseline (avg. +6.2). Beyond accuracy, DyTopo yields an interpretable coordination trace via the evolving graphs, enabling qualitative inspection of how communication pathways reconfigure across rounds.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DyTopo：通过语义匹配实现多智能体推理的动态拓扑路由</div>
<div class="mono" style="margin-top:8px">由提示大型语言模型构建的多智能体系统可以提升多轮推理能力，但大多数现有流程依赖于固定的、轨迹范围内的通信模式，这些模式与迭代问题解决阶段相关的需要不匹配。我们引入了DyTopo，这是一个由管理者引导的多智能体框架，它在每一轮中重构一个稀疏的有向通信图。基于管理者的轮次目标，每个智能体输出轻量级的自然语言查询（need）和关键词（offer）描述符；DyTopo将这些描述符嵌入并进行语义匹配，仅沿诱导的边路由私有消息。在代码生成和数学推理基准测试以及四个LLM主干模型上，DyTopo始终优于最强基线（平均提升+6.2）。除了准确性，DyTopo还通过演变的图生成可解释的协调轨迹，使人们能够定性地检查通信路径如何在轮次间重新配置。</div>
</details>
</div>
<div class="card">
<div class="title">SwimBird: Eliciting Switchable Reasoning Mode in Hybrid Autoregressive MLLMs</div>
<div class="meta-line">Authors: Jintao Tong, Shilin Yan, Hongwei Xue, Xiaojun Tang, Kunyu Shi, Guannan Zhang, Ruixuan Li, Yixiong Zou</div>
<div class="meta-line">First: 2026-02-05T18:59:51+00:00 · Latest: 2026-02-05T18:59:51+00:00</div>
<div class="meta-line">Comments: Project Page: https://accio-lab.github.io/SwimBird</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06040v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06040v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://accio-lab.github.io/SwimBird">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language. However, most existing MLLMs perform reasoning primarily with textual CoT, which limits their effectiveness on vision-intensive tasks. Recent approaches inject a fixed number of continuous hidden states as &quot;visual thoughts&quot; into the reasoning process and improve visual performance, but often at the cost of degraded text-based logical reasoning. We argue that the core limitation lies in a rigid, pre-defined reasoning pattern that cannot adaptively choose the most suitable thinking modality for different user queries. We introduce SwimBird, a reasoning-switchable MLLM that dynamically switches among three reasoning modes conditioned on the input: (1) text-only reasoning, (2) vision-only reasoning (continuous hidden states as visual thoughts), and (3) interleaved vision-text reasoning. To enable this capability, we adopt a hybrid autoregressive formulation that unifies next-token prediction for textual thoughts with next-embedding prediction for visual thoughts, and design a systematic reasoning-mode curation strategy to construct SwimBird-SFT-92K, a diverse supervised fine-tuning dataset covering all three reasoning patterns. By enabling flexible, query-adaptive mode selection, SwimBird preserves strong textual logic while substantially improving performance on vision-dense tasks. Experiments across diverse benchmarks covering textual reasoning and challenging visual understanding demonstrate that SwimBird achieves state-of-the-art results and robust gains over prior fixed-pattern multimodal reasoning methods.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>SwimBird：在混合自回归多模态大语言模型中引入可切换的推理模式</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）通过连接视觉和语言，在多模态感知和推理方面取得了显著进展。然而，大多数现有MLLMs主要依赖文本形式的链式推理（CoT），这限制了它们在视觉密集型任务中的有效性。最近的方法通过在推理过程中注入固定数量的连续隐藏状态作为“视觉思维”来提升视觉性能，但往往以牺牲基于文本的逻辑推理为代价。我们认为，核心限制在于一种刚性且预定义的推理模式，无法根据不同的用户查询自适应地选择最合适的思维模态。我们引入了SwimBird，这是一种可切换推理模式的MLLM，能够根据输入动态切换三种推理模式：（1）仅文本推理，（2）仅视觉推理（连续隐藏状态作为视觉思维），（3）视觉-文本交错推理。为了实现这一能力，我们采用了一种混合自回归框架，将文本思维的下一个词预测与视觉思维的下一个嵌入预测统一起来，并设计了一种系统化的推理模式策略，构建了涵盖所有三种推理模式的多样化监督微调数据集SwimBird-SFT-92K。通过实现灵活且适应查询的模式选择，SwimBird在保持强大文本逻辑的同时，显著提升了在视觉密集型任务中的性能。在涵盖文本推理和具有挑战性的视觉理解的多样化基准测试中，实验表明SwimBird在现有固定模式多模态推理方法的基础上取得了最先进的结果和稳健的提升。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) have made remarkable progress in multimodal perception and reasoning by bridging vision and language.</div>
</details>
</div>
<div class="card">
<div class="title">CommCP: Efficient Multi-Agent Coordination via LLM-Based Communication with Conformal Prediction</div>
<div class="meta-line">Authors: Xiaopan Zhang, Zejin Wang, Zhixu Li, Jianpeng Yao, Jiachen Li</div>
<div class="meta-line">Venue: ICRA 2026</div>
<div class="meta-line">First: 2026-02-05T18:59:45+00:00 · Latest: 2026-02-05T18:59:45+00:00</div>
<div class="meta-line">Comments: IEEE International Conference on Robotics and Automation (ICRA 2026); Project Website: https://comm-cp.github.io/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06038v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06038v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://comm-cp.github.io/">Project1</a> · <a href="https://comm-cp.github.io">Project2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">To complete assignments provided by humans in natural language, robots must interpret commands, generate and answer relevant questions for scene understanding, and manipulate target objects. Real-world deployments often require multiple heterogeneous robots with different manipulation capabilities to handle different assignments cooperatively. Beyond the need for specialized manipulation skills, effective information gathering is important in completing these assignments. To address this component of the problem, we formalize the information-gathering process in a fully cooperative setting as an underexplored multi-agent multi-task Embodied Question Answering (MM-EQA) problem, which is a novel extension of canonical Embodied Question Answering (EQA), where effective communication is crucial for coordinating efforts without redundancy. To address this problem, we propose CommCP, a novel LLM-based decentralized communication framework designed for MM-EQA. Our framework employs conformal prediction to calibrate the generated messages, thereby minimizing receiver distractions and enhancing communication reliability. To evaluate our framework, we introduce an MM-EQA benchmark featuring diverse, photo-realistic household scenarios with embodied questions. Experimental results demonstrate that CommCP significantly enhances the task success rate and exploration efficiency over baselines. The experiment videos, code, and dataset are available on our project website: https://comm-cp.github.io.</div></details>
</div>
<div class="card">
<div class="title">Thinking with Geometry: Active Geometry Integration for Spatial Reasoning</div>
<div class="meta-line">Authors: Haoyuan Li, Qihang Cao, Tao Tang, Kun Xiang, Zihan Guo, Jianhua Han, Hang Xu, Xiaodan Liang</div>
<div class="meta-line">First: 2026-02-05T18:59:32+00:00 · Latest: 2026-02-05T18:59:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06037v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06037v1">PDF</a> · <a href="https://github.com/Li-Hao-yuan/GeoThinker">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders. However, most existing integration strategies remain passive: geometry is exposed as a global stream and fused in an indiscriminate manner, which often induces semantic-geometry misalignment and redundant signals. We propose GeoThinker, a framework that shifts the paradigm from passive fusion to active perception. Instead of feature mixing, GeoThinker enables the model to selectively retrieve geometric evidence conditioned on its internal reasoning demands. GeoThinker achieves this through Spatial-Grounded Fusion applied at carefully selected VLM layers, where semantic visual priors selectively query and integrate task-relevant geometry via frame-strict cross-attention, further calibrated by Importance Gating that biases per-frame attention toward task-relevant structures. Comprehensive evaluation results show that GeoThinker sets a new state-of-the-art in spatial intelligence, achieving a peak score of 72.6 on the VSI-Bench. Furthermore, GeoThinker demonstrates robust generalization and significantly improved spatial perception across complex downstream scenarios, including embodied referring and autonomous driving. Our results indicate that the ability to actively integrate spatial structures is essential for next-generation spatial intelligence. Code can be found at https://github.com/Li-Hao-yuan/GeoThinker.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>基于几何的思考：用于空间推理的主动几何整合</div>
<div class="mono" style="margin-top:8px">近年来，多模态大语言模型（MLLMs）在空间推理方面取得了进展，越来越多地利用3D编码器中的几何先验知识。然而，大多数现有的整合策略仍然是被动的：几何信息被作为全局流暴露出来，并以无差别的方式进行融合，这通常会导致语义与几何的不匹配以及冗余信号。我们提出了GeoThinker框架，将整合范式从被动融合转变为主动感知。与特征混合不同，GeoThinker使模型能够根据其内部推理需求选择性地检索几何证据。GeoThinker通过在精心选择的VLM层上应用空间锚定融合实现这一目标，其中语义视觉先验知识通过帧严格交叉注意力选择性地查询和整合任务相关的几何信息，并进一步通过重要性门控进行校准，该门控机制会将每帧的注意力偏向任务相关的结构。全面的评估结果表明，GeoThinker在空间智能方面设定了新的最先进水平，在VSI-Bench上取得了72.6的峰值分数。此外，GeoThinker在复杂下游场景中展示了强大的泛化能力和显著提升的空间感知能力，包括具身指称和自动驾驶。我们的结果表明，能够主动整合空间结构的能力对于下一代空间智能至关重要。代码可在https://github.com/Li-Hao-yuan/GeoThinker上找到。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent progress in spatial reasoning with Multimodal Large Language Models (MLLMs) increasingly leverages geometric priors from 3D encoders.</div>
</details>
</div>
<div class="card">
<div class="title">DFlash: Block Diffusion for Flash Speculative Decoding</div>
<div class="meta-line">Authors: Jian Chen, Yesheng Liang, Zhijian Liu</div>
<div class="meta-line">First: 2026-02-05T18:59:30+00:00 · Latest: 2026-02-05T18:59:30+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06036v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06036v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization. Speculative decoding mitigates this bottleneck by using a fast draft model whose outputs are verified in parallel by the target LLM; however, existing methods still rely on autoregressive drafting, which remains sequential and limits practical speedups. Diffusion LLMs offer a promising alternative by enabling parallel generation, but current diffusion models typically underperform compared with autoregressive models. In this paper, we introduce DFlash, a speculative decoding framework that employs a lightweight block diffusion model for parallel drafting. By generating draft tokens in a single forward pass and conditioning the draft model on context features extracted from the target model, DFlash enables efficient drafting with high-quality outputs and higher acceptance rates. Experiments show that DFlash achieves over 6x lossless acceleration across a range of models and tasks, delivering up to 2.5x higher speedup than the state-of-the-art speculative decoding method EAGLE-3.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>DFlash：用于Flash推测解码的块扩散</div>
<div class="mono" style="margin-top:8px">自回归大语言模型（LLMs）表现出色，但需要本质上顺序的解码过程，导致推理延迟高且GPU利用率低。推测解码通过使用一个快速的草稿模型来缓解这一瓶颈，其输出由目标LLM并行验证；然而，现有方法仍依赖于自回归草稿生成，这仍然是顺序的，限制了实际的速度提升。扩散LLMs提供了一种有前景的替代方案，能够实现并行生成，但当前的扩散模型通常在性能上不如自回归模型。本文介绍了DFlash，一个采用轻量级块扩散模型的推测解码框架，通过单次前向传播生成草稿标记，并利用目标模型提取的上下文特征对草稿模型进行条件化，从而实现高效草稿生成，同时保持高质量输出和更高的接受率。实验表明，DFlash在多种模型和任务上实现了超过6倍的无损加速，其速度提升比最先进的推测解码方法EAGLE-3高出高达2.5倍。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Autoregressive large language models (LLMs) deliver strong performance but require inherently sequential decoding, leading to high inference latency and poor GPU utilization.</div>
</details>
</div>
<div class="card">
<div class="title">InterPrior: Scaling Generative Control for Physics-Based Human-Object Interactions</div>
<div class="meta-line">Authors: Sirui Xu, Samuel Schulter, Morteza Ziyadi, Xialin He, Xiaohan Fei, Yu-Xiong Wang, Liangyan Gui</div>
<div class="meta-line">First: 2026-02-05T18:59:27+00:00 · Latest: 2026-02-05T18:59:27+00:00</div>
<div class="meta-line">Comments: Webpage: https://sirui-xu.github.io/InterPrior/</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06035v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06035v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://sirui-xu.github.io/InterPrior/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements. High-level intentions, such as affordance, define the goal, while coordinated balance, contact, and manipulation can emerge naturally from underlying physical and motor priors. Scaling such priors is key to enabling humanoids to compose and generalize loco-manipulation skills across diverse contexts while maintaining physically coherent whole-body coordination. To this end, we introduce InterPrior, a scalable framework that learns a unified generative controller through large-scale imitation pretraining and post-training by reinforcement learning. InterPrior first distills a full-reference imitation expert into a versatile, goal-conditioned variational policy that reconstructs motion from multimodal observations and high-level intent. While the distilled policy reconstructs training behaviors, it does not generalize reliably due to the vast configuration space of large-scale human-object interactions. To address this, we apply data augmentation with physical perturbations, and then perform reinforcement learning finetuning to improve competence on unseen goals and initializations. Together, these steps consolidate the reconstructed latent skills into a valid manifold, yielding a motion prior that generalizes beyond the training data, e.g., it can incorporate new behaviors such as interactions with unseen objects. We further demonstrate its effectiveness for user-interactive control and its potential for real robot deployment.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>InterPrior: 扩展基于物理的人机交互生成控制</div>
<div class="mono" style="margin-top:8px">人类很少在显式的全身运动层面规划与物体的全身交互。高层次意图，如可利用性，定义目标，而协调的平衡、接触和操作则可以从底层的物理和运动先验中自然涌现。扩展此类先验是使人形机器人能够在各种情境中组合和泛化移动与操作技能的关键。为此，我们引入了InterPrior，一个可扩展的框架，通过大规模模仿预训练和强化学习微调来学习统一的生成控制器。InterPrior首先将一个完整的参考模仿专家蒸馏为一个通用、目标条件的变分策略，该策略能够从多模态观测和高层次意图中重建运动。虽然蒸馏策略可以重建训练行为，但由于大规模人机交互的庞大配置空间，其泛化能力并不可靠。为了解决这一问题，我们应用了带有物理扰动的数据增强，并进行强化学习微调以提升对未见过的目标和初始状态的适应能力。这些步骤共同将重建的潜在技能整合为一个有效的流形，从而得到一个能够超越训练数据泛化的运动先验，例如，它可以包含与未见过物体的交互等新行为。我们进一步展示了其在用户交互控制中的有效性及其在真实机器人部署中的潜力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Humans rarely plan whole-body interactions with objects at the level of explicit whole-body movements.</div>
</details>
</div>
<div class="card">
<div class="title">V-Retrver: Evidence-Driven Agentic Reasoning for Universal Multimodal Retrieval</div>
<div class="meta-line">Authors: Dongyang Chen, Chaoyang Wang, Dezhao SU, Xi Xiao, Zeyu Zhang, Jing Xiong, Qing Li, Yuzhang Shang, Shichao Ka</div>
<div class="meta-line">First: 2026-02-05T18:59:21+00:00 · Latest: 2026-02-05T18:59:21+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06034v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06034v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multimodal Large Language Models (MLLMs) have recently been applied to universal multimodal retrieval, where Chain-of-Thought (CoT) reasoning improves candidate reranking. However, existing approaches remain largely language-driven, relying on static visual encodings and lacking the ability to actively verify fine-grained visual evidence, which often leads to speculative reasoning in visually ambiguous cases. We propose V-Retrver, an evidence-driven retrieval framework that reformulates multimodal retrieval as an agentic reasoning process grounded in visual inspection. V-Retrver enables an MLLM to selectively acquire visual evidence during reasoning via external visual tools, performing a multimodal interleaved reasoning process that alternates between hypothesis generation and targeted visual verification.To train such an evidence-gathering retrieval agent, we adopt a curriculum-based learning strategy combining supervised reasoning activation, rejection-based refinement, and reinforcement learning with an evidence-aligned objective. Experiments across multiple multimodal retrieval benchmarks demonstrate consistent improvements in retrieval accuracy (with 23.0% improvements on average), perception-driven reasoning reliability, and generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>V-Retrver：面向通用多模态检索的证据驱动代理推理</div>
<div class="mono" style="margin-top:8px">多模态大语言模型（MLLMs）最近被应用于通用多模态检索，其中思维链（Chain-of-Thought, CoT）推理提升了候选结果的重排序效果。然而，现有方法大多依赖语言驱动，使用静态视觉编码，并缺乏主动验证细粒度视觉证据的能力，这在视觉模糊的情况下常导致推测性推理。我们提出V-Retrver，一个基于证据的检索框架，将多模态检索重新表述为以视觉检查为基础的代理推理过程。V-Retrver使MLLM能够在推理过程中通过外部视觉工具选择性地获取视觉证据，执行交替生成假设和针对性视觉验证的多模态交错推理过程。为训练此类证据收集检索代理，我们采用基于课程的学习策略，结合监督推理激活、基于拒绝的优化以及与证据对齐的目标的强化学习。在多个多模态检索基准上的实验表明，该方法在检索准确率（平均提升23.0%）、感知驱动推理的可靠性以及泛化能力方面均表现出一致的改进。</div>
</details>
</div>
<div class="card">
<div class="title">Can vision language models learn intuitive physics from interaction?</div>
<div class="meta-line">Authors: Luca M. Schulze Buschoff, Konstantinos Voudouris, Can Demircan, Eric Schulz</div>
<div class="meta-line">First: 2026-02-05T18:59:20+00:00 · Latest: 2026-02-05T18:59:20+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06033v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06033v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-trained vision language models do not have good intuitions about the physical world. Recent work has shown that supervised fine-tuning can improve model performance on simple physical tasks. However, fine-tuned models do not appear to learn robust physical rules that can generalize to new contexts. Based on research in cognitive science, we hypothesize that models need to interact with an environment to properly learn its physical dynamics. We train models that learn through interaction with the environment using reinforcement learning. While learning from interaction allows models to improve their within-task performance, it fails to produce models with generalizable physical intuitions. We find that models trained on one task do not reliably generalize to related tasks, even if the tasks share visual statistics and physical principles, and regardless of whether the models are trained through interaction.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>视觉语言模型能否通过交互学习直观物理？</div>
<div class="mono" style="margin-top:8px">预训练的视觉语言模型对物理世界缺乏良好的直觉。近期研究表明，监督微调可以提升模型在简单物理任务上的表现。然而，微调后的模型似乎未能学习到能够推广到新情境的稳健物理规则。基于认知科学研究，我们假设模型需要与环境进行交互才能正确学习其物理动态。我们使用强化学习训练模型，使其通过与环境的交互来学习。虽然通过交互学习能够让模型在任务内表现提升，但它未能产生具有可推广物理直觉的模型。我们发现，即使任务共享视觉统计信息和物理原理，仅在单一任务上训练的模型也无法可靠地推广到相关任务，无论其是否通过交互进行训练。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Pre-trained vision language models do not have good intuitions about the physical world.</div>
</details>
</div>
<div class="card">
<div class="title">Splat and Distill: Augmenting Teachers with Feed-Forward 3D Reconstruction For 3D-Aware Distillation</div>
<div class="meta-line">Authors: David Shavin, Sagie Benaim</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-05T18:59:05+00:00 · Latest: 2026-02-05T18:59:05+00:00</div>
<div class="meta-line">Comments: Accepted to ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06032v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06032v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://davidshavin4.github.io/Splat-and-Distill/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Vision Foundation Models (VFMs) have achieved remarkable success when applied to various downstream 2D tasks. Despite their effectiveness, they often exhibit a critical lack of 3D awareness. To this end, we introduce Splat and Distill, a framework that instills robust 3D awareness into 2D VFMs by augmenting the teacher model with a fast, feed-forward 3D reconstruction pipeline. Given 2D features produced by a teacher model, our method first lifts these features into an explicit 3D Gaussian representation, in a feedforward manner. These 3D features are then ``splatted&quot; onto novel viewpoints, producing a set of novel 2D feature maps used to supervise the student model, ``distilling&quot; geometrically grounded knowledge. By replacing slow per-scene optimization of prior work with our feed-forward lifting approach, our framework avoids feature-averaging artifacts, creating a dynamic learning process where the teacher&#x27;s consistency improves alongside that of the student. We conduct a comprehensive evaluation on a suite of downstream tasks, including monocular depth estimation, surface normal estimation, multi-view correspondence, and semantic segmentation. Our method significantly outperforms prior works, not only achieving substantial gains in 3D awareness but also enhancing the underlying semantic richness of 2D features. Project page is available at https://davidshavin4.github.io/Splat-and-Distill/</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>Splat and Distill：通过前馈3D重建增强教师模型以实现3D感知的知识蒸馏</div>
<div class="mono" style="margin-top:8px">视觉基础模型（VFMs）在各种下游2D任务中取得了显著成功。尽管它们在效果上表现良好，但通常缺乏关键的3D感知能力。为此，我们提出了Splat and Distill框架，通过为教师模型添加一个快速的前馈3D重建流程，使2D VFMs具备强大的3D感知能力。给定教师模型生成的2D特征，我们的方法首先以前馈方式将这些特征提升为显式的3D高斯表示。随后，这些3D特征被“投影”到新的视角，生成一组新的2D特征图，用于监督学生模型，从而实现几何知识的“蒸馏”。通过用我们的前馈提升方法替代先前工作中缓慢的每场景优化过程，我们的框架避免了特征平均的伪影，创造了一个动态的学习过程，其中教师和学生的保持性同步提升。我们在一系列下游任务上进行了全面评估，包括单目深度估计、表面法线估计、多视角对应和语义分割。我们的方法显著优于先前的工作，不仅在3D感知方面取得了显著提升，还增强了底层2D特征的语义丰富性。项目页面可在 https://davidshavin4.github.io/Splat-and-Distill/ 查看。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Vision Foundation Models (VFMs) have achieved remarkable success when applied to various downstream 2D tasks.</div>
</details>
</div>
<div class="card">
<div class="title">PhysicsAgentABM: Physics-Guided Generative Agent-Based Modeling</div>
<div class="meta-line">Authors: Kavana Venkatesh, Yinhan He, Jundong Li, Jiaming Cui</div>
<div class="meta-line">First: 2026-02-05T18:59:01+00:00 · Latest: 2026-02-05T18:59:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06030v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06030v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors. We propose PhysicsAgentABM, which shifts inference to behaviorally coherent agent clusters: state-specialized symbolic agents encode mechanistic transition priors, a multimodal neural transition model captures temporal and interaction dynamics, and uncertainty-aware epistemic fusion yields calibrated cluster-level transition distributions. Individual agents then stochastically realize transitions under local constraints, decoupling population inference from entity-level variability. We further introduce ANCHOR, an LLM agent-driven clustering strategy based on cross-contextual behavioral responses and a novel contrastive loss, reducing LLM calls by up to 6-8 times. Experiments across public health, finance, and social sciences show consistent gains in event-time accuracy and calibration over mechanistic, neural, and LLM baselines. By re-architecting generative ABM around population-level inference with uncertainty-aware neuro-symbolic fusion, PhysicsAgentABM establishes a new paradigm for scalable and calibrated simulation with LLMs.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>PhysicsAgentABM：基于物理引导的生成式多智能体建模</div>
<div class="mono" style="margin-top:8px">基于大语言模型（LLM）的多智能体系统能够实现丰富的智能体推理，但在扩展时成本较高，并且在时间步对齐的状态转移模拟中校准效果不佳。而经典智能体建模（ABM）虽然具有可解释性，但难以整合丰富的个体信号和非平稳行为。我们提出PhysicsAgentABM，将推理转移到行为一致的智能体集群：状态专用的符号智能体编码了机制性的状态转移先验，多模态神经状态转移模型捕捉了时间动态和交互动态，不确定性感知的知识融合方法生成了校准的集群级状态转移分布。个体智能体随后在局部约束下随机实现状态转移，从而将群体推理与实体级变化解耦。我们进一步引入ANCHOR，这是一种基于跨上下文行为响应和新颖对比损失的LLM驱动的聚类策略，可将LLM调用次数减少高达6-8倍。在公共卫生、金融和社会科学领域的实验表明，与机制性、神经网络和LLM基线方法相比，PhysicsAgentABM在事件时间准确性和校准方面表现出一致的提升。通过围绕不确定性感知的神经符号融合方法重构生成式ABM，PhysicsAgentABM建立了一种基于LLM的可扩展且校准的模拟新范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language model (LLM)-based multi-agent systems enable expressive agent reasoning but are expensive to scale and poorly calibrated for timestep-aligned state-transition simulation, while classical agent-based models (ABMs) offer interpretability but struggle to integrate rich individual-level signals and non-stationary behaviors.</div>
</details>
</div>
<div class="card">
<div class="title">AP-OOD: Attention Pooling for Out-of-Distribution Detection</div>
<div class="meta-line">Authors: Claus Hofmann, Christian Huber, Bernhard Lehner, Daniel Klotz, Sepp Hochreiter, Werner Zellinger</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-05T18:59:01+00:00 · Latest: 2026-02-05T18:59:01+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06031v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06031v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Out-of-distribution (OOD) detection, which maps high-dimensional data into a scalar OOD score, is critical for the reliable deployment of machine learning models. A key challenge in recent research is how to effectively leverage and aggregate token embeddings from language models to obtain the OOD score. In this work, we propose AP-OOD, a novel OOD detection method for natural language that goes beyond simple average-based aggregation by exploiting token-level information. AP-OOD is a semi-supervised approach that flexibly interpolates between unsupervised and supervised settings, enabling the use of limited auxiliary outlier data. Empirically, AP-OOD sets a new state of the art in OOD detection for text: in the unsupervised setting, it reduces the FPR95 (false positive rate at 95% true positives) from 27.84% to 4.67% on XSUM summarization, and from 77.08% to 70.37% on WMT15 En-Fr translation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AP-OOD：用于分布外检测的注意力池化</div>
<div class="mono" style="margin-top:8px">将高维数据映射为标量分布外得分的分布外检测（OOD）对于机器学习模型的可靠部署至关重要。近期研究中的一个关键挑战是如何有效利用和聚合语言模型的token嵌入以获得分布外得分。在本工作中，我们提出了一种名为AP-OOD的新颖自然语言分布外检测方法，该方法超越了基于简单平均的聚合方式，利用了token级别的信息。AP-OOD是一种半监督方法，可以在无监督和监督设置之间灵活插值，从而利用有限的辅助异常数据。实验证明，AP-OOD在文本分布外检测中设定了新的基准：在无监督设置下，它在XSUM摘要任务中将FPR95（95%真实正例下的假阳性率）从27.84%降低至4.67%，在WMT15 En-Fr翻译任务中从77.08%降低至70.37%。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Out-of-distribution (OOD) detection, which maps high-dimensional data into a scalar OOD score, is critical for the reliable deployment of machine learning models.</div>
</details>
</div>
<div class="card">
<div class="title">Curiosity is Knowledge: Self-Consistent Learning and No-Regret Optimization with Active Inference</div>
<div class="meta-line">Authors: Yingke Li, Anjali Parashar, Enlu Zhou, Chuchu Fan</div>
<div class="meta-line">First: 2026-02-05T18:58:32+00:00 · Latest: 2026-02-05T18:58:32+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06029v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06029v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient. Yet it has been unclear when this balance yields both coherent learning and efficient decision-making: insufficient curiosity can drive myopic exploitation and prevent uncertainty resolution, while excessive curiosity can induce unnecessary exploration and regret. We establish the first theoretical guarantee for EFE-minimizing agents, showing that a single requirement--sufficient curiosity--simultaneously ensures self-consistent learning (Bayesian posterior consistency) and no-regret optimization (bounded cumulative regret). Our analysis characterizes how this mechanism depends on initial uncertainty, identifiability, and objective alignment, thereby connecting AIF to classical Bayesian experimental design and Bayesian optimization within one theoretical framework. We further translate these theories into practical design guidelines for tuning the epistemic-pragmatic trade-off in hybrid learning-optimization problems, validated through real-world experiments.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>好奇心即知识：基于主动推断的自洽学习与无悔优化</div>
<div class="mono" style="margin-top:8px">主动推断（AIF）通过最小化预期自由能（EFE）统一了探索与利用，利用好奇心系数在知识价值（信息增益）和实用价值（任务表现）之间进行平衡。然而，这种平衡何时能同时实现连贯的学习和高效的决策仍不清楚：好奇心不足可能导致短视的利用行为，阻碍不确定性解决；而好奇心过强则可能引发不必要的探索，导致后悔。我们建立了首个针对EFE最小化智能体的理论保证，证明只需满足一个条件——足够的好奇心——即可同时确保自洽学习（贝叶斯后验一致性）和无悔优化（累积后悔有界）。我们的分析阐明了该机制如何依赖初始不确定性、可识别性和目标一致性，从而将主动推断与经典贝叶斯实验设计和贝叶斯优化联系起来。我们进一步将这些理论转化为实际的设计指南，用于调整混合学习-优化问题中的知识-实用权衡，并通过现实世界实验进行了验证。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Active inference (AIF) unifies exploration and exploitation by minimizing the Expected Free Energy (EFE), balancing epistemic value (information gain) and pragmatic value (task performance) through a curiosity coefficient.</div>
</details>
</div>
<div class="card">
<div class="title">Language Models and Logic Programs for Trustworthy Tax Reasoning</div>
<div class="meta-line">Authors: William Jurayj, Nils Holzenberger, Benjamin Van Durme</div>
<div class="meta-line">Venue: AAAI 2026</div>
<div class="meta-line">First: 2025-08-28T17:55:07+00:00 · Latest: 2026-02-05T18:58:31+00:00</div>
<div class="meta-line">Comments: Accepted to AAAI 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2508.21051v3">Abs</a> · <a href="https://arxiv.org/pdf/2508.21051v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">According to the United States Internal Revenue Service, ``the average American spends $\$270$ and 13 hours filing their taxes&#x27;&#x27;. Even beyond the U.S., tax filing requires complex reasoning, combining application of overlapping rules with numerical calculations. Because errors can incur costly penalties, any automated system must deliver high accuracy and auditability, making modern large language models (LLMs) poorly suited for this task. We propose an approach that integrates LLMs with a symbolic solver to calculate tax obligations. We evaluate variants of this system on the challenging StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for estimating the cost of deploying such a system based on real-world penalties for tax errors. We further show how combining up-front translation of plain-text rules into formal logic programs, combined with intelligently retrieved exemplars for formal case representations, can dramatically improve performance on this task and reduce costs to well below real-world averages. Our results demonstrate the effectiveness of applying semantic parsing methods to statutory reasoning, and show promising economic feasibility of neuro-symbolic architectures for increasing access to reliable tax assistance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>语言模型与逻辑程序用于可信税务推理</div>
<div class="mono" style="margin-top:8px">根据美国国税局（IRS）的统计，『平均美国人花费$270和13小时来报税』。即使在其他国家，报税也需要复杂的推理，结合重叠规则的应用与数值计算。由于错误可能导致高昂的罚款，任何自动化系统都必须提供高准确性和可审计性，这使得现代大型语言模型（LLMs）不适合执行此类任务。我们提出了一种将LLMs与符号求解器结合的方法，用于计算税务义务。我们在具有挑战性的法定推理评估数据集（SARA）上评估了该系统的不同变体，并引入了一种基于现实世界税务错误罚款的新方法，用于估算部署此类系统的成本。我们进一步展示了如何通过将普通文本规则预先翻译为形式化逻辑程序，并结合智能检索的形式化案例表示示例，可以显著提升该任务的性能，并将成本降至远低于现实世界的平均水平。我们的结果证明了将语义解析方法应用于法定推理的有效性，并展示了神经符号架构在提高可靠税务援助可及性方面的有前景的经济可行性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">According to the United States Internal Revenue Service, ``the average American spends $\$270$ and 13 hours filing their taxes&#x27;&#x27;.</div>
</details>
</div>
<div class="card">
<div class="title">Context Forcing: Consistent Autoregressive Video Generation with Long Context</div>
<div class="meta-line">Authors: Shuo Chen, Cong Wei, Sun Sun, Ping Nie, Kai Zhou, Ge Zhang, Ming-Hsuan Yang, Wenhu Chen</div>
<div class="meta-line">First: 2026-02-05T18:58:01+00:00 · Latest: 2026-02-05T18:58:01+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06028v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06028v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher. In these frameworks, the student performs long rollouts but receives supervision from a teacher limited to short 5-second windows. This structural discrepancy creates a critical \textbf{student-teacher mismatch}: the teacher&#x27;s inability to access long-term history prevents it from guiding the student on global temporal dependencies, effectively capping the student&#x27;s context length. To resolve this, we propose \textbf{Context Forcing}, a novel framework that trains a long-context student via a long-context teacher. By ensuring the teacher is aware of the full generation history, we eliminate the supervision mismatch, enabling the robust training of models capable of long-term consistency. To make this computationally feasible for extreme durations (e.g., 2 minutes), we introduce a context management system that transforms the linearly growing context into a \textbf{Slow-Fast Memory} architecture, significantly reducing visual redundancy. Extensive results demonstrate that our method enables effective context lengths exceeding 20 seconds -- 2 to 10 times longer than state-of-the-art methods like LongLive and Infinite-RoPE. By leveraging this extended context, Context Forcing preserves superior consistency across long durations, surpassing state-of-the-art baselines on various long video evaluation metrics.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>上下文强制：具有长上下文的一致自回归视频生成</div>
<div class="mono" style="margin-top:8px">近期的实时长视频生成方法通常采用流式调优策略，尝试使用短上下文（无记忆）教师来训练长上下文学生。在这些框架中，学生执行长序列生成，但教师仅能提供短时5秒窗口的监督。这种结构性差异导致了关键的\textbf{学生-教师不匹配}问题：教师无法访问长期历史，因此无法指导学生处理全局时间依赖性，实际上限制了学生的上下文长度。为了解决这一问题，我们提出了\textbf{上下文强制}，一种新颖的框架，通过长上下文教师来训练长上下文学生。通过确保教师能够访问完整的生成历史，我们消除了监督不匹配，使得能够训练出具备长期一致性的模型。为了在极端时长（例如2分钟）下实现计算可行性，我们引入了一种上下文管理系统，将线性增长的上下文转换为\textbf{慢速-快速记忆}架构，显著减少了视觉冗余。大量实验结果表明，我们的方法实现了超过20秒的有效上下文长度，比现有方法如LongLive和Infinite-RoPE长2到10倍。通过利用这种扩展的上下文，上下文强制在各种长视频评估指标上保持了卓越的一致性，超越了现有最先进的基线方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent approaches to real-time long video generation typically employ streaming tuning strategies, attempting to train a long-context student using a short-context (memoryless) teacher.</div>
</details>
</div>
<div class="card">
<div class="title">Learning Query-Aware Budget-Tier Routing for Runtime Agent Memory</div>
<div class="meta-line">Authors: Haozhen Zhang, Haodong Yue, Tao Feng, Quanyu Long, Jianzhu Bao, Bowen Jin, Weizhi Zhang, Xiao Li, Jiaxuan You, Chengwei Qin, Wenya Wang</div>
<div class="meta-line">First: 2026-02-05T18:57:09+00:00 · Latest: 2026-02-05T18:57:09+00:00</div>
<div class="meta-line">Comments: Code is available at https://github.com/ViktorAxelsen/BudgetMem</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06025v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06025v1">PDF</a> · <a href="https://github.com/ViktorAxelsen/BudgetMem">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information. Although runtime memory utilization is a natural alternative, prior work often incurs substantial overhead and offers limited explicit control over the performance-cost trade-off. In this work, we present \textbf{BudgetMem}, a runtime agent memory framework for explicit, query-aware performance-cost control. BudgetMem structures memory processing as a set of memory modules, each offered in three budget tiers (i.e., \textsc{Low}/\textsc{Mid}/\textsc{High}). A lightweight router performs budget-tier routing across modules to balance task performance and memory construction cost, which is implemented as a compact neural policy trained with reinforcement learning. Using BudgetMem as a unified testbed, we study three complementary strategies for realizing budget tiers: implementation (method complexity), reasoning (inference behavior), and capacity (module model size). Across LoCoMo, LongMemEval, and HotpotQA, BudgetMem surpasses strong baselines when performance is prioritized (i.e., high-budget setting), and delivers better accuracy-cost frontiers under tighter budgets. Moreover, our analysis disentangles the strengths and weaknesses of different tiering strategies, clarifying when each axis delivers the most favorable trade-offs under varying budget regimes.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>学习查询感知的预算层级路由以实现运行时代理记忆</div>
<div class="mono" style="margin-top:8px">记忆在大型语言模型（LLM）代理运行超出单一上下文窗口时变得越来越关键，但大多数现有系统依赖于离线、查询无关的记忆构建方式，这可能效率低下并丢弃关键查询信息。尽管运行时记忆利用是自然的替代方案，但先前的工作通常会产生显著的开销，并且对性能-成本权衡的显式控制有限。在本工作中，我们提出了\textbf{BudgetMem}，这是一个用于运行时代理记忆的框架，支持显式的、查询感知的性能-成本控制。BudgetMem将记忆处理结构化为一组记忆模块，每个模块提供三种预算层级（即\textsc{Low}/\textsc{Mid}/\textsc{High}）。一个轻量级路由器在模块间进行预算层级路由，以平衡任务性能和记忆构建成本，该路由器通过强化学习训练了一个紧凑的神经策略来实现。使用BudgetMem作为统一的测试平台，我们研究了三种实现预算层级的互补策略：实现方式（方法复杂度）、推理行为（推理过程）和容量（模块模型大小）。在LoCoMo、LongMemEval和HotpotQA数据集上，当优先考虑性能（即高预算设置）时，BudgetMem超越了强大的基线模型，并在更严格的预算限制下提供了更优的准确率-成本前沿。此外，我们的分析解耦了不同层级策略的优势和劣势，明确了在不同预算制度下，每种策略在哪个维度上能实现最有利的权衡。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Memory is increasingly central to Large Language Model (LLM) agents operating beyond a single context window, yet most existing systems rely on offline, query-agnostic memory construction that can be inefficient and may discard query-critical information.</div>
</details>
</div>
<div class="card">
<div class="title">Learning Event-Based Shooter Models from Virtual Reality Experiments</div>
<div class="meta-line">Authors: Christopher A. McClurg, Alan R. Wagner</div>
<div class="meta-line">First: 2026-02-05T18:56:49+00:00 · Latest: 2026-02-05T18:56:49+00:00</div>
<div class="meta-line">Comments: Preprint under review for conference publication. 9 pages, 4 figures, 4 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06023v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06023v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Virtual reality (VR) has emerged as a powerful tool for evaluating school security measures in high-risk scenarios such as school shootings, offering experimental control and high behavioral fidelity. However, assessing new interventions in VR requires recruiting new participant cohorts for each condition, making large-scale or iterative evaluation difficult. These limitations are especially restrictive when attempting to learn effective intervention strategies, which typically require many training episodes. To address this challenge, we develop a data-driven discrete-event simulator (DES) that models shooter movement and in-region actions as stochastic processes learned from participant behavior in VR studies. We use the simulator to examine the impact of a robot-based shooter intervention strategy. Once shown to reproduce key empirical patterns, the DES enables scalable evaluation and learning of intervention strategies that are infeasible to train directly with human subjects. Overall, this work demonstrates a high-to-mid fidelity simulation workflow that provides a scalable surrogate for developing and evaluating autonomous school-security interventions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>从虚拟现实实验中学习基于事件的射击模型</div>
<div class="mono" style="margin-top:8px">虚拟现实（VR）已成为评估学校安全措施在高风险场景（如校园枪击事件）中的有力工具，提供了实验控制和高行为保真度。然而，在VR中评估新干预措施需要为每个条件招募新的参与者，使得大规模或迭代评估变得困难。这些限制在尝试学习有效的干预策略时尤为明显，因为这类策略通常需要大量的训练回合。为了解决这一挑战，我们开发了一个数据驱动的离散事件模拟器（DES），该模拟器将射击者的移动和区域内的行为建模为从VR研究中参与者行为学习得到的随机过程。我们使用该模拟器来研究基于机器人的射击干预策略的影响。一旦证明该DES能够再现关键的实证模式，它便能实现对无法直接以人类为受试者进行训练的干预策略的可扩展评估和学习。总体而言，本工作展示了一个高至中保真度的模拟流程，为开发和评估自主校园安全干预措施提供了一个可扩展的替代方案。</div>
</details>
</div>
<div class="card">
<div class="title">Correctness-Optimized Residual Activation Lens (CORAL): Transferrable and Calibration-Aware Inference-Time Steering</div>
<div class="meta-line">Authors: Miranda Muqing Miao, Young-Min Cho, Lyle Ungar</div>
<div class="meta-line">First: 2026-02-05T18:55:56+00:00 · Latest: 2026-02-05T18:55:56+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06022v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06022v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) exhibit persistent miscalibration, especially after instruction tuning and preference alignment. Modified training objectives can improve calibration, but retraining is expensive. Inference-time steering offers a lightweight alternative, yet most existing methods optimize proxies for correctness rather than correctness itself. We introduce CORAL (Correctness-Optimized Residual Activation Lens), a regularized inference-time steering method that captures distributed correctness signals from model internal activations using weight-decay MLP probes. We evaluate CORAL across three 7B-parameter models and find that it consistently improves accuracy by 10\% and expected calibration error (ECE) by 50\% on average. We additionally demonstrate that these gains transfer without retraining to the complete published test sets of four held-out benchmarks (ARC-Challenge, HellaSwag, Math-MC, OpenBookQA), averaging 14\% accuracy improvements and 49\% ECE improvements. Our results support the hypothesis that distributed information in model internals can be extracted using regularized probes when individual neurons are insufficient. CORAL thus provides a compute-efficient, transferable, and calibration-aware approach to improve MCQA performance during inference.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>正确性优化残差激活透镜（CORAL）：可迁移且校准感知的推理时引导</div>
<div class="mono" style="margin-top:8px">大型语言模型（LLMs）在指令调优和偏好对齐后表现出持续的校准偏差。修改训练目标可以改善校准，但重新训练成本较高。推理时引导提供了一种轻量级替代方案，但大多数现有方法优化的是正确性的代理指标而非正确性本身。我们引入了CORAL（正确性优化残差激活透镜），这是一种正则化推理时引导方法，通过权重衰减的MLP探针从模型内部激活中捕捉分布式正确性信号。我们在三个7B参数模型上评估了CORAL，发现其在准确率上平均提升10%，在预期校准误差（ECE）上平均降低50%。此外，我们还证明这些改进可以在不重新训练的情况下迁移到四个基准测试集（ARC-Challenge、HellaSwag、Math-MC、OpenBookQA）的完整发布测试集上，平均准确率提升14%，ECE降低49%。我们的结果支持了假设：当单个神经元不足以提取信息时，可以通过正则化探针从模型内部提取分布式信息。因此，CORAL为在推理过程中提升多项选择题问答（MCQA）性能提供了一种计算高效、可迁移且校准感知的方法。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) exhibit persistent miscalibration, especially after instruction tuning and preference alignment.</div>
</details>
</div>
<div class="card">
<div class="title">Diffusion Model&#x27;s Generalization Can Be Characterized by Inductive Biases toward a Data-Dependent Ridge Manifold</div>
<div class="meta-line">Authors: Ye He, Yitong Qiu, Molei Tao</div>
<div class="meta-line">First: 2026-02-05T18:55:03+00:00 · Latest: 2026-02-05T18:55:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06021v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06021v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">When a diffusion model is not memorizing the training data set, how does it generalize exactly? A quantitative understanding of the distribution it generates would be beneficial to, for example, an assessment of the model&#x27;s performance for downstream applications. We thus explicitly characterize what diffusion model generates, by proposing a log-density ridge manifold and quantifying how the generated data relate to this manifold as inference dynamics progresses. More precisely, inference undergoes a reach-align-slide process centered around the ridge manifold: trajectories first reach a neighborhood of the manifold, then align as being pushed toward or away from the manifold in normal directions, and finally slide along the manifold in tangent directions. Within the scope of this general behavior, different training errors will lead to different normal and tangent motions, which can be quantified, and these detailed motions characterize when inter-mode generations emerge. More detailed understanding of training dynamics will lead to more accurate quantification of the generation inductive bias, and an example of random feature model will be considered, for which we can explicitly illustrate how diffusion model&#x27;s inductive biases originate as a composition of architectural bias and training accuracy, and how they evolve with the inference dynamics. Experiments on synthetic multimodal distributions and MNIST latent diffusion support the predicted directional effects, in both low- and high-dimensions.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>扩散模型的泛化能力可通过向数据依赖的脊流形的归纳偏置来表征</div>
<div class="mono" style="margin-top:8px">当扩散模型没有记忆训练数据集时，它如何进行泛化？对它生成的分布进行定量理解，有助于评估其在下游应用中的性能。因此，我们通过提出一个对数密度脊流形，并量化生成数据在推理过程中如何与该流形相关联，来明确表征扩散模型生成的内容。更具体地说，推理过程围绕脊流形进行，经历一个“到达-对齐-滑动”的过程：轨迹首先到达流形的邻域，然后沿着法向方向被推近或推离流形，最后在切向方向沿流形滑动。在这一总体行为范围内，不同的训练误差会导致不同的法向和切向运动，这些运动可以被量化，并且这些详细的运动决定了多模式生成何时出现。对训练动态的更深入理解将有助于更准确地量化生成的归纳偏置，并以随机特征模型为例，明确说明扩散模型的归纳偏置是如何由架构偏置和训练精度组成的，以及它们如何随着推理动态演变。在合成多模态分布和MNIST潜在扩散模型上的实验支持了预测的方向性效应，无论是在低维还是高维空间中。</div>
</details>
</div>
<div class="card">
<div class="title">Mechanisms of AI Protein Folding in ESMFold</div>
<div class="meta-line">Authors: Kevin Lu, Jannik Brinkmann, Stefan Huber, Aaron Mueller, Yonatan Belinkov, David Bau, Chris Wendler</div>
<div class="meta-line">First: 2026-02-05T18:54:54+00:00 · Latest: 2026-02-05T18:54:54+00:00</div>
<div class="meta-line">Comments: Our code, data, and results are available at https://folding.baulab.info</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06020v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06020v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">How do protein structure prediction models fold proteins? We investigate this question by tracing how ESMFold folds a beta hairpin, a prevalent structural motif. Through counterfactual interventions on model latents, we identify two computational stages in the folding trunk. In the first stage, early blocks initialize pairwise biochemical signals: residue identities and associated biochemical features such as charge flow from sequence representations into pairwise representations. In the second stage, late blocks develop pairwise spatial features: distance and contact information accumulate in the pairwise representation. We demonstrate that the mechanisms underlying structural decisions of ESMFold can be localized, traced through interpretable representations, and manipulated with strong causal effects.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>ESMFold中AI蛋白质折叠的机制</div>
<div class="mono" style="margin-top:8px">蛋白质结构预测模型是如何折叠蛋白质的？我们通过追踪ESMFold如何折叠一种常见的结构模体——β发夹，来探讨这一问题。通过对模型潜变量进行反事实干预，我们识别出折叠过程中的两个计算阶段。在第一阶段，早期模块初始化成对的生化信号：残基身份及其相关的生化特征，如电荷流，从序列表示转化为成对表示。在第二阶段，晚期模块发展成对的空间特征：距离和接触信息在成对表示中积累。我们证明了ESMFold结构决策背后的机制可以被定位、通过可解释的表示进行追踪，并通过强因果效应进行操控。</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Token Prediction via Self-Distillation</div>
<div class="meta-line">Authors: John Kirchenbauer, Abhimanyu Hans, Brian Bartoldson, Micah Goldblum, Ashwinee Panda, Tom Goldstein</div>
<div class="meta-line">First: 2026-02-05T18:54:48+00:00 · Latest: 2026-02-05T18:54:48+00:00</div>
<div class="meta-line">Comments: 8 pages and 5 figures in the main body</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06019v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06019v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Existing techniques for accelerating language model inference, such as speculative decoding, require training auxiliary speculator models and building and deploying complex inference pipelines. We consider a new approach for converting a pretrained autoregressive language model from a slow single next token prediction model into a fast standalone multi-token prediction model using a simple online distillation objective. The final model retains the exact same implementation as the pretrained initial checkpoint and is deployable without the addition of any auxiliary verifier or other specialized inference code. On GSM8K, our method produces models that can decode more than $3\times$ faster on average at $&lt;5\%$ drop in accuracy relative to single token decoding performance.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>通过自蒸馏实现多标记预测</div>
<div class="mono" style="margin-top:8px">现有的加速语言模型推理技术，如推测解码，需要训练辅助的推测模型并构建和部署复杂的推理管道。我们提出了一种新方法，通过一个简单的在线蒸馏目标，将一个预训练的自回归语言模型从一个缓慢的单标记预测模型转换为一个快速的独立多标记预测模型。最终模型保留了预训练初始检查点的完全相同实现，无需添加任何辅助验证器或其他专用推理代码即可部署。在GSM8K数据集上，我们的方法生成的模型在准确率下降不到5%的情况下，平均解码速度比单标记解码快3倍以上。</div>
</details>
</div>
<div class="card">
<div class="title">MambaVF: State Space Model for Efficient Video Fusion</div>
<div class="meta-line">Authors: Zixiang Zhao, Yukun Cui, Lilun Deng, Haowen Bai, Haotong Qin, Tao Feng, Konrad Schindler</div>
<div class="meta-line">First: 2026-02-05T18:53:47+00:00 · Latest: 2026-02-05T18:53:47+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06017v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06017v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a> · <a href="https://mambavf.github.io">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Video fusion is a fundamental technique in various video processing tasks. However, existing video fusion methods heavily rely on optical flow estimation and feature warping, resulting in severe computational overhead and limited scalability. This paper presents MambaVF, an efficient video fusion framework based on state space models (SSMs) that performs temporal modeling without explicit motion estimation. First, by reformulating video fusion as a sequential state update process, MambaVF captures long-range temporal dependencies with linear complexity while significantly reducing computation and memory costs. Second, MambaVF proposes a lightweight SSM-based fusion module that replaces conventional flow-guided alignment via a spatio-temporal bidirectional scanning mechanism. This module enables efficient information aggregation across frames. Extensive experiments across multiple benchmarks demonstrate that our MambaVF achieves state-of-the-art performance in multi-exposure, multi-focus, infrared-visible, and medical video fusion tasks. We highlight that MambaVF enjoys high efficiency, reducing up to 92.25% of parameters and 88.79% of computational FLOPs and a 2.1x speedup compared to existing methods. Project page: https://mambavf.github.io</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>MambaVF：用于高效视频融合的状态空间模型</div>
<div class="mono" style="margin-top:8px">视频融合是各种视频处理任务中的基础技术。然而，现有的视频融合方法严重依赖光流估计和特征变形，导致计算开销大且可扩展性有限。本文提出MambaVF，一种基于状态空间模型（SSMs）的高效视频融合框架，能够在不显式进行运动估计的情况下进行时序建模。首先，通过将视频融合重新表述为一个序列状态更新过程，MambaVF以线性复杂度捕捉长时序依赖关系，同时显著降低计算和内存成本。其次，MambaVF提出了一种轻量级的基于SSM的融合模块，通过时空双向扫描机制替代传统的流引导对齐方式。该模块能够高效地在帧之间聚合信息。在多个基准测试中进行的大量实验表明，我们的MambaVF在多曝光、多焦点、红外-可见和医学视频融合任务中均取得了最先进的性能。我们强调，MambaVF具有高效率，相比现有方法，参数减少了高达92.25%，计算FLOPs减少了88.79%，速度提升了2.1倍。项目页面：https://mambavf.github.io</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Video fusion is a fundamental technique in various video processing tasks.</div>
</details>
</div>
<div class="card">
<div class="title">Convex unions and completions from simplicial pseudomanifolds</div>
<div class="meta-line">Authors: Soohyun Park</div>
<div class="meta-line">First: 2026-02-05T18:53:23+00:00 · Latest: 2026-02-05T18:53:23+00:00</div>
<div class="meta-line">Comments: 56 pages</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06016v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06016v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While intersections of convex sets are convex, their unions have rather complicated behavior. Some natural contexts where they appear include duality arguments involving boundaries of convex sets and valuations, which have an Euler characteristic-like structure. However, there are certain settings where the convexity property itself is important to consider. For example, this includes (preservation of) positivity properties of divisors on toric varieties under blowdowns. In the case of (restrictions of) conormal bundles, this can be interpreted in terms of interactions between local convexity data stored in rational equivalence relations. We consider generalizations to realizations of simplicial pseudomanifolds and replace rational equivalence with effects of PL homeomorphisms.
  Decomposing the PL homeomorphisms into edge subdivisions and contractions, we characterize the space of suitable contraction points compatible with local convexity properties in terms of convex unions and completions. This gives rise to certain external edge subdivisions that make this ``contraction space&#x27;&#x27; of the starting edge empty, which is unexpected given the expected ``increased convexity&#x27;&#x27; from edge subdivisions. We also obtain strong affine/linear restrictions on realizations of facets containing nearby edges preserving local convexity. This implies that contracting certain nearby edges results in a very large or very small contraction space of the starting edge. As for boundary behavior, there are parallels between effects of PL homeomorphisms on induced 4-cycles in the 1-skeleton. Finally, we find effects of PL homeomorphisms and suspensions on analogues of local convexity properties stored by linear systems of parameters. This indicates that simplicial spheres PL homeomorphic to the boundary of a cross polytope store record local convexity changes in the most natural way.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>凸集的并集与完备性：来自单纯伪流形的推广</div>
<div class="mono" style="margin-top:8px">虽然凸集的交集仍然是凸的，但它们的并集则表现出较为复杂的行为。这些并集在一些自然的上下文中出现，例如涉及凸集边界和估值的对偶论论证，这些具有类似欧拉特征的结构。然而，在某些情况下，凸性本身是一个需要考虑的重要性质。例如，这包括在对称锥流形上通过吹胀保持的除子的正性性质。在共法线丛的（限制）情形下，这可以被解释为局部凸性数据在有理等价关系中的相互作用。我们考虑了单纯伪流形的实现中的推广，并将有理等价替换为PL同胚的影响。
通过将PL同胚分解为边细分和收缩，我们以凸并集和完备性来刻画与局部凸性性质兼容的合适收缩点的空间。这导致了某些外部边细分，使得起始边的『收缩空间』为空，这与预期的『增加凸性』从边细分中得出的结果相矛盾。我们还得到了关于保持局部凸性的边邻近面的实现的强仿射/线性限制。这表明收缩某些邻近边会导致起始边的收缩空间变得非常大或非常小。至于边界行为，PL同胚对诱导的1-骨架中的4-环的影响存在类比。最后，我们发现了PL同胚和悬垂对存储在参数线性系统中的局部凸性性质类比的影响。这表明，与交叉多面体边界PL同胚的单纯球面以最自然的方式存储了局部凸性变化的记录。</div>
</details>
</div>
<div class="card">
<div class="title">A Systematic Evaluation of Large Language Models for PTSD Severity Estimation: The Role of Contextual Knowledge and Modeling Strategies</div>
<div class="meta-line">Authors: Panagiotis Kaliosis, Adithya V Ganesan, Oscar N. E. Kjell, Whitney Ringwald, Scott Feltman, Melissa A. Carr, Dimitris Samaras, Camilo Ruggero, Benjamin J. Luft, Roman Kotov, Andrew H. Schwartz</div>
<div class="meta-line">First: 2026-02-05T18:53:17+00:00 · Latest: 2026-02-05T18:53:17+00:00</div>
<div class="meta-line">Comments: 18 pages, 3 figures, 5 tables</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06015v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06015v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models (LLMs) are increasingly being used in a zero-shot fashion to assess mental health conditions, yet we have limited knowledge on what factors affect their accuracy. In this study, we utilize a clinical dataset of natural language narratives and self-reported PTSD severity scores from 1,437 individuals to comprehensively evaluate the performance of 11 state-of-the-art LLMs. To understand the factors affecting accuracy, we systematically varied (i) contextual knowledge like subscale definitions, distribution summary, and interview questions, and (ii) modeling strategies including zero-shot vs few shot, amount of reasoning effort, model sizes, structured subscales vs direct scalar prediction, output rescaling and nine ensemble methods. Our findings indicate that (a) LLMs are most accurate when provided with detailed construct definitions and context of the narrative; (b) increased reasoning effort leads to better estimation accuracy; (c) performance of open-weight models (Llama, Deepseek), plateau beyond 70B parameters while closed-weight (o3-mini, gpt-5) models improve with newer generations; and (d) best performance is achieved when ensembling a supervised model with the zero-shot LLMs. Taken together, the results suggest choice of contextual knowledge and modeling strategies is important for deploying LLMs to accurately assess mental health.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>用于PTSD严重程度评估的大语言模型系统性评价：情境知识与建模策略的作用</div>
<div class="mono" style="margin-top:8px">大语言模型（LLMs）越来越多地以零样本方式用于评估心理健康状况，但我们对影响其准确性的因素了解有限。本研究利用包含1,437名个体的临床数据集，其中包含自然语言叙述和自述的PTSD严重程度评分，全面评估了11种最先进的LLMs性能。为了解影响准确性的因素，我们系统地变化了（i）情境知识，如子维度定义、分布摘要和访谈问题，以及（ii）建模策略，包括零样本与少样本、推理努力程度、模型规模、结构化子维度与直接标量预测、输出重缩放和九种集成方法。我们的研究结果表明：（a）当提供详细的构念定义和叙述情境时，LLMs的准确性最高；（b）增加推理努力程度可提高估计准确性；（c）开源模型（如Llama、Deepseek）在超过700亿参数后性能趋于平稳，而闭源模型（如o3-mini、gpt-5）则随着新版本的推出性能不断提升；（d）当将监督模型与零样本LLMs集成时，可获得最佳性能。综上所述，这些结果表明，在部署LLMs以准确评估心理健康状况时，选择合适的情境知识和建模策略至关重要。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models (LLMs) are increasingly being used in a zero-shot fashion to assess mental health conditions, yet we have limited knowledge on what factors affect their accuracy.</div>
</details>
</div>
<div class="card">
<div class="title">Optimism Stabilizes Thompson Sampling for Adaptive Inference</div>
<div class="meta-line">Authors: Shunxing Yan, Han Zhong</div>
<div class="meta-line">First: 2026-02-05T18:52:54+00:00 · Latest: 2026-02-05T18:52:54+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06014v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06014v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Thompson sampling (TS) is widely used for stochastic multi-armed bandits, yet its inferential properties under adaptive data collection are subtle. Classical asymptotic theory for sample means can fail because arm-specific sample sizes are random and coupled with the rewards through the action-selection rule. We study this phenomenon in the $K$-armed Gaussian bandit and identify \emph{optimism} as a key mechanism for restoring \emph{stability}, a sufficient condition for valid asymptotic inference requiring each arm&#x27;s pull count to concentrate around a deterministic scale. First, we prove that variance-inflated TS \citep{halder2025stable} is stable for any $K \ge 2$, including the challenging regime where multiple arms are optimal. This resolves the open question raised by \citet{halder2025stable} through extending their results from the two-armed setting to the general $K$-armed setting. Second, we analyze an alternative optimistic modification that keeps the posterior variance unchanged but adds an explicit mean bonus to posterior mean, and establish the same stability conclusion. In summary, suitably implemented optimism stabilizes Thompson sampling and enables asymptotically valid inference in multi-armed bandits, while incurring only a mild additional regret cost.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>乐观性使汤普森采样在自适应推断中稳定</div>
<div class="mono" style="margin-top:8px">汤普森采样（TS）广泛用于随机多臂老虎机问题，但其在自适应数据收集下的推断性质较为微妙。经典样本均值渐近理论可能失效，因为每个臂的样本量是随机的，并通过动作选择规则与奖励耦合。我们研究了这一现象在 $K$ 臂高斯多臂老虎机中的表现，并识别出乐观性（optimism）是恢复稳定性（stability）的关键机制，而稳定性是有效渐近推断的充分条件，要求每个臂的拉取次数集中在一个确定性尺度上。首先，我们证明了方差膨胀的汤普森采样（TS）在任何 $K \ge 2$ 的情况下都是稳定的，包括多个臂均为最优的具有挑战性的场景。这通过将他们的结果从两臂设置扩展到一般的 $K$ 臂设置，解决了 \citet{halder2025stable} 提出的开放问题。其次，我们分析了一种替代的乐观性修改方法，该方法保持后验方差不变，但向后验均值添加了一个显式的均值奖励，并建立了相同的稳定性结论。总之，适当实现的乐观性使汤普森采样在多臂老虎机中实现渐近有效的推断，同时仅带来轻微的额外遗憾成本。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Thompson sampling (TS) is widely used for stochastic multi-armed bandits, yet its inferential properties under adaptive data collection are subtle.</div>
</details>
</div>
<div class="card">
<div class="title">GenArena: How Can We Achieve Human-Aligned Evaluation for Visual Generation Tasks?</div>
<div class="meta-line">Authors: Ruihang Li, Leigang Qu, Jingxu Zhang, Dongnan Gui, Mengde Xu, Xiaosong Zhang, Han Hu, Wenjie Wang, Jiaqi Wang</div>
<div class="meta-line">First: 2026-02-05T18:52:48+00:00 · Latest: 2026-02-05T18:52:48+00:00</div>
<div class="meta-line">Comments: Project Page: https://genarena.github.io/, Code: https://github.com/ruihanglix/genarena</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.06013v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.06013v1">PDF</a> · <a href="https://github.com/ruihanglix/genarena">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a> · <a href="https://genarena.github.io/">Project1</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid advancement of visual generation models has outpaced traditional evaluation approaches, necessitating the adoption of Vision-Language Models as surrogate judges. In this work, we systematically investigate the reliability of the prevailing absolute pointwise scoring standard, across a wide spectrum of visual generation tasks. Our analysis reveals that this paradigm is limited due to stochastic inconsistency and poor alignment with human perception. To resolve these limitations, we introduce GenArena, a unified evaluation framework that leverages a pairwise comparison paradigm to ensure stable and human-aligned evaluation. Crucially, our experiments uncover a transformative finding that simply adopting this pairwise protocol enables off-the-shelf open-source models to outperform top-tier proprietary models. Notably, our method boosts evaluation accuracy by over 20% and achieves a Spearman correlation of 0.86 with the authoritative LMArena leaderboard, drastically surpassing the 0.36 correlation of pointwise methods. Based on GenArena, we benchmark state-of-the-art visual generation models across diverse tasks, providing the community with a rigorous and automated evaluation standard for visual generation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>GenArena：如何实现视觉生成任务中与人类对齐的评估？</div>
<div class="mono" style="margin-top:8px">随着视觉生成模型的快速发展，传统评估方法已无法满足需求，因此需要采用视觉-语言模型作为替代评估者。本文系统地研究了当前主流的绝对点评估标准在广泛视觉生成任务中的可靠性。我们的分析表明，这种范式存在随机不一致性和与人类感知对齐不足的问题。为了解决这些问题，我们提出了GenArena，一个统一的评估框架，利用成对比较范式确保评估的稳定性和与人类对齐。关键的是，我们的实验发现，仅仅采用这种成对比较协议，即可使现成的开源模型超越顶级的专有模型。值得注意的是，我们的方法将评估准确性提升了超过20%，并在权威的LMArena排行榜上实现了0.86的斯皮尔曼相关系数，远超点评估方法的0.36。基于GenArena，我们对最先进的视觉生成模型在多样化任务上的表现进行了基准测试，为社区提供了一个严格且自动化的视觉生成评估标准。</div>
</details>
</div>
<div class="card">
<div class="title">Lyapunov stability of the Euler method</div>
<div class="meta-line">Authors: Cédric Josz</div>
<div class="meta-line">First: 2025-09-14T20:08:15+00:00 · Latest: 2026-02-05T18:50:46+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2509.11415v3">Abs</a> · <a href="https://arxiv.org/pdf/2509.11415v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We extend the Lyapunov stability criterion to Euler discretizations of differential inclusions. It relies on a pair of Lyapunov functions, one in continuous time and one in discrete time. In the context of optimization, this yields sufficient conditions for the stability of nonisolated local minima when using the Bouligand subgradient method.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>欧拉方法的李雅普诺夫稳定性</div>
<div class="mono" style="margin-top:8px">我们将李雅普诺夫稳定性准则扩展到微分包含的欧拉离散化。它依赖于一对李雅普诺夫函数，一个用于连续时间，一个用于离散时间。在优化的背景下，这为使用 Bouligand 次梯度方法时非孤立局部极小值的稳定性提供了充分条件。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">We extend the Lyapunov stability criterion to Euler discretizations of differential inclusions.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260208_0353.html">20260208_0353</a>
<a href="archive/20260207_0410.html">20260207_0410</a>
<a href="archive/20260206_0412.html">20260206_0412</a>
<a href="archive/20260205_0417.html">20260205_0417</a>
<a href="archive/20260204_0421.html">20260204_0421</a>
<a href="archive/20260202_0402.html">20260202_0402</a>
<a href="archive/20260201_0354.html">20260201_0354</a>
<a href="archive/20260131_0408.html">20260131_0408</a>
<a href="archive/20260130_0406.html">20260130_0406</a>
<a href="archive/20260129_0407.html">20260129_0407</a>
<a href="archive/20260128_0407.html">20260128_0407</a>
<a href="archive/20260127_0401.html">20260127_0401</a>
<a href="archive/20260126_0356.html">20260126_0356</a>
<a href="archive/20260125_0354.html">20260125_0354</a>
<a href="archive/20260123_0403.html">20260123_0403</a>
<a href="archive/20260122_0405.html">20260122_0405</a>
<a href="archive/20260121_0450.html">20260121_0450</a>
<a href="archive/20260120_0352.html">20260120_0352</a>
<a href="archive/20260119_0349.html">20260119_0349</a>
<a href="archive/20260118_0350.html">20260118_0350</a>
<a href="archive/20260117_0356.html">20260117_0356</a>
<a href="archive/20260116_0403.html">20260116_0403</a>
<a href="archive/20260115_0356.html">20260115_0356</a>
<a href="archive/20260114_0402.html">20260114_0402</a>
<a href="archive/20260113_0342.html">20260113_0342</a>
<a href="archive/20260112_0355.html">20260112_0355</a>
<a href="archive/20260111_0354.html">20260111_0354</a>
<a href="archive/20260110_0400.html">20260110_0400</a>
<a href="archive/20260109_0356.html">20260109_0356</a>
<a href="archive/20260108_0356.html">20260108_0356</a>
<a href="archive/20260107_0355.html">20260107_0355</a>
<a href="archive/20260106_0358.html">20260106_0358</a>
<a href="archive/20260105_0351.html">20260105_0351</a>
<a href="archive/20260104_0353.html">20260104_0353</a>
<a href="archive/20260103_0351.html">20260103_0351</a>
<a href="archive/20260102_0354.html">20260102_0354</a>
<a href="archive/20260101_0352.html">20260101_0352</a>
<a href="archive/20251231_0356.html">20251231_0356</a>
<a href="archive/20251230_0356.html">20251230_0356</a>
<a href="archive/20251229_0355.html">20251229_0355</a>
<a href="archive/20251228_0354.html">20251228_0354</a>
<a href="archive/20251227_0355.html">20251227_0355</a>
<a href="archive/20251226_0355.html">20251226_0355</a>
<a href="archive/20251225_0355.html">20251225_0355</a>
<a href="archive/20251224_0355.html">20251224_0355</a>
<a href="archive/20251223_0354.html">20251223_0354</a>
<a href="archive/20251222_0354.html">20251222_0354</a>
<a href="archive/20251221_0354.html">20251221_0354</a>
<a href="archive/20251220_0356.html">20251220_0356</a>
<a href="archive/20251219_0354.html">20251219_0354</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0346.html">20251217_0346</a>
<a href="archive/20251216_0347.html">20251216_0347</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
